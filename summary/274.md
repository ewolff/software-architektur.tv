# Das Eichhörnchen im Kopf: KI-Architektur zwischen Hype und Realität

## Wichtige Keytakeaways

- LLMs haben mittlerweile einen reifen Stand erreicht, bei dem nicht mehr nur die Größe entscheidend ist
- Es gibt keine grundsätzlichen Limits der KI-Technologie, aber nicht alles macht Sinn
- Spezialisierte, kleinere Modelle sind oft sinnvoller als große generische Modelle
- Context Management ist ein zentrales Thema bei der Arbeit mit LLMs
- LLMs sind probabilistische, keine deterministische Systeme
- Die Kombination aus LLMs und Reinforcement Learning ermöglicht "motiviertes" Verhalten
- KI-Systeme sind nie neutral, da sie auf menschlichen Daten trainiert werden

## Behandelte Kernfragen

- Wie entwickeln sich große Sprachmodelle weiter?
- Welche Rolle spielt der Kontext bei LLMs?
- Wie können Modelle spezialisiert und verkleinert werden?
- Wie funktioniert die "Motivation" von KI-Systemen?
- Welche Grenzen haben aktuelle KI-Systeme?
- Wie verhalten sich deterministische und probabilistische Ansätze zueinander?
- Wie kann man KI-Systeme sinnvoll in Unternehmen einsetzen?

## Glossar wichtiger Begriffe

- LLM (Large Language Model): Großes Sprachmodell wie GPT oder Claude
- Quantisierung: Technik zur Verkleinerung von Modellen durch Reduktion der numerischen Präzision
- Model Distillation: Technik zum Training eines kleineren Modells durch ein größeres
- Reinforcement Learning: Maschinelles Lernen durch Belohnungssignale
- Dark Patterns: Algorithmen zur Beeinflussung von Nutzerverhalten
- Context Window: Maximale Textmenge, die ein LLM gleichzeitig verarbeiten kann
- Hybrid-Modelle: KI-Modelle die zwischen verschiedenen Verarbeitungsmodi wechseln können