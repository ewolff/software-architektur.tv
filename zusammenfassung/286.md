# KI in der Softwareentwicklung: Praktische Erfahrungen mit LLMs bei der Website-Optimierung

Künstliche Intelligenz durchdringt inzwischen alle Bereiche der Softwareentwicklung. Doch wie funktioniert es wirklich, wenn man KI-Modelle wie Claude oder GitHub Copilot für konkrete Entwicklungsprojekte einsetzt? Eberhard Wolff und Ralf haben das bei einem faszinierendem Experiment zu untersuchen begonnen: Sie haben ihre Website "Software-Architektur im Stream" um automatische Transkriptionen, Zusammenfassungen und weitere KI-gestützte Features erweitert. Ihre Erfahrungen bieten wertvolle Einsichten in die Möglichkeiten, aber auch die Grenzen von Large Language Models in der realen Anwendung.

## Die Motivation: Ein Experiment mit echtem Mehrwert

Das Projekt entstand aus einer simplen Beobachtung heraus: Die Website des erfolgreichen Podcast- und Stream-Formats "Software-Architektur im Stream" verfügte über viele ungenutzte Features. Gleichzeitig erkannte Wolff die Gelegenheit, seine umfangreiche Erfahrung mit KI-Systemen praktisch zu testen. "Ich probiere so viel aus mit KI, ich erzähle so viel über KI", erklärt er seinen Ansatz. "Und ich wollte halt tatsächlich mal ein Projekt machen, was in der Öffentlichkeit ist, wo man tatsächlich die Fähigkeiten von der KI auf verschiedenen Ebenen ausprobiert."

Die zentrale Idee war bestechend einfach: Warum nicht automatisierte Transkriptionen für alle Episoden erstellen? Dies hätte nicht nur Vorteile für die Barrierefreiheit, sondern würde auch die Auffindbarkeit von Inhalten erheblich verbessern. Doch wie würde man das technisch umsetzten?

## Prompt-Driven Development: Der praktische Weg zur Lösung

Wolff wandte sich an Claude – einen modernen Language Model – mit einer klaren Anforderung: "Ich möchte hier Transcriptions haben. Hier ist eine Website, da findest du die ganzen MP3-Files." Das war der Startschuss für das, was man heute "Prompt-Driven Development" nennt – oder im neueren Sprachgebrauch auch als "Web-Coding" bezeichnet.

Das besondere an dieser Herangehensweise: Ralf musste nicht selbst tiefgreifende Domänenkenntnisse mitbringen. Claude zeigte hier eine bemerkenswerte Fähigkeit zur Problemlösung. Was zunächst als zehnzeiliger Code-Schnipsel geplant war, entwickelte sich zu einer komplexeren Herausforderung. Die MP3-Dateien waren zu groß für die Whisper-API von OpenAI.

Hier offenbarte sich eine entscheidende Stärke von Claude: Das Modell zeigte autonomes Problemlösen. Statt die Sache aufzugeben, analysierte Claude selbstständig das Problem und schlug Lösungen vor. Erst versuchte es, die Audio-Qualität zu reduzieren – eine Mono-Konvertierung statt Stereo-Audio könnte Speicherplatz einsparen. Als das noch nicht ausreichte, kam Claude zu einer eleganten Lösung: Das System sollte die Audio-Dateien nicht brutal komprimieren, sondern intelligent aufteilen.

Claude erkannte dabei Pausen in den Audioaufnahmen und schlug vor, die Dateien an diesen Stellen in ungefähr gleich große Segmente zu unterteilen. Eine intelligente Strategie, die zeigt, wie LLMs nicht linear, sondern kontextuell denken können. Für Ralf, der selbst keine Erfahrung mit Audio-Verarbeitung hatte, war dies ein echter "Enablement-Moment" – die KI ermöglichte es ihm, ein Projekt zu realisieren, das sonst unmöglich gewesen wäre.

## Die Grenzen von Whisper: Worterkennung statt Kontextverstehen

Bei aller Begeisterung zeigte sich aber auch schnell eine wichtige Limitation: Whisper, Openais Transkriptions-Modell, scheint zu arbeiten wie ein phonetisches System ohne tieferes Kontextverständnis. Fachbegriffe wie "ChatGPT" wurden konsequent falsch erkannt – und zwar jedes Mal anders. Der Name von Simon Wardley wurde zu "Wortly Maps" transfomiert, ein Gast-Name wurde mehrfach unterschiedlich geschrieben.

Wolff testete auch das Hinzufügen von Hint-Wörtern, um Whisper auf die richtige Spur zu bringen – ohne durchschlagenden Erfolg. "Whisper versucht die Wörter zu erkennen und arbeitet nicht mit dem Kontext", fasst er das Problem zusammen. Im Gegensatz dazu könnten fortgeschrittenere Modelle wie GPT-4 vermutlich den Gesprächskontext nutzen, um genauere Transkriptionen zu erzeugen.

## GitHub Copilot als Qualitätskontrolle: KI überprüft KI

Ein besonders faszinierendes Element des Projekts war die Verwendung von GitHub Copilot zur Überprüfung der Transkriptionen. Während Whisper phonetisch arbeitete, konnte Copilot den Kontext verstehen. Als Copilot über die Pull Request auf GitHub die Transkription reviewte, erkannte es sofort: "Ihr redet hier über Wordley Maps. Ihr habt aber Simon Wardley falsch geschrieben." 

Dieses "KI kontrolliert KI"-Szenario war nicht nur technisch interessant, sondern illustrierte auch einen tieferen Punkt: Unterschiedliche KI-Systeme haben unterschiedliche Stärken. Während Whisper gut in Pattern-Matching ist, excelt Copilot im Kontextverstehen. Das wirft interessante Fragen auf: Können verschiedene spezialisierte KI-Systeme sich gegenseitig korrigieren? Und sollten komplexere Review-Prozesse zukünftig mehrschichtig aufgebaut sein?

## Zusammenfassungen und Key Takeaways: Ein echtes Highlight

Ein zweites großes Feature waren automatisierte Zusammenfassungen der Episoden mit den wichtigsten Punkten (Key Takeaways). Hier zeigten sich deutlich bessere Ergebnisse als bei den reinen Transkriptionen. Die Zusammenfassungen halfen Nutzern, schnell zu erfassen, worum es in einer Episode ging.

Doch auch hier offenbarte sich ein kritisches Problem: Die menschliche Überprüfung (Human in the Loop) ist essential. Ein prägnantes Beispiel war eine Episode über die Dunbar-Zahl – eine soziologische Hypothese über die Größe stabiler Gruppen. Die KI hatte summariert, dass die Episode von dieser Dunbar-Zahl als Maßstab für Gruppengröße handelt. Die Realität war genau das Gegenteil: Die Episode argumentierte, dass die Dunbar-Zahl gerade nichts über optimale Gruppengrößen aussagt. Ohne manuelle Überprüfung hätten Nutzer die Episode auf falscher Grundlage erwarten und nicht gefunden.

Ein anderes Beispiel: Bei einer Episode über Produktmanagement mit Tanja Friedl ging es um Produktkonfiguration. Friedl erwähnte ein Versicherungsfall-Beispiel kurz als Illustration. Die KI war zu dem Schluss gekommen, dass es um ein "System zur Bearbeitung von Versicherungsschäden" geht. Der Kontext war komplett verloren gegangen.

## Die psychologische Falle: Anthropomorphisierung von KI

Während des gesamten Prozesses offenbarte sich ein psychologisches Phänomen, das gleichermaßen faszinierend und problematisch ist: Die menschliche Tendenz, KI-Systeme zu vermenschlichen. Im Podcast-Chat stellte sich die Frage, ob Claude männlich oder weiblich sei – worauf Wolff klug anmerkt, dass dies bereits ein Problem darstellt.

Diese Anthropomorphisierung führt zu realen Problemen. Wenn Claude sagt "Hey, das hast du aber gut gemacht, da wäre ich selbst nicht drauf gekommen", erzeugt das eine emotionale Bindung, die zu geringerer kritischer Distanz führt. Menschen neigen dann dazu, Ergebnisse weniger gründlich zu überprüfen – ein klassisches "Confirmation Bias"-Problem. 

Eberhard Wolff gibt zu, sich dieser Anthropomorphisierung bewusst zu sein, aber auch zu sagen: "Ich gebe da auch für meine Kommunikation selbst nicht mehr viel drauf." Der Punkt ist jedoch wichtig: Während wir es bei persönlichen KI-Interaktionen vielleicht verkraften können, wird es problematisch in professionellen oder kritischen Kontexten.

## Fehlertoleranz und der Stellenwert von Ungenauigkeit

Nach Durchsicht von etwa 50-60 Episoden stellte sich die Frage: Wie viele unbemerkte Fehler befinden sich in den Zusammenfassungen? Wolff ist pragmatisch: "Ich glaube, wir haben keine krass sinnentstellenden Zusammenfassungen, das würde mich überraschen."

Der Schlüssel ist hier die Funktionalität: Die Zusammenfassung soll nicht das Original ersetzen, sondern als Navigationshilfe dienen. Sie antwortet auf die Frage "Soll ich diese Episode anhören?" Ein Fehler in der Zusammenfassung ist akzeptabel, solange das Original verfügbar bleibt und die grobe Richtung stimmt. Manche Fehler sind auch einfach irrelevant – wenn die KI Stereo zu Mono konvertierte, war das technisch ein Fehler, aber sachlich ohne Belang.

## Teamwork hätte geholfen: Die wichtigste Lesson

Ein kritischer Punkt wird gegen Ende deutlich: Ralf war bei der Entwicklung allein vorangeschritten. Er nutzte öffentlich verfügbare Daten und baute schnell eine funktionierende Lösung. Das ist einerseits bewundernswert – aber hätte ein früher Austausch zwischen Ralf und Eberhard stattgefunden, wären viele kleine Probleme vermieden worden.

"Du bist ja allein losgelaufen", merkt Wolff an. "Wenn wir vorher drüber geredet hätten, dann hätten wir es halt herausgefunden." Das ist eine wichtige Erinnerung an ein klassisches Prinzip guter Softwareentwicklung: Kommunikation übertrumpft Einzelleistung. Auch im Zeitalter von KI-assistierter Entwicklung bleibt das wahr.

## Fazit: KI als Werkzeug, nicht als Ersatz

Das Projekt zeigt eindrucksvoll, wie Large Language Models die Softwareentwicklung transformieren können. Sie ermöglichen es Entwicklern, schnell in neue Domänen zu stoßen und komplexe Probleme durch intelligente Problemvermeidung und -zerlegung zu lösen.

Gleichzeitig werden die Grenzen deutlich: Whisper erkennt nur Phonetik, nicht Kontext. Zusammenfassungen können den Kern verfehlen. KI kann andere KI reviewen, aber nicht vollständig ersetzen. Und die menschliche Tendenz zur Anthropomorphisierung führt zu geringerer kritischer Prüfung.

Das goldene Mittel liegt im bewussten "Human in the Loop"-Prozess: KI übernimmt die schwere Lifting beim Code-Generieren und der initialen Datenverarbeitung. Menschen führen die kritische Überprüfung durch. Teams kommunizieren früh, um Fehler zu vermeiden. In dieser Kombination liegt die Zukunft intelligenter Softwareentwicklung – nicht in der vollständigen Automatisierung, sondern in der intelligenten Ergänzung menschlicher und künstlicher Fähigkeiten.