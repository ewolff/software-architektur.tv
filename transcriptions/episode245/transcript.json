{
  "text": "Hallo, ich bin Eberhard Wolff. Freitags mache ich oder Lisa Moritz einen Livestream zum Thema Software-Architektur, oft zusammen mit Gästen. Dieser Podcast ist das Audio des Streams. Weitere Folgen, Sketchnotes und vieles mehr findet ihr unter software-architektur.tv. Hallo und herzlich willkommen zu einer neuen Episode von Software-Architektur im Stream. Heute seht ihr schon, dass eine ganze Bande an Leuten hier und wir besprechen das Thema KI in der Softwareentwicklung über oder unter Hyped. Ich sage schon mal Hallo André, Hallo Stefan, Hallo Ralf und Hallo Eberhard. Ihr fragt euch vielleicht, wie es überhaupt zu dieser Zusammenstellung kommt und zu dieser Episode. Eberhard hatte einen Post bei Heise veröffentlicht. Ich hoffe, ich erzähle es jetzt auch richtig rum. Diesen Post hat er irgendwann ins Englische übersetzt. Dann gab es dazu einen LinkedIn-Post und dieser LinkedIn-Post ist mit ganz vielen Kommentaren überhäuft worden. Und Eberhard hat dann beschlossen, dass es hier diese Folge geben soll mit eben André, Stefan und Ralf. Genau. Bevor wir so richtig ins Thema anfangen, würde ich sagen, dass ihr beiden André und Stefan euch einmal kurz vorstellt, wer ihr seid, was ihr so macht. Stefan, leg doch mal los. Ja, Stefan. Coder by heart, sage ich immer. Ich habe vor 40 Jahren mir Programmieren beigebracht, weil ich Videospiele programmieren wollte. Hab dann Videospiele programmiert. Programmier seitdem. Manchmal hauptberuflich, manchmal nebenberuflich. War lange Jahre Engineering Manager, war CTO und bin seit ein paar Jahren CTO-Coach, hauptberuflich. Sehr gut. André, machst du weiter? Na klar, gerne. Erstmal danke für die Einladung. Ich war auch schon mal hier. Das ist schon ein paar Tage her, aber ich mache es noch mal kurz. Also André macht das so ungefähr jetzt gute 20 Jahre. Ich würde sagen, nicht Coder by heart, aber Techie by heart. Mich interessieren vor allem disruptive Technologien und wie man das auch in die Organisation bringt. Ich war viele Jahre lang quasi so Individual Contributor, also Software-Ingenieur und habe mich dann irgendwann für die Management-Seite entschieden. Und mache das jetzt so die letzten 10, 15 Jahre für verschiedene Unternehmen, verschiedenen Rollen. Und versuche dann halt eben die Technologien halt auch ins Operative zu kriegen. Sehr gut. Genau. Wir haben schon gesagt, KI in der Softwareentwicklung über- oder unterhyped ist quasi das Grundthema. Und ihr alle vier hattet dazu eine These. Und um es halbwegs fair zu machen, würde ich einfach mal alphabetisch nach Vornamen vorgehen und euch bitten zu sagen, was eure Grundthese zu diesem Statement über- oder unterhyped von KI ist. André, fang doch gerne noch mal an. Ja, also meine These ist quasi underhyped. Also wir haben noch nicht verstanden, was damit möglich ist. Ich glaube, wir befinden uns mit einer wahnsinnigen Geschwindigkeit in der Entwicklung, wo das Ende einfach noch nicht abzusehen ist. Und deswegen einfach zu sagen, es ist underhyped, glaube ich, ist quasi für den Moment, max. situativ betrachtet, richtig sein. Ist, glaube ich, aber für das, was da kommen mag, glaube ich, falsch. Oder zumindestens kann man das in Frage stellen. Ich würde eine Sache hinzufügen, es ist underhyped. Aus meiner Sicht ist es aber auch kompliziert, es einzuführen. Wenn ich ein bisschen aus meiner täglichen Arbeit berichte, dann ist es, wie sagt man so schön, Change ist toll, solange es nicht selbst einen betrifft. Und dieser Change, den wir hier vor uns haben, der ist allumfassend. Wir reden nicht davon, dass wir jetzt jeden Tag deployen wollen. Wir reden nicht von automatisierten Tests, sondern wir reden davon, dass grundlegend die Art und Weise, wie wir Software entwickeln, sich verändert. Und das sägt im Zweifelsfall auch an dem eigenen Stuhl. Und sich da dann quasi damit auseinanderzusetzen, glaube ich, ist verdammt, verdammt schwer. Das wäre so eine These, also quasi warum es schwer ist, das in Unternehmen zu etablieren, obwohl es eigentlich sehr powerful ist. Lass mich da auch nochmal vielleicht auf die Mächtigkeit eingehen, weil das, was wir sehen, ist eine rasende Geschwindigkeit. Also getrieben meiner Meinung nach im Endeffekt aus drei Dimensionen. Das ist einmal die Hardware, die immer effizienter wird und auch von oben nach unten durchläuft. Also das, was quasi zuletzt bloß irgendwo im Rechenzentrum möglich ist, ist dann demnächst irgendwo eine Edge möglich. Also auf meinem Laptop. Also ich kann ohne Probleme ein LLM auf meinem Laptop laufen lassen. Das Zweite ist halt LLMs, die Größe und die Fähigkeiten, die diese LLMs haben. Also angefangen Parameter auf der einen Seite, aber auch Kontext Windows. Am Anfang konnte man halt irgendwie eine Klasse da reinladen und sagen oder ein File ergänzt das mal. Jetzt kann ich mittlerweile eine ganze Source-Code-Basis da reinpumpen. Also Global Gemini 1.5 Pro sind quasi 300.000 Token. Können wir gleich nochmal nachgucken. Kann ich gleich nochmal nachliefern. Aber die Kontext-Windows sind mittlerweile so groß, dass halt einfach große Code-Basen da reinpassen. Und das verändert halt einfach alles. Also deswegen kann man halt quasi Stand heute sagen. Heute ist es vielleicht noch nicht da, wo es halt ist. Aber die Wahrscheinlichkeit, dass es sich rasend schnell entwickelt, ist halt einfach gegeben. Und das sieht man unter anderem auch bei den ganzen Benchmarks. Quasi gibt es immer wieder neue Benchmarks, weil die alten Benchmarks dann quasi gelöst wurden. Und das kann man auch relativ leicht nachvollziehen, wenn man sich mal einfach so den, den ich ganz gern verfolge. So SWE Bench. Da gibt es so ein Leaderboard. Und da sieht man auch, mit welcher Geschwindigkeit, also im Monatstakt quasi da immer ein neues Set an LLM und Tools quasi da in den ersten Platz belegt. Genau. Und das bringt mich letztendlich zur letzten These. Ich glaube, wir haben noch nicht verstanden, wie man das bestmöglich nutzt. Am Anfang gab es so ein bisschen die Idee davon, wir müssen jetzt alle Prompt-Engineer werden. Ich glaube, da ist auch was dran. Also quasi Shit in, Shit aus. Wenn der Prompt nicht gut ist, dann kann das Ergebnis nicht gut sein oder es beeinflusst zumindest das Ergebnis. Jetzt wird die Idee von Agents gerade durch die Welt getragen. Und die einfache Frage ist, was ist das Nächste? Was ist das Nächste, was quasi uns in der Softwareentwicklung vielleicht einfach alles verändert? Gewisse Teile werden bleiben. Da glaube ich wahrscheinlich auch eine andere These als das, was vielleicht Stefan Eberhard gleich sagen. Aber ich glaube, wir haben noch nicht verstanden, wozu LLMs oder quasi Generated AI, weil da zählen ja nicht nur LLMs rein, quasi in der Lage sind. Sehr gut. Ich habe noch eine kurze Nachfrage, weil ich glaube, das ist auch noch ganz interessant. Nutzt du schon KI in deiner täglichen Arbeit? Ja. Das ist, würde ich auch sagen, für mich gar nicht mehr wegzudenken. Ist für mich tatsächlich ein Sparrings-Partner, viele Sachen einfach auch schnell zu erledigen. Also im Sinne von auch so Boring-Arbeit halt einfach zu erledigen. Und ich nutze auch verschiedene Sachen. Das habe ich für mich mittlerweile gelernt. Da gibt es nicht dieses One-Tool-to-rule-them-all, sondern kontextbezogen braucht man das eine und mal das andere. Sehr cool. Genau. Ich wünsche mir gleich von euch allen das auch nochmal, wenn ihr die These gesagt habt, sagt gerne auch nochmal dabei, ob ihr schon KI in eurer täglichen Arbeit nutzt. Ich glaube, das ist auch nochmal spannend für die Einschätzung eurer Thesen. Eberhard ist der Nächste im Alphabet. Ja, genau. Vielen Dank. Eigentlich müsste ich ja jetzt sagen, dass es overhyped ist. Ich habe gestern das nochmal korrigiert. Ich glaube, es ist ein bisschen overhyped bei Investoren und bei Management, die jetzt anfangen und sagen, das wird uns grundsätzlich zu ganz neuen Dingen führen und Sachen fundamental ändern. Gleichzeitig ist es so, dass ich das Gefühl habe, dass Entwickler so ein bisschen müde geworden sind und jetzt eher sagen, naja, das ist keine besonders spannende Entwicklung. Ich glaube, da spielt auch die Enttäuschung von Blockchain eine Rolle und die Probleme, die da existieren. Blockchain ist am Ende eine Technologie, die für eine extreme Nische ermöglicht, im Wesentlichen Spekulationen und das Bezahlen von Menschen, die in kriminellen Aktivitäten verwickelt sind. Das ist mit KI ein ganz anderes Ding. Also wenn man Chatshipping ausprobiert hat und die anderen KI-Tools, wird man feststellen, dass die eben deutlich mehr können und viel helfen. Also wie André gerade eben sagte, das ist auch bei mir etwas, was in der täglichen Arbeit eine Rolle spielt. Ich arbeite viel mit Texten, dafür ist es super. Da stelle ich mir der Schrecken fest, dass es Menschen gibt, gerade Techniker, die es noch nicht mal ernsthaft ausprobiert haben und das ist schwierig. Jetzt ist die Frage, warum glaube ich, dass es Overhyped ist. Ich habe diesen Heiser-Artikel geschrieben, der im Wesentlichen die These aufstellt, eigentlich bedeutet Softwareentwicklung, dass wir viele Menschen koordinieren, um gemeinsam an einem Projekt zu arbeiten. Dazu gehören nicht nur Techniker, sondern wir Entwickler und Architekten, sondern vor allem auch Stakeholder, Benutzer und wer auch immer dazugehört. Das bedeutet, dass das Hauptproblem von Softwareentwicklung ist, Menschen zu koordinieren und sie gemeinsam an einem Projekt arbeiten zu lassen. Das ist das, was ich seit sehr langer Zeit auch tatsächlich so erlebe. Das heißt, ich sitze da und rede mit irgendwelchen Leuten, so wie jetzt, über irgendwelche Ideen, irgendwelche Themen, irgendwelche Sachen, die wir bauen wollen und das ist mein wesentlicher Lebensinhalt. André hatte schon einen ganz guten Punkt gemacht. Man muss ein bisschen aufpassen, weil es gibt eine gewisse Versuchung zu sagen, mich tangiert das nicht, ich bin sicher. Ich weiß noch nicht, ob ich da ein Opfer bin, aber ich würde behaupten, diese Tätigkeit, die ich dort betreibe und so, wie ich Softwareentwicklung erlebe und so, wie ich auch die Hauptprobleme erlebe, die ist ein Thema, was KI für mich nicht offensichtlich erschlägt. Ich glaube sofort, dass KI-Tools die Produktivität von Entwicklern und Technikern verbessern, also eben das Produzieren von Code. Wir hatten ja vor 14 Tagen die Episode mit Ralf gemacht, wo er darüber berichtet hat, wie ChatGPT für ihn im Prinzip ein Feature implementiert hat. Das bedeutet, die Entwicklung von Code wird sicher viel einfacher, viel schneller funktionieren. Das sind aber Dinge, die gar nicht so einmalig sind. Wir haben eine Tradition davon, Produktivität zu verbessern durch bestimmte technische Maßnahmen. Hochsprachen haben wir deswegen implementiert. Wir haben angefangen, Betriebssysteme zu implementieren. Wir haben angefangen, Datenbanken zu implementieren. Wir haben Mitarbeiter, die Cloud, wo halt Komponentenbibliotheken sind, die ich einfach benutzen kann. Ich kann eben Machine Learning-Modelle aus der Cloud benutzen. Ich kann auch irgendwelche anderen Dinge, halbfertige Dinge aus der Cloud benutzen als Komponenten. Das sind alles Dinge, die dafür sorgen, dass sich die Produktivität von Entwicklern erheblich verbessert. Es gibt diese traditionelle Meinung von Fred Brooks, der gesagt hat, nicht eine einzige Maßnahme wird die Produktivität in der Softwareentwicklung um eine Größenordnung verbessern, also um einen Faktor von 10. Das belegt er nicht richtig. Das ist eher eine Vermutung. Jetzt ist die Frage, ob KI daran kratzt. Aber selbst wenn, und das ist der letzte Blogartikel, den ich geschrieben habe, ist die Frage, was passiert denn dann? Meine These wäre, wir werden mehr Software in noch mehr Bereichen sehen. Dinge, die wir heute mit Software ledigen, haben wir früher ohne gemacht. Mittlerweile haben wir alle diese lustigen Telefone. Ich habe hier eine Uhr, die hat auch Software. Das sind Dinge, die wir früher nicht gemacht haben. Das ist dieser Rebound-Effekt. Weil etwas effizienter wird, weil wir es einfacher hinbekommen, bedeutet das nicht, dass weniger davon passiert, sondern im Gegenteil, möglicherweise sogar mehr. Wir haben effizientere Autos. Ergebnis, es wird mehr Benzin verbraucht, weil mehr Leute Auto fahren und mehr Strecken fahren. Das ist vielleicht bei Softwareentwicklung ähnlich. Wir sind hier in der schwierigen Situation, dass wir versuchen, die Zukunft vorherzusagen. Das heißt, ich kann jetzt irgendwelche Dinge behaupten. Und ich würde behaupten, wir werden vielleicht eher einen Rebound-Effekt sehen. Und wir werden die Hauptprobleme nicht lösen. Kommunikation, Stakeholder. Unser Hauptproblem ist zu sagen, was genau soll implementiert werden. Unser Problem ist nicht in erster Linie den Code zu produzieren. Und ich glaube, davon können wir alle ein Lied singen, die wir täglich im Meeting sitzen und genau versuchen, das herauszufinden. Nutzt du in deiner täglichen Arbeit KI? Ja, und insbesondere für Arbeiten mit Texten. Also Abstracts zusammenbasteln, Zusammenfassungen bauen, solche Geschichten. Und das funktioniert hervorragend. Und als ich vor zwei Jahren das erste Mal JGPT genutzt habe, hätte ich vorher gesagt, sowas geht gar nicht. Und von daher nicht. Also dass es eine Revolution ist und eine Disruption. Das ist, glaube ich, unzweifelhaft so. Danke, Eberhard. Dann Alphabet. Ralf, glaube ich, PQRS. Genau. Also wenn ich bei der Technologie irgendwie schon Grenzen sehen würde, dann würde ich sagen Overhyped. Aber mein Standpunkt ist Underhyped, weil ich sehe halt momentan keine Grenzen. Ich versuche, die Grenzen zu finden, und ich finde sie nicht. Es ist auf jeden Fall klar, dass GenAI, die Art und Weise, wie wir Software entwickeln, werden fundamental verändert und dass diese Veränderung in vielen Bereichen schnell und zeitnah erfolgen wird. Momentan sehe ich noch, dass die KI oft falsch, weil punktuell, unterstützend und gegen die Regeln der Softwareentwicklung eingesetzt wird. Also wenn ich zum Beispiel Code habe und dann meinen Copilot bitte, mir den Test schreiben zu lassen, dann ist es nicht mehr Test-Driven Development, sondern Development-Driven Testing. Und so sehe ich in manchen Bereichen, dass wir über die KI, weil wir sie punktuell einsetzen, Fehler machen. Aber meine These ist, wenn GenAI richtig eingesetzt wird und wir nicht an irgendwelche Grenzen stoßen, dann wird die GenAI die Softwareentwicklung für uns in Zukunft übernehmen können. Das heißt, wir fallen da meines Erachtens komplett raus. Das ist dieses, was Eberhard ja auch schon gesagt hat. Wir programmieren nicht mehr in Assembler, wir nutzen Hochsprachen. In Zukunft werden wir aus meiner Sicht natürlich sprachlich unsere Requirements, unsere Anforderungen stellen und dann wird die Maschine das umsetzen. Das Ganze geht in Richtung Low-Code, No-Code und das führt wahrscheinlich anfangs nur wieder zu weiteren Problemen, weil wenn wir mit diesem Low-Code, No-Code Ansatz an die Softwareentwicklung rangehen, dann werden wir auf komplexere Probleme stoßen, die wir dann mit dem Low-Code Wissen nicht umsetzen können, sondern wo wir dann eben wieder entsprechende Experten brauchen. Und das heißt, dieses Expertentum wird auch noch länger bestehen bleiben. Davon werden dann einige extrem profitieren, andere weniger. Was mich noch beschäftigt ist, dass wir weltweit momentan nur ein paar wenige LLMs haben, die dominieren. Wir haben zwar eine riesige Anzahl an irgendwelchen Sprachmodellen, aber die Dominanten sind sehr wenig. Und wenn all diese wenigen dominanten LLMs für uns in Zukunft die Entscheidung treffen, welche Technologien wir verwenden, welchen Code-Stil wir verwenden, dann wird es technologisch zu Monokulturen kommen. Wir haben es jetzt schon, Python ist die Sprache der KI, es wird immer mehr KI eingesetzt. Deswegen haben wir immer mehr Python-Repositories und die Sprache gewinnt an Popularität. Und andere fallen dann hinten runter, weil sie eben auch von den Maschinen nicht unterstützt werden. Und das ist eine sehr interessante Entwicklung. Es war jetzt auch gerade die Woche eine Entscheidung in einem großen Repository, dass man eine Dokumentation nicht für den Menschen lesbarer gestalten möchte, sondern bei dem Lesbaren für die Maschine bleiben möchte. Und das wird sehr spannend. Und dich braucht man gar nicht fragen, du nutzt KI in deiner täglichen Arbeit, oder? Nein, ich habe die ganze Zeit schon nachgedacht. Und ja, ich versuche KI zu verwenden, also Gen AI, wo es möglich ist. Wenn ich aber die Coding-Tools verwende, dann habe ich eher das Gefühl, dass sie mich verwenden. Sie sagen, bitte den Code jetzt rüber kopieren und jetzt mal die Tests ausführen, ob es noch läuft. Und davon will ich wieder weg. Und da bin ich eben mit meinem eigenen System auch entsprechend weg, weil dieses System selbst den Code modifizieren darf und selbst die Tests ausführen darf und deswegen entsprechend in die Iterationen reinkommt. Und das ist so mein Traum, dass eben die Maschine nebenbei entwickelt. Manchmal Rückfragen stellt, du soll ich links oder rechts gehen? Und so soll es in Zukunft sein. Aus meiner Sicht. Sehr cool. Danke, Ralf. Und dann darfst du noch den grünen Abschluss mit deinen Thesen machen, Stefan. Ja, immer, immer am Ende wie in der Schule. Bei mir ist aber egal, ob man das mit Vor- oder Nachnamen macht. Okay, meine Thesen. Also erstens, ich benutze in der täglichen Programmierung Cursor als IDE nach ungefähr 20 Jahren, über 20 Jahren JetBrains Produkten. Und ich bin ziemlich fasziniert davon, wie Cursor mich unterstützt, bestimmte Sachen zu programmieren. Das heißt, ich ändere in einer Datei etwas, gehe in eine andere Datei und Cursor macht Vorschläge, was ich hier ändern müsste, damit es zu dem anderen passt. Also es grenzt bei mir schon ab und an an Magie, wo ich denke, wie kommt man da jetzt drauf? Aber das hätte ich auch gemacht. Und die Sachen erscheinen mir auch nicht als trivial. Also ich bin da fasziniert und ich halte es teilweise für Magie im Sinne von hinreichend komplizierte Technologie, die man nicht versteht, stellt sich magisch dar. Ich hatte zwei vorige Momente. Das erste Mal im Moment eben vor 40 Jahren, wo ich zum ersten Mal programmiert habe, was eingetippt habe und da kam am Bildschirm irgendwas Buntes. Das war auch ein magischer Moment. Und ein magischer Moment war auch Anfang der 90er, als das Internet kam und ich die ersten IRC Bots programmiert habe, die dann im Chat irgendwelche Sachen gemacht haben und Leute in Amerika darauf geantwortet haben. Also ich sehe das in ähnlicher Art und Weise. Mich hat zwischendrin relativ wenig technisch fasziniert. Mich fasziniert es unglaublich. Ich glaube aber, dass diese ganze Gen AI für die Softwareentwicklung im Sinne von Code Generierung letztlich nur eine Brückentechnologie ist, die kommt und geht. Eine Auswirkung dieser Brückentechnologie wird sein, dass die Programmierer den reinen manuellen Anteil, also Codeschreiben immer schneller machen. Und deswegen glaube ich, diese Zeiten immer kürzer werden und daraus resultiert, dass die Interaktionen mit Leuten mittelfristig vielleicht auch in Richtung Eberhard nochmal zunehmen, weil wenn ich immer schneller fertig bin mit dem, was ich machen soll oder will oder darf oder kann und dann wieder zurückkommen muss, darf, soll, muss, kann, um mit jemandem zu sprechen, dann werden diese Interaktionen erstmal zunehmen. Also im Prinzip so ein bisschen anders law. Das hatte ich mal von Andrej vor vielen, vielen Jahren. Weiterer Punkt ist, ich glaube, wir betrachten die Software zu sehr als als gegeben und vergessen, dass Software ein Werkzeug ist, das Probleme lösen soll. Also unter dem ganzen Stichwort Jobs to be done ist Software eben ein Ding, was Jobs löst. Mit Software kann ich bestimmte Jobs lösen, meine Aufgaben lösen. Aber vielleicht brauche ich eben, wenn AI weiter fortschritt, brauche ich gar keine Software mehr. Also die Frage ist nicht, löst AI Softwareentwicklung ab, sondern löst AI Software ab im Sinne von Jobs to be done. Und vielleicht sind wir alle ein bisschen zu sehr drauf verhaftet. Softwareentwicklung muss bleiben. Ich würde sagen, was sie schon immer gibt. Das stimmt bei Softwareentwicklung nicht ganz. Aber da ein bisschen weiter wegzugehen, halte ich für einen wichtigen Punkt. Generell glaube ich, es ist aber weitere Zukunft, dass Software, dass einzelne Systeme weggehen und ich in der AI halt sagen werde, irgendwann mal, was sie machen soll. Also speichere mal alle E-Mail-Adressen von allen Kunden oder aggregiere die oder sag mir, in welchen Ländern die sind, ohne dass ich ein CRM-System programmieren müsste. Wie man es vielleicht heute machen würde oder ohne dass man ein CMS machen müsste oder etwas wie HubSpot programmieren. Sondern alle diese Aufgaben kann letztlich eine AI generisch lösen, ohne dass dafür Code generiert werden muss. Der erste Schritt in diese Richtung, glaube ich, werden Datensysteme sein. Ich habe in der Vergangenheit auch mehrmals Firmen geholfen und CTOs geholfen, ihre Datensysteme zu modernisieren und Datateams aufzusetzen. Da glaube ich aber, das werden die ersten sein, die wahrscheinlich massiv darunter zu leiden haben. Das heißt, Datensysteme als erste Instanz etwas, wo ich kein spezielles System mehr habe, sondern eine generische Lösung auf einem Data Lake und dann sage ich, mach mal für Montagmorgen die KPI-Diagramme und dann kommen da fünf Slides raus und dann können wir uns die angucken. Da brauche ich aber keinen Data-Analysten dazu und keinen SQL-Menschen und keinen Plumber, der die Sachen zusammensteckt. Vielleicht der Plumber bleibt vielleicht, brauchen vielleicht immer, kann sein. Das war es, meine Meinung. Super. Dann vielen, vielen Dank euch vier schon mal für eure Meinung. Wir haben im Chat schon ziemlich viel Nachrichten bekommen, dass zum Beispiel Christian Beuthenmüller, der hat als allererstes mal geschrieben, Overhype zumindest bei LLMs, also das ist seine Meinung und nachher, ich glaube, als Ralf gesprochen hat, hat er das ungefähr geschrieben, wir haben jetzt schon Grenzen, Modelle sind nicht besser geworden seit GPT-4. Sind sie das nicht? Also ich bin da nicht so drin. Kann mich da jemand aufschlauen? Also ich glaube, wir sind mit den Modellen echt ganz weit vorne, also die können schon so verdammt viel, dass jeder einzelne Prozentpunkt, den wir irgendwie in den Benchmarks gewinnen, schon ein großer Schritt nach vorne ist. Aber ich glaube auch nicht, dass wir die Modelle so stark verbessern müssen, sondern die Frontends und die Art und Weise, wie wir mit den Modellen arbeiten. Die Modelle brauchen den Kontext, um Software zu entwickeln und nicht nur einen Ausschnitt aus dem Code und wir müssen lernen, wie wir diesen Kontext der Maschine geben. Und da ist, glaube ich, dann eben eine gute Architekturarbeit wieder von Vorteil, wenn wir, also ich habe zum Beispiel mal die Maschine gefragt, hier das Arc 42 Template für Software Architektur Dokumentation. Welche Bereiche aus diesem Template sind für dich interessant? Welche weniger? Und das war ganz super spannend, weil es eigentlich gesagt hat, fast alle Bereiche. Mir ist hängen geblieben, dass die Maschine gesagt hat, das Glossar, das ist jetzt sowas, die Fachbegriffe kennt die Maschine. Aber ansonsten ist es halt wichtig, diesen Kontext für die Softwareentwicklung der Maschine mitzugeben. André, du hattest auch noch gesagt, dass du so Benchmarks beobachtest. Ja, also ich würde widersprechen, dass die LLMs nicht besser werden. Also ich glaube, sie werden in jeder Dimension aktuell noch immer besser. Und nach LLMs gibt es dann die nächsten Modelle. Und vielleicht ist doch das Transformer-Modell, auf dem das alles basiert, quasi nicht mehr dann quasi ausreichend. Aber ich glaube, wir reiten gerade eine große Welle, gerade die Menge an LLMs und Updates. Ich gebe dir recht, gibt nicht so viele große. Tendenziell könnte man das auch als kritisch einstufen. Aber wenn man sich anschaut, was wirklich die großen, die Tech, die Magnificent Seven, mein Gott, man sollte Wörter wählen, die man aussprechen kann. Also die großen Tech-Companies quasi immer wieder deployen und zur Verfügung stellen. Dann würde ich sagen, ist es signifikant, dass was man in diesem Benchmark sieht, vielleicht das als Einschub noch, das sind natürlich immer getunte Modelle und das ist sicherlich auch eine Ebene, also ich würde jetzt zum Beispiel nicht sagen, dass man in den ganz normalen Gemini 1 5 Pro halt einfach seinen Software-Repo reinschmeißen sollte und darauf dann quasi basierend entwickelt. Also das, was man in diesen Benchmarks quasi sieht, sind eigentlich durch die Bank weg alles getunte Modelle, alles quasi auch mit entsprechendem Tooling versehen. Da bin ich bei euch. Das fehlt. Aber vor dem Hintergrund, klar, jede Meinung akzeptiert. Ich würde es anders sehen, einfach aus der puren Erfahrung. Ich versuche wirklich alles auszuprobieren. Ich muss Stefan ein bisschen beipflichten, wirkt ab und zu ein bisschen wie Magie, wenn man sieht, wie gewisse Tools halt quasi einem da auch Vorschläge machen. Ich würde vielleicht kurz einwerfen, wenn ich eine Schwäche beim Thema LLM sehe, ist denn den Fokus auf LLM. Ich bin mir nicht ganz sicher. Das ist ein bisschen wie mit Elektroautos und mit Wasserstoffautos. Wasserstoff ist vielleicht bestimmte Sachen besser, aber die Masse an Ladeinfrastruktur und so weiter ist ja klar, dass das Elektro besser ist, bequemer und andere Sachen hat. Und die Frage ist, ob das mit LLM und anderen Modellen ähnlich ist. Also gibt es bessere Modelle für bestimmte Sachen oder eigene Modelle? Sollte ich eigene Modelle machen oder trainieren oder eben nicht? Kann das alles LLM? Da bin ich mir zum Beispiel noch nicht so sicher. Ich habe einen sehr erfolgreichen Kunden, der im Sinne von Jobs to be done. Früher Software entwickelt hat für Kunden und jetzt Probleme für Kunden löst, indem er eigene AIs trainiert, die dem Kunden bestimmte Arbeiten abnehmen. Also er hat vorher sehr tolle Tools gebaut, wo der Kunde so SaaS mäßig Sachen machen kann. Und jetzt nimmt er den Jobs ab und das macht er aber nicht mit LLM, sondern mit eigenen Modellen. Und da bin ich nicht sicher, wo es dahin geht. Super, genau, Poliboy hat auch noch kurz ergänzt, angeblich ist die Hardware Skalierbarkeit für LLMs fast erreicht, durch die noch Fortschritte gemacht werden können. Das hat noch jemand geschrieben. Und was ich wollte, Ralf hat sich entmutet. Du willst dazu was sagen? Ich wollte es nur eben bestätigen. Also wir denken jetzt immer an Gen AI und die LLMs. Und das ist natürlich ein großes Problem. Das war eigentlich auch schon vorher ein Problem. Also ich habe mal gesucht, wie man Barcodes am besten decoden kann und bin auf viele Blogposts gestoßen, wo ein neuronales Netz trainiert worden ist, um die Barcodes zu erkennen. Das ist natürlich blöd. Ja, und genauso könnte ich jetzt LLMs für solche Tätigkeiten nutzen. Aber damit verschwende ich nur Energie. Und wir benutzen jetzt vor allem die LLMs, weil sie eben schon da sind und weil sie uns die Programmierprobleme lösen. Aber es sind so viele Leute an den Unis und in den Firmen damit beschäftigt, diese Large-Language-Models in Small-Language-Models umzuwandeln, damit sie eben, wie André vorhin auch sagte, on the edge laufen können und auf kleineren Devices, damit man diese Technologie jetzt in den Griff bekommt. Und das ist weiterhin, deswegen finde ich diese Unterscheidung in Gen-AI und KI auch so wichtig. Das ganz normale Machine Learning ist weiterhin sehr, sehr wichtig, weil wir damit schon vor Gen-AI sehr gut Probleme gelöst haben und das nicht außer Acht lassen sollten. Also hier im Chat ist total viel, aber ich glaube, wenn ich jetzt hier alles vorlese, was gerade im Chat passiert, dann kommen wir nie zu dem nächsten Thema, was wir eigentlich besprechen würden und wollten, und zwar, was können wir denn überhaupt heute schon tun? Also was bedeutet das heute für uns, das ganze Thema mit der KI? Ihr hattet schon, als ich in den Call kam, alle fleißig diskutiert. Von daher würde ich einfach mal die Frage in den Raum werfen. Was bedeutet das heute für mich? Was sollte ich heute schon tun? Und Eberhard sah aus, als wollte er reden, ist den Kopf zur Seite gedreht und kurz den Mund aufgehoben. Finde ich, ist eine spannende Frage. Ich glaube, eine Sache ist, sich mit den Technologien vertraut zu machen und sie tatsächlich ernsthaft zu nutzen. Und da ist halt etwas am Werden. Und wie gesagt, einer der Gründe, warum ich glaube, dass es sinnvoll ist, diese Episode zu machen, ist, weil ich immer noch das Gefühl habe, dass viele Leute sich darauf nicht einlassen und da nahezu technologiefeindlich sind. Und das ist irgendwie schwierig, nicht? Also ich glaube halt, dass sich da einiges ändern wird. Ich bin ehrlich gesagt nicht sicher, was die konkreten jetzigen Hinweise sind. Ich glaube, also jetzt auch nach der Diskussion, die wir bis jetzt geführt haben, dass, also wie soll ich sagen, ich hatte bisher gedacht, dass man irgendwie sagt, naja, wir haben halt eine höhere Ebene von Abstraktion und sagen halt, implementiert das mal, was ja eben genau das ist, was Ralf gemacht hat mit diesem Linter für SGDoc. Und dann haben wir eben tatsächlich eine höhere Ebene von Abstraktion. Also dann ist es eben so, wie man einem Entwicklern sagt, implementiert das mal und die macht es dann halt. Unter der Voraussetzung bedeutet das halt, dass wir uns noch stärker orientieren müssen an den Dingen, die ja dann wie wichtig sind, also mit Kundinnen zum Beispiel sprechen, verstehen, was fachlich tatsächlich der Fall ist, was exakt die Veranforderungen sind und von daraus sozusagen eine Lösung entwerfen. Und das ist meiner Ansicht nach eben traditionell das schwierige Thema. Und das wird, glaube ich, auch das schwierige Thema weiterhin bleiben. Und darauf müssen wir uns noch stärker fokussieren. Ich glaube, die andere Sache und daraus ergibt sich dann eben auch. Das ist auch so etwas, was ich so ein bisschen aus der Diskussion mitgenommen habe. Eigentlich ist Code ja ein Mechanismus, mit dem Menschen miteinander kommunizieren. Also ich schreibe es halt einmal. Es wird vielfach gelesen. Es wird geändert. Wir müssen jetzt irgendwie weg davon kommen. Und das ist halt genau diese Sache, die Ralf auch angesprochen hat. Das ist halt jetzt so weit, dass man sagt, okay, dieses Diagramm will ich aber maschinenkompatibel haben. Also wir müssen halt irgendwie dafür sorgen, das wird vielleicht ersetzt in Richtung von Menschen und Maschinen kommunizieren über Code und über bestimmte andere Dinge. Da haben wir übrigens meiner Ansicht nach in der Episode mit Ralf von Pfeder gemacht. Wir hätten nämlich eigentlich sagen müssen, hey, hier sind Qualitätsszenarien. Wir haben aber Qualitätsszenarien erstellen lassen. Also da sind Dinge, die wir, glaube ich, da noch besser machen können. Und ich fand auch den Hinweis interessant von Stefan, dass das halt so ein allgemein generisches Ding ist, was halt einfach ein Problem lösen kann. Und das bedeutet, dass wir halt noch was anderes haben. Also wir haben halt nicht eine höhere Ebene von Abstraktionen, sondern was anderes, wo wir anders Dinge ausdrücken können. Und da muss man sich, glaube ich, auch mal Gedanken darüber machen, was das denn nun tatsächlich bedeutet für die Projekte. Und nicht also diese Hinweise, die auch Stefan da gegeben hat, nicht, dass vielleicht so eine Datenanalyse in Hochsprache, also normaler deutscher Sprache oder englischer Sprache oder was auch immer, dass das halt vielleicht ein Zukunftsding ist, nicht? Guter Punkt. Und das wäre etwas, was wir im Moment nicht so einfach umsetzen können. Ich habe vielleicht eine etwas schwammige Nachfrage. Du hast ganz am Anfang gesagt, dass du, dass es Leute gibt, die sich gerade mit dem Thema nicht auseinandersetzen wollen und dass, dass sie sich quasi, dass sie dagegen sind, mehr oder weniger. Ist es denn so schlimm, wenn ich mich jetzt heute noch nicht damit auseinandersetzen möchte, wenn ich quasi noch ein bisschen warte, bis der Hype etwas abgeebbt ist? Also ich sehe noch nicht so ganz den, dieses, was daran schlimm ist, wenn ich mich jetzt heute noch nicht damit auseinandersetzen möchte. Was wir noch nicht angesprochen haben, ist, wir haben einen massiven Energieverbrauch und das wird ein großes Problem werden. Das ist jetzt schon ein großes Problem und das ist sicherlich ein Nachteil von AI und AI wie alle Technologien hat irgendwie Nachteile. Ich glaube, das, wo ich ein bisschen Angst habe, ist, dass wenn man sich damit auseinandersetzt, dass man dann wirklich irgendwann obsolet wird. Und ich bin da, glaube ich, nicht so wie Stefan, der ja sagt, nicht. Also das ist halt das neue Ding, was mich hat total begeistert. Aber ich warne, glaube ich, vor der Fehlwahrnehmung, dass das halt nur ein Hype ist. Also nicht Shichibiti ausprobieren. Ich habe es halt irgendwie eine Stunde ausprobiert, als es halt rauskam. Und ich war nach einer Stunde überzeugt, dass das halt ziemlich krasse Sachen sind, die ich vorher nicht für möglich gehalten habe. Und ich habe ein Problem. Also wie soll ich sagen, wenn man jetzt irgendwie sagt Nein, es ist halt nur ein Hype, dann ist das halt meiner Ansicht nach objektiv einfach falsch. Und das bedeutet, dass man sich möglicherweise in seiner Karriere und halt in dem, was man halt so tut, in Anführungsstrichen falsch verhält, also nicht Dinge halt irgendwie falsch einschätzt. Und das finde ich halt einfach schwierig. Also ich glaube halt nicht grundloser, grundloses Hype-Wissen ist halt ein Problem oder Grundloser, ein Hypes-Glauben ist halt ein echtes Problem. Und da ist halt Blockchain für mich halt ein, da gab es halt Leute, die irgendwie an diesen Hype geglaubt haben. Und das war eben grundlos. Das andere, wo ich irgendwie mich mit der Technologie nicht ausreichend kritisch beschäftigt habe, war EGB am Anfang. Ganz anderes Thema. Und ich glaube, bei KI ist es umgekehrt. Also gibt es ein bisschen diese umgekehrte Tendenz. Für mich ist immer, ich würde noch, darf ich kurz, Stefan? Immer. Okay, ansonsten schlägt das Alphabet, also Vorname. Was ich gerade mitbekomme oder was ich, was ich immer wieder höre, ist also gegen was das halt antritt, also der Vergleich. Und für mich ist die Frage, also tritt das gegen, also tritt Gen-AI gegen Cloud an oder gegen Microservices oder tritt das gegen, ich mach jetzt mal das krasse Gegenteil, tritt das gegen Feuer und Elektrizität an? Also quasi über, also wo ordnet sich das, das Thema halt ein? Und das, ich glaube, wenn es sich eher quasi bei dem Cloud-Thema einordnet, dann würde ich sagen, okay, brauchst du dich im Zweifelsfall nicht beschäftigen. Das wird auch wahrscheinlich noch in 100 Jahren quasi Data Center geben, die quasi anders funktionieren. Gibt immer gewisse Use Cases, die vielleicht eben nicht in der Cloud laufen können oder wo man Infrastruktur nicht so weitestgehend automatisieren kann. Wenn das allerdings gegen die andere Seite antritt, dann ist es schon ziemlich fundamental. Und jetzt ist vielleicht Feuer und Elektrizität also sehr, sehr, sehr weit quasi auf der anderen Seite. Aber man könnte auch sagen, es tritt quasi gegen die Dampfmaschine halt irgendwie an. Und das ist vielleicht dann schon irgendwas, was wir alle auch dann wiederum nicht kennen, aber was vielleicht ein bisschen greifbarer ist. Und da muss man schon sagen, da sind ganze Industrien weggefallen. Und vor dem Hintergrund und das vermisse ich so ein bisschen. Meine Frage, die Frage war ja so ein bisschen, also was müsste man heute schon tun? Und das verstehe ich nicht, weil ich quasi eigentlich unsere Disziplin immer als sehr neugierig wahrnehme. Quasi diese Offenheit gegenüber einer Sache. Und was ich halt häufig wahrnehme, ist entweder so eine relativ theoretische Sache. Habe ich irgendwas gelesen? Aber ich habe es nie probiert. Und eine der Sachen, die wir machen, um da mal ein konkretes Beispiel zu machen, wir haben einmal im Monat so einen Self-Education-Friday und eigentlich versuchen wir da, die Leute zu ermutigen, mal Sachen auszuprobieren. Das heißt, es gibt verschiedene Themen, aber unter anderem halt auch Gen-AI, wo man mal sehen, guck mal, man kann halt auch einen LLM auf seinem Notebook laufen lassen, kann ja halt irgendwie deiner täglichen Arbeit halt irgendwie helfen. Ich will jetzt nicht sagen spielerische Herangehensweise, aber ich habe unsere Disziplin eigentlich immer so verstanden, Neugier, Automating all the things und halt einfach Sachen auszuprobieren und natürlich quasi dieses ganz große Bild, was das Szenario, was Stefan gerade gezeichnet hat, also quasi vielleicht gibt es gar keine Software, das mag irgendwann mal kommen. Und da kann man sich natürlich jetzt vortrefflich drüber streiten, weil man es noch nicht, weil es ist nicht around the corner wahrscheinlich. Aber quasi was kann ich heute quasi einfacher machen? Womit kann ich mein Leben leichter machen? Wo kann ich halt irgendwie das wegautomatisieren oder auslagern, was mir vielleicht auch persönlich gar keinen Spaß macht? Das vermisse ich. Also für mich beginnt das tatsächlich individuell im Kopf. Ich habe gerade kam im Chat ein Kommentar zu dem Dampfmaschinenvergleich und zwar schreibt Happy Tree, der Unterschied zu dem Dampfmaschinenvergleich ist, dass wenn das funktioniert, es keinen neuen Arbeitsbereich gibt. Ich weiß nicht mehr, wer von euch es sagt, aber irgendjemand sagte schon vorhin, dass man die Beine von seinem eigenen Stühlchen abschneidet oder so. Du warst das, genau. Ich war beide, beide sogar, genau. Nee, ich wollte bloß kurz was sagen. Also ich, jetzt geht es ja um die Softwareentwicklung so ein bisschen weniger um allgemeine. Aber für das Allgemeine glaube ich, dass da würde ich dem Kommentar aus dem Chat sehr, sehr zustimmen. Wir haben durch Rationalisierung alle Leute aus dem Agrarsektor in den Industriesektor, der glücklicherweise durch die gleichen Bedingungen zur gleichen Zeit entstanden ist, verfrachtet. Und dann haben wir alle Leute aus dem Industriesektor in den Dienstleistungssektor verfrachtet. Wenn man sich heute eine Fabrik anguckt, dann ist die quasi komplett leer, kaufen ganz wenig Leute rum. Und jetzt verlagern wir Leute aus dem Dienstleistungssektor irgendwo hin. Aber leider ist halt, das war jetzt so der dritte Sektor. Aber es ist unklar, wo die Leute hinverlagert werden. Also das ist, glaube ich, der ganz große Unterschied zur Dampfmaschine. Und da würde ich mich dem ein bisschen dem Kommentar anschließen. Ich habe noch einen Punkt zu dem vorher zu dem Energiethema, was mir immer ein bisschen aufstößt, also jetzt hier nur allgemein. Es wird sehr, sehr stark vermischt an Training und Inferenz. Also das hat natürlich jetzt auch stark mit dem aktuell, wie wir arbeiten, zu tun, mit LLMs und mit dem quasi mit dem Batch arbeiten, also mit dem Trainieren und mit dem Benutzen und mit dem Trainieren und mit dem Benutzen. Aber trotzdem, glaube ich, muss man schon auch immer ein bisschen unterscheiden, weil Ralf vorher, glaube ich, es gesagt hat oder Andreas gesagt mit dem Laptop. Aber letztlich kann ich es auf meinem Handy haben. Also die Energienutzung von LLMs nutzen und von LLMs trainieren ist potenziell auch nochmal eine andere. Das wird auch bei Performance oft und bei Hardware Performance oft irgendwie alles in einen in einen Topf geworfen. Für mich sind es aber aktuell durch den Batch Modus zwei fundamental unterschiedliche Operationsmodelle von von AIs mit unterschiedlicher Performance, unterschiedlichen Energieverbrauch und so weiter und so fort. Vielleicht ein paar Gedanken, also weil du es gerade sagtest, André, von wegen, wo trägt das an? Also wir haben das ja im Stream exerziert. Das heißt, wir haben gesagt, liebes, liebes Chetchipi Team, mach doch mal die SAP Beispielaufgabe. So, damit lassen wir es halt in mir antreten gegen Software Architekten, die sich halt zertifizieren lassen. Und das Schlimme dabei ist, dass die Ergebnisse zumindest scheinbar gar nicht so schlicht sind. Und das ist genau das, was wir jetzt vor 14 Tagen in dem in dem Stream nochmal gesehen haben. Also wenn ich mir Qualitätsszenarien angucke, also was sind die Dinge, die dieses Verfahren leisten soll? Da kommt halt eine ein über ein überzeugend klingender Text raus. Und wenn mir jetzt jemand fünf Sachen vorgeben würde und halt sagen würde Okay, guck dir mal die die an und sag mir mal, welches davon irgendwie von Chetchipi ist, bin ich mir nicht sicher, ob ich es rausfinden könnte. Und das ist enttäuschend, weil das, was da irgendwie rauskommt, ist halt aus den Fingern rausgesogen. Also kommen halt Performance Ideen raus, die halt irgendwie vom Himmel fallen. So und das ist eine nicht positive Aussage über unsere Branche. Wir müssen da besser werden. Wir müssen halt solche Dinge wie Qualitätsszenarien, Anforderungen, solche Sachen halt besser hinbekommen und deutlich unterscheidbar sein von irgendwelchen AIs, sonst haben wir ein Problem. Und das ist und das ist aber das, was also nicht, weil du sagtest, wo tritt das an? Also da draußen sind garantiert Leute, die genau das machen und die halt dann irgendwie genau dieses, die halt dann sagen Okay, reicht ja. Also nicht haben wir ja. Und ich bin mal gespannt, wie das sozusagen rausgeht. Und wir müssen da irgendwie, glaube ich, qualifizierter werden und besser werden. Und bezüglich der Produktivität. Ich finde das auch ein schwieriges Thema, weil nicht vor langer Zeit war es so, da gibt es immer wieder solche Schilde. Wir haben schon diskutiert, die Fabriken sind mittlerweile leer. Trotzdem gibt es noch Facharbeiter, die Autos bauen oder solche Dinge tun. Ich erinnere mich an die Zeit, wo ich Anfang der 2000er war, wo die Aussage war, wir machen Offshoring und Nearshoring und dann ist das Thema mit Softwareentwicklung in Deutschland tot. Das ist einfach nicht so. Wir sind eine der größten Branchen, auch in Deutschland, auch in diesem Hochlohnland, obwohl das von der Autoindustrie so dominiert ist. Zitiert mich nicht, aber ich meine, wir sind auf Augenhöhe mit der Autoindustrie. Es gibt halt Produktivitätszuwächse und die führen scheinbar dazu, dass – oder nicht, wenn man das sozusagen extrapoliert von man hat – wir in diesem Bereich nicht mehr arbeiten. Ich habe nur das Gefühl, das ist nicht so. Es ist immer noch so, dass du als Facharbeiter in einem Automobilwerk gutes Geld verdienen kannst und du kannst immer noch als Entwickler hin gutes Geld verdienen, obwohl du im direkten Wettbewerb stehst zu irgendwelchen Leuten, die Offshoring machen. Ich bin nicht sicher, ob das das Ding ist, was unser gesamten Brancheit das Licht ausbläst. Wäre überraschend, weil die anderen Ansätze haben es im Jahr auch nicht geschafft. Das war jetzt ein langes Statement, wenn ich das mal so sagen darf. Du hast da sehr viele Punkte angesprochen, gerade jetzt zum Beispiel, die KI kann innerhalb kurzer Zeit eine Architektur erstellen und wir brauchen relativ lange, um diese Architektur zu validieren. Zeit ist ein teures Gut und das merke ich eben auch, wenn die KI mir Code generiert innerhalb von Sekunden und ich brauche Minuten zum Verifizieren. Gerade wenn ich jetzt sowas wie den ASCII-Doc Linter so nebenbei erstellen lasse, die Zeit, das alles zu verifizieren, ist teuer, ist aufwendig. Und sobald ich da einen gewissen Trust in die Maschine habe, werde ich das bleiben lassen. Aber interessant ist eben auch bei der Arbeit mit der Maschine. Das, was ich kontrollieren kann, das kann ich ganz gut mit der Maschine bearbeiten, weil sie erzeugt mir etwas. Ich gucke drüber, sage Ja, passt, die Qualität stimmt. Noch besser ist es, wenn die Maschine das selbst kontrollieren kann. Und da sind wir halt mit der Softwareentwicklung in einer Situation, die einfach toll ist. Dadurch, dass wir automatisierte Tests machen können, sind gerade diese Bereiche wie der ASCII-Doc Linter. Ich habe ein einfaches Interface, ich kann Tests schreiben und die Maschine kann selbst gegen die Tests ausprobieren, ob ihre Software funktioniert, kann selbst iterieren. Jetzt bin ich natürlich noch einen Schritt weiter gegangen und habe gesagt, Maschine, schreibt dir selbst die Tests. Und da brauche ich dann wieder das Vertrauen oder ich brauche den Aufwand, um es zu überprüfen, was die Maschine da geschrieben hat. Und das geht halt in den Bereich, der wirklich viel mit mit Trust in die Maschine zu tun hat und wo wir eben auch mit der Zeit sehen werden, wie sich das entwickelt. Und auch weil die Maschine so schnell manche Sachen machen kann, Aufgaben, die wir nicht gerne machen, laufen wir teilweise Gefahr, sie falsch einzusetzen. Also neue Features entwickeln, macht Spaß. Die Tests dafür zu schreiben und die Dokumentation zu erstellen, macht keinen Spaß. Also übergeben wir das der Maschine. Wir schreiben erst den Code, erst die Features, sagen dann der Maschine, du dokumentiere das mal und schreib mir die Tests. Also hatten wir ja vorhin schon Test-Driven geht anders, ist das erste Problem. Mit der Dokumentation ist das zweite Problem, weil die Maschine sieht den Code, kann nur sagen, was der Code macht, also das was dokumentieren. Warum ich das Ganze so aufgebaut habe, warum ich folgende Libraries benutzt habe, das kann sie in dem Moment nicht sagen. Es sei denn, ich mache den Kontext wieder groß und dieses Warum, das ist teilweise eben in der Architektur, dass wir uns für Technologien entscheiden aufgrund der Qualitätskriterien. Und das wird dann eben spannend, wenn wir das mit der Maschine machen. Aber diese diese Vorgänge sind wieder zeitlich aufwendig und es wird längere Zeit dauern, bis wir eben verschiedene Experimente gemacht haben, wie wir mit den Maschinen arbeiten können. Und dann eben diese Experimente auch entsprechend bewerten können, um zu sagen, das war der richtige Weg oder eben auch nicht. Zum Thema Verifikation hat HappyTree gerade noch auf YouTube geschrieben, die Verifikation ist auch der Game Changer. Bis wir das haben, wird zum Beispiel im Medizinbereich und so weiter auch nicht KI zum Einsatz kommen. Aber das wird auf jeden Fall kommen, denkt er. Und er hat gerade noch geschrieben, man muss sich ja nun mal angucken, was im Modellchecker-Bereich im RE, wahrscheinlich Requirements Engineering Bereich geht, wie viele der Requirements am Ende unvereinbar sind. Hat er noch geschrieben. Stefan, du hast, glaube ich, noch gar nichts gesagt jetzt hier zu der Frage. Wie ungewöhnlich. Also vielleicht noch mal ganz kurz zum Hintergrund. Ich habe zwar in größeren Firmen auch ein bisschen gearbeitet, aber mein Hintergrund ist eigentlich so bis 50, 60 Entwicklern in meistens stark wachsenden Startups, wo ich entweder Cyber Engineering Manager war oder jetzt meine Kunden sind. Das heißt, das sind wahrscheinlich auch so ein bisschen unterschiedliche Welten, vielleicht auch vom Programmier-Niveau, vom Herangehensniveau von anderen. Da unterscheiden wir uns sicherlich in dem Hintergrund oder unterscheide ich mich vielleicht ein bisschen im Hintergrund von anderen Leuten hier. Und da muss ich sagen, beim Thema Test und AI, immer wenn, da hatte ich mich auch mit jemandem vor kurzem drüber unterhalten. Wir waren der Meinung, in dem Kontext, in dem wir uns bewegen, schreibt AI bessere Tests. Also besser, weil mehr. Also besser durch Quantität und besser durch Qualität. Das heißt, da muss man natürlich immer noch mal gucken, was will man mit den Tests? Will man mit den Tests Requirements abdecken? Will man Regressionen vermeiden? Also was ist denn vielleicht die Aufgabe? Und dann ist auch innerhalb von dem Test, wo die AI manche Sachen besser kann und manche Sachen schlechter kann. Aber irgendwie scheint die AI meine Code-Basis und andere Klassen mehr zu verstehen und durch Anbindung potenziell an größere LLMs die Welt besser zu verstehen und an Sachen zu denken, an die ich nicht denke. Und von dem Hintergrund, wenn ich vielleicht nicht sehr stark, jetzt mal eben kein Medizinbereich, vielleicht auch kein Rüstungsbereich oder keine kritischen Bereiche, würde ich eher dazu gehen, dass die AI bessere Tests schreibt als der Durchschnittsentwickler und meistens bessere Tests schreibt als ich, vielleicht eben als Durchschnittsentwickler. Durch den größeren Hintergrund an Wissen und an was kann denn eigentlich schief gehen? Du hast da zwei interessante Punkte genannt. Also zum einen, die AI schreibt halt schnell viele Tests. Dadurch habe ich schnell eine hohe Testabdeckung. Meistens kriegen die Entwickler aufgrund von Zeitdruck gar nicht die Möglichkeit, sich mal über die Tests groß Gedanken zu machen. Zumindest ist das so meine Erfahrung. Wir haben die Theorie, wie es laufen sollte und wir haben die Praxis. Das andere, was du gesagt hast, ist, dass du das Gefühl hast, dass die AI dein Repository gut versteht und ein gutes Weltbild hat. Da finde ich es interessant, dass es dazwischen noch etwas gibt, dass ich zum Beispiel mit der AI in einem Repository arbeite, welches zu einem größeren Projekt gehört. Und das kennt die KI momentan meistens nicht, weil sie sich auf ein Repository konzentriert. Das heißt, sie kennt gar nicht die Schnittstellen zu der anderen Software. Oder wenn wir jetzt die Architektur Dokumentation in einem Wiki haben, kommt die KI nicht dran. Wenn wir aber den DOCSIS Code Ansatz verwenden, dann kann es passieren, dass unsere Architektur oder andere Dokumentation im Repository liegt und die KI, die aufgreift, die Qualitätskriterien vielleicht findet und deswegen sagt, oh, dann nehme ich lieber die Library anstatt jener. Oder Zugriff auf Bug Reports hat und Critical Incidents und Postmortems. Und da fällt mir auch gerade wieder ein, ich habe gerade wieder was entwickeln lassen und die KI hat ja einen alten Stand von Libraries benutzt, weil sie eben ein Knowledge Cutoff von vor einem Jahr hat. Und dann haben wir da auch wieder ein Problem. Wie gehen wir mit neueren Versionen um? Das sind alles Sachen, auf die sich heutige Entwickler einstellen müssen. Wir brauchen Antworten auf diese Fragen. Was aber by the way bedeutet, und das ist auch etwas, was mir auffällt, dass die Anforderungen an Menschen in diesem ganzen Bereich eigentlich eher steigen. Das, was du jetzt zum Beispiel als Beispiel gesagt hast, ist, hat eine alte Library benutzt, suboptimale Idee, muss ich halt irgendwie korrigieren. Das heißt, ich muss nicht das Problem lösen, sondern ich muss erkennen, dass das Problem nicht vernünftig gelöst ist und es noch besser lösen. Und das ist halt anspruchsvoll, wobei man auch natürlich argumentieren kann, aber es funktioniert ja und dann ist ja gut. Und vielleicht ist das auch ein Teil der Komponente. Also wenn man sagt, ich will am Ende nur etwas haben, was funktioniert und mein konkretes Problem löst, vielleicht definiert durch Tests, dann kriege ich es halt. Und das ist vielleicht intern gar nicht so wahnsinnig schön. Du hattest noch einen Kommentar bekommen, der bezog sich eventuell nur auf deinen Kommentar, den du in YouTube geschrieben hast, aber er passt, glaube ich, generell noch ganz gut von Christian Beuthmüller. Er hat nämlich geschrieben, KI generierter Code basiert auf GitHub und Stackoverflows wird nicht besser werden als der Durchschnitt. Und ich finde, der Kommentar passt auch nochmal gut zu den generierten Tests, die Stefan erwähnt hat. Ich bin nämlich immer noch, also in meinem Kopf bin ich immer noch bei diesen Tests, die besser sind als die menschgeschriebenen Tests. Und ich frage mich, wie das dazu kommen kann, dass sie besser sind. Also ich hänge da, glaube ich, im Kopf noch ein bisschen dran. Genau, und was ich jetzt noch gerade kam, noch ein von Happy Tree auch sehr interessant, finde ich auch die Frage, was mit Problemen wie dem XZ-Exploit passieren wird in diesem Kontext. Da habt ihr da eine Meinung zu? Also Richtung Exploits, Security, KI? Also da sind jetzt verschiedene Punkte. Der eine Punkt ist halt, also weil du bei den Tests sozusagen noch hängst, ich glaube, also wie soll ich sagen, ich bin nicht sicher, ob mich das überrascht, weil das bedeutet, dass halt viele Dinge, die wir halt in unserer Branche machen, eben qualitativ nicht so gut sind. Und das führt irgendwie dazu, dass halt eine AI über diese Hürde drüber springt und halt besser ist. Und ich weiß nicht, ob mich das überrascht. Ich frage mich nur, was das bedeutet. Also wenn jetzt einer von diesen generierten Tests fehlschlägt, also sind die, kann man die gut verstehen? Versteht ihr dann gut, wo jetzt das Problem liegt? Also das sind ja auch nochmal Sachen, also ich schreibe ja auch einen Test, der mir nochmal sagt, okay, ich habe das Problem verstanden, ich weiß, was die Lösung sein soll. Also das Schreiben des Tests hat ja einen positiven Effekt auf das Verständnis des Gesamtcodes. Und wenn ich dann sehe, dieser Test schlägt fehl, weiß ich ja warum. Und wenn ich mich jetzt so daran zurückerinnere, auch nur wenn ein anderer den Test geschrieben hat und genau dieser Test schlägt dann fehl, je nachdem, wie komplex oder unumständlich der geschrieben ist, hampel ich ja dann daran länger rum. Und ich frage mich nur gerade, wie gut das noch so für die restliche Nachvollziehbarkeit der Problemlösung ist, wenn dann mal ein Test fehlschlägt, weil wir einen Change gemacht haben. Stefan, du hättest dich gemeldet. Das mit dem Fehlschlagen ist, denke ich, ja üblich jetzt schon ein Problem, wenn jemand einen anderen Test geschrieben hat und ich habe den nicht geschrieben. Das passiert ja öfter mal in Firmen bei Integrationstests oder sowas und dann hält das auf, weil ich nicht verstehe. Also ich glaube, das kann ich nachvollziehen. Ich wollte aber zu dem, wie kann es besser sein, als ich einwerfen. Es gab mal diese Blogposts, wo dort stand, 100 Mythen, die Entwickler über Adressen glauben oder über Namen glauben oder sowas. Also das weiß ich nicht. Und vor dem Hintergrund glaube ich schon, dass eine KI bessere Tests schreiben kann als der Entwickler. Ich glaube, ich bin bei besser eher so ins Stocken geraten. Nicht ob der Vielfältigkeit der Tests und gerade bei Validierung glaube ich sofort, dass Entwickler nicht anders denken, was wirklich validiert werden soll, sondern eher in Richtung der Lesbarkeit der Ergebnisse, eben weil die Dinge über Stack Overflow und GitHub gezogen werden und nicht alle EntwicklerInnen bisher groß auf Lesbarkeit von Dingen geachtet haben. Also ich hatte gerade jetzt das Problem, dass die Tests, die die Maschine mir erzeugt hat, tatsächlich für mich nicht so schön lesbar waren. Ich habe der Maschine dann gesagt, sie soll sie überarbeiten und bitte mit Behavior Driven Development arbeiten, dass ich eben sehe, gegeben das und das. Wenn das passiert, dann erwarte ich Folgendes. Und dadurch wurde es wieder lesbarer. Also ich meine, das ist dann eben jetzt auch so eine Entwicklung, dass ich der Maschine Verhalten beibringe, dass ich ihr sage, wie ich Software entwickeln möchte. Und das landet dann im System prompt und das nächste Mal macht sie es dann eben genauso, wie ich das brauche. Das spricht so ein bisschen für die These, dass man quasi prompt Engineering auf jeden Fall besser werden soll. Wer hat das vorhin gesagt? Shit in, shit out. Das ist ja mit fast allem eh so. Also wenn ich schlechte Requirements habe, dann kriege ich ja auch eine schlechte, eine unpassende Software. Das ist ja nicht nur bei KI. Ja, wir brauchen halt vor allem Mechanismen, wie wir die KI uns unseren Bedürfnissen anpassen können. Und André hatte vorhin dieses Feintuning genannt. Also zumindest André, du sagtest, dass die ganzen Modelle in den Benchmarks getunt sind. Und ja, ist das dieses Feintuning, was man in den in der Dokumentation liest? Oder meinst du, das sind quasi auf spezialisierte, mit spezialisierten Trainingsdaten gefütterte Modelle? Sowohl also sowohl das als halt auch die Gewichtung, die die Modelle halt irgendwie nützlich bringen. Also im Zweifelsfall ist ja aus so einem Large-Language-Modell nur ein Bruchteil der Informationen halt irrelevant und die werden einfach höher gewichtet. Das ist eine Sache. Da muss ich auch fairerweise zugeben, stehe ich jetzt nicht im Detail drin. Man sieht bloß diesen Trend, dass es eben nicht ein JGPT 4.0 Mini ist, was da halt irgendwie antritt, sondern das sind alles quasi. Ich würde fast sogar soweit gehen, zu sagen, das sind alles Forschungsmodelle oder Forschungsansätze, wo halt wirklich sehr genau getweakt wird. Wie kann man jetzt diese Cases halt irgendwie lösen? Im Chat kam gerade noch ein spannendes Statement. Das möchte ich jetzt noch eben loswerden, obwohl wir schon überziehen. Christian Beuthmüller hat geschrieben, meine aktuelle Erfahrung ist Senior Developer werden schneller und Junior eher das Gegenteil. Habt ihr die Erfahrung auch gemacht in eurem beruflichen Kontext? Ich habe ja mit 100 geantwortet, also insofern ja. Ich glaube, es ist ein großes Problem oder nicht ein großes Problem, aber es ist ein Problem, was ich auch sehe. Es ist halt quasi, dass halt einfach du der Sache blind folgst und das nicht quasi einhornen kannst. Die Frage ist halt – das hatte ich auch geschrieben – so what? Das fügt ja trotzdem jetzt keinen Weg dran vorbei. Man sollte sich trotzdem mit dem Thema auseinandersetzen. Das ist für mich immer so eine Frage nach eines bewussten Handelns. So würde ich es wahrscheinlich einordnen. Ich muss mir dessen bewusst sein oder ich muss den Leuten bewusst machen, den Kollegen bewusst machen, dass es da etwas gibt. Aber dass man halt auch quasi mit einem gewissen – wie soll ich sagen – einfach nicht blind einsetzt. Ich glaube auch, dass der Christian hat darauf geantwortet und würde ich auch zustimmen. Man muss trotzdem die Grundlagen halt irgendwie beherrschen. Solange wir in einem Szenario sind, wo wir auch eher das in einem Co-Pilot oder auch irgendwann mal in einem Pilotenmodus haben, muss man trotzdem verstehen, was da passiert. Ich glaube spannend wird dann erst quasi so dieses Szenario, was Stefan ganz am Anfang aufgemacht hat, quasi Software verschwingelt. Und da gibt es noch ein paar Episoden vorher. Das Thema mit den Grundlagen ist aus meiner Sicht ganz wichtig, weil wenn ich die Grundlagen verstanden habe, dann kann ich GenAI sehr gut verwenden, um mir neue Sachen beizubringen. Also ich habe jetzt Python verwendet, weil ich sage, GenAI kann am besten Python. Und deswegen habe ich in meinem Zeitprojekt eben Python verwendet, obwohl ich überhaupt keine Ahnung davon hatte. Aber ich habe mir einiges erklären lassen. Die Maschine weiß, dass ich eigentlich aus der Java-Welt komme und erklärt mir dann, wie eben Vererbung oder Klassen oder Entkapselierung eben in Python funktionieren. Und wow, es hilft mir. Aber die Grundlagen muss ich wissen. Lustiger Kommentar im Chat von HappyTree. KI macht dumme dümmer und schlau schlauer wie das Fernsehen. Deswegen musste ich lachen. Wir haben schon leicht überzogen. Hat noch jemand von euch ein Schlusswort, was er noch loswerden möchte? Also Eberhard sehen wir jetzt nicht, ob er reden möchte, aber ich glaube, wir würden ihn hören, wenn er es wollte. Stefan hatte länger nichts gesagt. Möchtest du noch was loswerden, was dir auf der Zunge brennt? Ja, ich würde noch zwei Sachen kurz loswerden. Das eine ist die Diskussion um Taschenrechner ab der ersten Klasse. Das ist so die gleiche Diskussion. Und das andere ist Punkt, den ich loswerden will. Alle Engineering Manager, deren Entwickler heute nicht State of the Art AI und Gen AI benutzen, würde ich gleich am Montag irgendwie alle einladen, Pizza kaufen und zeigen, was geht. Also das wäre schon mein dringender Appell. Ich hoffe, Montag sind nicht so viele Menschen im Büro. Vielleicht lieber nächstes Jahr. Am ersten Montag an dem Leute wieder überhaupt im Büro. Ich weiß ja nicht, ob jemand ins Büro geht oder so. Dann würde ich sagen, wir haben jetzt hier eine Stunde spannende KI-Diskussion gehabt. Ich bin immer noch fasziniert, dass es im Chat auch die ganze Zeit so gut abging und dass ihr Ralf und André auch noch nebenbei im Chat dabei wart. Respekt, dass ihr zwei Diskussionen verfolgt habt oder sogar drei. Ich bin sehr dankbar, dass ich heute dabei sein durfte und das moderieren durfte. Ich weiß, dass Ralf gleich noch ankündigen will, womit es weitergeht im nächsten Jahr. Erstmal aber danke, André, dass du da warst und danke, Stefan, dass du dabei warst. Danke euch allen im Chat, dass ihr dabei wart. Ralf, wie startet Software Architektur im Stream im neuen Jahr? Ja, wir werden den als ersten Gast den Christian Weiher haben und da wird es wieder um KI in der Software Architektur gehen. Das wird am 9.1. sein. Mit der Uhrzeit sind wir noch ein bisschen am Gucken, was da am besten passt, aber es wird auf jeden Fall spannend. Schaltet wieder ein. Genau. Ich packe den Chat auf jeden Fall nochmal zum Download in dieses Internet, also auf diese Webseite, die wir haben, dass man das nachlesen kann. Super, gute Idee. Ja, dann danke euch allen, danke euch ZuschauerInnen und schöne Feiertage, erholsame Feiertage und einen guten Rutsch ins neue Jahr. Genau, guten Rutsch.",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 6.480000019073486,
      "text": " Hallo, ich bin Eberhard Wolff. Freitags mache ich oder Lisa Moritz einen Livestream zum Thema Software-Architektur, oft zusammen mit Gästen.",
      "tokens": [
        50364,
        21242,
        11,
        1893,
        5171,
        462,
        607,
        21491,
        19925,
        602,
        13,
        6142,
        270,
        12109,
        28289,
        1893,
        4513,
        12252,
        5146,
        6862,
        4891,
        31738,
        377,
        1572,
        5919,
        16306,
        27428,
        12,
        10683,
        339,
        642,
        2320,
        374,
        11,
        11649,
        14311,
        2194,
        460,
        737,
        6266,
        13,
        50688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28723767399787903,
      "compression_ratio": 1.3517588376998901,
      "no_speech_prob": 0.04219088330864906
    },
    {
      "id": 1,
      "seek": 0,
      "start": 6.480000019073486,
      "end": 15.119999885559082,
      "text": " Dieser Podcast ist das Audio des Streams. Weitere Folgen, Sketchnotes und vieles mehr findet ihr unter software-architektur.tv.",
      "tokens": [
        50688,
        39609,
        29972,
        1418,
        1482,
        25706,
        730,
        24904,
        82,
        13,
        492,
        270,
        323,
        15255,
        1766,
        11,
        49245,
        2247,
        279,
        674,
        5891,
        279,
        5417,
        27752,
        5553,
        8662,
        4722,
        12,
        1178,
        642,
        2320,
        374,
        13,
        24641,
        13,
        51120
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28723767399787903,
      "compression_ratio": 1.3517588376998901,
      "no_speech_prob": 0.04219088330864906
    },
    {
      "id": 2,
      "seek": 1512,
      "start": 15.119999885559082,
      "end": 27.600000381469727,
      "text": " Hallo und herzlich willkommen zu einer neuen Episode von Software-Architektur im Stream.",
      "tokens": [
        50364,
        21242,
        674,
        45919,
        46439,
        2164,
        6850,
        21387,
        19882,
        2957,
        27428,
        12,
        10683,
        339,
        642,
        2320,
        374,
        566,
        24904,
        13,
        50988
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34871166944503784,
      "compression_ratio": 1.4128440618515015,
      "no_speech_prob": 0.2669925093650818
    },
    {
      "id": 3,
      "seek": 1512,
      "start": 27.600000381469727,
      "end": 35.599998474121094,
      "text": " Heute seht ihr schon, dass eine ganze Bande an Leuten hier und wir besprechen das Thema KI in der Softwareentwicklung über oder unter Hyped.",
      "tokens": [
        50988,
        27978,
        369,
        357,
        5553,
        4981,
        11,
        2658,
        3018,
        18898,
        363,
        11123,
        364,
        42301,
        3296,
        674,
        1987,
        4097,
        38951,
        1482,
        16306,
        47261,
        294,
        1163,
        27428,
        317,
        16038,
        17850,
        4502,
        4513,
        8662,
        5701,
        3452,
        13,
        51388
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34871166944503784,
      "compression_ratio": 1.4128440618515015,
      "no_speech_prob": 0.2669925093650818
    },
    {
      "id": 4,
      "seek": 1512,
      "start": 35.599998474121094,
      "end": 39.599998474121094,
      "text": " Ich sage schon mal Hallo André, Hallo Stefan, Hallo Ralf und Hallo Eberhard.",
      "tokens": [
        51388,
        3141,
        19721,
        4981,
        2806,
        21242,
        400,
        10521,
        11,
        21242,
        32158,
        11,
        21242,
        497,
        1678,
        674,
        21242,
        462,
        607,
        21491,
        13,
        51588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34871166944503784,
      "compression_ratio": 1.4128440618515015,
      "no_speech_prob": 0.2669925093650818
    },
    {
      "id": 5,
      "seek": 3960,
      "start": 40.08000183105469,
      "end": 45.08000183105469,
      "text": " Ihr fragt euch vielleicht, wie es überhaupt zu dieser Zusammenstellung kommt und zu dieser Episode.",
      "tokens": [
        50388,
        14773,
        9241,
        83,
        10403,
        12547,
        11,
        3355,
        785,
        20023,
        2164,
        9053,
        29442,
        30016,
        10047,
        674,
        2164,
        9053,
        19882,
        13,
        50638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2589505612850189,
      "compression_ratio": 1.5704917907714844,
      "no_speech_prob": 0.25017040967941284
    },
    {
      "id": 6,
      "seek": 3960,
      "start": 45.08000183105469,
      "end": 49.599998474121094,
      "text": " Eberhard hatte einen Post bei Heise veröffentlicht. Ich hoffe, ich erzähle es jetzt auch richtig rum.",
      "tokens": [
        50638,
        462,
        607,
        21491,
        13299,
        4891,
        10223,
        4643,
        634,
        908,
        1306,
        30453,
        317,
        20238,
        13,
        3141,
        34903,
        11,
        1893,
        28337,
        306,
        785,
        4354,
        2168,
        13129,
        8347,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2589505612850189,
      "compression_ratio": 1.5704917907714844,
      "no_speech_prob": 0.25017040967941284
    },
    {
      "id": 7,
      "seek": 3960,
      "start": 49.599998474121094,
      "end": 58.08000183105469,
      "text": " Diesen Post hat er irgendwann ins Englische übersetzt. Dann gab es dazu einen LinkedIn-Post und dieser LinkedIn-Post ist mit ganz vielen Kommentaren überhäuft worden.",
      "tokens": [
        50864,
        10796,
        268,
        10223,
        2385,
        1189,
        34313,
        1028,
        2469,
        75,
        7864,
        45022,
        3524,
        13,
        7455,
        17964,
        785,
        13034,
        4891,
        20657,
        12,
        47,
        555,
        674,
        9053,
        20657,
        12,
        47,
        555,
        1418,
        2194,
        6312,
        19885,
        33708,
        4484,
        4502,
        71,
        737,
        25005,
        14054,
        13,
        51288
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2589505612850189,
      "compression_ratio": 1.5704917907714844,
      "no_speech_prob": 0.25017040967941284
    },
    {
      "id": 8,
      "seek": 3960,
      "start": 58.08000183105469,
      "end": 65.5999984741211,
      "text": " Und Eberhard hat dann beschlossen, dass es hier diese Folge geben soll mit eben André, Stefan und Ralf.",
      "tokens": [
        51288,
        2719,
        462,
        607,
        21491,
        2385,
        3594,
        4097,
        29002,
        11,
        2658,
        785,
        3296,
        6705,
        43597,
        17191,
        7114,
        2194,
        11375,
        400,
        10521,
        11,
        32158,
        674,
        497,
        1678,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2589505612850189,
      "compression_ratio": 1.5704917907714844,
      "no_speech_prob": 0.25017040967941284
    },
    {
      "id": 9,
      "seek": 6560,
      "start": 65.5999984741211,
      "end": 74.5999984741211,
      "text": " Genau. Bevor wir so richtig ins Thema anfangen, würde ich sagen, dass ihr beiden André und Stefan euch einmal kurz vorstellt, wer ihr seid, was ihr so macht.",
      "tokens": [
        50364,
        22340,
        13,
        879,
        8453,
        1987,
        370,
        13129,
        1028,
        16306,
        33709,
        10784,
        11,
        11942,
        1893,
        8360,
        11,
        2658,
        5553,
        23446,
        400,
        10521,
        674,
        32158,
        10403,
        11078,
        20465,
        4245,
        372,
        12783,
        11,
        2612,
        5553,
        38041,
        11,
        390,
        5553,
        370,
        10857,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2626489996910095,
      "compression_ratio": 1.468468427658081,
      "no_speech_prob": 0.02225269377231598
    },
    {
      "id": 10,
      "seek": 6560,
      "start": 74.5999984741211,
      "end": 77.5999984741211,
      "text": " Stefan, leg doch mal los.",
      "tokens": [
        50814,
        32158,
        11,
        1676,
        9243,
        2806,
        1750,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2626489996910095,
      "compression_ratio": 1.468468427658081,
      "no_speech_prob": 0.02225269377231598
    },
    {
      "id": 11,
      "seek": 6560,
      "start": 77.5999984741211,
      "end": 86.5999984741211,
      "text": " Ja, Stefan. Coder by heart, sage ich immer. Ich habe vor 40 Jahren mir Programmieren beigebracht, weil ich Videospiele programmieren wollte.",
      "tokens": [
        50964,
        3530,
        11,
        32158,
        13,
        383,
        19866,
        538,
        1917,
        11,
        19721,
        1893,
        5578,
        13,
        3141,
        6015,
        4245,
        3356,
        13080,
        3149,
        48244,
        5695,
        40274,
        23404,
        11,
        7689,
        1893,
        7926,
        2763,
        15949,
        37648,
        5695,
        24509,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2626489996910095,
      "compression_ratio": 1.468468427658081,
      "no_speech_prob": 0.02225269377231598
    },
    {
      "id": 12,
      "seek": 8660,
      "start": 86.5999984741211,
      "end": 94.5999984741211,
      "text": " Hab dann Videospiele programmiert. Programmier seitdem. Manchmal hauptberuflich, manchmal nebenberuflich.",
      "tokens": [
        50364,
        14225,
        3594,
        7926,
        2763,
        15949,
        37648,
        4859,
        13,
        48244,
        811,
        16452,
        10730,
        13,
        2458,
        25751,
        324,
        84,
        662,
        607,
        2947,
        1739,
        11,
        32092,
        36098,
        607,
        2947,
        1739,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22252002358436584,
      "compression_ratio": 1.431952714920044,
      "no_speech_prob": 0.28736287355422974
    },
    {
      "id": 13,
      "seek": 8660,
      "start": 94.5999984741211,
      "end": 101.5999984741211,
      "text": " War lange Jahre Engineering Manager, war CTO und bin seit ein paar Jahren CTO-Coach, hauptberuflich.",
      "tokens": [
        50764,
        3630,
        18131,
        15557,
        16215,
        13821,
        11,
        1516,
        383,
        15427,
        674,
        5171,
        16452,
        1343,
        16509,
        13080,
        383,
        15427,
        12,
        21141,
        608,
        11,
        324,
        84,
        662,
        607,
        2947,
        1739,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22252002358436584,
      "compression_ratio": 1.431952714920044,
      "no_speech_prob": 0.28736287355422974
    },
    {
      "id": 14,
      "seek": 8660,
      "start": 101.5999984741211,
      "end": 104.5999984741211,
      "text": " Sehr gut. André, machst du weiter?",
      "tokens": [
        51114,
        32028,
        5228,
        13,
        400,
        10521,
        11,
        43350,
        1581,
        8988,
        30,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22252002358436584,
      "compression_ratio": 1.431952714920044,
      "no_speech_prob": 0.28736287355422974
    },
    {
      "id": 15,
      "seek": 10460,
      "start": 104.5999984741211,
      "end": 112.5999984741211,
      "text": " Na klar, gerne. Erstmal danke für die Einladung. Ich war auch schon mal hier. Das ist schon ein paar Tage her, aber ich mache es noch mal kurz.",
      "tokens": [
        50364,
        6056,
        14743,
        11,
        15689,
        13,
        31183,
        5579,
        46434,
        2959,
        978,
        6391,
        9290,
        1063,
        13,
        3141,
        1516,
        2168,
        4981,
        2806,
        3296,
        13,
        2846,
        1418,
        4981,
        1343,
        16509,
        29724,
        720,
        11,
        4340,
        1893,
        28289,
        785,
        3514,
        2806,
        20465,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.293938547372818,
      "compression_ratio": 1.4501992464065552,
      "no_speech_prob": 0.17040663957595825
    },
    {
      "id": 16,
      "seek": 10460,
      "start": 112.5999984741211,
      "end": 121.5999984741211,
      "text": " Also André macht das so ungefähr jetzt gute 20 Jahre. Ich würde sagen, nicht Coder by heart, aber Techie by heart.",
      "tokens": [
        50764,
        2743,
        400,
        10521,
        10857,
        1482,
        370,
        41285,
        4354,
        21476,
        945,
        15557,
        13,
        3141,
        11942,
        8360,
        11,
        1979,
        383,
        19866,
        538,
        1917,
        11,
        4340,
        13795,
        414,
        538,
        1917,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.293938547372818,
      "compression_ratio": 1.4501992464065552,
      "no_speech_prob": 0.17040663957595825
    },
    {
      "id": 17,
      "seek": 10460,
      "start": 121.5999984741211,
      "end": 127.5999984741211,
      "text": " Mich interessieren vor allem disruptive Technologien und wie man das auch in die Organisation bringt.",
      "tokens": [
        51214,
        3392,
        12478,
        5695,
        4245,
        17585,
        37865,
        8337,
        1132,
        1053,
        674,
        3355,
        587,
        1482,
        2168,
        294,
        978,
        49425,
        36008,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.293938547372818,
      "compression_ratio": 1.4501992464065552,
      "no_speech_prob": 0.17040663957595825
    },
    {
      "id": 18,
      "seek": 12760,
      "start": 128.60000610351562,
      "end": 136.60000610351562,
      "text": " Ich war viele Jahre lang quasi so Individual Contributor, also Software-Ingenieur und habe mich dann irgendwann für die Management-Seite entschieden.",
      "tokens": [
        50414,
        3141,
        1516,
        9693,
        15557,
        2265,
        20954,
        370,
        37292,
        4839,
        2024,
        22163,
        11,
        611,
        27428,
        12,
        40,
        872,
        268,
        20679,
        674,
        6015,
        6031,
        3594,
        34313,
        2959,
        978,
        14781,
        12,
        10637,
        642,
        49807,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31580814719200134,
      "compression_ratio": 1.4887892007827759,
      "no_speech_prob": 0.23896171152591705
    },
    {
      "id": 19,
      "seek": 12760,
      "start": 136.60000610351562,
      "end": 142.60000610351562,
      "text": " Und mache das jetzt so die letzten 10, 15 Jahre für verschiedene Unternehmen, verschiedenen Rollen.",
      "tokens": [
        50814,
        2719,
        28289,
        1482,
        4354,
        370,
        978,
        18226,
        1266,
        11,
        2119,
        15557,
        2959,
        35411,
        27577,
        11,
        41043,
        9926,
        268,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31580814719200134,
      "compression_ratio": 1.4887892007827759,
      "no_speech_prob": 0.23896171152591705
    },
    {
      "id": 20,
      "seek": 12760,
      "start": 142.60000610351562,
      "end": 150.60000610351562,
      "text": " Und versuche dann halt eben die Technologien halt auch ins Operative zu kriegen.",
      "tokens": [
        51114,
        2719,
        1774,
        17545,
        3594,
        12479,
        11375,
        978,
        8337,
        1132,
        1053,
        12479,
        2168,
        1028,
        12480,
        1166,
        2164,
        46882,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31580814719200134,
      "compression_ratio": 1.4887892007827759,
      "no_speech_prob": 0.23896171152591705
    },
    {
      "id": 21,
      "seek": 15060,
      "start": 150.60000610351562,
      "end": 157.60000610351562,
      "text": " Sehr gut. Genau. Wir haben schon gesagt, KI in der Softwareentwicklung über- oder unterhyped ist quasi das Grundthema.",
      "tokens": [
        50364,
        32028,
        5228,
        13,
        22340,
        13,
        4347,
        3084,
        4981,
        12260,
        11,
        47261,
        294,
        1163,
        27428,
        317,
        16038,
        17850,
        4502,
        12,
        4513,
        8662,
        3495,
        3452,
        1418,
        20954,
        1482,
        13941,
        392,
        5619,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20399878919124603,
      "compression_ratio": 1.5533596277236938,
      "no_speech_prob": 0.06937413662672043
    },
    {
      "id": 22,
      "seek": 15060,
      "start": 157.60000610351562,
      "end": 160.60000610351562,
      "text": " Und ihr alle vier hattet dazu eine These.",
      "tokens": [
        50714,
        2719,
        5553,
        5430,
        17634,
        276,
        1591,
        302,
        13034,
        3018,
        1981,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20399878919124603,
      "compression_ratio": 1.5533596277236938,
      "no_speech_prob": 0.06937413662672043
    },
    {
      "id": 23,
      "seek": 15060,
      "start": 160.60000610351562,
      "end": 174.60000610351562,
      "text": " Und um es halbwegs fair zu machen, würde ich einfach mal alphabetisch nach Vornamen vorgehen und euch bitten zu sagen, was eure Grundthese zu diesem Statement über- oder unterhyped von KI ist.",
      "tokens": [
        50864,
        2719,
        1105,
        785,
        7523,
        65,
        32793,
        3143,
        2164,
        7069,
        11,
        11942,
        1893,
        7281,
        2806,
        23339,
        5494,
        5168,
        691,
        1865,
        22403,
        4245,
        24985,
        674,
        10403,
        34608,
        2164,
        8360,
        11,
        390,
        32845,
        13941,
        42678,
        2164,
        10975,
        16249,
        1712,
        4502,
        12,
        4513,
        8662,
        3495,
        3452,
        2957,
        47261,
        1418,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20399878919124603,
      "compression_ratio": 1.5533596277236938,
      "no_speech_prob": 0.06937413662672043
    },
    {
      "id": 24,
      "seek": 15060,
      "start": 174.60000610351562,
      "end": 176.60000610351562,
      "text": " André, fang doch gerne noch mal an.",
      "tokens": [
        51564,
        400,
        10521,
        11,
        283,
        656,
        9243,
        15689,
        3514,
        2806,
        364,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20399878919124603,
      "compression_ratio": 1.5533596277236938,
      "no_speech_prob": 0.06937413662672043
    },
    {
      "id": 25,
      "seek": 17660,
      "start": 176.60000610351562,
      "end": 180.60000610351562,
      "text": " Ja, also meine These ist quasi underhyped.",
      "tokens": [
        50364,
        3530,
        11,
        611,
        10946,
        1981,
        1418,
        20954,
        833,
        3495,
        3452,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1943109631538391,
      "compression_ratio": 1.383720874786377,
      "no_speech_prob": 0.3494703471660614
    },
    {
      "id": 26,
      "seek": 17660,
      "start": 180.60000610351562,
      "end": 186.60000610351562,
      "text": " Also wir haben noch nicht verstanden, was damit möglich ist.",
      "tokens": [
        50564,
        2743,
        1987,
        3084,
        3514,
        1979,
        1306,
        33946,
        11,
        390,
        9479,
        16294,
        1418,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1943109631538391,
      "compression_ratio": 1.383720874786377,
      "no_speech_prob": 0.3494703471660614
    },
    {
      "id": 27,
      "seek": 17660,
      "start": 186.60000610351562,
      "end": 195.60000610351562,
      "text": " Ich glaube, wir befinden uns mit einer wahnsinnigen Geschwindigkeit in der Entwicklung, wo das Ende einfach noch nicht abzusehen ist.",
      "tokens": [
        50864,
        3141,
        13756,
        11,
        1987,
        21312,
        10291,
        2693,
        2194,
        6850,
        31979,
        46134,
        3213,
        14241,
        12199,
        16626,
        294,
        1163,
        39654,
        11,
        6020,
        1482,
        15152,
        7281,
        3514,
        1979,
        410,
        89,
        438,
        2932,
        1418,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1943109631538391,
      "compression_ratio": 1.383720874786377,
      "no_speech_prob": 0.3494703471660614
    },
    {
      "id": 28,
      "seek": 19560,
      "start": 195.60000610351562,
      "end": 204.60000610351562,
      "text": " Und deswegen einfach zu sagen, es ist underhyped, glaube ich, ist quasi für den Moment, max. situativ betrachtet, richtig sein.",
      "tokens": [
        50364,
        2719,
        26482,
        7281,
        2164,
        8360,
        11,
        785,
        1418,
        833,
        3495,
        3452,
        11,
        13756,
        1893,
        11,
        1418,
        20954,
        2959,
        1441,
        19093,
        11,
        11469,
        13,
        2054,
        10662,
        778,
        81,
        48833,
        11,
        13129,
        6195,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2549665570259094,
      "compression_ratio": 1.6194690465927124,
      "no_speech_prob": 0.6427402496337891
    },
    {
      "id": 29,
      "seek": 19560,
      "start": 204.60000610351562,
      "end": 208.60000610351562,
      "text": " Ist, glaube ich, aber für das, was da kommen mag, glaube ich, falsch.",
      "tokens": [
        50814,
        12810,
        11,
        13756,
        1893,
        11,
        4340,
        2959,
        1482,
        11,
        390,
        1120,
        11729,
        2258,
        11,
        13756,
        1893,
        11,
        43340,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2549665570259094,
      "compression_ratio": 1.6194690465927124,
      "no_speech_prob": 0.6427402496337891
    },
    {
      "id": 30,
      "seek": 19560,
      "start": 208.60000610351562,
      "end": 212.60000610351562,
      "text": " Oder zumindestens kann man das in Frage stellen.",
      "tokens": [
        51014,
        20988,
        38082,
        694,
        4028,
        587,
        1482,
        294,
        13685,
        24407,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2549665570259094,
      "compression_ratio": 1.6194690465927124,
      "no_speech_prob": 0.6427402496337891
    },
    {
      "id": 31,
      "seek": 19560,
      "start": 212.60000610351562,
      "end": 219.60000610351562,
      "text": " Ich würde eine Sache hinzufügen, es ist underhyped. Aus meiner Sicht ist es aber auch kompliziert, es einzuführen.",
      "tokens": [
        51214,
        3141,
        11942,
        3018,
        31452,
        14102,
        39467,
        45336,
        11,
        785,
        1418,
        833,
        3495,
        3452,
        13,
        9039,
        20529,
        36615,
        1418,
        785,
        4340,
        2168,
        24526,
        43590,
        11,
        785,
        21586,
        2947,
        29540,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2549665570259094,
      "compression_ratio": 1.6194690465927124,
      "no_speech_prob": 0.6427402496337891
    },
    {
      "id": 32,
      "seek": 21960,
      "start": 219.60000610351562,
      "end": 231.60000610351562,
      "text": " Wenn ich ein bisschen aus meiner täglichen Arbeit berichte, dann ist es, wie sagt man so schön, Change ist toll, solange es nicht selbst einen betrifft.",
      "tokens": [
        50364,
        7899,
        1893,
        1343,
        10763,
        3437,
        20529,
        14619,
        8856,
        268,
        18604,
        5948,
        18972,
        11,
        3594,
        1418,
        785,
        11,
        3355,
        15764,
        587,
        370,
        13527,
        11,
        15060,
        1418,
        16629,
        11,
        1404,
        933,
        785,
        1979,
        13053,
        4891,
        778,
        22575,
        844,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2526843845844269,
      "compression_ratio": 1.4693877696990967,
      "no_speech_prob": 0.6901976466178894
    },
    {
      "id": 33,
      "seek": 21960,
      "start": 231.60000610351562,
      "end": 239.60000610351562,
      "text": " Und dieser Change, den wir hier vor uns haben, der ist allumfassend.",
      "tokens": [
        50964,
        2719,
        9053,
        15060,
        11,
        1441,
        1987,
        3296,
        4245,
        2693,
        3084,
        11,
        1163,
        1418,
        439,
        449,
        69,
        640,
        521,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2526843845844269,
      "compression_ratio": 1.4693877696990967,
      "no_speech_prob": 0.6901976466178894
    },
    {
      "id": 34,
      "seek": 21960,
      "start": 239.60000610351562,
      "end": 243.60000610351562,
      "text": " Wir reden nicht davon, dass wir jetzt jeden Tag deployen wollen.",
      "tokens": [
        51364,
        4347,
        26447,
        1979,
        18574,
        11,
        2658,
        1987,
        4354,
        12906,
        11204,
        7274,
        268,
        11253,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2526843845844269,
      "compression_ratio": 1.4693877696990967,
      "no_speech_prob": 0.6901976466178894
    },
    {
      "id": 35,
      "seek": 24360,
      "start": 243.60000610351562,
      "end": 250.60000610351562,
      "text": " Wir reden nicht von automatisierten Tests, sondern wir reden davon, dass grundlegend die Art und Weise, wie wir Software entwickeln, sich verändert.",
      "tokens": [
        50364,
        4347,
        26447,
        1979,
        2957,
        28034,
        271,
        29632,
        314,
        4409,
        11,
        11465,
        1987,
        26447,
        18574,
        11,
        2658,
        30886,
        6363,
        521,
        978,
        5735,
        674,
        41947,
        11,
        3355,
        1987,
        27428,
        28449,
        32099,
        11,
        3041,
        45990,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18106240034103394,
      "compression_ratio": 1.5750916004180908,
      "no_speech_prob": 0.4797040522098541
    },
    {
      "id": 36,
      "seek": 24360,
      "start": 250.60000610351562,
      "end": 256.6000061035156,
      "text": " Und das sägt im Zweifelsfall auch an dem eigenen Stuhl.",
      "tokens": [
        50714,
        2719,
        1482,
        15316,
        10463,
        566,
        32475,
        351,
        1625,
        6691,
        2168,
        364,
        1371,
        28702,
        745,
        3232,
        75,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18106240034103394,
      "compression_ratio": 1.5750916004180908,
      "no_speech_prob": 0.4797040522098541
    },
    {
      "id": 37,
      "seek": 24360,
      "start": 256.6000061035156,
      "end": 262.6000061035156,
      "text": " Und sich da dann quasi damit auseinanderzusetzen, glaube ich, ist verdammt, verdammt schwer.",
      "tokens": [
        51014,
        2719,
        3041,
        1120,
        3594,
        20954,
        9479,
        257,
        438,
        20553,
        16236,
        24797,
        11,
        13756,
        1893,
        11,
        1418,
        6387,
        5136,
        83,
        11,
        6387,
        5136,
        83,
        23809,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18106240034103394,
      "compression_ratio": 1.5750916004180908,
      "no_speech_prob": 0.4797040522098541
    },
    {
      "id": 38,
      "seek": 24360,
      "start": 262.6000061035156,
      "end": 270.6000061035156,
      "text": " Das wäre so eine These, also quasi warum es schwer ist, das in Unternehmen zu etablieren, obwohl es eigentlich sehr powerful ist.",
      "tokens": [
        51314,
        2846,
        14558,
        370,
        3018,
        1981,
        11,
        611,
        20954,
        24331,
        785,
        23809,
        1418,
        11,
        1482,
        294,
        27577,
        2164,
        1030,
        455,
        2753,
        268,
        11,
        48428,
        785,
        10926,
        5499,
        4005,
        1418,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18106240034103394,
      "compression_ratio": 1.5750916004180908,
      "no_speech_prob": 0.4797040522098541
    },
    {
      "id": 39,
      "seek": 27060,
      "start": 270.6000061035156,
      "end": 278.6000061035156,
      "text": " Lass mich da auch nochmal vielleicht auf die Mächtigkeit eingehen, weil das, was wir sehen, ist eine rasende Geschwindigkeit.",
      "tokens": [
        50364,
        441,
        640,
        6031,
        1120,
        2168,
        26509,
        12547,
        2501,
        978,
        376,
        737,
        4701,
        16626,
        30061,
        2932,
        11,
        7689,
        1482,
        11,
        390,
        1987,
        11333,
        11,
        1418,
        3018,
        26815,
        5445,
        14241,
        12199,
        16626,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21428395807743073,
      "compression_ratio": 1.5735849142074585,
      "no_speech_prob": 0.5603877305984497
    },
    {
      "id": 40,
      "seek": 27060,
      "start": 278.6000061035156,
      "end": 283.6000061035156,
      "text": " Also getrieben meiner Meinung nach im Endeffekt aus drei Dimensionen.",
      "tokens": [
        50764,
        2743,
        483,
        24027,
        20529,
        36519,
        5168,
        566,
        15152,
        602,
        8192,
        3437,
        16809,
        20975,
        3378,
        268,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21428395807743073,
      "compression_ratio": 1.5735849142074585,
      "no_speech_prob": 0.5603877305984497
    },
    {
      "id": 41,
      "seek": 27060,
      "start": 283.6000061035156,
      "end": 288.6000061035156,
      "text": " Das ist einmal die Hardware, die immer effizienter wird und auch von oben nach unten durchläuft.",
      "tokens": [
        51014,
        2846,
        1418,
        11078,
        978,
        11817,
        3039,
        11,
        978,
        5578,
        1244,
        590,
        1196,
        260,
        4578,
        674,
        2168,
        2957,
        21279,
        5168,
        25693,
        7131,
        22882,
        25005,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21428395807743073,
      "compression_ratio": 1.5735849142074585,
      "no_speech_prob": 0.5603877305984497
    },
    {
      "id": 42,
      "seek": 27060,
      "start": 288.6000061035156,
      "end": 294.6000061035156,
      "text": " Also das, was quasi zuletzt bloß irgendwo im Rechenzentrum möglich ist, ist dann demnächst irgendwo eine Edge möglich.",
      "tokens": [
        51264,
        2743,
        1482,
        11,
        390,
        20954,
        43238,
        3524,
        1749,
        2536,
        40865,
        566,
        1300,
        2470,
        14185,
        6247,
        16294,
        1418,
        11,
        1418,
        3594,
        1371,
        77,
        28679,
        40865,
        3018,
        19328,
        16294,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21428395807743073,
      "compression_ratio": 1.5735849142074585,
      "no_speech_prob": 0.5603877305984497
    },
    {
      "id": 43,
      "seek": 29460,
      "start": 294.6000061035156,
      "end": 300.6000061035156,
      "text": " Also auf meinem Laptop. Also ich kann ohne Probleme ein LLM auf meinem Laptop laufen lassen.",
      "tokens": [
        50364,
        2743,
        2501,
        24171,
        441,
        2796,
        404,
        13,
        2743,
        1893,
        4028,
        15716,
        32891,
        1343,
        441,
        43,
        44,
        2501,
        24171,
        441,
        2796,
        404,
        41647,
        16168,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22291168570518494,
      "compression_ratio": 1.512930989265442,
      "no_speech_prob": 0.22498080134391785
    },
    {
      "id": 44,
      "seek": 29460,
      "start": 300.6000061035156,
      "end": 306.6000061035156,
      "text": " Das Zweite ist halt LLMs, die Größe und die Fähigkeiten, die diese LLMs haben.",
      "tokens": [
        50664,
        2846,
        32475,
        642,
        1418,
        12479,
        441,
        43,
        26386,
        11,
        978,
        45778,
        11451,
        674,
        978,
        479,
        6860,
        37545,
        11,
        978,
        6705,
        441,
        43,
        26386,
        3084,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22291168570518494,
      "compression_ratio": 1.512930989265442,
      "no_speech_prob": 0.22498080134391785
    },
    {
      "id": 45,
      "seek": 29460,
      "start": 306.6000061035156,
      "end": 310.6000061035156,
      "text": " Also angefangen Parameter auf der einen Seite, aber auch Kontext Windows.",
      "tokens": [
        50964,
        2743,
        43907,
        10784,
        34882,
        2398,
        2501,
        1163,
        4891,
        19748,
        11,
        4340,
        2168,
        20629,
        3828,
        8591,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22291168570518494,
      "compression_ratio": 1.512930989265442,
      "no_speech_prob": 0.22498080134391785
    },
    {
      "id": 46,
      "seek": 29460,
      "start": 310.6000061035156,
      "end": 317.6000061035156,
      "text": " Am Anfang konnte man halt irgendwie eine Klasse da reinladen und sagen oder ein File ergänzt das mal.",
      "tokens": [
        51164,
        2012,
        25856,
        24058,
        587,
        12479,
        20759,
        3018,
        591,
        40255,
        1120,
        6561,
        9290,
        268,
        674,
        8360,
        4513,
        1343,
        26196,
        26585,
        4029,
        2682,
        1482,
        2806,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22291168570518494,
      "compression_ratio": 1.512930989265442,
      "no_speech_prob": 0.22498080134391785
    },
    {
      "id": 47,
      "seek": 31760,
      "start": 317.6000061035156,
      "end": 321.6000061035156,
      "text": " Jetzt kann ich mittlerweile eine ganze Source-Code-Basis da reinpumpen.",
      "tokens": [
        50364,
        12592,
        4028,
        1893,
        41999,
        3018,
        18898,
        29629,
        12,
        34,
        1429,
        12,
        33,
        26632,
        1120,
        6561,
        79,
        1420,
        268,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27408304810523987,
      "compression_ratio": 1.6049822568893433,
      "no_speech_prob": 0.3480100929737091
    },
    {
      "id": 48,
      "seek": 31760,
      "start": 321.6000061035156,
      "end": 328.6000061035156,
      "text": " Also Global Gemini 1.5 Pro sind quasi 300.000 Token.",
      "tokens": [
        50564,
        2743,
        14465,
        22894,
        3812,
        502,
        13,
        20,
        1705,
        3290,
        20954,
        6641,
        13,
        1360,
        314,
        8406,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27408304810523987,
      "compression_ratio": 1.6049822568893433,
      "no_speech_prob": 0.3480100929737091
    },
    {
      "id": 49,
      "seek": 31760,
      "start": 328.6000061035156,
      "end": 331.6000061035156,
      "text": " Können wir gleich nochmal nachgucken. Kann ich gleich nochmal nachliefern.",
      "tokens": [
        50914,
        29077,
        2866,
        1987,
        11699,
        26509,
        5168,
        70,
        49720,
        13,
        29074,
        1893,
        11699,
        26509,
        5168,
        6302,
        28958,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27408304810523987,
      "compression_ratio": 1.6049822568893433,
      "no_speech_prob": 0.3480100929737091
    },
    {
      "id": 50,
      "seek": 31760,
      "start": 331.6000061035156,
      "end": 336.6000061035156,
      "text": " Aber die Kontext-Windows sind mittlerweile so groß, dass halt einfach große Code-Basen da reinpassen.",
      "tokens": [
        51064,
        5992,
        978,
        20629,
        3828,
        12,
        45813,
        1509,
        3290,
        41999,
        370,
        17253,
        11,
        2658,
        12479,
        7281,
        19691,
        15549,
        12,
        33,
        296,
        268,
        1120,
        6561,
        44270,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27408304810523987,
      "compression_ratio": 1.6049822568893433,
      "no_speech_prob": 0.3480100929737091
    },
    {
      "id": 51,
      "seek": 31760,
      "start": 336.6000061035156,
      "end": 338.6000061035156,
      "text": " Und das verändert halt einfach alles.",
      "tokens": [
        51314,
        2719,
        1482,
        45990,
        12479,
        7281,
        7874,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27408304810523987,
      "compression_ratio": 1.6049822568893433,
      "no_speech_prob": 0.3480100929737091
    },
    {
      "id": 52,
      "seek": 31760,
      "start": 338.6000061035156,
      "end": 341.6000061035156,
      "text": " Also deswegen kann man halt quasi Stand heute sagen.",
      "tokens": [
        51414,
        2743,
        26482,
        4028,
        587,
        12479,
        20954,
        9133,
        9801,
        8360,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27408304810523987,
      "compression_ratio": 1.6049822568893433,
      "no_speech_prob": 0.3480100929737091
    },
    {
      "id": 53,
      "seek": 31760,
      "start": 341.6000061035156,
      "end": 343.6000061035156,
      "text": " Heute ist es vielleicht noch nicht da, wo es halt ist.",
      "tokens": [
        51564,
        27978,
        1418,
        785,
        12547,
        3514,
        1979,
        1120,
        11,
        6020,
        785,
        12479,
        1418,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27408304810523987,
      "compression_ratio": 1.6049822568893433,
      "no_speech_prob": 0.3480100929737091
    },
    {
      "id": 54,
      "seek": 34360,
      "start": 343.6000061035156,
      "end": 347.6000061035156,
      "text": " Aber die Wahrscheinlichkeit, dass es sich rasend schnell entwickelt, ist halt einfach gegeben.",
      "tokens": [
        50364,
        5992,
        978,
        36357,
        25553,
        9238,
        11,
        2658,
        785,
        3041,
        26815,
        521,
        17589,
        43208,
        11,
        1418,
        12479,
        7281,
        32572,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2503664493560791,
      "compression_ratio": 1.633858323097229,
      "no_speech_prob": 0.259257972240448
    },
    {
      "id": 55,
      "seek": 34360,
      "start": 347.6000061035156,
      "end": 351.6000061035156,
      "text": " Und das sieht man unter anderem auch bei den ganzen Benchmarks.",
      "tokens": [
        50564,
        2719,
        1482,
        14289,
        587,
        8662,
        293,
        7333,
        2168,
        4643,
        1441,
        23966,
        3964,
        339,
        37307,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2503664493560791,
      "compression_ratio": 1.633858323097229,
      "no_speech_prob": 0.259257972240448
    },
    {
      "id": 56,
      "seek": 34360,
      "start": 351.6000061035156,
      "end": 357.6000061035156,
      "text": " Quasi gibt es immer wieder neue Benchmarks, weil die alten Benchmarks dann quasi gelöst wurden.",
      "tokens": [
        50764,
        2326,
        8483,
        6089,
        785,
        5578,
        6216,
        16842,
        3964,
        339,
        37307,
        11,
        7689,
        978,
        41217,
        3964,
        339,
        37307,
        3594,
        20954,
        4087,
        36995,
        21105,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2503664493560791,
      "compression_ratio": 1.633858323097229,
      "no_speech_prob": 0.259257972240448
    },
    {
      "id": 57,
      "seek": 34360,
      "start": 357.6000061035156,
      "end": 363.6000061035156,
      "text": " Und das kann man auch relativ leicht nachvollziehen, wenn man sich mal einfach so den, den ich ganz gern verfolge.",
      "tokens": [
        51064,
        2719,
        1482,
        4028,
        587,
        2168,
        21960,
        28333,
        5168,
        20654,
        28768,
        11,
        4797,
        587,
        3041,
        2806,
        7281,
        370,
        1441,
        11,
        1441,
        1893,
        6312,
        38531,
        1306,
        7082,
        432,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2503664493560791,
      "compression_ratio": 1.633858323097229,
      "no_speech_prob": 0.259257972240448
    },
    {
      "id": 58,
      "seek": 34360,
      "start": 363.6000061035156,
      "end": 367.6000061035156,
      "text": " So SWE Bench. Da gibt es so ein Leaderboard.",
      "tokens": [
        51364,
        407,
        318,
        37937,
        3964,
        339,
        13,
        3933,
        6089,
        785,
        370,
        1343,
        22650,
        3787,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2503664493560791,
      "compression_ratio": 1.633858323097229,
      "no_speech_prob": 0.259257972240448
    },
    {
      "id": 59,
      "seek": 36760,
      "start": 367.6000061035156,
      "end": 379.6000061035156,
      "text": " Und da sieht man auch, mit welcher Geschwindigkeit, also im Monatstakt quasi da immer ein neues Set an LLM und Tools quasi da in den ersten Platz belegt.",
      "tokens": [
        50364,
        2719,
        1120,
        14289,
        587,
        2168,
        11,
        2194,
        2214,
        6759,
        14241,
        12199,
        16626,
        11,
        611,
        566,
        4713,
        267,
        372,
        5886,
        20954,
        1120,
        5578,
        1343,
        43979,
        8928,
        364,
        441,
        43,
        44,
        674,
        30302,
        20954,
        1120,
        294,
        1441,
        17324,
        27595,
        312,
        22745,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22153261303901672,
      "compression_ratio": 1.529197096824646,
      "no_speech_prob": 0.30350974202156067
    },
    {
      "id": 60,
      "seek": 36760,
      "start": 379.6000061035156,
      "end": 384.6000061035156,
      "text": " Genau. Und das bringt mich letztendlich zur letzten These.",
      "tokens": [
        50964,
        22340,
        13,
        2719,
        1482,
        36008,
        6031,
        35262,
        521,
        1739,
        7147,
        18226,
        1981,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22153261303901672,
      "compression_ratio": 1.529197096824646,
      "no_speech_prob": 0.30350974202156067
    },
    {
      "id": 61,
      "seek": 36760,
      "start": 384.6000061035156,
      "end": 387.6000061035156,
      "text": " Ich glaube, wir haben noch nicht verstanden, wie man das bestmöglich nutzt.",
      "tokens": [
        51214,
        3141,
        13756,
        11,
        1987,
        3084,
        3514,
        1979,
        1306,
        33946,
        11,
        3355,
        587,
        1482,
        1151,
        76,
        16277,
        5393,
        2682,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22153261303901672,
      "compression_ratio": 1.529197096824646,
      "no_speech_prob": 0.30350974202156067
    },
    {
      "id": 62,
      "seek": 36760,
      "start": 387.6000061035156,
      "end": 392.6000061035156,
      "text": " Am Anfang gab es so ein bisschen die Idee davon, wir müssen jetzt alle Prompt-Engineer werden.",
      "tokens": [
        51364,
        2012,
        25856,
        17964,
        785,
        370,
        1343,
        10763,
        978,
        32651,
        18574,
        11,
        1987,
        9013,
        4354,
        5430,
        15833,
        662,
        12,
        31254,
        533,
        260,
        4604,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22153261303901672,
      "compression_ratio": 1.529197096824646,
      "no_speech_prob": 0.30350974202156067
    },
    {
      "id": 63,
      "seek": 36760,
      "start": 392.6000061035156,
      "end": 393.6000061035156,
      "text": " Ich glaube, da ist auch was dran.",
      "tokens": [
        51614,
        3141,
        13756,
        11,
        1120,
        1418,
        2168,
        390,
        32801,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22153261303901672,
      "compression_ratio": 1.529197096824646,
      "no_speech_prob": 0.30350974202156067
    },
    {
      "id": 64,
      "seek": 39360,
      "start": 393.6000061035156,
      "end": 395.6000061035156,
      "text": " Also quasi Shit in, Shit aus.",
      "tokens": [
        50364,
        2743,
        20954,
        19593,
        294,
        11,
        19593,
        3437,
        13,
        50464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23215657472610474,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.5292345881462097
    },
    {
      "id": 65,
      "seek": 39360,
      "start": 395.6000061035156,
      "end": 402.6000061035156,
      "text": " Wenn der Prompt nicht gut ist, dann kann das Ergebnis nicht gut sein oder es beeinflusst zumindest das Ergebnis.",
      "tokens": [
        50464,
        7899,
        1163,
        15833,
        662,
        1979,
        5228,
        1418,
        11,
        3594,
        4028,
        1482,
        46229,
        1979,
        5228,
        6195,
        4513,
        785,
        17479,
        19920,
        3063,
        372,
        38082,
        1482,
        46229,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23215657472610474,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.5292345881462097
    },
    {
      "id": 66,
      "seek": 39360,
      "start": 402.6000061035156,
      "end": 406.6000061035156,
      "text": " Jetzt wird die Idee von Agents gerade durch die Welt getragen.",
      "tokens": [
        50814,
        12592,
        4578,
        978,
        32651,
        2957,
        2725,
        791,
        12117,
        7131,
        978,
        14761,
        483,
        20663,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23215657472610474,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.5292345881462097
    },
    {
      "id": 67,
      "seek": 39360,
      "start": 406.6000061035156,
      "end": 410.6000061035156,
      "text": " Und die einfache Frage ist, was ist das Nächste?",
      "tokens": [
        51014,
        2719,
        978,
        38627,
        6000,
        13685,
        1418,
        11,
        390,
        1418,
        1482,
        426,
        10168,
        2941,
        30,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23215657472610474,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.5292345881462097
    },
    {
      "id": 68,
      "seek": 39360,
      "start": 410.6000061035156,
      "end": 419.6000061035156,
      "text": " Was ist das Nächste, was quasi uns in der Softwareentwicklung vielleicht einfach alles verändert?",
      "tokens": [
        51214,
        3027,
        1418,
        1482,
        426,
        10168,
        2941,
        11,
        390,
        20954,
        2693,
        294,
        1163,
        27428,
        317,
        16038,
        17850,
        12547,
        7281,
        7874,
        45990,
        30,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23215657472610474,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.5292345881462097
    },
    {
      "id": 69,
      "seek": 39360,
      "start": 419.6000061035156,
      "end": 420.6000061035156,
      "text": " Gewisse Teile werden bleiben.",
      "tokens": [
        51664,
        19063,
        7746,
        1989,
        794,
        4604,
        24912,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23215657472610474,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.5292345881462097
    },
    {
      "id": 70,
      "seek": 42060,
      "start": 420.6000061035156,
      "end": 426.6000061035156,
      "text": " Da glaube ich wahrscheinlich auch eine andere These als das, was vielleicht Stefan Eberhard gleich sagen.",
      "tokens": [
        50364,
        3933,
        13756,
        1893,
        30957,
        2168,
        3018,
        10490,
        1981,
        3907,
        1482,
        11,
        390,
        12547,
        32158,
        462,
        607,
        21491,
        11699,
        8360,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22130896151065826,
      "compression_ratio": 1.5625,
      "no_speech_prob": 0.2504565715789795
    },
    {
      "id": 71,
      "seek": 42060,
      "start": 426.6000061035156,
      "end": 435.6000061035156,
      "text": " Aber ich glaube, wir haben noch nicht verstanden, wozu LLMs oder quasi Generated AI, weil da zählen ja nicht nur LLMs rein, quasi in der Lage sind.",
      "tokens": [
        50664,
        5992,
        1893,
        13756,
        11,
        1987,
        3084,
        3514,
        1979,
        1306,
        33946,
        11,
        6020,
        11728,
        441,
        43,
        26386,
        4513,
        20954,
        15409,
        770,
        7318,
        11,
        7689,
        1120,
        710,
        6860,
        6698,
        2784,
        1979,
        4343,
        441,
        43,
        26386,
        6561,
        11,
        20954,
        294,
        1163,
        41555,
        3290,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22130896151065826,
      "compression_ratio": 1.5625,
      "no_speech_prob": 0.2504565715789795
    },
    {
      "id": 72,
      "seek": 42060,
      "start": 435.6000061035156,
      "end": 439.6000061035156,
      "text": " Sehr gut. Ich habe noch eine kurze Nachfrage, weil ich glaube, das ist auch noch ganz interessant.",
      "tokens": [
        51114,
        32028,
        5228,
        13,
        3141,
        6015,
        3514,
        3018,
        10072,
        1381,
        11815,
        40449,
        11,
        7689,
        1893,
        13756,
        11,
        1482,
        1418,
        2168,
        3514,
        6312,
        37748,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22130896151065826,
      "compression_ratio": 1.5625,
      "no_speech_prob": 0.2504565715789795
    },
    {
      "id": 73,
      "seek": 42060,
      "start": 439.6000061035156,
      "end": 442.6000061035156,
      "text": " Nutzt du schon KI in deiner täglichen Arbeit?",
      "tokens": [
        51314,
        19861,
        2682,
        1581,
        4981,
        47261,
        294,
        368,
        4564,
        14619,
        8856,
        268,
        18604,
        30,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22130896151065826,
      "compression_ratio": 1.5625,
      "no_speech_prob": 0.2504565715789795
    },
    {
      "id": 74,
      "seek": 44260,
      "start": 442.6000061035156,
      "end": 449.6000061035156,
      "text": " Ja. Das ist, würde ich auch sagen, für mich gar nicht mehr wegzudenken.",
      "tokens": [
        50364,
        3530,
        13,
        2846,
        1418,
        11,
        11942,
        1893,
        2168,
        8360,
        11,
        2959,
        6031,
        3691,
        1979,
        5417,
        15565,
        89,
        32940,
        2653,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2741851508617401,
      "compression_ratio": 1.552884578704834,
      "no_speech_prob": 0.5172552466392517
    },
    {
      "id": 75,
      "seek": 44260,
      "start": 449.6000061035156,
      "end": 455.6000061035156,
      "text": " Ist für mich tatsächlich ein Sparrings-Partner, viele Sachen einfach auch schnell zu erledigen.",
      "tokens": [
        50714,
        12810,
        2959,
        6031,
        20796,
        1343,
        1738,
        2284,
        1109,
        12,
        37012,
        1193,
        11,
        9693,
        26074,
        7281,
        2168,
        17589,
        2164,
        1189,
        1493,
        3213,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2741851508617401,
      "compression_ratio": 1.552884578704834,
      "no_speech_prob": 0.5172552466392517
    },
    {
      "id": 76,
      "seek": 44260,
      "start": 455.6000061035156,
      "end": 460.6000061035156,
      "text": " Also im Sinne von auch so Boring-Arbeit halt einfach zu erledigen.",
      "tokens": [
        51014,
        2743,
        566,
        47041,
        2957,
        2168,
        370,
        363,
        3662,
        12,
        10683,
        9407,
        12479,
        7281,
        2164,
        1189,
        1493,
        3213,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2741851508617401,
      "compression_ratio": 1.552884578704834,
      "no_speech_prob": 0.5172552466392517
    },
    {
      "id": 77,
      "seek": 44260,
      "start": 460.6000061035156,
      "end": 462.6000061035156,
      "text": " Und ich nutze auch verschiedene Sachen.",
      "tokens": [
        51264,
        2719,
        1893,
        5393,
        1381,
        2168,
        35411,
        26074,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2741851508617401,
      "compression_ratio": 1.552884578704834,
      "no_speech_prob": 0.5172552466392517
    },
    {
      "id": 78,
      "seek": 44260,
      "start": 462.6000061035156,
      "end": 465.6000061035156,
      "text": " Das habe ich für mich mittlerweile gelernt.",
      "tokens": [
        51364,
        2846,
        6015,
        1893,
        2959,
        6031,
        41999,
        49224,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2741851508617401,
      "compression_ratio": 1.552884578704834,
      "no_speech_prob": 0.5172552466392517
    },
    {
      "id": 79,
      "seek": 46560,
      "start": 465.6000061035156,
      "end": 472.6000061035156,
      "text": " Da gibt es nicht dieses One-Tool-to-rule-them-all, sondern kontextbezogen braucht man das eine und mal das andere.",
      "tokens": [
        50364,
        3933,
        6089,
        785,
        1979,
        12113,
        1485,
        12,
        51,
        1092,
        12,
        1353,
        12,
        44197,
        12,
        47959,
        12,
        336,
        11,
        11465,
        14373,
        3828,
        650,
        42120,
        22623,
        587,
        1482,
        3018,
        674,
        2806,
        1482,
        10490,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23554474115371704,
      "compression_ratio": 1.5524475574493408,
      "no_speech_prob": 0.5572347640991211
    },
    {
      "id": 80,
      "seek": 46560,
      "start": 472.6000061035156,
      "end": 480.6000061035156,
      "text": " Sehr cool. Genau. Ich wünsche mir gleich von euch allen das auch nochmal, wenn ihr die These gesagt habt, sagt gerne auch nochmal dabei, ob ihr schon KI in eurer täglichen Arbeit nutzt.",
      "tokens": [
        50714,
        32028,
        1627,
        13,
        22340,
        13,
        3141,
        30841,
        12287,
        3149,
        11699,
        2957,
        10403,
        18440,
        1482,
        2168,
        26509,
        11,
        4797,
        5553,
        978,
        1981,
        12260,
        23660,
        11,
        15764,
        15689,
        2168,
        26509,
        14967,
        11,
        1111,
        5553,
        4981,
        47261,
        294,
        308,
        9858,
        14619,
        8856,
        268,
        18604,
        5393,
        2682,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23554474115371704,
      "compression_ratio": 1.5524475574493408,
      "no_speech_prob": 0.5572347640991211
    },
    {
      "id": 81,
      "seek": 46560,
      "start": 480.6000061035156,
      "end": 485.6000061035156,
      "text": " Ich glaube, das ist auch nochmal spannend für die Einschätzung eurer Thesen.",
      "tokens": [
        51114,
        3141,
        13756,
        11,
        1482,
        1418,
        2168,
        26509,
        49027,
        2959,
        978,
        22790,
        339,
        3628,
        27667,
        308,
        9858,
        334,
        17403,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23554474115371704,
      "compression_ratio": 1.5524475574493408,
      "no_speech_prob": 0.5572347640991211
    },
    {
      "id": 82,
      "seek": 46560,
      "start": 485.6000061035156,
      "end": 487.6000061035156,
      "text": " Eberhard ist der Nächste im Alphabet.",
      "tokens": [
        51364,
        462,
        607,
        21491,
        1418,
        1163,
        426,
        10168,
        2941,
        566,
        967,
        20890,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23554474115371704,
      "compression_ratio": 1.5524475574493408,
      "no_speech_prob": 0.5572347640991211
    },
    {
      "id": 83,
      "seek": 46560,
      "start": 487.6000061035156,
      "end": 489.6000061035156,
      "text": " Ja, genau. Vielen Dank.",
      "tokens": [
        51464,
        3530,
        11,
        12535,
        13,
        22502,
        14148,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23554474115371704,
      "compression_ratio": 1.5524475574493408,
      "no_speech_prob": 0.5572347640991211
    },
    {
      "id": 84,
      "seek": 48960,
      "start": 489.6000061035156,
      "end": 493.6000061035156,
      "text": " Eigentlich müsste ich ja jetzt sagen, dass es overhyped ist.",
      "tokens": [
        50364,
        40561,
        7698,
        42962,
        1893,
        2784,
        4354,
        8360,
        11,
        2658,
        785,
        670,
        3495,
        3452,
        1418,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24465599656105042,
      "compression_ratio": 1.5204081535339355,
      "no_speech_prob": 0.7961936593055725
    },
    {
      "id": 85,
      "seek": 48960,
      "start": 493.6000061035156,
      "end": 496.6000061035156,
      "text": " Ich habe gestern das nochmal korrigiert.",
      "tokens": [
        50564,
        3141,
        6015,
        7219,
        1248,
        1482,
        26509,
        14784,
        7065,
        4859,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24465599656105042,
      "compression_ratio": 1.5204081535339355,
      "no_speech_prob": 0.7961936593055725
    },
    {
      "id": 86,
      "seek": 48960,
      "start": 496.6000061035156,
      "end": 510.6000061035156,
      "text": " Ich glaube, es ist ein bisschen overhyped bei Investoren und bei Management, die jetzt anfangen und sagen, das wird uns grundsätzlich zu ganz neuen Dingen führen und Sachen fundamental ändern.",
      "tokens": [
        50714,
        3141,
        13756,
        11,
        785,
        1418,
        1343,
        10763,
        670,
        3495,
        3452,
        4643,
        14008,
        10948,
        674,
        4643,
        14781,
        11,
        978,
        4354,
        33709,
        10784,
        674,
        8360,
        11,
        1482,
        4578,
        2693,
        30886,
        82,
        33373,
        2164,
        6312,
        21387,
        49351,
        35498,
        674,
        26074,
        8088,
        47775,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24465599656105042,
      "compression_ratio": 1.5204081535339355,
      "no_speech_prob": 0.7961936593055725
    },
    {
      "id": 87,
      "seek": 51060,
      "start": 510.6000061035156,
      "end": 523.5999755859375,
      "text": " Gleichzeitig ist es so, dass ich das Gefühl habe, dass Entwickler so ein bisschen müde geworden sind und jetzt eher sagen, naja, das ist keine besonders spannende Entwicklung.",
      "tokens": [
        50364,
        33858,
        32075,
        1418,
        785,
        370,
        11,
        2658,
        1893,
        1482,
        29715,
        6015,
        11,
        2658,
        29397,
        1918,
        370,
        1343,
        10763,
        6047,
        1479,
        26281,
        3290,
        674,
        4354,
        24332,
        8360,
        11,
        1667,
        2938,
        11,
        1482,
        1418,
        9252,
        25258,
        33360,
        5445,
        39654,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2231072187423706,
      "compression_ratio": 1.4393939971923828,
      "no_speech_prob": 0.660158097743988
    },
    {
      "id": 88,
      "seek": 51060,
      "start": 523.5999755859375,
      "end": 529.5999755859375,
      "text": " Ich glaube, da spielt auch die Enttäuschung von Blockchain eine Rolle und die Probleme, die da existieren.",
      "tokens": [
        51014,
        3141,
        13756,
        11,
        1120,
        39778,
        2168,
        978,
        3951,
        83,
        31611,
        339,
        1063,
        2957,
        48916,
        3018,
        35376,
        674,
        978,
        32891,
        11,
        978,
        1120,
        2514,
        5695,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2231072187423706,
      "compression_ratio": 1.4393939971923828,
      "no_speech_prob": 0.660158097743988
    },
    {
      "id": 89,
      "seek": 52960,
      "start": 530.5999755859375,
      "end": 541.5999755859375,
      "text": " Blockchain ist am Ende eine Technologie, die für eine extreme Nische ermöglicht, im Wesentlichen Spekulationen und das Bezahlen von Menschen, die in kriminellen Aktivitäten verwickelt sind.",
      "tokens": [
        50414,
        48916,
        1418,
        669,
        15152,
        3018,
        8337,
        20121,
        11,
        978,
        2959,
        3018,
        8084,
        426,
        7864,
        25253,
        50023,
        20238,
        11,
        566,
        23843,
        7698,
        268,
        3550,
        74,
        2776,
        268,
        674,
        1482,
        879,
        89,
        21128,
        2957,
        8397,
        11,
        978,
        294,
        350,
        5565,
        533,
        19191,
        32850,
        592,
        49289,
        1306,
        22295,
        25798,
        3290,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26328739523887634,
      "compression_ratio": 1.54952073097229,
      "no_speech_prob": 0.8963920474052429
    },
    {
      "id": 90,
      "seek": 52960,
      "start": 541.5999755859375,
      "end": 545.5999755859375,
      "text": " Das ist mit KI ein ganz anderes Ding.",
      "tokens": [
        50964,
        2846,
        1418,
        2194,
        47261,
        1343,
        6312,
        31426,
        20558,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26328739523887634,
      "compression_ratio": 1.54952073097229,
      "no_speech_prob": 0.8963920474052429
    },
    {
      "id": 91,
      "seek": 52960,
      "start": 545.5999755859375,
      "end": 553.5999755859375,
      "text": " Also wenn man Chatshipping ausprobiert hat und die anderen KI-Tools, wird man feststellen, dass die eben deutlich mehr können und viel helfen.",
      "tokens": [
        51164,
        2743,
        4797,
        587,
        761,
        1720,
        71,
        6297,
        3437,
        41990,
        4859,
        2385,
        674,
        978,
        11122,
        47261,
        12,
        51,
        29298,
        11,
        4578,
        587,
        6633,
        17538,
        11,
        2658,
        978,
        11375,
        24344,
        5417,
        6310,
        674,
        5891,
        29966,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26328739523887634,
      "compression_ratio": 1.54952073097229,
      "no_speech_prob": 0.8963920474052429
    },
    {
      "id": 92,
      "seek": 52960,
      "start": 553.5999755859375,
      "end": 558.5999755859375,
      "text": " Also wie André gerade eben sagte, das ist auch bei mir etwas, was in der täglichen Arbeit eine Rolle spielt.",
      "tokens": [
        51564,
        2743,
        3355,
        400,
        10521,
        12117,
        11375,
        36771,
        11,
        1482,
        1418,
        2168,
        4643,
        3149,
        9569,
        11,
        390,
        294,
        1163,
        14619,
        8856,
        268,
        18604,
        3018,
        35376,
        39778,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26328739523887634,
      "compression_ratio": 1.54952073097229,
      "no_speech_prob": 0.8963920474052429
    },
    {
      "id": 93,
      "seek": 55860,
      "start": 558.5999755859375,
      "end": 561.5999755859375,
      "text": " Ich arbeite viel mit Texten, dafür ist es super.",
      "tokens": [
        50364,
        3141,
        40476,
        642,
        5891,
        2194,
        18643,
        268,
        11,
        13747,
        1418,
        785,
        1687,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29168668389320374,
      "compression_ratio": 1.587837815284729,
      "no_speech_prob": 0.026742050424218178
    },
    {
      "id": 94,
      "seek": 55860,
      "start": 561.5999755859375,
      "end": 569.5999755859375,
      "text": " Da stelle ich mir der Schrecken fest, dass es Menschen gibt, gerade Techniker, die es noch nicht mal ernsthaft ausprobiert haben und das ist schwierig.",
      "tokens": [
        50514,
        3933,
        342,
        4434,
        1893,
        3149,
        1163,
        2065,
        265,
        13029,
        6633,
        11,
        2658,
        785,
        8397,
        6089,
        11,
        12117,
        8337,
        17314,
        11,
        978,
        785,
        3514,
        1979,
        2806,
        43412,
        25127,
        3437,
        41990,
        4859,
        3084,
        674,
        1482,
        1418,
        37845,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29168668389320374,
      "compression_ratio": 1.587837815284729,
      "no_speech_prob": 0.026742050424218178
    },
    {
      "id": 95,
      "seek": 55860,
      "start": 569.5999755859375,
      "end": 573.5999755859375,
      "text": " Jetzt ist die Frage, warum glaube ich, dass es Overhyped ist.",
      "tokens": [
        50914,
        12592,
        1418,
        978,
        13685,
        11,
        24331,
        13756,
        1893,
        11,
        2658,
        785,
        4886,
        3495,
        3452,
        1418,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29168668389320374,
      "compression_ratio": 1.587837815284729,
      "no_speech_prob": 0.026742050424218178
    },
    {
      "id": 96,
      "seek": 55860,
      "start": 573.5999755859375,
      "end": 584.5999755859375,
      "text": " Ich habe diesen Heiser-Artikel geschrieben, der im Wesentlichen die These aufstellt, eigentlich bedeutet Softwareentwicklung, dass wir viele Menschen koordinieren, um gemeinsam an einem Projekt zu arbeiten.",
      "tokens": [
        51114,
        3141,
        6015,
        12862,
        634,
        6694,
        12,
        32925,
        41486,
        47397,
        11,
        1163,
        566,
        23843,
        7698,
        268,
        978,
        1981,
        2501,
        372,
        12783,
        11,
        10926,
        27018,
        27428,
        317,
        16038,
        17850,
        11,
        2658,
        1987,
        9693,
        8397,
        8384,
        6241,
        5695,
        11,
        1105,
        29701,
        364,
        6827,
        34804,
        2164,
        23162,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29168668389320374,
      "compression_ratio": 1.587837815284729,
      "no_speech_prob": 0.026742050424218178
    },
    {
      "id": 97,
      "seek": 58460,
      "start": 585.5999755859375,
      "end": 594.5999755859375,
      "text": " Dazu gehören nicht nur Techniker, sondern wir Entwickler und Architekten, sondern vor allem auch Stakeholder, Benutzer und wer auch immer dazugehört.",
      "tokens": [
        50414,
        34667,
        13218,
        26377,
        1979,
        4343,
        8337,
        17314,
        11,
        11465,
        1987,
        29397,
        1918,
        674,
        10984,
        642,
        2320,
        268,
        11,
        11465,
        4245,
        17585,
        2168,
        745,
        619,
        20480,
        11,
        3964,
        325,
        4527,
        674,
        2612,
        2168,
        5578,
        274,
        921,
        7181,
        71,
        11454,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2624894976615906,
      "compression_ratio": 1.489878535270691,
      "no_speech_prob": 0.22490794956684113
    },
    {
      "id": 98,
      "seek": 58460,
      "start": 594.5999755859375,
      "end": 601.5999755859375,
      "text": " Das bedeutet, dass das Hauptproblem von Softwareentwicklung ist, Menschen zu koordinieren und sie gemeinsam an einem Projekt arbeiten zu lassen.",
      "tokens": [
        50864,
        2846,
        27018,
        11,
        2658,
        1482,
        30573,
        47419,
        2957,
        27428,
        317,
        16038,
        17850,
        1418,
        11,
        8397,
        2164,
        8384,
        6241,
        5695,
        674,
        2804,
        29701,
        364,
        6827,
        34804,
        23162,
        2164,
        16168,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2624894976615906,
      "compression_ratio": 1.489878535270691,
      "no_speech_prob": 0.22490794956684113
    },
    {
      "id": 99,
      "seek": 58460,
      "start": 601.5999755859375,
      "end": 606.5999755859375,
      "text": " Das ist das, was ich seit sehr langer Zeit auch tatsächlich so erlebe.",
      "tokens": [
        51214,
        2846,
        1418,
        1482,
        11,
        390,
        1893,
        16452,
        5499,
        2265,
        260,
        9394,
        2168,
        20796,
        370,
        26826,
        650,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2624894976615906,
      "compression_ratio": 1.489878535270691,
      "no_speech_prob": 0.22490794956684113
    },
    {
      "id": 100,
      "seek": 60660,
      "start": 606.5999755859375,
      "end": 617.5999755859375,
      "text": " Das heißt, ich sitze da und rede mit irgendwelchen Leuten, so wie jetzt, über irgendwelche Ideen, irgendwelche Themen, irgendwelche Sachen, die wir bauen wollen und das ist mein wesentlicher Lebensinhalt.",
      "tokens": [
        50364,
        2846,
        13139,
        11,
        1893,
        1394,
        1381,
        1120,
        674,
        14328,
        2194,
        26455,
        338,
        2470,
        42301,
        11,
        370,
        3355,
        4354,
        11,
        4502,
        26455,
        338,
        1876,
        13090,
        268,
        11,
        26455,
        338,
        1876,
        39229,
        11,
        26455,
        338,
        1876,
        26074,
        11,
        978,
        1987,
        43787,
        11253,
        674,
        1482,
        1418,
        10777,
        38384,
        7698,
        260,
        21530,
        10085,
        3198,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.277599573135376,
      "compression_ratio": 1.568464756011963,
      "no_speech_prob": 0.7110567688941956
    },
    {
      "id": 101,
      "seek": 60660,
      "start": 617.5999755859375,
      "end": 623.5999755859375,
      "text": " André hatte schon einen ganz guten Punkt gemacht.",
      "tokens": [
        50914,
        400,
        10521,
        13299,
        4981,
        4891,
        6312,
        31277,
        25487,
        12293,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.277599573135376,
      "compression_ratio": 1.568464756011963,
      "no_speech_prob": 0.7110567688941956
    },
    {
      "id": 102,
      "seek": 60660,
      "start": 623.5999755859375,
      "end": 633.5999755859375,
      "text": " Man muss ein bisschen aufpassen, weil es gibt eine gewisse Versuchung zu sagen, mich tangiert das nicht, ich bin sicher.",
      "tokens": [
        51214,
        2458,
        6425,
        1343,
        10763,
        2501,
        44270,
        11,
        7689,
        785,
        6089,
        3018,
        6906,
        7746,
        12226,
        625,
        1063,
        2164,
        8360,
        11,
        6031,
        10266,
        4859,
        1482,
        1979,
        11,
        1893,
        5171,
        18623,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.277599573135376,
      "compression_ratio": 1.568464756011963,
      "no_speech_prob": 0.7110567688941956
    },
    {
      "id": 103,
      "seek": 63360,
      "start": 633.5999755859375,
      "end": 653.5999755859375,
      "text": " Ich weiß noch nicht, ob ich da ein Opfer bin, aber ich würde behaupten, diese Tätigkeit, die ich dort betreibe und so, wie ich Softwareentwicklung erlebe und so, wie ich auch die Hauptprobleme erlebe, die ist ein Thema, was KI für mich nicht offensichtlich erschlägt.",
      "tokens": [
        50364,
        3141,
        13385,
        3514,
        1979,
        11,
        1111,
        1893,
        1120,
        1343,
        12011,
        612,
        5171,
        11,
        4340,
        1893,
        11942,
        1540,
        13343,
        268,
        11,
        6705,
        314,
        3628,
        16626,
        11,
        978,
        1893,
        15775,
        778,
        10271,
        650,
        674,
        370,
        11,
        3355,
        1893,
        27428,
        317,
        16038,
        17850,
        26826,
        650,
        674,
        370,
        11,
        3355,
        1893,
        2168,
        978,
        30573,
        47419,
        68,
        26826,
        650,
        11,
        978,
        1418,
        1343,
        16306,
        11,
        390,
        47261,
        2959,
        6031,
        1979,
        766,
        694,
        41971,
        33743,
        11439,
        737,
        10463,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20849885046482086,
      "compression_ratio": 1.5027624368667603,
      "no_speech_prob": 0.25043609738349915
    },
    {
      "id": 104,
      "seek": 65360,
      "start": 654.5999755859375,
      "end": 662.5999755859375,
      "text": " Ich glaube sofort, dass KI-Tools die Produktivität von Entwicklern und Technikern verbessern, also eben das Produzieren von Code.",
      "tokens": [
        50414,
        3141,
        13756,
        33168,
        11,
        2658,
        47261,
        12,
        51,
        29298,
        978,
        44599,
        592,
        14053,
        2957,
        29397,
        75,
        1248,
        674,
        8337,
        1035,
        1248,
        49112,
        1248,
        11,
        611,
        11375,
        1482,
        11793,
        89,
        5695,
        2957,
        15549,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2259867638349533,
      "compression_ratio": 1.3207547664642334,
      "no_speech_prob": 0.9419227838516235
    },
    {
      "id": 105,
      "seek": 65360,
      "start": 662.5999755859375,
      "end": 675.5999755859375,
      "text": " Wir hatten ja vor 14 Tagen die Episode mit Ralf gemacht, wo er darüber berichtet hat, wie ChatGPT für ihn im Prinzip ein Feature implementiert hat.",
      "tokens": [
        50814,
        4347,
        20441,
        2784,
        4245,
        3499,
        41721,
        978,
        19882,
        2194,
        497,
        1678,
        12293,
        11,
        6020,
        1189,
        21737,
        5948,
        40387,
        2385,
        11,
        3355,
        27503,
        38,
        47,
        51,
        2959,
        14534,
        566,
        47572,
        1343,
        3697,
        1503,
        4445,
        4859,
        2385,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2259867638349533,
      "compression_ratio": 1.3207547664642334,
      "no_speech_prob": 0.9419227838516235
    },
    {
      "id": 106,
      "seek": 67560,
      "start": 675.5999755859375,
      "end": 682.5999755859375,
      "text": " Das bedeutet, die Entwicklung von Code wird sicher viel einfacher, viel schneller funktionieren.",
      "tokens": [
        50364,
        2846,
        27018,
        11,
        978,
        39654,
        2957,
        15549,
        4578,
        18623,
        5891,
        38627,
        4062,
        11,
        5891,
        43865,
        20454,
        5695,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19436223804950714,
      "compression_ratio": 1.708053708076477,
      "no_speech_prob": 0.7101585268974304
    },
    {
      "id": 107,
      "seek": 67560,
      "start": 682.5999755859375,
      "end": 686.5999755859375,
      "text": " Das sind aber Dinge, die gar nicht so einmalig sind.",
      "tokens": [
        50714,
        2846,
        3290,
        4340,
        25102,
        11,
        978,
        3691,
        1979,
        370,
        11078,
        328,
        3290,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19436223804950714,
      "compression_ratio": 1.708053708076477,
      "no_speech_prob": 0.7101585268974304
    },
    {
      "id": 108,
      "seek": 67560,
      "start": 686.5999755859375,
      "end": 691.5999755859375,
      "text": " Wir haben eine Tradition davon, Produktivität zu verbessern durch bestimmte technische Maßnahmen.",
      "tokens": [
        50914,
        4347,
        3084,
        3018,
        22017,
        849,
        18574,
        11,
        44599,
        592,
        14053,
        2164,
        49112,
        1248,
        7131,
        35180,
        975,
        1537,
        7864,
        36626,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19436223804950714,
      "compression_ratio": 1.708053708076477,
      "no_speech_prob": 0.7101585268974304
    },
    {
      "id": 109,
      "seek": 67560,
      "start": 691.5999755859375,
      "end": 694.5999755859375,
      "text": " Hochsprachen haben wir deswegen implementiert.",
      "tokens": [
        51164,
        29193,
        18193,
        608,
        268,
        3084,
        1987,
        26482,
        4445,
        4859,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19436223804950714,
      "compression_ratio": 1.708053708076477,
      "no_speech_prob": 0.7101585268974304
    },
    {
      "id": 110,
      "seek": 67560,
      "start": 694.5999755859375,
      "end": 696.5999755859375,
      "text": " Wir haben angefangen, Betriebssysteme zu implementieren.",
      "tokens": [
        51314,
        4347,
        3084,
        43907,
        10784,
        11,
        6279,
        5469,
        929,
        28215,
        68,
        2164,
        4445,
        5695,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19436223804950714,
      "compression_ratio": 1.708053708076477,
      "no_speech_prob": 0.7101585268974304
    },
    {
      "id": 111,
      "seek": 67560,
      "start": 696.5999755859375,
      "end": 698.5999755859375,
      "text": " Wir haben angefangen, Datenbanken zu implementieren.",
      "tokens": [
        51414,
        4347,
        3084,
        43907,
        10784,
        11,
        31126,
        65,
        18493,
        2164,
        4445,
        5695,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19436223804950714,
      "compression_ratio": 1.708053708076477,
      "no_speech_prob": 0.7101585268974304
    },
    {
      "id": 112,
      "seek": 67560,
      "start": 698.5999755859375,
      "end": 703.5999755859375,
      "text": " Wir haben Mitarbeiter, die Cloud, wo halt Komponentenbibliotheken sind, die ich einfach benutzen kann.",
      "tokens": [
        51514,
        4347,
        3084,
        38324,
        11,
        978,
        8061,
        11,
        6020,
        12479,
        591,
        8586,
        30365,
        268,
        65,
        897,
        2081,
        24863,
        2653,
        3290,
        11,
        978,
        1893,
        7281,
        38424,
        2904,
        4028,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19436223804950714,
      "compression_ratio": 1.708053708076477,
      "no_speech_prob": 0.7101585268974304
    },
    {
      "id": 113,
      "seek": 70360,
      "start": 703.5999755859375,
      "end": 705.5999755859375,
      "text": " Ich kann eben Machine Learning-Modelle aus der Cloud benutzen.",
      "tokens": [
        50364,
        3141,
        4028,
        11375,
        22155,
        15205,
        12,
        44,
        378,
        4434,
        3437,
        1163,
        8061,
        38424,
        2904,
        13,
        50464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20613281428813934,
      "compression_ratio": 1.6296296119689941,
      "no_speech_prob": 0.0979539006948471
    },
    {
      "id": 114,
      "seek": 70360,
      "start": 705.5999755859375,
      "end": 712.5999755859375,
      "text": " Ich kann auch irgendwelche anderen Dinge, halbfertige Dinge aus der Cloud benutzen als Komponenten.",
      "tokens": [
        50464,
        3141,
        4028,
        2168,
        26455,
        338,
        1876,
        11122,
        25102,
        11,
        7523,
        65,
        34784,
        3969,
        25102,
        3437,
        1163,
        8061,
        38424,
        2904,
        3907,
        591,
        8586,
        30365,
        268,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20613281428813934,
      "compression_ratio": 1.6296296119689941,
      "no_speech_prob": 0.0979539006948471
    },
    {
      "id": 115,
      "seek": 70360,
      "start": 712.5999755859375,
      "end": 718.5999755859375,
      "text": " Das sind alles Dinge, die dafür sorgen, dass sich die Produktivität von Entwicklern erheblich verbessert.",
      "tokens": [
        50814,
        2846,
        3290,
        7874,
        25102,
        11,
        978,
        13747,
        47972,
        11,
        2658,
        3041,
        978,
        44599,
        592,
        14053,
        2957,
        29397,
        75,
        1248,
        1189,
        675,
        65,
        1739,
        49112,
        911,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20613281428813934,
      "compression_ratio": 1.6296296119689941,
      "no_speech_prob": 0.0979539006948471
    },
    {
      "id": 116,
      "seek": 70360,
      "start": 718.5999755859375,
      "end": 723.5999755859375,
      "text": " Es gibt diese traditionelle Meinung von Fred Brooks, der gesagt hat,",
      "tokens": [
        51114,
        2313,
        6089,
        6705,
        6994,
        4434,
        36519,
        2957,
        10112,
        33493,
        11,
        1163,
        12260,
        2385,
        11,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20613281428813934,
      "compression_ratio": 1.6296296119689941,
      "no_speech_prob": 0.0979539006948471
    },
    {
      "id": 117,
      "seek": 70360,
      "start": 723.5999755859375,
      "end": 732.5999755859375,
      "text": " nicht eine einzige Maßnahme wird die Produktivität in der Softwareentwicklung um eine Größenordnung verbessern, also um einen Faktor von 10.",
      "tokens": [
        51364,
        1979,
        3018,
        47743,
        28645,
        32796,
        4578,
        978,
        44599,
        592,
        14053,
        294,
        1163,
        27428,
        317,
        16038,
        17850,
        1105,
        3018,
        45778,
        8989,
        31829,
        49112,
        1248,
        11,
        611,
        1105,
        4891,
        479,
        5886,
        284,
        2957,
        1266,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20613281428813934,
      "compression_ratio": 1.6296296119689941,
      "no_speech_prob": 0.0979539006948471
    },
    {
      "id": 118,
      "seek": 73260,
      "start": 732.5999755859375,
      "end": 735.5999755859375,
      "text": " Das belegt er nicht richtig. Das ist eher eine Vermutung.",
      "tokens": [
        50364,
        2846,
        312,
        22745,
        1189,
        1979,
        13129,
        13,
        2846,
        1418,
        24332,
        3018,
        20185,
        325,
        1063,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2307521253824234,
      "compression_ratio": 1.5692883729934692,
      "no_speech_prob": 0.14594659209251404
    },
    {
      "id": 119,
      "seek": 73260,
      "start": 735.5999755859375,
      "end": 739.5999755859375,
      "text": " Jetzt ist die Frage, ob KI daran kratzt.",
      "tokens": [
        50514,
        12592,
        1418,
        978,
        13685,
        11,
        1111,
        47261,
        24520,
        350,
        4481,
        2682,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2307521253824234,
      "compression_ratio": 1.5692883729934692,
      "no_speech_prob": 0.14594659209251404
    },
    {
      "id": 120,
      "seek": 73260,
      "start": 739.5999755859375,
      "end": 747.5999755859375,
      "text": " Aber selbst wenn, und das ist der letzte Blogartikel, den ich geschrieben habe, ist die Frage, was passiert denn dann?",
      "tokens": [
        50714,
        5992,
        13053,
        4797,
        11,
        674,
        1482,
        1418,
        1163,
        35236,
        46693,
        446,
        41486,
        11,
        1441,
        1893,
        47397,
        6015,
        11,
        1418,
        978,
        13685,
        11,
        390,
        21671,
        10471,
        3594,
        30,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2307521253824234,
      "compression_ratio": 1.5692883729934692,
      "no_speech_prob": 0.14594659209251404
    },
    {
      "id": 121,
      "seek": 73260,
      "start": 747.5999755859375,
      "end": 752.5999755859375,
      "text": " Meine These wäre, wir werden mehr Software in noch mehr Bereichen sehen.",
      "tokens": [
        51114,
        22258,
        1981,
        14558,
        11,
        1987,
        4604,
        5417,
        27428,
        294,
        3514,
        5417,
        17684,
        18613,
        11333,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2307521253824234,
      "compression_ratio": 1.5692883729934692,
      "no_speech_prob": 0.14594659209251404
    },
    {
      "id": 122,
      "seek": 73260,
      "start": 752.5999755859375,
      "end": 757.5999755859375,
      "text": " Dinge, die wir heute mit Software ledigen, haben wir früher ohne gemacht.",
      "tokens": [
        51364,
        25102,
        11,
        978,
        1987,
        9801,
        2194,
        27428,
        476,
        67,
        3213,
        11,
        3084,
        1987,
        32349,
        15716,
        12293,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2307521253824234,
      "compression_ratio": 1.5692883729934692,
      "no_speech_prob": 0.14594659209251404
    },
    {
      "id": 123,
      "seek": 73260,
      "start": 757.5999755859375,
      "end": 760.5999755859375,
      "text": " Mittlerweile haben wir alle diese lustigen Telefone.",
      "tokens": [
        51614,
        18784,
        36055,
        3084,
        1987,
        5430,
        6705,
        24672,
        3213,
        14889,
        69,
        546,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2307521253824234,
      "compression_ratio": 1.5692883729934692,
      "no_speech_prob": 0.14594659209251404
    },
    {
      "id": 124,
      "seek": 76060,
      "start": 761.5999755859375,
      "end": 764.5999755859375,
      "text": " Ich habe hier eine Uhr, die hat auch Software.",
      "tokens": [
        50414,
        3141,
        6015,
        3296,
        3018,
        30084,
        11,
        978,
        2385,
        2168,
        27428,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19377319514751434,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.17752304673194885
    },
    {
      "id": 125,
      "seek": 76060,
      "start": 764.5999755859375,
      "end": 767.5999755859375,
      "text": " Das sind Dinge, die wir früher nicht gemacht haben.",
      "tokens": [
        50564,
        2846,
        3290,
        25102,
        11,
        978,
        1987,
        32349,
        1979,
        12293,
        3084,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19377319514751434,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.17752304673194885
    },
    {
      "id": 126,
      "seek": 76060,
      "start": 767.5999755859375,
      "end": 769.5999755859375,
      "text": " Das ist dieser Rebound-Effekt.",
      "tokens": [
        50714,
        2846,
        1418,
        9053,
        1300,
        18767,
        12,
        36,
        602,
        8192,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19377319514751434,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.17752304673194885
    },
    {
      "id": 127,
      "seek": 76060,
      "start": 769.5999755859375,
      "end": 775.5999755859375,
      "text": " Weil etwas effizienter wird, weil wir es einfacher hinbekommen, bedeutet das nicht, dass weniger davon passiert,",
      "tokens": [
        50814,
        18665,
        9569,
        1244,
        590,
        1196,
        260,
        4578,
        11,
        7689,
        1987,
        785,
        38627,
        4062,
        14102,
        650,
        13675,
        11,
        27018,
        1482,
        1979,
        11,
        2658,
        23224,
        18574,
        21671,
        11,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19377319514751434,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.17752304673194885
    },
    {
      "id": 128,
      "seek": 76060,
      "start": 775.5999755859375,
      "end": 778.5999755859375,
      "text": " sondern im Gegenteil, möglicherweise sogar mehr.",
      "tokens": [
        51114,
        11465,
        566,
        27826,
        1576,
        388,
        11,
        16294,
        44071,
        19485,
        5417,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19377319514751434,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.17752304673194885
    },
    {
      "id": 129,
      "seek": 76060,
      "start": 778.5999755859375,
      "end": 780.5999755859375,
      "text": " Wir haben effizientere Autos.",
      "tokens": [
        51264,
        4347,
        3084,
        1244,
        590,
        1196,
        323,
        6049,
        329,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19377319514751434,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.17752304673194885
    },
    {
      "id": 130,
      "seek": 76060,
      "start": 780.5999755859375,
      "end": 785.5999755859375,
      "text": " Ergebnis, es wird mehr Benzin verbraucht, weil mehr Leute Auto fahren und mehr Strecken fahren.",
      "tokens": [
        51364,
        46229,
        11,
        785,
        4578,
        5417,
        3964,
        23584,
        1306,
        6198,
        10084,
        11,
        7689,
        5417,
        13495,
        13738,
        25593,
        674,
        5417,
        19597,
        13029,
        25593,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19377319514751434,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.17752304673194885
    },
    {
      "id": 131,
      "seek": 76060,
      "start": 785.5999755859375,
      "end": 787.5999755859375,
      "text": " Das ist vielleicht bei Softwareentwicklung ähnlich.",
      "tokens": [
        51614,
        2846,
        1418,
        12547,
        4643,
        27428,
        317,
        16038,
        17850,
        49696,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19377319514751434,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.17752304673194885
    },
    {
      "id": 132,
      "seek": 78760,
      "start": 787.5999755859375,
      "end": 792.5999755859375,
      "text": " Wir sind hier in der schwierigen Situation, dass wir versuchen, die Zukunft vorherzusagen.",
      "tokens": [
        50364,
        4347,
        3290,
        3296,
        294,
        1163,
        27546,
        3213,
        22247,
        11,
        2658,
        1987,
        34749,
        11,
        978,
        22782,
        29195,
        32868,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2185099571943283,
      "compression_ratio": 1.598540186882019,
      "no_speech_prob": 0.2389405220746994
    },
    {
      "id": 133,
      "seek": 78760,
      "start": 792.5999755859375,
      "end": 796.5999755859375,
      "text": " Das heißt, ich kann jetzt irgendwelche Dinge behaupten.",
      "tokens": [
        50614,
        2846,
        13139,
        11,
        1893,
        4028,
        4354,
        26455,
        338,
        1876,
        25102,
        1540,
        13343,
        268,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2185099571943283,
      "compression_ratio": 1.598540186882019,
      "no_speech_prob": 0.2389405220746994
    },
    {
      "id": 134,
      "seek": 78760,
      "start": 796.5999755859375,
      "end": 800.5999755859375,
      "text": " Und ich würde behaupten, wir werden vielleicht eher einen Rebound-Effekt sehen.",
      "tokens": [
        50814,
        2719,
        1893,
        11942,
        1540,
        13343,
        268,
        11,
        1987,
        4604,
        12547,
        24332,
        4891,
        1300,
        18767,
        12,
        36,
        602,
        8192,
        11333,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2185099571943283,
      "compression_ratio": 1.598540186882019,
      "no_speech_prob": 0.2389405220746994
    },
    {
      "id": 135,
      "seek": 78760,
      "start": 800.5999755859375,
      "end": 804.5999755859375,
      "text": " Und wir werden die Hauptprobleme nicht lösen.",
      "tokens": [
        51014,
        2719,
        1987,
        4604,
        978,
        30573,
        47419,
        68,
        1979,
        25209,
        6748,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2185099571943283,
      "compression_ratio": 1.598540186882019,
      "no_speech_prob": 0.2389405220746994
    },
    {
      "id": 136,
      "seek": 78760,
      "start": 804.5999755859375,
      "end": 807.5999755859375,
      "text": " Kommunikation, Stakeholder.",
      "tokens": [
        51214,
        28832,
        1035,
        399,
        11,
        745,
        619,
        20480,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2185099571943283,
      "compression_ratio": 1.598540186882019,
      "no_speech_prob": 0.2389405220746994
    },
    {
      "id": 137,
      "seek": 78760,
      "start": 807.5999755859375,
      "end": 811.5999755859375,
      "text": " Unser Hauptproblem ist zu sagen, was genau soll implementiert werden.",
      "tokens": [
        51364,
        1156,
        12484,
        30573,
        47419,
        1418,
        2164,
        8360,
        11,
        390,
        12535,
        7114,
        4445,
        4859,
        4604,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2185099571943283,
      "compression_ratio": 1.598540186882019,
      "no_speech_prob": 0.2389405220746994
    },
    {
      "id": 138,
      "seek": 78760,
      "start": 811.5999755859375,
      "end": 815.5999755859375,
      "text": " Unser Problem ist nicht in erster Linie den Code zu produzieren.",
      "tokens": [
        51564,
        1156,
        12484,
        11676,
        1418,
        1979,
        294,
        1189,
        3120,
        9355,
        414,
        1441,
        15549,
        2164,
        582,
        378,
        3334,
        5695,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2185099571943283,
      "compression_ratio": 1.598540186882019,
      "no_speech_prob": 0.2389405220746994
    },
    {
      "id": 139,
      "seek": 81560,
      "start": 815.5999755859375,
      "end": 820.5999755859375,
      "text": " Und ich glaube, davon können wir alle ein Lied singen, die wir täglich im Meeting sitzen",
      "tokens": [
        50364,
        2719,
        1893,
        13756,
        11,
        18574,
        6310,
        1987,
        5430,
        1343,
        441,
        1091,
        1522,
        268,
        11,
        978,
        1987,
        14619,
        8856,
        566,
        33217,
        44998,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2161811888217926,
      "compression_ratio": 1.439130425453186,
      "no_speech_prob": 0.280767023563385
    },
    {
      "id": 140,
      "seek": 81560,
      "start": 820.5999755859375,
      "end": 822.5999755859375,
      "text": " und genau versuchen, das herauszufinden.",
      "tokens": [
        50614,
        674,
        12535,
        34749,
        11,
        1482,
        25089,
        39467,
        10291,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2161811888217926,
      "compression_ratio": 1.439130425453186,
      "no_speech_prob": 0.280767023563385
    },
    {
      "id": 141,
      "seek": 81560,
      "start": 822.5999755859375,
      "end": 825.5999755859375,
      "text": " Nutzt du in deiner täglichen Arbeit KI?",
      "tokens": [
        50714,
        19861,
        2682,
        1581,
        294,
        368,
        4564,
        14619,
        8856,
        268,
        18604,
        47261,
        30,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2161811888217926,
      "compression_ratio": 1.439130425453186,
      "no_speech_prob": 0.280767023563385
    },
    {
      "id": 142,
      "seek": 81560,
      "start": 825.5999755859375,
      "end": 830.5999755859375,
      "text": " Ja, und insbesondere für Arbeiten mit Texten.",
      "tokens": [
        50864,
        3530,
        11,
        674,
        48694,
        2959,
        1587,
        16779,
        2194,
        18643,
        268,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2161811888217926,
      "compression_ratio": 1.439130425453186,
      "no_speech_prob": 0.280767023563385
    },
    {
      "id": 143,
      "seek": 81560,
      "start": 830.5999755859375,
      "end": 836.5999755859375,
      "text": " Also Abstracts zusammenbasteln, Zusammenfassungen bauen, solche Geschichten.",
      "tokens": [
        51114,
        2743,
        46853,
        1897,
        82,
        14311,
        65,
        525,
        9878,
        11,
        29442,
        69,
        640,
        5084,
        43787,
        11,
        29813,
        14241,
        24681,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2161811888217926,
      "compression_ratio": 1.439130425453186,
      "no_speech_prob": 0.280767023563385
    },
    {
      "id": 144,
      "seek": 81560,
      "start": 836.5999755859375,
      "end": 840.5999755859375,
      "text": " Und das funktioniert hervorragend.",
      "tokens": [
        51414,
        2719,
        1482,
        26160,
        720,
        8453,
        3731,
        521,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2161811888217926,
      "compression_ratio": 1.439130425453186,
      "no_speech_prob": 0.280767023563385
    },
    {
      "id": 145,
      "seek": 84060,
      "start": 840.5999755859375,
      "end": 845.5999755859375,
      "text": " Und als ich vor zwei Jahren das erste Mal JGPT genutzt habe,",
      "tokens": [
        50364,
        2719,
        3907,
        1893,
        4245,
        12002,
        13080,
        1482,
        20951,
        5746,
        508,
        38,
        47,
        51,
        1049,
        325,
        2682,
        6015,
        11,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 146,
      "seek": 84060,
      "start": 845.5999755859375,
      "end": 848.5999755859375,
      "text": " hätte ich vorher gesagt, sowas geht gar nicht.",
      "tokens": [
        50614,
        20041,
        1893,
        29195,
        12260,
        11,
        19766,
        296,
        7095,
        3691,
        1979,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 147,
      "seek": 84060,
      "start": 848.5999755859375,
      "end": 850.5999755859375,
      "text": " Und von daher nicht.",
      "tokens": [
        50764,
        2719,
        2957,
        36971,
        1979,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 148,
      "seek": 84060,
      "start": 850.5999755859375,
      "end": 853.5999755859375,
      "text": " Also dass es eine Revolution ist und eine Disruption.",
      "tokens": [
        50864,
        2743,
        2658,
        785,
        3018,
        16617,
        1418,
        674,
        3018,
        4208,
        11266,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 149,
      "seek": 84060,
      "start": 853.5999755859375,
      "end": 856.5999755859375,
      "text": " Das ist, glaube ich, unzweifelhaft so.",
      "tokens": [
        51014,
        2846,
        1418,
        11,
        13756,
        1893,
        11,
        517,
        89,
        826,
        351,
        338,
        25127,
        370,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 150,
      "seek": 84060,
      "start": 856.5999755859375,
      "end": 858.5999755859375,
      "text": " Danke, Eberhard.",
      "tokens": [
        51164,
        26508,
        11,
        462,
        607,
        21491,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 151,
      "seek": 84060,
      "start": 858.5999755859375,
      "end": 862.5999755859375,
      "text": " Dann Alphabet. Ralf, glaube ich, PQRS.",
      "tokens": [
        51264,
        7455,
        967,
        20890,
        13,
        497,
        1678,
        11,
        13756,
        1893,
        11,
        430,
        48,
        43580,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 152,
      "seek": 84060,
      "start": 862.5999755859375,
      "end": 864.5999755859375,
      "text": " Genau.",
      "tokens": [
        51464,
        22340,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 153,
      "seek": 84060,
      "start": 864.5999755859375,
      "end": 868.5999755859375,
      "text": " Also wenn ich bei der Technologie irgendwie schon Grenzen sehen würde,",
      "tokens": [
        51564,
        2743,
        4797,
        1893,
        4643,
        1163,
        8337,
        20121,
        20759,
        4981,
        24913,
        2904,
        11333,
        11942,
        11,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3203882575035095,
      "compression_ratio": 1.4752066135406494,
      "no_speech_prob": 0.4639628529548645
    },
    {
      "id": 154,
      "seek": 86860,
      "start": 868.5999755859375,
      "end": 870.5999755859375,
      "text": " dann würde ich sagen Overhyped.",
      "tokens": [
        50364,
        3594,
        11942,
        1893,
        8360,
        4886,
        3495,
        3452,
        13,
        50464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20117594301700592,
      "compression_ratio": 1.5929824113845825,
      "no_speech_prob": 0.018540622666478157
    },
    {
      "id": 155,
      "seek": 86860,
      "start": 870.5999755859375,
      "end": 875.5999755859375,
      "text": " Aber mein Standpunkt ist Underhyped, weil ich sehe halt momentan keine Grenzen.",
      "tokens": [
        50464,
        5992,
        10777,
        9133,
        31744,
        1418,
        6974,
        3495,
        3452,
        11,
        7689,
        1893,
        35995,
        12479,
        1623,
        282,
        9252,
        24913,
        2904,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20117594301700592,
      "compression_ratio": 1.5929824113845825,
      "no_speech_prob": 0.018540622666478157
    },
    {
      "id": 156,
      "seek": 86860,
      "start": 875.5999755859375,
      "end": 879.5999755859375,
      "text": " Ich versuche, die Grenzen zu finden, und ich finde sie nicht.",
      "tokens": [
        50714,
        3141,
        1774,
        17545,
        11,
        978,
        24913,
        2904,
        2164,
        20734,
        11,
        674,
        1893,
        17841,
        2804,
        1979,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20117594301700592,
      "compression_ratio": 1.5929824113845825,
      "no_speech_prob": 0.018540622666478157
    },
    {
      "id": 157,
      "seek": 86860,
      "start": 879.5999755859375,
      "end": 883.5999755859375,
      "text": " Es ist auf jeden Fall klar, dass GenAI, die Art und Weise, wie wir Software entwickeln,",
      "tokens": [
        50914,
        2313,
        1418,
        2501,
        12906,
        7465,
        14743,
        11,
        2658,
        3632,
        48698,
        11,
        978,
        5735,
        674,
        41947,
        11,
        3355,
        1987,
        27428,
        28449,
        32099,
        11,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20117594301700592,
      "compression_ratio": 1.5929824113845825,
      "no_speech_prob": 0.018540622666478157
    },
    {
      "id": 158,
      "seek": 86860,
      "start": 883.5999755859375,
      "end": 890.5999755859375,
      "text": " werden fundamental verändert und dass diese Veränderung in vielen Bereichen schnell und zeitnah erfolgen wird.",
      "tokens": [
        51114,
        4604,
        8088,
        45990,
        674,
        2658,
        6705,
        4281,
        16082,
        1063,
        294,
        19885,
        17684,
        18613,
        17589,
        674,
        49367,
        12411,
        1189,
        7082,
        1766,
        4578,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20117594301700592,
      "compression_ratio": 1.5929824113845825,
      "no_speech_prob": 0.018540622666478157
    },
    {
      "id": 159,
      "seek": 86860,
      "start": 890.5999755859375,
      "end": 896.5999755859375,
      "text": " Momentan sehe ich noch, dass die KI oft falsch, weil punktuell, unterstützend",
      "tokens": [
        51464,
        19093,
        282,
        35995,
        1893,
        3514,
        11,
        2658,
        978,
        47261,
        11649,
        43340,
        11,
        7689,
        39561,
        13789,
        11,
        30007,
        89,
        521,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20117594301700592,
      "compression_ratio": 1.5929824113845825,
      "no_speech_prob": 0.018540622666478157
    },
    {
      "id": 160,
      "seek": 89660,
      "start": 896.5999755859375,
      "end": 899.5999755859375,
      "text": " und gegen die Regeln der Softwareentwicklung eingesetzt wird.",
      "tokens": [
        50364,
        674,
        13953,
        978,
        4791,
        9878,
        1163,
        27428,
        317,
        16038,
        17850,
        49821,
        3524,
        4578,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17139199376106262,
      "compression_ratio": 1.5,
      "no_speech_prob": 0.055761225521564484
    },
    {
      "id": 161,
      "seek": 89660,
      "start": 899.5999755859375,
      "end": 906.5999755859375,
      "text": " Also wenn ich zum Beispiel Code habe und dann meinen Copilot bitte, mir den Test schreiben zu lassen,",
      "tokens": [
        50514,
        2743,
        4797,
        1893,
        5919,
        13772,
        15549,
        6015,
        674,
        3594,
        22738,
        11579,
        31516,
        23231,
        11,
        3149,
        1441,
        9279,
        48546,
        2164,
        16168,
        11,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17139199376106262,
      "compression_ratio": 1.5,
      "no_speech_prob": 0.055761225521564484
    },
    {
      "id": 162,
      "seek": 89660,
      "start": 906.5999755859375,
      "end": 911.5999755859375,
      "text": " dann ist es nicht mehr Test-Driven Development, sondern Development-Driven Testing.",
      "tokens": [
        50864,
        3594,
        1418,
        785,
        1979,
        5417,
        9279,
        12,
        35,
        470,
        553,
        15041,
        11,
        11465,
        15041,
        12,
        35,
        470,
        553,
        45517,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17139199376106262,
      "compression_ratio": 1.5,
      "no_speech_prob": 0.055761225521564484
    },
    {
      "id": 163,
      "seek": 89660,
      "start": 911.5999755859375,
      "end": 919.5999755859375,
      "text": " Und so sehe ich in manchen Bereichen, dass wir über die KI, weil wir sie punktuell einsetzen, Fehler machen.",
      "tokens": [
        51114,
        2719,
        370,
        35995,
        1893,
        294,
        587,
        2470,
        17684,
        18613,
        11,
        2658,
        1987,
        4502,
        978,
        47261,
        11,
        7689,
        1987,
        2804,
        39561,
        13789,
        21889,
        24797,
        11,
        48101,
        7069,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17139199376106262,
      "compression_ratio": 1.5,
      "no_speech_prob": 0.055761225521564484
    },
    {
      "id": 164,
      "seek": 91960,
      "start": 919.5999755859375,
      "end": 926.5999755859375,
      "text": " Aber meine These ist, wenn GenAI richtig eingesetzt wird und wir nicht an irgendwelche Grenzen stoßen,",
      "tokens": [
        50364,
        5992,
        10946,
        1981,
        1418,
        11,
        4797,
        3632,
        48698,
        13129,
        49821,
        3524,
        4578,
        674,
        1987,
        1979,
        364,
        26455,
        338,
        1876,
        24913,
        2904,
        22784,
        8989,
        11,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16371166706085205,
      "compression_ratio": 1.4527559280395508,
      "no_speech_prob": 0.020618654787540436
    },
    {
      "id": 165,
      "seek": 91960,
      "start": 926.5999755859375,
      "end": 931.5999755859375,
      "text": " dann wird die GenAI die Softwareentwicklung für uns in Zukunft übernehmen können.",
      "tokens": [
        50714,
        3594,
        4578,
        978,
        3632,
        48698,
        978,
        27428,
        317,
        16038,
        17850,
        2959,
        2693,
        294,
        22782,
        4502,
        14669,
        6310,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16371166706085205,
      "compression_ratio": 1.4527559280395508,
      "no_speech_prob": 0.020618654787540436
    },
    {
      "id": 166,
      "seek": 91960,
      "start": 931.5999755859375,
      "end": 936.5999755859375,
      "text": " Das heißt, wir fallen da meines Erachtens komplett raus.",
      "tokens": [
        50964,
        2846,
        13139,
        11,
        1987,
        11547,
        1120,
        385,
        1652,
        3300,
        3589,
        694,
        32261,
        17202,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16371166706085205,
      "compression_ratio": 1.4527559280395508,
      "no_speech_prob": 0.020618654787540436
    },
    {
      "id": 167,
      "seek": 91960,
      "start": 936.5999755859375,
      "end": 939.5999755859375,
      "text": " Das ist dieses, was Eberhard ja auch schon gesagt hat.",
      "tokens": [
        51214,
        2846,
        1418,
        12113,
        11,
        390,
        462,
        607,
        21491,
        2784,
        2168,
        4981,
        12260,
        2385,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16371166706085205,
      "compression_ratio": 1.4527559280395508,
      "no_speech_prob": 0.020618654787540436
    },
    {
      "id": 168,
      "seek": 91960,
      "start": 939.5999755859375,
      "end": 943.5999755859375,
      "text": " Wir programmieren nicht mehr in Assembler, wir nutzen Hochsprachen.",
      "tokens": [
        51364,
        4347,
        37648,
        5695,
        1979,
        5417,
        294,
        1018,
        15750,
        1918,
        11,
        1987,
        36905,
        29193,
        18193,
        11646,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16371166706085205,
      "compression_ratio": 1.4527559280395508,
      "no_speech_prob": 0.020618654787540436
    },
    {
      "id": 169,
      "seek": 94360,
      "start": 943.5999755859375,
      "end": 948.5999755859375,
      "text": " In Zukunft werden wir aus meiner Sicht natürlich sprachlich unsere Requirements,",
      "tokens": [
        50364,
        682,
        22782,
        4604,
        1987,
        3437,
        20529,
        36615,
        8762,
        6103,
        608,
        1739,
        14339,
        42029,
        621,
        1117,
        11,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16400083899497986,
      "compression_ratio": 1.5170940160751343,
      "no_speech_prob": 0.004537141416221857
    },
    {
      "id": 170,
      "seek": 94360,
      "start": 948.5999755859375,
      "end": 953.5999755859375,
      "text": " unsere Anforderungen stellen und dann wird die Maschine das umsetzen.",
      "tokens": [
        50614,
        14339,
        1107,
        30943,
        5084,
        24407,
        674,
        3594,
        4578,
        978,
        5224,
        36675,
        1482,
        1105,
        3854,
        2904,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16400083899497986,
      "compression_ratio": 1.5170940160751343,
      "no_speech_prob": 0.004537141416221857
    },
    {
      "id": 171,
      "seek": 94360,
      "start": 953.5999755859375,
      "end": 962.5999755859375,
      "text": " Das Ganze geht in Richtung Low-Code, No-Code und das führt wahrscheinlich anfangs nur wieder zu weiteren Problemen,",
      "tokens": [
        50864,
        2846,
        35206,
        7095,
        294,
        33023,
        17078,
        12,
        34,
        1429,
        11,
        883,
        12,
        34,
        1429,
        674,
        1482,
        39671,
        30957,
        364,
        19134,
        82,
        4343,
        6216,
        2164,
        44036,
        11676,
        268,
        11,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16400083899497986,
      "compression_ratio": 1.5170940160751343,
      "no_speech_prob": 0.004537141416221857
    },
    {
      "id": 172,
      "seek": 94360,
      "start": 962.5999755859375,
      "end": 970.5999755859375,
      "text": " weil wenn wir mit diesem Low-Code, No-Code Ansatz an die Softwareentwicklung rangehen,",
      "tokens": [
        51314,
        7689,
        4797,
        1987,
        2194,
        10975,
        17078,
        12,
        34,
        1429,
        11,
        883,
        12,
        34,
        1429,
        14590,
        10300,
        364,
        978,
        27428,
        317,
        16038,
        17850,
        3613,
        2932,
        11,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16400083899497986,
      "compression_ratio": 1.5170940160751343,
      "no_speech_prob": 0.004537141416221857
    },
    {
      "id": 173,
      "seek": 97060,
      "start": 970.5999755859375,
      "end": 978.5999755859375,
      "text": " dann werden wir auf komplexere Probleme stoßen, die wir dann mit dem Low-Code Wissen nicht umsetzen können,",
      "tokens": [
        50364,
        3594,
        4604,
        1987,
        2501,
        5207,
        18945,
        323,
        32891,
        22784,
        8989,
        11,
        978,
        1987,
        3594,
        2194,
        1371,
        17078,
        12,
        34,
        1429,
        343,
        10987,
        1979,
        1105,
        3854,
        2904,
        6310,
        11,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17708325386047363,
      "compression_ratio": 1.4669811725616455,
      "no_speech_prob": 0.002472029533237219
    },
    {
      "id": 174,
      "seek": 97060,
      "start": 978.5999755859375,
      "end": 982.5999755859375,
      "text": " sondern wo wir dann eben wieder entsprechende Experten brauchen.",
      "tokens": [
        50764,
        11465,
        6020,
        1987,
        3594,
        11375,
        6216,
        29967,
        5445,
        12522,
        1147,
        19543,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17708325386047363,
      "compression_ratio": 1.4669811725616455,
      "no_speech_prob": 0.002472029533237219
    },
    {
      "id": 175,
      "seek": 97060,
      "start": 982.5999755859375,
      "end": 988.5999755859375,
      "text": " Und das heißt, dieses Expertentum wird auch noch länger bestehen bleiben.",
      "tokens": [
        50964,
        2719,
        1482,
        13139,
        11,
        12113,
        41255,
        317,
        449,
        4578,
        2168,
        3514,
        40935,
        22245,
        2932,
        24912,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17708325386047363,
      "compression_ratio": 1.4669811725616455,
      "no_speech_prob": 0.002472029533237219
    },
    {
      "id": 176,
      "seek": 97060,
      "start": 988.5999755859375,
      "end": 992.5999755859375,
      "text": " Davon werden dann einige extrem profitieren, andere weniger.",
      "tokens": [
        51264,
        3724,
        266,
        4604,
        3594,
        28338,
        4040,
        7475,
        5695,
        11,
        10490,
        23224,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17708325386047363,
      "compression_ratio": 1.4669811725616455,
      "no_speech_prob": 0.002472029533237219
    },
    {
      "id": 177,
      "seek": 99260,
      "start": 992.5999755859375,
      "end": 1001.5999755859375,
      "text": " Was mich noch beschäftigt ist, dass wir weltweit momentan nur ein paar wenige LLMs haben, die dominieren.",
      "tokens": [
        50364,
        3027,
        6031,
        3514,
        38768,
        5828,
        1418,
        11,
        2658,
        1987,
        43119,
        28019,
        1623,
        282,
        4343,
        1343,
        16509,
        11472,
        3969,
        441,
        43,
        26386,
        3084,
        11,
        978,
        8859,
        5695,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16390179097652435,
      "compression_ratio": 1.3439490795135498,
      "no_speech_prob": 0.11740022897720337
    },
    {
      "id": 178,
      "seek": 99260,
      "start": 1001.5999755859375,
      "end": 1010.5999755859375,
      "text": " Wir haben zwar eine riesige Anzahl an irgendwelchen Sprachmodellen, aber die Dominanten sind sehr wenig.",
      "tokens": [
        50814,
        4347,
        3084,
        19054,
        3018,
        23932,
        3969,
        1107,
        39670,
        364,
        26455,
        338,
        2470,
        7702,
        608,
        8014,
        8581,
        11,
        4340,
        978,
        18027,
        29646,
        3290,
        5499,
        20911,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16390179097652435,
      "compression_ratio": 1.3439490795135498,
      "no_speech_prob": 0.11740022897720337
    },
    {
      "id": 179,
      "seek": 101060,
      "start": 1010.5999755859375,
      "end": 1020.5999755859375,
      "text": " Und wenn all diese wenigen dominanten LLMs für uns in Zukunft die Entscheidung treffen,",
      "tokens": [
        50364,
        2719,
        4797,
        439,
        6705,
        11472,
        3213,
        15657,
        268,
        441,
        43,
        26386,
        2959,
        2693,
        294,
        22782,
        978,
        44667,
        37620,
        11,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21718749403953552,
      "compression_ratio": 1.4466019868850708,
      "no_speech_prob": 0.08497706055641174
    },
    {
      "id": 180,
      "seek": 101060,
      "start": 1020.5999755859375,
      "end": 1029.5999755859375,
      "text": " welche Technologien wir verwenden, welchen Code-Stil wir verwenden, dann wird es technologisch zu Monokulturen kommen.",
      "tokens": [
        50864,
        24311,
        8337,
        1132,
        1053,
        1987,
        24615,
        8896,
        11,
        2214,
        2470,
        15549,
        12,
        4520,
        388,
        1987,
        24615,
        8896,
        11,
        3594,
        4578,
        785,
        1537,
        1132,
        5494,
        2164,
        4713,
        453,
        8389,
        77,
        11729,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21718749403953552,
      "compression_ratio": 1.4466019868850708,
      "no_speech_prob": 0.08497706055641174
    },
    {
      "id": 181,
      "seek": 101060,
      "start": 1029.5999755859375,
      "end": 1037.5999755859375,
      "text": " Wir haben es jetzt schon, Python ist die Sprache der KI, es wird immer mehr KI eingesetzt.",
      "tokens": [
        51314,
        4347,
        3084,
        785,
        4354,
        4981,
        11,
        15329,
        1418,
        978,
        7702,
        6000,
        1163,
        47261,
        11,
        785,
        4578,
        5578,
        5417,
        47261,
        49821,
        3524,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21718749403953552,
      "compression_ratio": 1.4466019868850708,
      "no_speech_prob": 0.08497706055641174
    },
    {
      "id": 182,
      "seek": 103760,
      "start": 1037.5999755859375,
      "end": 1044.5999755859375,
      "text": " Deswegen haben wir immer mehr Python-Repositories und die Sprache gewinnt an Popularität.",
      "tokens": [
        50364,
        24864,
        3084,
        1987,
        5578,
        5417,
        15329,
        12,
        25554,
        9598,
        2083,
        674,
        978,
        7702,
        6000,
        6906,
        259,
        580,
        364,
        37637,
        14053,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19760750234127045,
      "compression_ratio": 1.4375,
      "no_speech_prob": 0.02367377281188965
    },
    {
      "id": 183,
      "seek": 103760,
      "start": 1044.5999755859375,
      "end": 1051.5999755859375,
      "text": " Und andere fallen dann hinten runter, weil sie eben auch von den Maschinen nicht unterstützt werden.",
      "tokens": [
        50714,
        2719,
        10490,
        11547,
        3594,
        36417,
        33295,
        11,
        7689,
        2804,
        11375,
        2168,
        2957,
        1441,
        5224,
        339,
        5636,
        1979,
        30007,
        2682,
        4604,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19760750234127045,
      "compression_ratio": 1.4375,
      "no_speech_prob": 0.02367377281188965
    },
    {
      "id": 184,
      "seek": 103760,
      "start": 1051.5999755859375,
      "end": 1059.5999755859375,
      "text": " Und das ist eine sehr interessante Entwicklung. Es war jetzt auch gerade die Woche eine Entscheidung in einem großen Repository,",
      "tokens": [
        51064,
        2719,
        1482,
        1418,
        3018,
        5499,
        24372,
        39654,
        13,
        2313,
        1516,
        4354,
        2168,
        12117,
        978,
        24511,
        3018,
        44667,
        294,
        6827,
        23076,
        3696,
        9598,
        827,
        11,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19760750234127045,
      "compression_ratio": 1.4375,
      "no_speech_prob": 0.02367377281188965
    },
    {
      "id": 185,
      "seek": 105960,
      "start": 1059.5999755859375,
      "end": 1071.5999755859375,
      "text": " dass man eine Dokumentation nicht für den Menschen lesbarer gestalten möchte, sondern bei dem Lesbaren für die Maschine bleiben möchte.",
      "tokens": [
        50364,
        2658,
        587,
        3018,
        29768,
        2206,
        399,
        1979,
        2959,
        1441,
        8397,
        1512,
        5356,
        260,
        7219,
        23276,
        14570,
        11,
        11465,
        4643,
        1371,
        6965,
        43552,
        2959,
        978,
        5224,
        36675,
        24912,
        14570,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22301135957241058,
      "compression_ratio": 1.522449016571045,
      "no_speech_prob": 0.06091112270951271
    },
    {
      "id": 186,
      "seek": 105960,
      "start": 1071.5999755859375,
      "end": 1074.5999755859375,
      "text": " Und das wird sehr spannend.",
      "tokens": [
        50964,
        2719,
        1482,
        4578,
        5499,
        49027,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22301135957241058,
      "compression_ratio": 1.522449016571045,
      "no_speech_prob": 0.06091112270951271
    },
    {
      "id": 187,
      "seek": 105960,
      "start": 1074.5999755859375,
      "end": 1079.5999755859375,
      "text": " Und dich braucht man gar nicht fragen, du nutzt KI in deiner täglichen Arbeit, oder?",
      "tokens": [
        51114,
        2719,
        10390,
        22623,
        587,
        3691,
        1979,
        39129,
        11,
        1581,
        5393,
        2682,
        47261,
        294,
        368,
        4564,
        14619,
        8856,
        268,
        18604,
        11,
        4513,
        30,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22301135957241058,
      "compression_ratio": 1.522449016571045,
      "no_speech_prob": 0.06091112270951271
    },
    {
      "id": 188,
      "seek": 105960,
      "start": 1079.5999755859375,
      "end": 1087.5999755859375,
      "text": " Nein, ich habe die ganze Zeit schon nachgedacht. Und ja, ich versuche KI zu verwenden, also Gen AI, wo es möglich ist.",
      "tokens": [
        51364,
        18878,
        11,
        1893,
        6015,
        978,
        18898,
        9394,
        4981,
        5168,
        3004,
        3589,
        13,
        2719,
        2784,
        11,
        1893,
        1774,
        17545,
        47261,
        2164,
        24615,
        8896,
        11,
        611,
        3632,
        7318,
        11,
        6020,
        785,
        16294,
        1418,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22301135957241058,
      "compression_ratio": 1.522449016571045,
      "no_speech_prob": 0.06091112270951271
    },
    {
      "id": 189,
      "seek": 108760,
      "start": 1087.5999755859375,
      "end": 1093.5999755859375,
      "text": " Wenn ich aber die Coding-Tools verwende, dann habe ich eher das Gefühl, dass sie mich verwenden.",
      "tokens": [
        50364,
        7899,
        1893,
        4340,
        978,
        383,
        8616,
        12,
        51,
        29298,
        24615,
        5445,
        11,
        3594,
        6015,
        1893,
        24332,
        1482,
        29715,
        11,
        2658,
        2804,
        6031,
        24615,
        8896,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2027180939912796,
      "compression_ratio": 1.7022900581359863,
      "no_speech_prob": 0.01363489031791687
    },
    {
      "id": 190,
      "seek": 108760,
      "start": 1093.5999755859375,
      "end": 1099.5999755859375,
      "text": " Sie sagen, bitte den Code jetzt rüber kopieren und jetzt mal die Tests ausführen, ob es noch läuft.",
      "tokens": [
        50664,
        3559,
        8360,
        11,
        23231,
        1441,
        15549,
        4354,
        367,
        12670,
        28920,
        5695,
        674,
        4354,
        2806,
        978,
        314,
        4409,
        3437,
        69,
        29540,
        11,
        1111,
        785,
        3514,
        31807,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2027180939912796,
      "compression_ratio": 1.7022900581359863,
      "no_speech_prob": 0.01363489031791687
    },
    {
      "id": 191,
      "seek": 108760,
      "start": 1099.5999755859375,
      "end": 1105.5999755859375,
      "text": " Und davon will ich wieder weg. Und da bin ich eben mit meinem eigenen System auch entsprechend weg,",
      "tokens": [
        50964,
        2719,
        18574,
        486,
        1893,
        6216,
        15565,
        13,
        2719,
        1120,
        5171,
        1893,
        11375,
        2194,
        24171,
        28702,
        8910,
        2168,
        47823,
        15565,
        11,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2027180939912796,
      "compression_ratio": 1.7022900581359863,
      "no_speech_prob": 0.01363489031791687
    },
    {
      "id": 192,
      "seek": 108760,
      "start": 1105.5999755859375,
      "end": 1114.5999755859375,
      "text": " weil dieses System selbst den Code modifizieren darf und selbst die Tests ausführen darf und deswegen entsprechend in die Iterationen reinkommt.",
      "tokens": [
        51264,
        7689,
        12113,
        8910,
        13053,
        1441,
        15549,
        1072,
        351,
        590,
        5695,
        19374,
        674,
        13053,
        978,
        314,
        4409,
        3437,
        69,
        29540,
        19374,
        674,
        26482,
        47823,
        294,
        978,
        286,
        391,
        399,
        268,
        319,
        475,
        22230,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2027180939912796,
      "compression_ratio": 1.7022900581359863,
      "no_speech_prob": 0.01363489031791687
    },
    {
      "id": 193,
      "seek": 111460,
      "start": 1114.5999755859375,
      "end": 1119.5999755859375,
      "text": " Und das ist so mein Traum, dass eben die Maschine nebenbei entwickelt.",
      "tokens": [
        50364,
        2719,
        1482,
        1418,
        370,
        10777,
        5403,
        449,
        11,
        2658,
        11375,
        978,
        5224,
        36675,
        36098,
        21845,
        43208,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26095208525657654,
      "compression_ratio": 1.4832713603973389,
      "no_speech_prob": 0.06553180515766144
    },
    {
      "id": 194,
      "seek": 111460,
      "start": 1119.5999755859375,
      "end": 1125.5999755859375,
      "text": " Manchmal Rückfragen stellt, du soll ich links oder rechts gehen? Und so soll es in Zukunft sein.",
      "tokens": [
        50614,
        2458,
        25751,
        35001,
        69,
        20663,
        38582,
        11,
        1581,
        7114,
        1893,
        6123,
        4513,
        34305,
        13230,
        30,
        2719,
        370,
        7114,
        785,
        294,
        22782,
        6195,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26095208525657654,
      "compression_ratio": 1.4832713603973389,
      "no_speech_prob": 0.06553180515766144
    },
    {
      "id": 195,
      "seek": 111460,
      "start": 1125.5999755859375,
      "end": 1128.5999755859375,
      "text": " Aus meiner Sicht. Sehr cool. Danke, Ralf.",
      "tokens": [
        50914,
        9039,
        20529,
        36615,
        13,
        32028,
        1627,
        13,
        26508,
        11,
        497,
        1678,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26095208525657654,
      "compression_ratio": 1.4832713603973389,
      "no_speech_prob": 0.06553180515766144
    },
    {
      "id": 196,
      "seek": 111460,
      "start": 1128.5999755859375,
      "end": 1132.5999755859375,
      "text": " Und dann darfst du noch den grünen Abschluss mit deinen Thesen machen, Stefan.",
      "tokens": [
        51064,
        2719,
        3594,
        19374,
        372,
        1581,
        3514,
        1441,
        677,
        774,
        2866,
        5813,
        41372,
        2194,
        49362,
        334,
        17403,
        7069,
        11,
        32158,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26095208525657654,
      "compression_ratio": 1.4832713603973389,
      "no_speech_prob": 0.06553180515766144
    },
    {
      "id": 197,
      "seek": 111460,
      "start": 1132.5999755859375,
      "end": 1138.5999755859375,
      "text": " Ja, immer, immer am Ende wie in der Schule. Bei mir ist aber egal, ob man das mit Vor- oder Nachnamen macht.",
      "tokens": [
        51264,
        3530,
        11,
        5578,
        11,
        5578,
        669,
        15152,
        3355,
        294,
        1163,
        32953,
        13,
        16188,
        3149,
        1418,
        4340,
        31528,
        11,
        1111,
        587,
        1482,
        2194,
        12231,
        12,
        4513,
        11815,
        5378,
        268,
        10857,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26095208525657654,
      "compression_ratio": 1.4832713603973389,
      "no_speech_prob": 0.06553180515766144
    },
    {
      "id": 198,
      "seek": 113860,
      "start": 1139.5999755859375,
      "end": 1156.5999755859375,
      "text": " Okay, meine Thesen. Also erstens, ich benutze in der täglichen Programmierung Cursor als IDE nach ungefähr 20 Jahren, über 20 Jahren JetBrains Produkten.",
      "tokens": [
        50414,
        1033,
        11,
        10946,
        334,
        17403,
        13,
        2743,
        11301,
        694,
        11,
        1893,
        38424,
        1381,
        294,
        1163,
        14619,
        8856,
        268,
        48244,
        11651,
        383,
        2156,
        284,
        3907,
        40930,
        5168,
        41285,
        945,
        13080,
        11,
        4502,
        945,
        13080,
        28730,
        45606,
        1292,
        11793,
        47120,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2350725680589676,
      "compression_ratio": 1.3684210777282715,
      "no_speech_prob": 0.06272797286510468
    },
    {
      "id": 199,
      "seek": 113860,
      "start": 1156.5999755859375,
      "end": 1166.5999755859375,
      "text": " Und ich bin ziemlich fasziniert davon, wie Cursor mich unterstützt, bestimmte Sachen zu programmieren.",
      "tokens": [
        51264,
        2719,
        1893,
        5171,
        28901,
        283,
        19601,
        259,
        4859,
        18574,
        11,
        3355,
        383,
        2156,
        284,
        6031,
        30007,
        2682,
        11,
        35180,
        975,
        26074,
        2164,
        37648,
        5695,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2350725680589676,
      "compression_ratio": 1.3684210777282715,
      "no_speech_prob": 0.06272797286510468
    },
    {
      "id": 200,
      "seek": 116660,
      "start": 1166.5999755859375,
      "end": 1175.5999755859375,
      "text": " Das heißt, ich ändere in einer Datei etwas, gehe in eine andere Datei und Cursor macht Vorschläge, was ich hier ändern müsste, damit es zu dem anderen passt.",
      "tokens": [
        50364,
        2846,
        13139,
        11,
        1893,
        24981,
        323,
        294,
        6850,
        31805,
        72,
        9569,
        11,
        34252,
        294,
        3018,
        10490,
        31805,
        72,
        674,
        383,
        2156,
        284,
        10857,
        31438,
        11439,
        737,
        432,
        11,
        390,
        1893,
        3296,
        47775,
        42962,
        11,
        9479,
        785,
        2164,
        1371,
        11122,
        37154,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383376836776733,
      "compression_ratio": 1.5110132694244385,
      "no_speech_prob": 0.04881259426474571
    },
    {
      "id": 201,
      "seek": 116660,
      "start": 1175.5999755859375,
      "end": 1181.5999755859375,
      "text": " Also es grenzt bei mir schon ab und an an Magie, wo ich denke, wie kommt man da jetzt drauf?",
      "tokens": [
        50814,
        2743,
        785,
        20313,
        2682,
        4643,
        3149,
        4981,
        410,
        674,
        364,
        364,
        6395,
        414,
        11,
        6020,
        1893,
        27245,
        11,
        3355,
        10047,
        587,
        1120,
        4354,
        22763,
        30,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383376836776733,
      "compression_ratio": 1.5110132694244385,
      "no_speech_prob": 0.04881259426474571
    },
    {
      "id": 202,
      "seek": 116660,
      "start": 1181.5999755859375,
      "end": 1187.5999755859375,
      "text": " Aber das hätte ich auch gemacht. Und die Sachen erscheinen mir auch nicht als trivial.",
      "tokens": [
        51114,
        5992,
        1482,
        20041,
        1893,
        2168,
        12293,
        13,
        2719,
        978,
        26074,
        33743,
        1876,
        5636,
        3149,
        2168,
        1979,
        3907,
        26703,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383376836776733,
      "compression_ratio": 1.5110132694244385,
      "no_speech_prob": 0.04881259426474571
    },
    {
      "id": 203,
      "seek": 118760,
      "start": 1188.5999755859375,
      "end": 1201.5999755859375,
      "text": " Also ich bin da fasziniert und ich halte es teilweise für Magie im Sinne von hinreichend komplizierte Technologie, die man nicht versteht, stellt sich magisch dar.",
      "tokens": [
        50414,
        2743,
        1893,
        5171,
        1120,
        283,
        19601,
        259,
        4859,
        674,
        1893,
        7523,
        975,
        785,
        46748,
        2959,
        6395,
        414,
        566,
        47041,
        2957,
        14102,
        12594,
        521,
        24526,
        590,
        23123,
        8337,
        20121,
        11,
        978,
        587,
        1979,
        22442,
        357,
        11,
        38582,
        3041,
        2258,
        5494,
        4072,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2020636647939682,
      "compression_ratio": 1.4784482717514038,
      "no_speech_prob": 0.580223798751831
    },
    {
      "id": 204,
      "seek": 118760,
      "start": 1201.5999755859375,
      "end": 1210.5999755859375,
      "text": " Ich hatte zwei vorige Momente. Das erste Mal im Moment eben vor 40 Jahren, wo ich zum ersten Mal programmiert habe, was eingetippt habe und da kam am Bildschirm irgendwas Buntes.",
      "tokens": [
        51064,
        3141,
        13299,
        12002,
        4245,
        3969,
        5576,
        1576,
        13,
        2846,
        20951,
        5746,
        566,
        19093,
        11375,
        4245,
        3356,
        13080,
        11,
        6020,
        1893,
        5919,
        17324,
        5746,
        37648,
        4859,
        6015,
        11,
        390,
        17002,
        302,
        2488,
        83,
        6015,
        674,
        1120,
        9727,
        669,
        15746,
        6145,
        3692,
        47090,
        363,
        2760,
        279,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2020636647939682,
      "compression_ratio": 1.4784482717514038,
      "no_speech_prob": 0.580223798751831
    },
    {
      "id": 205,
      "seek": 121060,
      "start": 1210.5999755859375,
      "end": 1227.5999755859375,
      "text": " Das war auch ein magischer Moment. Und ein magischer Moment war auch Anfang der 90er, als das Internet kam und ich die ersten IRC Bots programmiert habe, die dann im Chat irgendwelche Sachen gemacht haben und Leute in Amerika darauf geantwortet haben.",
      "tokens": [
        50364,
        2846,
        1516,
        2168,
        1343,
        2258,
        19674,
        19093,
        13,
        2719,
        1343,
        2258,
        19674,
        19093,
        1516,
        2168,
        25856,
        1163,
        4289,
        260,
        11,
        3907,
        1482,
        7703,
        9727,
        674,
        1893,
        978,
        17324,
        16486,
        34,
        47224,
        37648,
        4859,
        6015,
        11,
        978,
        3594,
        566,
        27503,
        26455,
        338,
        1876,
        26074,
        12293,
        3084,
        674,
        13495,
        294,
        42345,
        18654,
        1519,
        21655,
        302,
        3084,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21797482669353485,
      "compression_ratio": 1.5708502531051636,
      "no_speech_prob": 0.24172307550907135
    },
    {
      "id": 206,
      "seek": 121060,
      "start": 1227.5999755859375,
      "end": 1235.5999755859375,
      "text": " Also ich sehe das in ähnlicher Art und Weise. Mich hat zwischendrin relativ wenig technisch fasziniert. Mich fasziniert es unglaublich.",
      "tokens": [
        51214,
        2743,
        1893,
        35995,
        1482,
        294,
        3078,
        12071,
        25215,
        5735,
        674,
        41947,
        13,
        3392,
        2385,
        11873,
        5494,
        521,
        12629,
        21960,
        20911,
        1537,
        5494,
        283,
        19601,
        259,
        4859,
        13,
        3392,
        283,
        19601,
        259,
        4859,
        785,
        49087,
        1739,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21797482669353485,
      "compression_ratio": 1.5708502531051636,
      "no_speech_prob": 0.24172307550907135
    },
    {
      "id": 207,
      "seek": 123560,
      "start": 1235.5999755859375,
      "end": 1245.5999755859375,
      "text": " Ich glaube aber, dass diese ganze Gen AI für die Softwareentwicklung im Sinne von Code Generierung letztlich nur eine Brückentechnologie ist, die kommt und geht.",
      "tokens": [
        50364,
        3141,
        13756,
        4340,
        11,
        2658,
        6705,
        18898,
        3632,
        7318,
        2959,
        978,
        27428,
        317,
        16038,
        17850,
        566,
        47041,
        2957,
        15549,
        15409,
        11651,
        35262,
        1739,
        4343,
        3018,
        1603,
        6536,
        1576,
        1377,
        20121,
        1418,
        11,
        978,
        10047,
        674,
        7095,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21692606806755066,
      "compression_ratio": 1.5072463750839233,
      "no_speech_prob": 0.5412529706954956
    },
    {
      "id": 208,
      "seek": 123560,
      "start": 1245.5999755859375,
      "end": 1256.5999755859375,
      "text": " Eine Auswirkung dieser Brückentechnologie wird sein, dass die Programmierer den reinen manuellen Anteil, also Codeschreiben immer schneller machen.",
      "tokens": [
        50864,
        17664,
        48500,
        18610,
        1063,
        9053,
        1603,
        6536,
        1576,
        1377,
        20121,
        4578,
        6195,
        11,
        2658,
        978,
        48244,
        811,
        260,
        1441,
        319,
        5636,
        587,
        13789,
        268,
        5130,
        30684,
        11,
        611,
        383,
        4789,
        339,
        25946,
        5578,
        43865,
        7069,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21692606806755066,
      "compression_ratio": 1.5072463750839233,
      "no_speech_prob": 0.5412529706954956
    },
    {
      "id": 209,
      "seek": 125660,
      "start": 1257.5999755859375,
      "end": 1281.5999755859375,
      "text": " Und deswegen glaube ich, diese Zeiten immer kürzer werden und daraus resultiert, dass die Interaktionen mit Leuten mittelfristig vielleicht auch in Richtung Eberhard nochmal zunehmen, weil wenn ich immer schneller fertig bin mit dem, was ich machen soll oder will oder darf oder kann und dann wieder zurückkommen muss, darf, soll, muss, kann, um mit jemandem zu sprechen, dann werden diese Interaktionen erstmal zunehmen.",
      "tokens": [
        50414,
        2719,
        26482,
        13756,
        1893,
        11,
        6705,
        48334,
        5578,
        350,
        1655,
        4527,
        4604,
        674,
        274,
        46483,
        1874,
        4859,
        11,
        2658,
        978,
        5751,
        5886,
        17068,
        2194,
        42301,
        19130,
        1967,
        12940,
        328,
        12547,
        2168,
        294,
        33023,
        462,
        607,
        21491,
        26509,
        710,
        2613,
        9547,
        11,
        7689,
        4797,
        1893,
        5578,
        43865,
        31362,
        5171,
        2194,
        1371,
        11,
        390,
        1893,
        7069,
        7114,
        4513,
        486,
        4513,
        19374,
        4513,
        4028,
        674,
        3594,
        6216,
        15089,
        13675,
        6425,
        11,
        19374,
        11,
        7114,
        11,
        6425,
        11,
        4028,
        11,
        1105,
        2194,
        21717,
        443,
        2164,
        27853,
        11,
        3594,
        4604,
        6705,
        5751,
        5886,
        17068,
        38607,
        710,
        2613,
        9547,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2272437959909439,
      "compression_ratio": 1.6785714626312256,
      "no_speech_prob": 0.8177565336227417
    },
    {
      "id": 210,
      "seek": 128160,
      "start": 1282.5999755859375,
      "end": 1289.5999755859375,
      "text": " Also im Prinzip so ein bisschen anders law. Das hatte ich mal von Andrej vor vielen, vielen Jahren.",
      "tokens": [
        50414,
        2743,
        566,
        47572,
        370,
        1343,
        10763,
        17999,
        2101,
        13,
        2846,
        13299,
        1893,
        2806,
        2957,
        20667,
        73,
        4245,
        19885,
        11,
        19885,
        13080,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.41279536485671997,
      "compression_ratio": 1.100000023841858,
      "no_speech_prob": 0.635188102722168
    },
    {
      "id": 211,
      "seek": 128960,
      "start": 1290.5999755859375,
      "end": 1303.5999755859375,
      "text": " Weiterer Punkt ist, ich glaube, wir betrachten die Software zu sehr als als gegeben und vergessen, dass Software ein Werkzeug ist, das Probleme lösen soll.",
      "tokens": [
        50414,
        48050,
        260,
        25487,
        1418,
        11,
        1893,
        13756,
        11,
        1987,
        778,
        81,
        20806,
        978,
        27428,
        2164,
        5499,
        3907,
        3907,
        32572,
        674,
        42418,
        11,
        2658,
        27428,
        1343,
        42911,
        19303,
        1418,
        11,
        1482,
        32891,
        25209,
        6748,
        7114,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25242719054222107,
      "compression_ratio": 1.401129961013794,
      "no_speech_prob": 0.5647936463356018
    },
    {
      "id": 212,
      "seek": 128960,
      "start": 1303.5999755859375,
      "end": 1311.5999755859375,
      "text": " Also unter dem ganzen Stichwort Jobs to be done ist Software eben ein Ding, was Jobs löst.",
      "tokens": [
        51064,
        2743,
        8662,
        1371,
        23966,
        745,
        480,
        13802,
        29169,
        281,
        312,
        1096,
        1418,
        27428,
        11375,
        1343,
        20558,
        11,
        390,
        29169,
        25209,
        372,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25242719054222107,
      "compression_ratio": 1.401129961013794,
      "no_speech_prob": 0.5647936463356018
    },
    {
      "id": 213,
      "seek": 131160,
      "start": 1311.5999755859375,
      "end": 1315.5999755859375,
      "text": " Mit Software kann ich bestimmte Jobs lösen, meine Aufgaben lösen.",
      "tokens": [
        50364,
        10821,
        27428,
        4028,
        1893,
        35180,
        975,
        29169,
        25209,
        6748,
        11,
        10946,
        29648,
        25071,
        25209,
        6748,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21649761497974396,
      "compression_ratio": 1.7435897588729858,
      "no_speech_prob": 0.6247778534889221
    },
    {
      "id": 214,
      "seek": 131160,
      "start": 1315.5999755859375,
      "end": 1320.5999755859375,
      "text": " Aber vielleicht brauche ich eben, wenn AI weiter fortschritt, brauche ich gar keine Software mehr.",
      "tokens": [
        50564,
        5992,
        12547,
        1548,
        17545,
        1893,
        11375,
        11,
        4797,
        7318,
        8988,
        30589,
        339,
        18579,
        11,
        1548,
        17545,
        1893,
        3691,
        9252,
        27428,
        5417,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21649761497974396,
      "compression_ratio": 1.7435897588729858,
      "no_speech_prob": 0.6247778534889221
    },
    {
      "id": 215,
      "seek": 131160,
      "start": 1320.5999755859375,
      "end": 1327.5999755859375,
      "text": " Also die Frage ist nicht, löst AI Softwareentwicklung ab, sondern löst AI Software ab im Sinne von Jobs to be done.",
      "tokens": [
        50814,
        2743,
        978,
        13685,
        1418,
        1979,
        11,
        25209,
        372,
        7318,
        27428,
        317,
        16038,
        17850,
        410,
        11,
        11465,
        25209,
        372,
        7318,
        27428,
        410,
        566,
        47041,
        2957,
        29169,
        281,
        312,
        1096,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21649761497974396,
      "compression_ratio": 1.7435897588729858,
      "no_speech_prob": 0.6247778534889221
    },
    {
      "id": 216,
      "seek": 131160,
      "start": 1327.5999755859375,
      "end": 1331.5999755859375,
      "text": " Und vielleicht sind wir alle ein bisschen zu sehr drauf verhaftet.",
      "tokens": [
        51164,
        2719,
        12547,
        3290,
        1987,
        5430,
        1343,
        10763,
        2164,
        5499,
        22763,
        1306,
        25127,
        302,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21649761497974396,
      "compression_ratio": 1.7435897588729858,
      "no_speech_prob": 0.6247778534889221
    },
    {
      "id": 217,
      "seek": 131160,
      "start": 1331.5999755859375,
      "end": 1334.5999755859375,
      "text": " Softwareentwicklung muss bleiben.",
      "tokens": [
        51364,
        27428,
        317,
        16038,
        17850,
        6425,
        24912,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21649761497974396,
      "compression_ratio": 1.7435897588729858,
      "no_speech_prob": 0.6247778534889221
    },
    {
      "id": 218,
      "seek": 131160,
      "start": 1334.5999755859375,
      "end": 1337.5999755859375,
      "text": " Ich würde sagen, was sie schon immer gibt.",
      "tokens": [
        51514,
        3141,
        11942,
        8360,
        11,
        390,
        2804,
        4981,
        5578,
        6089,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21649761497974396,
      "compression_ratio": 1.7435897588729858,
      "no_speech_prob": 0.6247778534889221
    },
    {
      "id": 219,
      "seek": 131160,
      "start": 1337.5999755859375,
      "end": 1339.5999755859375,
      "text": " Das stimmt bei Softwareentwicklung nicht ganz.",
      "tokens": [
        51664,
        2846,
        37799,
        4643,
        27428,
        317,
        16038,
        17850,
        1979,
        6312,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21649761497974396,
      "compression_ratio": 1.7435897588729858,
      "no_speech_prob": 0.6247778534889221
    },
    {
      "id": 220,
      "seek": 133960,
      "start": 1339.5999755859375,
      "end": 1346.5999755859375,
      "text": " Aber da ein bisschen weiter wegzugehen, halte ich für einen wichtigen Punkt.",
      "tokens": [
        50364,
        5992,
        1120,
        1343,
        10763,
        8988,
        15565,
        46285,
        2932,
        11,
        7523,
        975,
        1893,
        2959,
        4891,
        26244,
        3213,
        25487,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2616521716117859,
      "compression_ratio": 1.529197096824646,
      "no_speech_prob": 0.025541331619024277
    },
    {
      "id": 221,
      "seek": 133960,
      "start": 1346.5999755859375,
      "end": 1357.5999755859375,
      "text": " Generell glaube ich, es ist aber weitere Zukunft, dass Software, dass einzelne Systeme weggehen und ich in der AI halt sagen werde, irgendwann mal, was sie machen soll.",
      "tokens": [
        50714,
        3632,
        323,
        285,
        13756,
        1893,
        11,
        785,
        1418,
        4340,
        30020,
        22782,
        11,
        2658,
        27428,
        11,
        2658,
        36731,
        716,
        8910,
        68,
        15565,
        24985,
        674,
        1893,
        294,
        1163,
        7318,
        12479,
        8360,
        24866,
        11,
        34313,
        2806,
        11,
        390,
        2804,
        7069,
        7114,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2616521716117859,
      "compression_ratio": 1.529197096824646,
      "no_speech_prob": 0.025541331619024277
    },
    {
      "id": 222,
      "seek": 133960,
      "start": 1357.5999755859375,
      "end": 1368.5999755859375,
      "text": " Also speichere mal alle E-Mail-Adressen von allen Kunden oder aggregiere die oder sag mir, in welchen Ländern die sind, ohne dass ich ein CRM-System programmieren müsste.",
      "tokens": [
        51264,
        2743,
        768,
        480,
        323,
        2806,
        5430,
        462,
        12,
        44,
        864,
        12,
        15830,
        735,
        268,
        2957,
        18440,
        38192,
        4513,
        16743,
        14412,
        978,
        4513,
        15274,
        3149,
        11,
        294,
        2214,
        2470,
        48321,
        978,
        3290,
        11,
        15716,
        2658,
        1893,
        1343,
        14123,
        44,
        12,
        50,
        9321,
        37648,
        5695,
        42962,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2616521716117859,
      "compression_ratio": 1.529197096824646,
      "no_speech_prob": 0.025541331619024277
    },
    {
      "id": 223,
      "seek": 136860,
      "start": 1368.5999755859375,
      "end": 1374.5999755859375,
      "text": " Wie man es vielleicht heute machen würde oder ohne dass man ein CMS machen müsste oder etwas wie HubSpot programmieren.",
      "tokens": [
        50364,
        9233,
        587,
        785,
        12547,
        9801,
        7069,
        11942,
        4513,
        15716,
        2658,
        587,
        1343,
        33124,
        7069,
        42962,
        4513,
        9569,
        3355,
        18986,
        14014,
        310,
        37648,
        5695,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2355603277683258,
      "compression_ratio": 1.3905324935913086,
      "no_speech_prob": 0.004396800883114338
    },
    {
      "id": 224,
      "seek": 136860,
      "start": 1374.5999755859375,
      "end": 1381.5999755859375,
      "text": " Sondern alle diese Aufgaben kann letztlich eine AI generisch lösen, ohne dass dafür Code generiert werden muss.",
      "tokens": [
        50664,
        318,
        10881,
        5430,
        6705,
        29648,
        25071,
        4028,
        35262,
        1739,
        3018,
        7318,
        1337,
        5494,
        25209,
        6748,
        11,
        15716,
        2658,
        13747,
        15549,
        1337,
        4859,
        4604,
        6425,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2355603277683258,
      "compression_ratio": 1.3905324935913086,
      "no_speech_prob": 0.004396800883114338
    },
    {
      "id": 0,
      "seek": 0,
      "start": 1381.61,
      "end": 1388.5700000381469,
      "text": " Der erste Schritt in diese Richtung, glaube ich, werden Datensysteme sein.",
      "tokens": [
        50364,
        5618,
        20951,
        33062,
        294,
        6705,
        33023,
        11,
        13756,
        1893,
        11,
        4604,
        9315,
        694,
        9321,
        68,
        6195,
        13,
        50712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26229405403137207,
      "compression_ratio": 1.6326531171798706,
      "no_speech_prob": 0.7544340491294861
    },
    {
      "id": 1,
      "seek": 0,
      "start": 1388.5700000381469,
      "end": 1394.9299996948241,
      "text": " Ich habe in der Vergangenheit auch mehrmals Firmen geholfen und CTOs geholfen, ihre Datensysteme",
      "tokens": [
        50712,
        3141,
        6015,
        294,
        1163,
        26610,
        10784,
        8480,
        2168,
        5417,
        34978,
        28164,
        2558,
        1519,
        5449,
        6570,
        674,
        383,
        15427,
        82,
        1519,
        5449,
        6570,
        11,
        14280,
        9315,
        694,
        9321,
        68,
        51030
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26229405403137207,
      "compression_ratio": 1.6326531171798706,
      "no_speech_prob": 0.7544340491294861
    },
    {
      "id": 2,
      "seek": 0,
      "start": 1394.9299996948241,
      "end": 1398.0099996185302,
      "text": " zu modernisieren und Datateams aufzusetzen.",
      "tokens": [
        51030,
        2164,
        4363,
        271,
        5695,
        674,
        9315,
        473,
        4070,
        2501,
        16236,
        24797,
        13,
        51184
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26229405403137207,
      "compression_ratio": 1.6326531171798706,
      "no_speech_prob": 0.7544340491294861
    },
    {
      "id": 3,
      "seek": 0,
      "start": 1398.0099996185302,
      "end": 1402.9299996948241,
      "text": " Da glaube ich aber, das werden die ersten sein, die wahrscheinlich massiv darunter zu",
      "tokens": [
        51184,
        3933,
        13756,
        1893,
        4340,
        11,
        1482,
        4604,
        978,
        17324,
        6195,
        11,
        978,
        30957,
        2758,
        592,
        4072,
        21777,
        2164,
        51430
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26229405403137207,
      "compression_ratio": 1.6326531171798706,
      "no_speech_prob": 0.7544340491294861
    },
    {
      "id": 4,
      "seek": 0,
      "start": 1402.9299996948241,
      "end": 1403.9299996948241,
      "text": " leiden haben.",
      "tokens": [
        51430,
        476,
        4380,
        3084,
        13,
        51480
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26229405403137207,
      "compression_ratio": 1.6326531171798706,
      "no_speech_prob": 0.7544340491294861
    },
    {
      "id": 5,
      "seek": 0,
      "start": 1403.9299996948241,
      "end": 1408.5699990844726,
      "text": " Das heißt, Datensysteme als erste Instanz etwas, wo ich kein spezielles System mehr",
      "tokens": [
        51480,
        2846,
        13139,
        11,
        9315,
        694,
        9321,
        68,
        3907,
        20951,
        2730,
        3910,
        9569,
        11,
        6020,
        1893,
        13424,
        48682,
        19126,
        8910,
        5417,
        51712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26229405403137207,
      "compression_ratio": 1.6326531171798706,
      "no_speech_prob": 0.7544340491294861
    },
    {
      "id": 6,
      "seek": 2696,
      "start": 1408.5699990844726,
      "end": 1414.330001220703,
      "text": " habe, sondern eine generische Lösung auf einem Data Lake und dann sage ich, mach mal",
      "tokens": [
        50364,
        6015,
        11,
        11465,
        3018,
        1337,
        7864,
        46934,
        2501,
        6827,
        11888,
        10582,
        674,
        3594,
        19721,
        1893,
        11,
        2246,
        2806,
        50652
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40341201424598694,
      "compression_ratio": 1.6384615898132324,
      "no_speech_prob": 0.564411997795105
    },
    {
      "id": 7,
      "seek": 2696,
      "start": 1414.330001220703,
      "end": 1419.0899995422362,
      "text": " für Montagmorgen die KPI-Diagramme und dann kommen da fünf Slides raus und dann können",
      "tokens": [
        50652,
        2959,
        7947,
        559,
        40220,
        1766,
        978,
        591,
        31701,
        12,
        35,
        72,
        3914,
        1398,
        674,
        3594,
        11729,
        1120,
        28723,
        6187,
        1875,
        17202,
        674,
        3594,
        6310,
        50890
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40341201424598694,
      "compression_ratio": 1.6384615898132324,
      "no_speech_prob": 0.564411997795105
    },
    {
      "id": 8,
      "seek": 2696,
      "start": 1419.0899995422362,
      "end": 1420.0899995422362,
      "text": " wir uns die angucken.",
      "tokens": [
        50890,
        1987,
        2693,
        978,
        2562,
        49720,
        13,
        50940
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40341201424598694,
      "compression_ratio": 1.6384615898132324,
      "no_speech_prob": 0.564411997795105
    },
    {
      "id": 9,
      "seek": 2696,
      "start": 1420.0899995422362,
      "end": 1425.6900018310546,
      "text": " Da brauche ich aber keinen Data-Analysten dazu und keinen SQL-Menschen und keinen Plumber,",
      "tokens": [
        50940,
        3933,
        1548,
        17545,
        1893,
        4340,
        20624,
        11888,
        12,
        32,
        4660,
        38593,
        268,
        13034,
        674,
        20624,
        19200,
        12,
        44,
        694,
        2470,
        674,
        20624,
        2149,
        4182,
        11,
        51220
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40341201424598694,
      "compression_ratio": 1.6384615898132324,
      "no_speech_prob": 0.564411997795105
    },
    {
      "id": 10,
      "seek": 2696,
      "start": 1425.6900018310546,
      "end": 1426.6900018310546,
      "text": " der die Sachen zusammensteckt.",
      "tokens": [
        51220,
        1163,
        978,
        26074,
        14311,
        2941,
        19951,
        13,
        51270
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40341201424598694,
      "compression_ratio": 1.6384615898132324,
      "no_speech_prob": 0.564411997795105
    },
    {
      "id": 11,
      "seek": 2696,
      "start": 1426.6900018310546,
      "end": 1431.8899987792968,
      "text": " Vielleicht der Plumber bleibt vielleicht, brauchen vielleicht immer, kann sein.",
      "tokens": [
        51270,
        29838,
        1163,
        2149,
        4182,
        24814,
        12547,
        11,
        19543,
        12547,
        5578,
        11,
        4028,
        6195,
        13,
        51530
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40341201424598694,
      "compression_ratio": 1.6384615898132324,
      "no_speech_prob": 0.564411997795105
    },
    {
      "id": 12,
      "seek": 2696,
      "start": 1431.8899987792968,
      "end": 1435.4500001525878,
      "text": " Das war es, meine Meinung.",
      "tokens": [
        51530,
        2846,
        1516,
        785,
        11,
        10946,
        36519,
        13,
        51708
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40341201424598694,
      "compression_ratio": 1.6384615898132324,
      "no_speech_prob": 0.564411997795105
    },
    {
      "id": 13,
      "seek": 5384,
      "start": 1435.4500001525878,
      "end": 1436.4500001525878,
      "text": " Super.",
      "tokens": [
        50364,
        4548,
        13,
        50414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 14,
      "seek": 5384,
      "start": 1436.4500001525878,
      "end": 1438.9700006103515,
      "text": " Dann vielen, vielen Dank euch vier schon mal für eure Meinung.",
      "tokens": [
        50414,
        7455,
        19885,
        11,
        19885,
        14148,
        10403,
        17634,
        4981,
        2806,
        2959,
        32845,
        36519,
        13,
        50540
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 15,
      "seek": 5384,
      "start": 1438.9700006103515,
      "end": 1445.7700036621093,
      "text": " Wir haben im Chat schon ziemlich viel Nachrichten bekommen, dass zum Beispiel Christian Beuthenmüller,",
      "tokens": [
        50540,
        4347,
        3084,
        566,
        27503,
        4981,
        28901,
        5891,
        11815,
        40825,
        19256,
        11,
        2658,
        5919,
        13772,
        5778,
        879,
        2910,
        268,
        76,
        774,
        4658,
        11,
        50880
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 16,
      "seek": 5384,
      "start": 1445.7700036621093,
      "end": 1450.6500009155272,
      "text": " der hat als allererstes mal geschrieben, Overhype zumindest bei LLMs, also das ist seine Meinung",
      "tokens": [
        50880,
        1163,
        2385,
        3907,
        8722,
        16398,
        279,
        2806,
        47397,
        11,
        4886,
        3495,
        494,
        38082,
        4643,
        441,
        43,
        26386,
        11,
        611,
        1482,
        1418,
        15925,
        36519,
        51124
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 17,
      "seek": 5384,
      "start": 1450.6500009155272,
      "end": 1454.5699990844726,
      "text": " und nachher, ich glaube, als Ralf gesprochen hat, hat er das ungefähr geschrieben, wir",
      "tokens": [
        51124,
        674,
        5168,
        511,
        11,
        1893,
        13756,
        11,
        3907,
        497,
        1678,
        42714,
        2385,
        11,
        2385,
        1189,
        1482,
        41285,
        47397,
        11,
        1987,
        51320
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 18,
      "seek": 5384,
      "start": 1454.5699990844726,
      "end": 1458.2900003051757,
      "text": " haben jetzt schon Grenzen, Modelle sind nicht besser geworden seit GPT-4.",
      "tokens": [
        51320,
        3084,
        4354,
        4981,
        24913,
        2904,
        11,
        6583,
        4434,
        3290,
        1979,
        18021,
        26281,
        16452,
        26039,
        51,
        12,
        19,
        13,
        51506
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 19,
      "seek": 5384,
      "start": 1458.2900003051757,
      "end": 1460.4899972534179,
      "text": " Sind sie das nicht?",
      "tokens": [
        51506,
        35405,
        2804,
        1482,
        1979,
        30,
        51616
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 20,
      "seek": 5384,
      "start": 1460.4899972534179,
      "end": 1462.3700021362304,
      "text": " Also ich bin da nicht so drin.",
      "tokens": [
        51616,
        2743,
        1893,
        5171,
        1120,
        1979,
        370,
        24534,
        13,
        51710
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 21,
      "seek": 5384,
      "start": 1462.3700021362304,
      "end": 1463.9700006103515,
      "text": " Kann mich da jemand aufschlauen?",
      "tokens": [
        51710,
        29074,
        6031,
        1120,
        21717,
        2501,
        6145,
        875,
        7801,
        30,
        51790
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3618934154510498,
      "compression_ratio": 1.615625023841858,
      "no_speech_prob": 0.2980990409851074
    },
    {
      "id": 22,
      "seek": 8236,
      "start": 1464.4899972534179,
      "end": 1472.0900033569335,
      "text": " Also ich glaube, wir sind mit den Modellen echt ganz weit vorne, also die können schon",
      "tokens": [
        50390,
        2743,
        1893,
        13756,
        11,
        1987,
        3290,
        2194,
        1441,
        6583,
        8581,
        13972,
        6312,
        15306,
        32025,
        11,
        611,
        978,
        6310,
        4981,
        50770
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2755358815193176,
      "compression_ratio": 1.5778688192367554,
      "no_speech_prob": 0.015414770692586899
    },
    {
      "id": 23,
      "seek": 8236,
      "start": 1472.0900033569335,
      "end": 1477.6900018310546,
      "text": " so verdammt viel, dass jeder einzelne Prozentpunkt, den wir irgendwie in den Benchmarks gewinnen,",
      "tokens": [
        50770,
        370,
        6387,
        5136,
        83,
        5891,
        11,
        2658,
        19610,
        36731,
        716,
        29726,
        31744,
        11,
        1441,
        1987,
        20759,
        294,
        1441,
        3964,
        339,
        37307,
        6906,
        11399,
        11,
        51050
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2755358815193176,
      "compression_ratio": 1.5778688192367554,
      "no_speech_prob": 0.015414770692586899
    },
    {
      "id": 24,
      "seek": 8236,
      "start": 1477.6900018310546,
      "end": 1479.9700006103515,
      "text": " schon ein großer Schritt nach vorne ist.",
      "tokens": [
        51050,
        4981,
        1343,
        46220,
        33062,
        5168,
        32025,
        1418,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2755358815193176,
      "compression_ratio": 1.5778688192367554,
      "no_speech_prob": 0.015414770692586899
    },
    {
      "id": 25,
      "seek": 8236,
      "start": 1479.9700006103515,
      "end": 1487.4499963378905,
      "text": " Aber ich glaube auch nicht, dass wir die Modelle so stark verbessern müssen, sondern die Frontends",
      "tokens": [
        51164,
        5992,
        1893,
        13756,
        2168,
        1979,
        11,
        2658,
        1987,
        978,
        6583,
        4434,
        370,
        17417,
        49112,
        1248,
        9013,
        11,
        11465,
        978,
        17348,
        2581,
        51538
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2755358815193176,
      "compression_ratio": 1.5778688192367554,
      "no_speech_prob": 0.015414770692586899
    },
    {
      "id": 26,
      "seek": 8236,
      "start": 1487.4499963378905,
      "end": 1490.809996948242,
      "text": " und die Art und Weise, wie wir mit den Modellen arbeiten.",
      "tokens": [
        51538,
        674,
        978,
        5735,
        674,
        41947,
        11,
        3355,
        1987,
        2194,
        1441,
        6583,
        8581,
        23162,
        13,
        51706
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2755358815193176,
      "compression_ratio": 1.5778688192367554,
      "no_speech_prob": 0.015414770692586899
    },
    {
      "id": 27,
      "seek": 10920,
      "start": 1491.209998474121,
      "end": 1497.6500009155272,
      "text": " Die Modelle brauchen den Kontext, um Software zu entwickeln und nicht nur einen Ausschnitt",
      "tokens": [
        50384,
        3229,
        6583,
        4434,
        19543,
        1441,
        20629,
        3828,
        11,
        1105,
        27428,
        2164,
        28449,
        32099,
        674,
        1979,
        4343,
        4891,
        21286,
        32064,
        50706
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2684193253517151,
      "compression_ratio": 1.6007194519042969,
      "no_speech_prob": 0.04078681394457817
    },
    {
      "id": 28,
      "seek": 10920,
      "start": 1497.6500009155272,
      "end": 1502.2499993896483,
      "text": " aus dem Code und wir müssen lernen, wie wir diesen Kontext der Maschine geben.",
      "tokens": [
        50706,
        3437,
        1371,
        15549,
        674,
        1987,
        9013,
        36082,
        11,
        3355,
        1987,
        12862,
        20629,
        3828,
        1163,
        5224,
        36675,
        17191,
        13,
        50936
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2684193253517151,
      "compression_ratio": 1.6007194519042969,
      "no_speech_prob": 0.04078681394457817
    },
    {
      "id": 29,
      "seek": 10920,
      "start": 1502.2499993896483,
      "end": 1507.9700006103515,
      "text": " Und da ist, glaube ich, dann eben eine gute Architekturarbeit wieder von Vorteil, wenn",
      "tokens": [
        50936,
        2719,
        1120,
        1418,
        11,
        13756,
        1893,
        11,
        3594,
        11375,
        3018,
        21476,
        10984,
        642,
        2320,
        28586,
        9407,
        6216,
        2957,
        46968,
        388,
        11,
        4797,
        51222
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2684193253517151,
      "compression_ratio": 1.6007194519042969,
      "no_speech_prob": 0.04078681394457817
    },
    {
      "id": 30,
      "seek": 10920,
      "start": 1507.9700006103515,
      "end": 1513.7299951171874,
      "text": " wir, also ich habe zum Beispiel mal die Maschine gefragt, hier das Arc 42 Template für Software",
      "tokens": [
        51222,
        1987,
        11,
        611,
        1893,
        6015,
        5919,
        13772,
        2806,
        978,
        5224,
        36675,
        42638,
        11,
        3296,
        1482,
        21727,
        14034,
        39563,
        473,
        2959,
        27428,
        51510
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2684193253517151,
      "compression_ratio": 1.6007194519042969,
      "no_speech_prob": 0.04078681394457817
    },
    {
      "id": 31,
      "seek": 10920,
      "start": 1513.7299951171874,
      "end": 1515.7700036621093,
      "text": " Architektur Dokumentation.",
      "tokens": [
        51510,
        10984,
        642,
        2320,
        374,
        29768,
        2206,
        399,
        13,
        51612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2684193253517151,
      "compression_ratio": 1.6007194519042969,
      "no_speech_prob": 0.04078681394457817
    },
    {
      "id": 32,
      "seek": 10920,
      "start": 1515.7700036621093,
      "end": 1519.6499932861327,
      "text": " Welche Bereiche aus diesem Template sind für dich interessant?",
      "tokens": [
        51612,
        3778,
        1876,
        17684,
        9304,
        3437,
        10975,
        39563,
        473,
        3290,
        2959,
        10390,
        37748,
        30,
        51806
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2684193253517151,
      "compression_ratio": 1.6007194519042969,
      "no_speech_prob": 0.04078681394457817
    },
    {
      "id": 33,
      "seek": 13804,
      "start": 1519.6499932861327,
      "end": 1525.6499932861327,
      "text": " Welche weniger? Und das war ganz super spannend, weil es eigentlich gesagt hat, fast alle",
      "tokens": [
        50364,
        3778,
        1876,
        23224,
        30,
        2719,
        1482,
        1516,
        6312,
        1687,
        49027,
        11,
        7689,
        785,
        10926,
        12260,
        2385,
        11,
        2370,
        5430,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22518830001354218,
      "compression_ratio": 1.5419846773147583,
      "no_speech_prob": 0.014496960677206516
    },
    {
      "id": 34,
      "seek": 13804,
      "start": 1525.6499932861327,
      "end": 1530.8899987792968,
      "text": " Bereiche. Mir ist hängen geblieben, dass die Maschine gesagt hat, das Glossar, das ist",
      "tokens": [
        50664,
        17684,
        9304,
        13,
        9421,
        1418,
        276,
        43921,
        1519,
        5199,
        38243,
        11,
        2658,
        978,
        5224,
        36675,
        12260,
        2385,
        11,
        1482,
        5209,
        772,
        289,
        11,
        1482,
        1418,
        50926
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22518830001354218,
      "compression_ratio": 1.5419846773147583,
      "no_speech_prob": 0.014496960677206516
    },
    {
      "id": 35,
      "seek": 13804,
      "start": 1530.8899987792968,
      "end": 1535.1699975585936,
      "text": " jetzt sowas, die Fachbegriffe kennt die Maschine.",
      "tokens": [
        50926,
        4354,
        19766,
        296,
        11,
        978,
        38213,
        650,
        861,
        31387,
        37682,
        978,
        5224,
        36675,
        13,
        51140
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22518830001354218,
      "compression_ratio": 1.5419846773147583,
      "no_speech_prob": 0.014496960677206516
    },
    {
      "id": 36,
      "seek": 13804,
      "start": 1535.1699975585936,
      "end": 1540.7299951171874,
      "text": " Aber ansonsten ist es halt wichtig, diesen Kontext für die Softwareentwicklung der Maschine",
      "tokens": [
        51140,
        5992,
        1567,
        4068,
        268,
        1418,
        785,
        12479,
        13621,
        11,
        12862,
        20629,
        3828,
        2959,
        978,
        27428,
        317,
        16038,
        17850,
        1163,
        5224,
        36675,
        51418
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22518830001354218,
      "compression_ratio": 1.5419846773147583,
      "no_speech_prob": 0.014496960677206516
    },
    {
      "id": 37,
      "seek": 13804,
      "start": 1540.7299951171874,
      "end": 1548.61,
      "text": " mitzugeben. André, du hattest auch noch gesagt, dass du so Benchmarks beobachtest.",
      "tokens": [
        51418,
        2194,
        46285,
        1799,
        13,
        400,
        10521,
        11,
        1581,
        276,
        1591,
        377,
        2168,
        3514,
        12260,
        11,
        2658,
        1581,
        370,
        3964,
        339,
        37307,
        312,
        996,
        3589,
        377,
        13,
        51812
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22518830001354218,
      "compression_ratio": 1.5419846773147583,
      "no_speech_prob": 0.014496960677206516
    },
    {
      "id": 38,
      "seek": 16804,
      "start": 1550.6499932861327,
      "end": 1555.6499932861327,
      "text": " Ja, also ich würde widersprechen, dass die LLMs nicht besser werden.",
      "tokens": [
        50414,
        3530,
        11,
        611,
        1893,
        11942,
        5274,
        433,
        38951,
        11,
        2658,
        978,
        441,
        43,
        26386,
        1979,
        18021,
        4604,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28228482604026794,
      "compression_ratio": 1.6507352590560913,
      "no_speech_prob": 0.012220497243106365
    },
    {
      "id": 39,
      "seek": 16804,
      "start": 1555.6499932861327,
      "end": 1559.0500024414061,
      "text": " Also ich glaube, sie werden in jeder Dimension aktuell noch immer besser.",
      "tokens": [
        50664,
        2743,
        1893,
        13756,
        11,
        2804,
        4604,
        294,
        19610,
        20975,
        3378,
        36267,
        3514,
        5578,
        18021,
        13,
        50834
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28228482604026794,
      "compression_ratio": 1.6507352590560913,
      "no_speech_prob": 0.012220497243106365
    },
    {
      "id": 40,
      "seek": 16804,
      "start": 1559.0500024414061,
      "end": 1561.7700036621093,
      "text": " Und nach LLMs gibt es dann die nächsten Modelle.",
      "tokens": [
        50834,
        2719,
        5168,
        441,
        43,
        26386,
        6089,
        785,
        3594,
        978,
        19101,
        6583,
        4434,
        13,
        50970
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28228482604026794,
      "compression_ratio": 1.6507352590560913,
      "no_speech_prob": 0.012220497243106365
    },
    {
      "id": 41,
      "seek": 16804,
      "start": 1561.7700036621093,
      "end": 1566.850005493164,
      "text": " Und vielleicht ist doch das Transformer-Modell, auf dem das alles basiert, quasi nicht mehr",
      "tokens": [
        50970,
        2719,
        12547,
        1418,
        9243,
        1482,
        27938,
        260,
        12,
        44,
        378,
        898,
        11,
        2501,
        1371,
        1482,
        7874,
        987,
        4859,
        11,
        20954,
        1979,
        5417,
        51224
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28228482604026794,
      "compression_ratio": 1.6507352590560913,
      "no_speech_prob": 0.012220497243106365
    },
    {
      "id": 42,
      "seek": 16804,
      "start": 1566.850005493164,
      "end": 1569.089995727539,
      "text": " dann quasi ausreichend.",
      "tokens": [
        51224,
        3594,
        20954,
        3437,
        12594,
        521,
        13,
        51336
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28228482604026794,
      "compression_ratio": 1.6507352590560913,
      "no_speech_prob": 0.012220497243106365
    },
    {
      "id": 43,
      "seek": 16804,
      "start": 1569.089995727539,
      "end": 1575.1300042724608,
      "text": " Aber ich glaube, wir reiten gerade eine große Welle, gerade die Menge an LLMs und",
      "tokens": [
        51336,
        5992,
        1893,
        13756,
        11,
        1987,
        319,
        6009,
        12117,
        3018,
        19691,
        1042,
        68,
        11,
        12117,
        978,
        40723,
        364,
        441,
        43,
        26386,
        674,
        51638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28228482604026794,
      "compression_ratio": 1.6507352590560913,
      "no_speech_prob": 0.012220497243106365
    },
    {
      "id": 44,
      "seek": 16804,
      "start": 1575.1300042724608,
      "end": 1577.9300073242186,
      "text": " Updates. Ich gebe dir recht, gibt nicht so viele große.",
      "tokens": [
        51638,
        5858,
        67,
        1024,
        13,
        3141,
        29073,
        4746,
        24261,
        11,
        6089,
        1979,
        370,
        9693,
        19691,
        13,
        51778
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28228482604026794,
      "compression_ratio": 1.6507352590560913,
      "no_speech_prob": 0.012220497243106365
    },
    {
      "id": 45,
      "seek": 19632,
      "start": 1578.570006713867,
      "end": 1581.8899987792968,
      "text": " Tendenziell könnte man das auch als kritisch einstufen.",
      "tokens": [
        50396,
        314,
        8896,
        3283,
        285,
        17646,
        587,
        1482,
        2168,
        3907,
        42825,
        5494,
        1343,
        372,
        19890,
        13,
        50562
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31703490018844604,
      "compression_ratio": 1.498084306716919,
      "no_speech_prob": 0.028846371918916702
    },
    {
      "id": 46,
      "seek": 19632,
      "start": 1581.8899987792968,
      "end": 1590.809996948242,
      "text": " Aber wenn man sich anschaut, was wirklich die großen, die Tech, die Magnificent Seven,",
      "tokens": [
        50562,
        5992,
        4797,
        587,
        3041,
        31508,
        1375,
        11,
        390,
        9696,
        978,
        23076,
        11,
        978,
        13795,
        11,
        978,
        19664,
        1089,
        317,
        14868,
        11,
        51008
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31703490018844604,
      "compression_ratio": 1.498084306716919,
      "no_speech_prob": 0.028846371918916702
    },
    {
      "id": 47,
      "seek": 19632,
      "start": 1590.809996948242,
      "end": 1593.5299981689452,
      "text": " mein Gott, man sollte Wörter wählen, die man aussprechen kann.",
      "tokens": [
        51008,
        10777,
        19133,
        11,
        587,
        18042,
        343,
        2311,
        391,
        24787,
        6698,
        11,
        978,
        587,
        5730,
        38951,
        4028,
        13,
        51144
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31703490018844604,
      "compression_ratio": 1.498084306716919,
      "no_speech_prob": 0.028846371918916702
    },
    {
      "id": 48,
      "seek": 19632,
      "start": 1593.5299981689452,
      "end": 1599.8899987792968,
      "text": " Also die großen Tech-Companies quasi immer wieder deployen und zur Verfügung stellen.",
      "tokens": [
        51144,
        2743,
        978,
        23076,
        13795,
        12,
        14627,
        6040,
        530,
        20954,
        5578,
        6216,
        7274,
        268,
        674,
        7147,
        43026,
        24407,
        13,
        51462
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31703490018844604,
      "compression_ratio": 1.498084306716919,
      "no_speech_prob": 0.028846371918916702
    },
    {
      "id": 49,
      "seek": 19632,
      "start": 1599.8899987792968,
      "end": 1605.2499993896483,
      "text": " Dann würde ich sagen, ist es signifikant, dass was man in diesem Benchmark sieht, vielleicht",
      "tokens": [
        51462,
        7455,
        11942,
        1893,
        8360,
        11,
        1418,
        785,
        1465,
        45475,
        394,
        11,
        2658,
        390,
        587,
        294,
        10975,
        3964,
        339,
        5638,
        14289,
        11,
        12547,
        51730
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31703490018844604,
      "compression_ratio": 1.498084306716919,
      "no_speech_prob": 0.028846371918916702
    },
    {
      "id": 50,
      "seek": 22364,
      "start": 1605.2899926757811,
      "end": 1609.850005493164,
      "text": " das als Einschub noch, das sind natürlich immer getunte Modelle und das ist sicherlich",
      "tokens": [
        50366,
        1482,
        3907,
        22790,
        339,
        836,
        3514,
        11,
        1482,
        3290,
        8762,
        5578,
        483,
        409,
        975,
        6583,
        4434,
        674,
        1482,
        1418,
        18623,
        1739,
        50594
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30447548627853394,
      "compression_ratio": 1.5841923952102661,
      "no_speech_prob": 0.014279285445809364
    },
    {
      "id": 51,
      "seek": 22364,
      "start": 1609.850005493164,
      "end": 1614.6900018310546,
      "text": " auch eine Ebene, also ich würde jetzt zum Beispiel nicht sagen, dass man in den ganz",
      "tokens": [
        50594,
        2168,
        3018,
        20418,
        1450,
        11,
        611,
        1893,
        11942,
        4354,
        5919,
        13772,
        1979,
        8360,
        11,
        2658,
        587,
        294,
        1441,
        6312,
        50836
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30447548627853394,
      "compression_ratio": 1.5841923952102661,
      "no_speech_prob": 0.014279285445809364
    },
    {
      "id": 52,
      "seek": 22364,
      "start": 1614.6900018310546,
      "end": 1620.2499993896483,
      "text": " normalen Gemini 1 5 Pro halt einfach seinen Software-Repo reinschmeißen sollte und darauf",
      "tokens": [
        50836,
        2710,
        268,
        22894,
        3812,
        502,
        1025,
        1705,
        12479,
        7281,
        24427,
        27428,
        12,
        25554,
        78,
        47200,
        339,
        1398,
        6230,
        268,
        18042,
        674,
        18654,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30447548627853394,
      "compression_ratio": 1.5841923952102661,
      "no_speech_prob": 0.014279285445809364
    },
    {
      "id": 53,
      "seek": 22364,
      "start": 1620.2499993896483,
      "end": 1622.330001220703,
      "text": " dann quasi basierend entwickelt.",
      "tokens": [
        51114,
        3594,
        20954,
        987,
        811,
        521,
        43208,
        13,
        51218
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30447548627853394,
      "compression_ratio": 1.5841923952102661,
      "no_speech_prob": 0.014279285445809364
    },
    {
      "id": 54,
      "seek": 22364,
      "start": 1622.330001220703,
      "end": 1627.6900018310546,
      "text": " Also das, was man in diesen Benchmarks quasi sieht, sind eigentlich durch die Bank weg",
      "tokens": [
        51218,
        2743,
        1482,
        11,
        390,
        587,
        294,
        12862,
        3964,
        339,
        37307,
        20954,
        14289,
        11,
        3290,
        10926,
        7131,
        978,
        8915,
        15565,
        51486
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30447548627853394,
      "compression_ratio": 1.5841923952102661,
      "no_speech_prob": 0.014279285445809364
    },
    {
      "id": 55,
      "seek": 22364,
      "start": 1628.0099938964843,
      "end": 1632.4100030517577,
      "text": " alles getunte Modelle, alles quasi auch mit entsprechendem Tooling versehen.",
      "tokens": [
        51502,
        7874,
        483,
        7070,
        6583,
        4434,
        11,
        7874,
        20954,
        2168,
        2194,
        47823,
        443,
        15934,
        278,
        7996,
        2932,
        13,
        51722
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30447548627853394,
      "compression_ratio": 1.5841923952102661,
      "no_speech_prob": 0.014279285445809364
    },
    {
      "id": 56,
      "seek": 25080,
      "start": 1632.4100030517577,
      "end": 1633.61,
      "text": " Da bin ich bei euch.",
      "tokens": [
        50364,
        3933,
        5171,
        1893,
        4643,
        10403,
        13,
        50424
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3044426143169403,
      "compression_ratio": 1.4853556156158447,
      "no_speech_prob": 0.12920795381069183
    },
    {
      "id": 57,
      "seek": 25080,
      "start": 1634.4499963378905,
      "end": 1635.570006713867,
      "text": " Das fehlt.",
      "tokens": [
        50466,
        2846,
        47994,
        13,
        50522
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3044426143169403,
      "compression_ratio": 1.4853556156158447,
      "no_speech_prob": 0.12920795381069183
    },
    {
      "id": 58,
      "seek": 25080,
      "start": 1637.570006713867,
      "end": 1641.370009765625,
      "text": " Aber vor dem Hintergrund, klar, jede Meinung akzeptiert.",
      "tokens": [
        50622,
        5992,
        4245,
        1371,
        35006,
        23701,
        11,
        14743,
        11,
        34039,
        36519,
        9308,
        32082,
        4859,
        13,
        50812
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3044426143169403,
      "compression_ratio": 1.4853556156158447,
      "no_speech_prob": 0.12920795381069183
    },
    {
      "id": 59,
      "seek": 25080,
      "start": 1641.5300134277343,
      "end": 1645.330001220703,
      "text": " Ich würde es anders sehen, einfach aus der puren Erfahrung.",
      "tokens": [
        50820,
        3141,
        11942,
        785,
        17999,
        11333,
        11,
        7281,
        3437,
        1163,
        6075,
        77,
        49318,
        13,
        51010
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3044426143169403,
      "compression_ratio": 1.4853556156158447,
      "no_speech_prob": 0.12920795381069183
    },
    {
      "id": 60,
      "seek": 25080,
      "start": 1646.370009765625,
      "end": 1648.1699975585936,
      "text": " Ich versuche wirklich alles auszuprobieren.",
      "tokens": [
        51062,
        3141,
        1774,
        17545,
        9696,
        7874,
        3437,
        89,
        1010,
        16614,
        5695,
        13,
        51152
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3044426143169403,
      "compression_ratio": 1.4853556156158447,
      "no_speech_prob": 0.12920795381069183
    },
    {
      "id": 61,
      "seek": 25080,
      "start": 1648.1699975585936,
      "end": 1652.6899865722655,
      "text": " Ich muss Stefan ein bisschen beipflichten, wirkt ab und zu ein bisschen wie Magie, wenn",
      "tokens": [
        51152,
        3141,
        6425,
        32158,
        1343,
        10763,
        312,
        647,
        69,
        20238,
        268,
        11,
        1987,
        2320,
        410,
        674,
        2164,
        1343,
        10763,
        3355,
        6395,
        414,
        11,
        4797,
        51378
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3044426143169403,
      "compression_ratio": 1.4853556156158447,
      "no_speech_prob": 0.12920795381069183
    },
    {
      "id": 62,
      "seek": 25080,
      "start": 1652.6899865722655,
      "end": 1656.6899865722655,
      "text": " man sieht, wie gewisse Tools halt quasi einem da auch Vorschläge machen.",
      "tokens": [
        51378,
        587,
        14289,
        11,
        3355,
        6906,
        7746,
        30302,
        12479,
        20954,
        6827,
        1120,
        2168,
        31438,
        11439,
        737,
        432,
        7069,
        13,
        51578
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3044426143169403,
      "compression_ratio": 1.4853556156158447,
      "no_speech_prob": 0.12920795381069183
    },
    {
      "id": 63,
      "seek": 27508,
      "start": 1657.6899865722655,
      "end": 1663.4900048828124,
      "text": " Ich würde vielleicht kurz einwerfen, wenn ich eine Schwäche beim Thema LLM sehe, ist",
      "tokens": [
        50414,
        3141,
        11942,
        12547,
        20465,
        1343,
        1554,
        6570,
        11,
        4797,
        1893,
        3018,
        17576,
        32664,
        13922,
        16306,
        441,
        43,
        44,
        35995,
        11,
        1418,
        50704
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38474950194358826,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.0074579548090696335
    },
    {
      "id": 64,
      "seek": 27508,
      "start": 1663.4900048828124,
      "end": 1665.4900048828124,
      "text": " denn den Fokus auf LLM.",
      "tokens": [
        50704,
        10471,
        1441,
        479,
        38480,
        2501,
        441,
        43,
        44,
        13,
        50804
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38474950194358826,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.0074579548090696335
    },
    {
      "id": 65,
      "seek": 27508,
      "start": 1665.4900048828124,
      "end": 1666.849990234375,
      "text": " Ich bin mir nicht ganz sicher.",
      "tokens": [
        50804,
        3141,
        5171,
        3149,
        1979,
        6312,
        18623,
        13,
        50872
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38474950194358826,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.0074579548090696335
    },
    {
      "id": 66,
      "seek": 27508,
      "start": 1667.8899987792968,
      "end": 1671.6500085449218,
      "text": " Das ist ein bisschen wie mit Elektroautos und mit Wasserstoffautos.",
      "tokens": [
        50924,
        2846,
        1418,
        1343,
        10763,
        3355,
        2194,
        40321,
        340,
        1375,
        329,
        674,
        2194,
        17351,
        23636,
        1375,
        329,
        13,
        51112
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38474950194358826,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.0074579548090696335
    },
    {
      "id": 67,
      "seek": 27508,
      "start": 1671.9300073242186,
      "end": 1677.61,
      "text": " Wasserstoff ist vielleicht bestimmte Sachen besser, aber die Masse an Ladeinfrastruktur",
      "tokens": [
        51126,
        17351,
        23636,
        1418,
        12547,
        35180,
        975,
        26074,
        18021,
        11,
        4340,
        978,
        5224,
        405,
        364,
        441,
        762,
        19920,
        4148,
        31543,
        51410
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38474950194358826,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.0074579548090696335
    },
    {
      "id": 68,
      "seek": 27508,
      "start": 1677.61,
      "end": 1683.8899987792968,
      "text": " und so weiter ist ja klar, dass das Elektro besser ist, bequemer und andere Sachen hat.",
      "tokens": [
        51410,
        674,
        370,
        8988,
        1418,
        2784,
        14743,
        11,
        2658,
        1482,
        40321,
        340,
        18021,
        1418,
        11,
        312,
        358,
        29660,
        674,
        10490,
        26074,
        2385,
        13,
        51724
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38474950194358826,
      "compression_ratio": 1.5909091234207153,
      "no_speech_prob": 0.0074579548090696335
    },
    {
      "id": 69,
      "seek": 30228,
      "start": 1684.8100122070311,
      "end": 1687.8899987792968,
      "text": " Und die Frage ist, ob das mit LLM und anderen Modellen ähnlich ist.",
      "tokens": [
        50410,
        2719,
        978,
        13685,
        1418,
        11,
        1111,
        1482,
        2194,
        441,
        43,
        44,
        674,
        11122,
        6583,
        8581,
        3078,
        12071,
        2081,
        339,
        1418,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980567216873169,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.025539526715874672
    },
    {
      "id": 70,
      "seek": 30228,
      "start": 1687.8899987792968,
      "end": 1692.6500085449218,
      "text": " Also gibt es bessere Modelle für bestimmte Sachen oder eigene Modelle?",
      "tokens": [
        50564,
        2743,
        6089,
        785,
        42410,
        323,
        6583,
        4434,
        2959,
        35180,
        975,
        26074,
        4513,
        38549,
        6583,
        4434,
        30,
        50802
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980567216873169,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.025539526715874672
    },
    {
      "id": 71,
      "seek": 30228,
      "start": 1692.6500085449218,
      "end": 1697.7700036621093,
      "text": " Sollte ich eigene Modelle machen oder trainieren oder eben nicht?",
      "tokens": [
        50802,
        407,
        285,
        975,
        1893,
        38549,
        6583,
        4434,
        7069,
        4513,
        3847,
        5695,
        4513,
        11375,
        1979,
        30,
        51058
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980567216873169,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.025539526715874672
    },
    {
      "id": 72,
      "seek": 30228,
      "start": 1698.4099877929686,
      "end": 1699.61,
      "text": " Kann das alles LLM?",
      "tokens": [
        51090,
        29074,
        1482,
        7874,
        441,
        43,
        44,
        30,
        51150
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980567216873169,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.025539526715874672
    },
    {
      "id": 73,
      "seek": 30228,
      "start": 1699.9699853515624,
      "end": 1701.7700036621093,
      "text": " Da bin ich mir zum Beispiel noch nicht so sicher.",
      "tokens": [
        51168,
        3933,
        5171,
        1893,
        3149,
        5919,
        13772,
        3514,
        1979,
        370,
        18623,
        13,
        51258
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980567216873169,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.025539526715874672
    },
    {
      "id": 74,
      "seek": 30228,
      "start": 1702.1699975585936,
      "end": 1706.4900048828124,
      "text": " Ich habe einen sehr erfolgreichen Kunden, der im Sinne von Jobs to be done.",
      "tokens": [
        51278,
        3141,
        6015,
        4891,
        5499,
        39447,
        29119,
        38192,
        11,
        1163,
        566,
        47041,
        2957,
        29169,
        281,
        312,
        1096,
        13,
        51494
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980567216873169,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.025539526715874672
    },
    {
      "id": 75,
      "seek": 30228,
      "start": 1707.6899865722655,
      "end": 1713.569991455078,
      "text": " Früher Software entwickelt hat für Kunden und jetzt Probleme für Kunden löst, indem",
      "tokens": [
        51554,
        1526,
        27775,
        27428,
        43208,
        2385,
        2959,
        38192,
        674,
        4354,
        32891,
        2959,
        38192,
        25209,
        372,
        11,
        37185,
        51848
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2980567216873169,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.025539526715874672
    },
    {
      "id": 76,
      "seek": 33196,
      "start": 1713.61,
      "end": 1720.2100061035155,
      "text": " er eigene AIs trainiert, die dem Kunden bestimmte Arbeiten abnehmen.",
      "tokens": [
        50366,
        1189,
        38549,
        316,
        6802,
        3847,
        4859,
        11,
        978,
        1371,
        38192,
        35180,
        975,
        1587,
        16779,
        410,
        14669,
        13,
        50696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3364880084991455,
      "compression_ratio": 1.4854369163513184,
      "no_speech_prob": 0.0020828633569180965
    },
    {
      "id": 77,
      "seek": 33196,
      "start": 1721.2500146484374,
      "end": 1725.9699853515624,
      "text": " Also er hat vorher sehr tolle Tools gebaut, wo der Kunde so SaaS mäßig Sachen machen",
      "tokens": [
        50748,
        2743,
        1189,
        2385,
        29195,
        5499,
        281,
        2447,
        30302,
        49203,
        11,
        6020,
        1163,
        591,
        13271,
        370,
        49733,
        25117,
        2536,
        328,
        26074,
        7069,
        50984
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3364880084991455,
      "compression_ratio": 1.4854369163513184,
      "no_speech_prob": 0.0020828633569180965
    },
    {
      "id": 78,
      "seek": 33196,
      "start": 1725.9699853515624,
      "end": 1731.1299890136718,
      "text": " kann. Und jetzt nimmt er den Jobs ab und das macht er aber nicht mit LLM, sondern mit",
      "tokens": [
        50984,
        4028,
        13,
        2719,
        4354,
        38891,
        1189,
        1441,
        29169,
        410,
        674,
        1482,
        10857,
        1189,
        4340,
        1979,
        2194,
        441,
        43,
        44,
        11,
        11465,
        2194,
        51242
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3364880084991455,
      "compression_ratio": 1.4854369163513184,
      "no_speech_prob": 0.0020828633569180965
    },
    {
      "id": 79,
      "seek": 33196,
      "start": 1731.1299890136718,
      "end": 1736.4900048828124,
      "text": " eigenen Modellen. Und da bin ich nicht sicher, wo es dahin geht.",
      "tokens": [
        51242,
        28702,
        6583,
        8581,
        13,
        2719,
        1120,
        5171,
        1893,
        1979,
        18623,
        11,
        6020,
        785,
        16800,
        259,
        7095,
        13,
        51510
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3364880084991455,
      "compression_ratio": 1.4854369163513184,
      "no_speech_prob": 0.0020828633569180965
    },
    {
      "id": 80,
      "seek": 35488,
      "start": 1736.4900048828124,
      "end": 1744.1699975585936,
      "text": " Super, genau, Poliboy hat auch noch kurz ergänzt, angeblich ist die Hardware",
      "tokens": [
        50364,
        4548,
        11,
        12535,
        11,
        3635,
        897,
        939,
        2385,
        2168,
        3514,
        20465,
        26585,
        4029,
        2682,
        11,
        364,
        10848,
        1739,
        1418,
        978,
        11817,
        3039,
        50748
      ],
      "temperature": 0.0,
      "avg_logprob": -0.36927491426467896,
      "compression_ratio": 1.4637681245803833,
      "no_speech_prob": 0.10504935681819916
    },
    {
      "id": 81,
      "seek": 35488,
      "start": 1744.1699975585936,
      "end": 1748.1299890136718,
      "text": " Skalierbarkeit für LLMs fast erreicht, durch die noch Fortschritte gemacht werden",
      "tokens": [
        50748,
        7324,
        304,
        811,
        5356,
        9238,
        2959,
        441,
        43,
        26386,
        2370,
        46250,
        11,
        7131,
        978,
        3514,
        479,
        3299,
        339,
        81,
        9786,
        12293,
        4604,
        50946
      ],
      "temperature": 0.0,
      "avg_logprob": -0.36927491426467896,
      "compression_ratio": 1.4637681245803833,
      "no_speech_prob": 0.10504935681819916
    },
    {
      "id": 82,
      "seek": 35488,
      "start": 1748.1299890136718,
      "end": 1751.4900048828124,
      "text": " können. Das hat noch jemand geschrieben.",
      "tokens": [
        50946,
        6310,
        13,
        2846,
        2385,
        3514,
        21717,
        47397,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.36927491426467896,
      "compression_ratio": 1.4637681245803833,
      "no_speech_prob": 0.10504935681819916
    },
    {
      "id": 83,
      "seek": 35488,
      "start": 1751.4900048828124,
      "end": 1753.7299951171874,
      "text": " Und was ich wollte, Ralf hat sich entmutet.",
      "tokens": [
        51114,
        2719,
        390,
        1893,
        24509,
        11,
        497,
        1678,
        2385,
        3041,
        948,
        76,
        20364,
        13,
        51226
      ],
      "temperature": 0.0,
      "avg_logprob": -0.36927491426467896,
      "compression_ratio": 1.4637681245803833,
      "no_speech_prob": 0.10504935681819916
    },
    {
      "id": 84,
      "seek": 35488,
      "start": 1753.7299951171874,
      "end": 1754.7299951171874,
      "text": " Du willst dazu was sagen?",
      "tokens": [
        51226,
        5153,
        48355,
        13034,
        390,
        8360,
        30,
        51276
      ],
      "temperature": 0.0,
      "avg_logprob": -0.36927491426467896,
      "compression_ratio": 1.4637681245803833,
      "no_speech_prob": 0.10504935681819916
    },
    {
      "id": 85,
      "seek": 35488,
      "start": 1755.7299951171874,
      "end": 1757.9300073242186,
      "text": " Ich wollte es nur eben bestätigen.",
      "tokens": [
        51326,
        3141,
        24509,
        785,
        4343,
        11375,
        1151,
        3628,
        3213,
        13,
        51436
      ],
      "temperature": 0.0,
      "avg_logprob": -0.36927491426467896,
      "compression_ratio": 1.4637681245803833,
      "no_speech_prob": 0.10504935681819916
    },
    {
      "id": 86,
      "seek": 35488,
      "start": 1757.9300073242186,
      "end": 1762.090010986328,
      "text": " Also wir denken jetzt immer an Gen AI und die LLMs.",
      "tokens": [
        51436,
        2743,
        1987,
        28780,
        4354,
        5578,
        364,
        3632,
        7318,
        674,
        978,
        441,
        43,
        26386,
        13,
        51644
      ],
      "temperature": 0.0,
      "avg_logprob": -0.36927491426467896,
      "compression_ratio": 1.4637681245803833,
      "no_speech_prob": 0.10504935681819916
    },
    {
      "id": 87,
      "seek": 35488,
      "start": 1762.090010986328,
      "end": 1764.7299951171874,
      "text": " Und das ist natürlich ein großes Problem.",
      "tokens": [
        51644,
        2719,
        1482,
        1418,
        8762,
        1343,
        48875,
        11676,
        13,
        51776
      ],
      "temperature": 0.0,
      "avg_logprob": -0.36927491426467896,
      "compression_ratio": 1.4637681245803833,
      "no_speech_prob": 0.10504935681819916
    },
    {
      "id": 88,
      "seek": 38312,
      "start": 1765.7299951171874,
      "end": 1767.9300073242186,
      "text": " Das war eigentlich auch schon vorher ein Problem.",
      "tokens": [
        50414,
        2846,
        1516,
        10926,
        2168,
        4981,
        29195,
        1343,
        11676,
        13,
        50524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22430002689361572,
      "compression_ratio": 1.4280155897140503,
      "no_speech_prob": 0.0017273375997319818
    },
    {
      "id": 89,
      "seek": 38312,
      "start": 1767.9300073242186,
      "end": 1773.370009765625,
      "text": " Also ich habe mal gesucht, wie man Barcodes am besten decoden kann und bin auf viele",
      "tokens": [
        50524,
        2743,
        1893,
        6015,
        2806,
        5019,
        10084,
        11,
        3355,
        587,
        4156,
        66,
        4789,
        669,
        30930,
        979,
        33482,
        4028,
        674,
        5171,
        2501,
        9693,
        50796
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22430002689361572,
      "compression_ratio": 1.4280155897140503,
      "no_speech_prob": 0.0017273375997319818
    },
    {
      "id": 90,
      "seek": 38312,
      "start": 1773.370009765625,
      "end": 1779.61,
      "text": " Blogposts gestoßen, wo ein neuronales Netz trainiert worden ist, um die Barcodes zu",
      "tokens": [
        50796,
        46693,
        23744,
        82,
        7219,
        78,
        8989,
        11,
        6020,
        1343,
        12087,
        21523,
        279,
        38889,
        3847,
        4859,
        14054,
        1418,
        11,
        1105,
        978,
        4156,
        66,
        4789,
        2164,
        51108
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22430002689361572,
      "compression_ratio": 1.4280155897140503,
      "no_speech_prob": 0.0017273375997319818
    },
    {
      "id": 91,
      "seek": 38312,
      "start": 1779.61,
      "end": 1781.5300134277343,
      "text": " erkennen. Das ist natürlich blöd.",
      "tokens": [
        51108,
        45720,
        13,
        2846,
        1418,
        8762,
        888,
        29747,
        13,
        51204
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22430002689361572,
      "compression_ratio": 1.4280155897140503,
      "no_speech_prob": 0.0017273375997319818
    },
    {
      "id": 92,
      "seek": 38312,
      "start": 1781.5300134277343,
      "end": 1786.4099877929686,
      "text": " Ja, und genauso könnte ich jetzt LLMs für solche Tätigkeiten nutzen.",
      "tokens": [
        51204,
        3530,
        11,
        674,
        37694,
        17646,
        1893,
        4354,
        441,
        43,
        26386,
        2959,
        29813,
        314,
        3628,
        37545,
        36905,
        13,
        51448
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22430002689361572,
      "compression_ratio": 1.4280155897140503,
      "no_speech_prob": 0.0017273375997319818
    },
    {
      "id": 93,
      "seek": 38312,
      "start": 1786.4099877929686,
      "end": 1789.4499963378905,
      "text": " Aber damit verschwende ich nur Energie.",
      "tokens": [
        51448,
        5992,
        9479,
        20563,
        86,
        5445,
        1893,
        4343,
        35309,
        13,
        51600
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22430002689361572,
      "compression_ratio": 1.4280155897140503,
      "no_speech_prob": 0.0017273375997319818
    },
    {
      "id": 94,
      "seek": 40784,
      "start": 1789.569991455078,
      "end": 1795.7700036621093,
      "text": " Und wir benutzen jetzt vor allem die LLMs, weil sie eben schon da sind und weil sie",
      "tokens": [
        50370,
        2719,
        1987,
        38424,
        2904,
        4354,
        4245,
        17585,
        978,
        441,
        43,
        26386,
        11,
        7689,
        2804,
        11375,
        4981,
        1120,
        3290,
        674,
        7689,
        2804,
        50680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2003272920846939,
      "compression_ratio": 1.5311203002929688,
      "no_speech_prob": 0.20901302993297577
    },
    {
      "id": 95,
      "seek": 40784,
      "start": 1795.7700036621093,
      "end": 1798.569991455078,
      "text": " uns die Programmierprobleme lösen.",
      "tokens": [
        50680,
        2693,
        978,
        48244,
        811,
        47419,
        68,
        25209,
        6748,
        13,
        50820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2003272920846939,
      "compression_ratio": 1.5311203002929688,
      "no_speech_prob": 0.20901302993297577
    },
    {
      "id": 96,
      "seek": 40784,
      "start": 1798.569991455078,
      "end": 1805.5300134277343,
      "text": " Aber es sind so viele Leute an den Unis und in den Firmen damit beschäftigt, diese",
      "tokens": [
        50820,
        5992,
        785,
        3290,
        370,
        9693,
        13495,
        364,
        1441,
        1156,
        271,
        674,
        294,
        1441,
        28164,
        2558,
        9479,
        38768,
        5828,
        11,
        6705,
        51168
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2003272920846939,
      "compression_ratio": 1.5311203002929688,
      "no_speech_prob": 0.20901302993297577
    },
    {
      "id": 97,
      "seek": 40784,
      "start": 1805.5300134277343,
      "end": 1812.2100061035155,
      "text": " Large-Language-Models in Small-Language-Models umzuwandeln, damit sie eben, wie",
      "tokens": [
        51168,
        33092,
        12,
        43,
        656,
        20473,
        12,
        44,
        378,
        1625,
        294,
        15287,
        12,
        43,
        656,
        20473,
        12,
        44,
        378,
        1625,
        1105,
        11728,
        33114,
        9878,
        11,
        9479,
        2804,
        11375,
        11,
        3355,
        51502
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2003272920846939,
      "compression_ratio": 1.5311203002929688,
      "no_speech_prob": 0.20901302993297577
    },
    {
      "id": 98,
      "seek": 40784,
      "start": 1812.2100061035155,
      "end": 1817.5300134277343,
      "text": " André vorhin auch sagte, on the edge laufen können und auf kleineren Devices, damit",
      "tokens": [
        51502,
        400,
        10521,
        4245,
        10876,
        2168,
        36771,
        11,
        322,
        264,
        4691,
        41647,
        6310,
        674,
        2501,
        39496,
        268,
        9096,
        1473,
        11,
        9479,
        51768
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2003272920846939,
      "compression_ratio": 1.5311203002929688,
      "no_speech_prob": 0.20901302993297577
    },
    {
      "id": 99,
      "seek": 43592,
      "start": 1817.5300134277343,
      "end": 1820.330001220703,
      "text": " man diese Technologie jetzt in den Griff bekommt.",
      "tokens": [
        50364,
        587,
        6705,
        8337,
        20121,
        4354,
        294,
        1441,
        23765,
        33429,
        13,
        50504
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28216081857681274,
      "compression_ratio": 1.547169804573059,
      "no_speech_prob": 0.15178260207176208
    },
    {
      "id": 100,
      "seek": 43592,
      "start": 1820.330001220703,
      "end": 1825.6899865722655,
      "text": " Und das ist weiterhin, deswegen finde ich diese Unterscheidung in Gen-AI und KI auch so",
      "tokens": [
        50504,
        2719,
        1482,
        1418,
        42480,
        11,
        26482,
        17841,
        1893,
        6705,
        30240,
        1876,
        327,
        1063,
        294,
        3632,
        12,
        48698,
        674,
        47261,
        2168,
        370,
        50772
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28216081857681274,
      "compression_ratio": 1.547169804573059,
      "no_speech_prob": 0.15178260207176208
    },
    {
      "id": 101,
      "seek": 43592,
      "start": 1825.6899865722655,
      "end": 1831.569991455078,
      "text": " wichtig. Das ganz normale Machine Learning ist weiterhin sehr, sehr wichtig, weil wir",
      "tokens": [
        50772,
        13621,
        13,
        2846,
        6312,
        43646,
        22155,
        15205,
        1418,
        42480,
        5499,
        11,
        5499,
        13621,
        11,
        7689,
        1987,
        51066
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28216081857681274,
      "compression_ratio": 1.547169804573059,
      "no_speech_prob": 0.15178260207176208
    },
    {
      "id": 102,
      "seek": 43592,
      "start": 1831.569991455078,
      "end": 1838.6899865722655,
      "text": " damit schon vor Gen-AI sehr gut Probleme gelöst haben und das nicht außer Acht lassen",
      "tokens": [
        51066,
        9479,
        4981,
        4245,
        3632,
        12,
        48698,
        5499,
        5228,
        32891,
        4087,
        36995,
        3084,
        674,
        1482,
        1979,
        39428,
        316,
        4701,
        16168,
        51422
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28216081857681274,
      "compression_ratio": 1.547169804573059,
      "no_speech_prob": 0.15178260207176208
    },
    {
      "id": 103,
      "seek": 43592,
      "start": 1838.6899865722655,
      "end": 1839.6899865722655,
      "text": " sollten.",
      "tokens": [
        51422,
        29096,
        13,
        51472
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28216081857681274,
      "compression_ratio": 1.547169804573059,
      "no_speech_prob": 0.15178260207176208
    },
    {
      "id": 104,
      "seek": 43592,
      "start": 1842.61,
      "end": 1846.61,
      "text": " Also hier im Chat ist total viel, aber ich glaube, wenn ich jetzt hier alles vorlese, was",
      "tokens": [
        51618,
        2743,
        3296,
        566,
        27503,
        1418,
        3217,
        5891,
        11,
        4340,
        1893,
        13756,
        11,
        4797,
        1893,
        4354,
        3296,
        7874,
        4245,
        904,
        68,
        11,
        390,
        51818
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28216081857681274,
      "compression_ratio": 1.547169804573059,
      "no_speech_prob": 0.15178260207176208
    },
    {
      "id": 105,
      "seek": 46500,
      "start": 1846.61,
      "end": 1850.370009765625,
      "text": " gerade im Chat passiert, dann kommen wir nie zu dem nächsten Thema, was wir eigentlich",
      "tokens": [
        50364,
        12117,
        566,
        27503,
        21671,
        11,
        3594,
        11729,
        1987,
        2838,
        2164,
        1371,
        19101,
        16306,
        11,
        390,
        1987,
        10926,
        50552
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2632986903190613,
      "compression_ratio": 1.6793650388717651,
      "no_speech_prob": 0.020950641483068466
    },
    {
      "id": 106,
      "seek": 46500,
      "start": 1850.370009765625,
      "end": 1854.1299890136718,
      "text": " besprechen würden und wollten, und zwar, was können wir denn überhaupt heute schon",
      "tokens": [
        50552,
        4097,
        38951,
        27621,
        674,
        46019,
        11,
        674,
        19054,
        11,
        390,
        6310,
        1987,
        10471,
        20023,
        9801,
        4981,
        50740
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2632986903190613,
      "compression_ratio": 1.6793650388717651,
      "no_speech_prob": 0.020950641483068466
    },
    {
      "id": 107,
      "seek": 46500,
      "start": 1854.1299890136718,
      "end": 1858.2899926757811,
      "text": " tun? Also was bedeutet das heute für uns, das ganze Thema mit der KI?",
      "tokens": [
        50740,
        4267,
        30,
        2743,
        390,
        27018,
        1482,
        9801,
        2959,
        2693,
        11,
        1482,
        18898,
        16306,
        2194,
        1163,
        47261,
        30,
        50948
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2632986903190613,
      "compression_ratio": 1.6793650388717651,
      "no_speech_prob": 0.020950641483068466
    },
    {
      "id": 108,
      "seek": 46500,
      "start": 1859.849990234375,
      "end": 1863.0099938964843,
      "text": " Ihr hattet schon, als ich in den Call kam, alle fleißig diskutiert.",
      "tokens": [
        51026,
        14773,
        276,
        1591,
        302,
        4981,
        11,
        3907,
        1893,
        294,
        1441,
        7807,
        9727,
        11,
        5430,
        7025,
        6230,
        328,
        36760,
        4859,
        13,
        51184
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2632986903190613,
      "compression_ratio": 1.6793650388717651,
      "no_speech_prob": 0.020950641483068466
    },
    {
      "id": 109,
      "seek": 46500,
      "start": 1863.0500024414061,
      "end": 1867.0500024414061,
      "text": " Von daher würde ich einfach mal die Frage in den Raum werfen.",
      "tokens": [
        51186,
        20700,
        36971,
        11942,
        1893,
        7281,
        2806,
        978,
        13685,
        294,
        1441,
        31359,
        2612,
        6570,
        13,
        51386
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2632986903190613,
      "compression_ratio": 1.6793650388717651,
      "no_speech_prob": 0.020950641483068466
    },
    {
      "id": 110,
      "seek": 46500,
      "start": 1867.1299890136718,
      "end": 1869.8100122070311,
      "text": " Was bedeutet das heute für mich?",
      "tokens": [
        51390,
        3027,
        27018,
        1482,
        9801,
        2959,
        6031,
        30,
        51524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2632986903190613,
      "compression_ratio": 1.6793650388717651,
      "no_speech_prob": 0.020950641483068466
    },
    {
      "id": 111,
      "seek": 46500,
      "start": 1869.8100122070311,
      "end": 1871.330001220703,
      "text": " Was sollte ich heute schon tun?",
      "tokens": [
        51524,
        3027,
        18042,
        1893,
        9801,
        4981,
        4267,
        30,
        51600
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2632986903190613,
      "compression_ratio": 1.6793650388717651,
      "no_speech_prob": 0.020950641483068466
    },
    {
      "id": 112,
      "seek": 46500,
      "start": 1871.5300134277343,
      "end": 1875.4499963378905,
      "text": " Und Eberhard sah aus, als wollte er reden, ist den Kopf zur Seite gedreht und kurz den",
      "tokens": [
        51610,
        2719,
        462,
        607,
        21491,
        19292,
        3437,
        11,
        3907,
        24509,
        1189,
        26447,
        11,
        1418,
        1441,
        28231,
        7147,
        19748,
        19238,
        265,
        357,
        674,
        20465,
        1441,
        51806
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2632986903190613,
      "compression_ratio": 1.6793650388717651,
      "no_speech_prob": 0.020950641483068466
    },
    {
      "id": 113,
      "seek": 49384,
      "start": 1875.4499963378905,
      "end": 1876.4499963378905,
      "text": " Mund aufgehoben.",
      "tokens": [
        50364,
        33317,
        2501,
        27057,
        46213,
        13,
        50414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2781140208244324,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.005553058348596096
    },
    {
      "id": 114,
      "seek": 49384,
      "start": 1879.6899865722655,
      "end": 1882.2500146484374,
      "text": " Finde ich, ist eine spannende Frage.",
      "tokens": [
        50576,
        479,
        8274,
        1893,
        11,
        1418,
        3018,
        33360,
        5445,
        13685,
        13,
        50704
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2781140208244324,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.005553058348596096
    },
    {
      "id": 115,
      "seek": 49384,
      "start": 1882.2500146484374,
      "end": 1886.9300073242186,
      "text": " Ich glaube, eine Sache ist, sich mit den Technologien vertraut zu machen und sie tatsächlich",
      "tokens": [
        50704,
        3141,
        13756,
        11,
        3018,
        31452,
        1418,
        11,
        3041,
        2194,
        1441,
        8337,
        1132,
        1053,
        6509,
        424,
        325,
        2164,
        7069,
        674,
        2804,
        20796,
        50938
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2781140208244324,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.005553058348596096
    },
    {
      "id": 116,
      "seek": 49384,
      "start": 1886.9300073242186,
      "end": 1890.4099877929686,
      "text": " ernsthaft zu nutzen. Und da ist halt etwas am Werden.",
      "tokens": [
        50938,
        43412,
        25127,
        2164,
        36905,
        13,
        2719,
        1120,
        1418,
        12479,
        9569,
        669,
        14255,
        1556,
        13,
        51112
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2781140208244324,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.005553058348596096
    },
    {
      "id": 117,
      "seek": 49384,
      "start": 1890.4099877929686,
      "end": 1895.7299951171874,
      "text": " Und wie gesagt, einer der Gründe, warum ich glaube, dass es sinnvoll ist, diese Episode",
      "tokens": [
        51112,
        2719,
        3355,
        12260,
        11,
        6850,
        1163,
        2606,
        25596,
        11,
        24331,
        1893,
        13756,
        11,
        2658,
        785,
        47066,
        20654,
        1418,
        11,
        6705,
        19882,
        51378
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2781140208244324,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.005553058348596096
    },
    {
      "id": 118,
      "seek": 49384,
      "start": 1895.7299951171874,
      "end": 1900.2500146484374,
      "text": " zu machen, ist, weil ich immer noch das Gefühl habe, dass viele Leute sich darauf nicht",
      "tokens": [
        51378,
        2164,
        7069,
        11,
        1418,
        11,
        7689,
        1893,
        5578,
        3514,
        1482,
        29715,
        6015,
        11,
        2658,
        9693,
        13495,
        3041,
        18654,
        1979,
        51604
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2781140208244324,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.005553058348596096
    },
    {
      "id": 119,
      "seek": 51864,
      "start": 1900.2500146484374,
      "end": 1905.9300073242186,
      "text": " einlassen und da nahezu technologiefeindlich",
      "tokens": [
        50364,
        1343,
        44898,
        674,
        1120,
        17170,
        4371,
        84,
        1537,
        20121,
        2106,
        471,
        1739,
        50648
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29674723744392395,
      "compression_ratio": 1.6138211488723755,
      "no_speech_prob": 0.12242484837770462
    },
    {
      "id": 120,
      "seek": 51864,
      "start": 1905.9300073242186,
      "end": 1909.329970703125,
      "text": " sind. Und das ist irgendwie schwierig, nicht?",
      "tokens": [
        50648,
        3290,
        13,
        2719,
        1482,
        1418,
        20759,
        37845,
        11,
        1979,
        30,
        50818
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29674723744392395,
      "compression_ratio": 1.6138211488723755,
      "no_speech_prob": 0.12242484837770462
    },
    {
      "id": 121,
      "seek": 51864,
      "start": 1909.370009765625,
      "end": 1912.9699853515624,
      "text": " Also ich glaube halt, dass sich da einiges ändern wird.",
      "tokens": [
        50820,
        2743,
        1893,
        13756,
        12479,
        11,
        2658,
        3041,
        1120,
        1343,
        20609,
        47775,
        4578,
        13,
        51000
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29674723744392395,
      "compression_ratio": 1.6138211488723755,
      "no_speech_prob": 0.12242484837770462
    },
    {
      "id": 122,
      "seek": 51864,
      "start": 1914.4500268554686,
      "end": 1917.849990234375,
      "text": " Ich bin ehrlich gesagt nicht sicher, was die konkreten jetzigen Hinweise sind.",
      "tokens": [
        51074,
        3141,
        5171,
        40872,
        12260,
        1979,
        18623,
        11,
        390,
        978,
        21428,
        35383,
        361,
        10074,
        3213,
        29571,
        13109,
        3290,
        13,
        51244
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29674723744392395,
      "compression_ratio": 1.6138211488723755,
      "no_speech_prob": 0.12242484837770462
    },
    {
      "id": 123,
      "seek": 51864,
      "start": 1917.890029296875,
      "end": 1922.4099877929686,
      "text": " Ich glaube, also jetzt auch nach der Diskussion, die wir bis jetzt geführt haben,",
      "tokens": [
        51246,
        3141,
        13756,
        11,
        611,
        4354,
        2168,
        5168,
        1163,
        45963,
        313,
        11,
        978,
        1987,
        7393,
        4354,
        11271,
        19647,
        3084,
        11,
        51472
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29674723744392395,
      "compression_ratio": 1.6138211488723755,
      "no_speech_prob": 0.12242484837770462
    },
    {
      "id": 124,
      "seek": 51864,
      "start": 1923.4099877929686,
      "end": 1928.0100244140624,
      "text": " dass, also wie soll ich sagen, ich hatte bisher gedacht, dass man irgendwie sagt, naja,",
      "tokens": [
        51522,
        2658,
        11,
        611,
        3355,
        7114,
        1893,
        8360,
        11,
        1893,
        13299,
        33598,
        33296,
        11,
        2658,
        587,
        20759,
        15764,
        11,
        1667,
        2938,
        11,
        51752
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29674723744392395,
      "compression_ratio": 1.6138211488723755,
      "no_speech_prob": 0.12242484837770462
    },
    {
      "id": 125,
      "seek": 54640,
      "start": 1929.0100244140624,
      "end": 1932.7299951171874,
      "text": " wir haben halt eine höhere Ebene von Abstraktion und sagen halt, implementiert das mal,",
      "tokens": [
        50414,
        1987,
        3084,
        12479,
        3018,
        13531,
        6703,
        20418,
        1450,
        2957,
        2847,
        19639,
        9780,
        674,
        8360,
        12479,
        11,
        4445,
        4859,
        1482,
        2806,
        11,
        50600
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3094169795513153,
      "compression_ratio": 1.7251908779144287,
      "no_speech_prob": 0.0007792857359163463
    },
    {
      "id": 126,
      "seek": 54640,
      "start": 1934.6900170898436,
      "end": 1938.61,
      "text": " was ja eben genau das ist, was Ralf gemacht hat mit diesem Linter für SGDoc.",
      "tokens": [
        50698,
        390,
        2784,
        11375,
        12535,
        1482,
        1418,
        11,
        390,
        497,
        1678,
        12293,
        2385,
        2194,
        10975,
        441,
        5106,
        2959,
        34520,
        35,
        905,
        13,
        50894
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3094169795513153,
      "compression_ratio": 1.7251908779144287,
      "no_speech_prob": 0.0007792857359163463
    },
    {
      "id": 127,
      "seek": 54640,
      "start": 1939.7699731445311,
      "end": 1942.13001953125,
      "text": " Und dann haben wir eben tatsächlich eine höhere Ebene von Abstraktion.",
      "tokens": [
        50952,
        2719,
        3594,
        3084,
        1987,
        11375,
        20796,
        3018,
        13531,
        6703,
        20418,
        1450,
        2957,
        2847,
        19639,
        9780,
        13,
        51070
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3094169795513153,
      "compression_ratio": 1.7251908779144287,
      "no_speech_prob": 0.0007792857359163463
    },
    {
      "id": 128,
      "seek": 54640,
      "start": 1942.13001953125,
      "end": 1947.0100244140624,
      "text": " Also dann ist es eben so, wie man einem Entwicklern sagt, implementiert das mal und die",
      "tokens": [
        51070,
        2743,
        3594,
        1418,
        785,
        11375,
        370,
        11,
        3355,
        587,
        6827,
        29397,
        75,
        1248,
        15764,
        11,
        4445,
        4859,
        1482,
        2806,
        674,
        978,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3094169795513153,
      "compression_ratio": 1.7251908779144287,
      "no_speech_prob": 0.0007792857359163463
    },
    {
      "id": 129,
      "seek": 54640,
      "start": 1947.0100244140624,
      "end": 1952.0500024414061,
      "text": " macht es dann halt. Unter der Voraussetzung",
      "tokens": [
        51314,
        10857,
        785,
        3594,
        12479,
        13,
        12065,
        1163,
        691,
        3252,
        2023,
        38584,
        51566
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3094169795513153,
      "compression_ratio": 1.7251908779144287,
      "no_speech_prob": 0.0007792857359163463
    },
    {
      "id": 130,
      "seek": 54640,
      "start": 1952.0500024414061,
      "end": 1957.0500024414061,
      "text": " bedeutet das halt, dass wir uns noch stärker orientieren müssen an den Dingen,",
      "tokens": [
        51566,
        27018,
        1482,
        12479,
        11,
        2658,
        1987,
        2693,
        3514,
        33527,
        5767,
        8579,
        5695,
        9013,
        364,
        1441,
        49351,
        11,
        51816
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3094169795513153,
      "compression_ratio": 1.7251908779144287,
      "no_speech_prob": 0.0007792857359163463
    },
    {
      "id": 131,
      "seek": 57544,
      "start": 1957.13001953125,
      "end": 1960.7699731445311,
      "text": " die ja dann wie wichtig sind, also mit Kundinnen zum Beispiel sprechen, verstehen, was",
      "tokens": [
        50368,
        978,
        2784,
        3594,
        26393,
        68,
        13621,
        3290,
        11,
        611,
        2194,
        49759,
        11399,
        5919,
        13772,
        27853,
        11,
        37352,
        11,
        390,
        50550
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25444820523262024,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.0003799762635026127
    },
    {
      "id": 132,
      "seek": 57544,
      "start": 1960.7699731445311,
      "end": 1965.4900048828124,
      "text": " fachlich tatsächlich der Fall ist, was exakt die Veranforderungen sind und",
      "tokens": [
        50550,
        283,
        608,
        1739,
        20796,
        1163,
        7465,
        1418,
        11,
        390,
        454,
        5886,
        978,
        4281,
        282,
        30943,
        5084,
        3290,
        674,
        50786
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25444820523262024,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.0003799762635026127
    },
    {
      "id": 133,
      "seek": 57544,
      "start": 1965.4900048828124,
      "end": 1968.890029296875,
      "text": " von daraus sozusagen eine Lösung entwerfen.",
      "tokens": [
        50786,
        2957,
        274,
        46483,
        33762,
        3018,
        46934,
        948,
        1554,
        6570,
        13,
        50956
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25444820523262024,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.0003799762635026127
    },
    {
      "id": 134,
      "seek": 57544,
      "start": 1968.890029296875,
      "end": 1973.0500024414061,
      "text": " Und das ist meiner Ansicht nach eben traditionell das schwierige Thema.",
      "tokens": [
        50956,
        2719,
        1482,
        1418,
        20529,
        14590,
        1405,
        5168,
        11375,
        6994,
        898,
        1482,
        27546,
        3969,
        16306,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25444820523262024,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.0003799762635026127
    },
    {
      "id": 135,
      "seek": 57544,
      "start": 1973.0500024414061,
      "end": 1976.2099755859374,
      "text": " Und das wird, glaube ich, auch das schwierige Thema weiterhin bleiben.",
      "tokens": [
        51164,
        2719,
        1482,
        4578,
        11,
        13756,
        1893,
        11,
        2168,
        1482,
        27546,
        3969,
        16306,
        42480,
        24912,
        13,
        51322
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25444820523262024,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.0003799762635026127
    },
    {
      "id": 136,
      "seek": 57544,
      "start": 1976.2099755859374,
      "end": 1978.4099877929686,
      "text": " Und darauf müssen wir uns noch stärker fokussieren.",
      "tokens": [
        51322,
        2719,
        18654,
        9013,
        1987,
        2693,
        3514,
        33527,
        5767,
        283,
        453,
        2023,
        5695,
        13,
        51432
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25444820523262024,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.0003799762635026127
    },
    {
      "id": 137,
      "seek": 57544,
      "start": 1979.0500024414061,
      "end": 1983.7299951171874,
      "text": " Ich glaube, die andere Sache und daraus ergibt sich dann eben auch.",
      "tokens": [
        51464,
        3141,
        13756,
        11,
        978,
        10490,
        31452,
        674,
        274,
        46483,
        26585,
        13651,
        3041,
        3594,
        11375,
        2168,
        13,
        51698
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25444820523262024,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.0003799762635026127
    },
    {
      "id": 138,
      "seek": 60212,
      "start": 1984.2500146484374,
      "end": 1987.5700219726561,
      "text": " Das ist auch so etwas, was ich so ein bisschen aus der Diskussion mitgenommen habe.",
      "tokens": [
        50390,
        2846,
        1418,
        2168,
        370,
        9569,
        11,
        390,
        1893,
        370,
        1343,
        10763,
        3437,
        1163,
        45963,
        313,
        2194,
        29270,
        6015,
        13,
        50556
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 139,
      "seek": 60212,
      "start": 1987.5700219726561,
      "end": 1991.329970703125,
      "text": " Eigentlich ist Code ja ein Mechanismus, mit dem Menschen miteinander kommunizieren.",
      "tokens": [
        50556,
        40561,
        7698,
        1418,
        15549,
        2784,
        1343,
        30175,
        25327,
        11,
        2194,
        1371,
        8397,
        43127,
        26275,
        590,
        5695,
        13,
        50744
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 140,
      "seek": 60212,
      "start": 1991.329970703125,
      "end": 1993.370009765625,
      "text": " Also ich schreibe es halt einmal. Es wird vielfach gelesen.",
      "tokens": [
        50744,
        2743,
        1893,
        956,
        10271,
        650,
        785,
        12479,
        11078,
        13,
        2313,
        4578,
        5891,
        6749,
        4087,
        17403,
        13,
        50846
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 141,
      "seek": 60212,
      "start": 1993.370009765625,
      "end": 1997.1699975585936,
      "text": " Es wird geändert. Wir müssen jetzt irgendwie weg davon kommen.",
      "tokens": [
        50846,
        2313,
        4578,
        1519,
        34945,
        13,
        4347,
        9013,
        4354,
        20759,
        15565,
        18574,
        11729,
        13,
        51036
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 142,
      "seek": 60212,
      "start": 1998.0100244140624,
      "end": 2000.7299951171874,
      "text": " Und das ist halt genau diese Sache, die Ralf auch angesprochen hat.",
      "tokens": [
        51078,
        2719,
        1482,
        1418,
        12479,
        12535,
        6705,
        31452,
        11,
        978,
        497,
        1678,
        2168,
        31138,
        23902,
        2385,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 143,
      "seek": 60212,
      "start": 2000.7299951171874,
      "end": 2003.4099877929686,
      "text": " Das ist halt jetzt so weit, dass man sagt, okay, dieses Diagramm will ich aber",
      "tokens": [
        51214,
        2846,
        1418,
        12479,
        4354,
        370,
        15306,
        11,
        2658,
        587,
        15764,
        11,
        1392,
        11,
        12113,
        8789,
        3914,
        76,
        486,
        1893,
        4340,
        51348
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 144,
      "seek": 60212,
      "start": 2003.4099877929686,
      "end": 2004.7299951171874,
      "text": " maschinenkompatibel haben.",
      "tokens": [
        51348,
        2300,
        339,
        5636,
        74,
        8586,
        267,
        44685,
        3084,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 145,
      "seek": 60212,
      "start": 2005.2899926757811,
      "end": 2009.0100244140624,
      "text": " Also wir müssen halt irgendwie dafür sorgen, das wird vielleicht ersetzt in Richtung von",
      "tokens": [
        51442,
        2743,
        1987,
        9013,
        12479,
        20759,
        13747,
        47972,
        11,
        1482,
        4578,
        12547,
        33743,
        3524,
        294,
        33023,
        2957,
        51628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 146,
      "seek": 60212,
      "start": 2009.890029296875,
      "end": 2013.329970703125,
      "text": " Menschen und Maschinen kommunizieren über Code und über bestimmte andere Dinge.",
      "tokens": [
        51672,
        8397,
        674,
        5224,
        339,
        5636,
        26275,
        590,
        5695,
        4502,
        15549,
        674,
        4502,
        35180,
        975,
        10490,
        25102,
        13,
        51844
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3087637424468994,
      "compression_ratio": 1.7999999523162842,
      "no_speech_prob": 0.22780200839042664
    },
    {
      "id": 147,
      "seek": 63212,
      "start": 2013.849990234375,
      "end": 2017.5700219726561,
      "text": " Da haben wir übrigens meiner Ansicht nach in der Episode mit Ralf von Pfeder gemacht.",
      "tokens": [
        50370,
        3933,
        3084,
        1987,
        38215,
        20529,
        14590,
        1405,
        5168,
        294,
        1163,
        19882,
        2194,
        497,
        1678,
        2957,
        17331,
        10020,
        12293,
        13,
        50556
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 148,
      "seek": 63212,
      "start": 2017.5700219726561,
      "end": 2021.4500268554686,
      "text": " Wir hätten nämlich eigentlich sagen müssen, hey, hier sind Qualitätsszenarien.",
      "tokens": [
        50556,
        4347,
        33278,
        21219,
        10926,
        8360,
        9013,
        11,
        4177,
        11,
        3296,
        3290,
        13616,
        13187,
        1373,
        82,
        2904,
        289,
        1053,
        13,
        50750
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 149,
      "seek": 63212,
      "start": 2021.61,
      "end": 2023.849990234375,
      "text": " Wir haben aber Qualitätsszenarien erstellen lassen.",
      "tokens": [
        50758,
        4347,
        3084,
        4340,
        13616,
        13187,
        1373,
        82,
        2904,
        289,
        1053,
        11301,
        8581,
        16168,
        13,
        50870
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 150,
      "seek": 63212,
      "start": 2023.849990234375,
      "end": 2027.08998046875,
      "text": " Also da sind Dinge, die wir, glaube ich, da noch besser machen können.",
      "tokens": [
        50870,
        2743,
        1120,
        3290,
        25102,
        11,
        978,
        1987,
        11,
        13756,
        1893,
        11,
        1120,
        3514,
        18021,
        7069,
        6310,
        13,
        51032
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 151,
      "seek": 63212,
      "start": 2027.5700219726561,
      "end": 2032.0500024414061,
      "text": " Und ich fand auch den Hinweis interessant von Stefan, dass das halt so ein allgemein",
      "tokens": [
        51056,
        2719,
        1893,
        38138,
        2168,
        1441,
        29571,
        35033,
        37748,
        2957,
        32158,
        11,
        2658,
        1482,
        12479,
        370,
        1343,
        439,
        31964,
        259,
        51280
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 152,
      "seek": 63212,
      "start": 2032.0500024414061,
      "end": 2034.7299951171874,
      "text": " generisches Ding ist, was halt einfach ein Problem lösen kann.",
      "tokens": [
        51280,
        1337,
        35889,
        20558,
        1418,
        11,
        390,
        12479,
        7281,
        1343,
        11676,
        25209,
        6748,
        4028,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 153,
      "seek": 63212,
      "start": 2035.4500268554686,
      "end": 2038.08998046875,
      "text": " Und das bedeutet, dass wir halt noch was anderes haben.",
      "tokens": [
        51450,
        2719,
        1482,
        27018,
        11,
        2658,
        1987,
        12479,
        3514,
        390,
        31426,
        3084,
        13,
        51582
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 154,
      "seek": 63212,
      "start": 2038.2899926757811,
      "end": 2041.7699731445311,
      "text": " Also wir haben halt nicht eine höhere Ebene von Abstraktionen, sondern was anderes,",
      "tokens": [
        51592,
        2743,
        1987,
        3084,
        12479,
        1979,
        3018,
        13531,
        6703,
        20418,
        1450,
        2957,
        2847,
        19639,
        9780,
        268,
        11,
        11465,
        390,
        31426,
        11,
        51766
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 155,
      "seek": 63212,
      "start": 2041.7699731445311,
      "end": 2043.4500268554686,
      "text": " wo wir anders Dinge ausdrücken können.",
      "tokens": [
        51766,
        6020,
        1987,
        17999,
        25102,
        3437,
        16753,
        26037,
        6310,
        13,
        51850
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2784402072429657,
      "compression_ratio": 1.7556179761886597,
      "no_speech_prob": 0.001187791465781629
    },
    {
      "id": 156,
      "seek": 66212,
      "start": 2044.370009765625,
      "end": 2047.13001953125,
      "text": " Und da muss man sich, glaube ich, auch mal Gedanken darüber machen, was das denn nun",
      "tokens": [
        50396,
        2719,
        1120,
        6425,
        587,
        3041,
        11,
        13756,
        1893,
        11,
        2168,
        2806,
        44612,
        21737,
        7069,
        11,
        390,
        1482,
        10471,
        8905,
        50534
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2778513431549072,
      "compression_ratio": 1.5993150472640991,
      "no_speech_prob": 0.00046547511010430753
    },
    {
      "id": 157,
      "seek": 66212,
      "start": 2047.13001953125,
      "end": 2049.4500268554684,
      "text": " tatsächlich bedeutet für die Projekte.",
      "tokens": [
        50534,
        20796,
        27018,
        2959,
        978,
        1705,
        27023,
        975,
        13,
        50650
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2778513431549072,
      "compression_ratio": 1.5993150472640991,
      "no_speech_prob": 0.00046547511010430753
    },
    {
      "id": 158,
      "seek": 66212,
      "start": 2049.769973144531,
      "end": 2054.529982910156,
      "text": " Und nicht also diese Hinweise, die auch Stefan da gegeben hat,",
      "tokens": [
        50666,
        2719,
        1979,
        611,
        6705,
        29571,
        13109,
        11,
        978,
        2168,
        32158,
        1120,
        1519,
        432,
        1799,
        2385,
        11,
        50904
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2778513431549072,
      "compression_ratio": 1.5993150472640991,
      "no_speech_prob": 0.00046547511010430753
    },
    {
      "id": 159,
      "seek": 66212,
      "start": 2054.570021972656,
      "end": 2058.289992675781,
      "text": " nicht, dass vielleicht so eine Datenanalyse in Hochsprache, also normaler deutscher",
      "tokens": [
        50906,
        1979,
        11,
        2658,
        12547,
        370,
        3018,
        31126,
        282,
        5222,
        405,
        294,
        29193,
        18193,
        6000,
        11,
        611,
        2710,
        260,
        23004,
        6759,
        51092
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2778513431549072,
      "compression_ratio": 1.5993150472640991,
      "no_speech_prob": 0.00046547511010430753
    },
    {
      "id": 160,
      "seek": 66212,
      "start": 2058.289992675781,
      "end": 2062.209975585937,
      "text": " Sprache oder englischer Sprache oder was auch immer, dass das halt vielleicht ein",
      "tokens": [
        51092,
        7702,
        6000,
        4513,
        1741,
        75,
        19674,
        7702,
        6000,
        4513,
        390,
        2168,
        5578,
        11,
        2658,
        1482,
        12479,
        12547,
        1343,
        51288
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2778513431549072,
      "compression_ratio": 1.5993150472640991,
      "no_speech_prob": 0.00046547511010430753
    },
    {
      "id": 161,
      "seek": 66212,
      "start": 2062.209975585937,
      "end": 2063.729995117187,
      "text": " Zukunftsding ist, nicht?",
      "tokens": [
        51288,
        22782,
        82,
        3584,
        1418,
        11,
        1979,
        30,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2778513431549072,
      "compression_ratio": 1.5993150472640991,
      "no_speech_prob": 0.00046547511010430753
    },
    {
      "id": 162,
      "seek": 66212,
      "start": 2063.769973144531,
      "end": 2067.6099999999997,
      "text": " Guter Punkt. Und das wäre etwas, was wir im Moment nicht so einfach umsetzen können.",
      "tokens": [
        51366,
        460,
        20314,
        25487,
        13,
        2719,
        1482,
        14558,
        9569,
        11,
        390,
        1987,
        566,
        19093,
        1979,
        370,
        7281,
        1105,
        3854,
        2904,
        6310,
        13,
        51558
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2778513431549072,
      "compression_ratio": 1.5993150472640991,
      "no_speech_prob": 0.00046547511010430753
    },
    {
      "id": 163,
      "seek": 68600,
      "start": 2068.250014648437,
      "end": 2074.250014648437,
      "text": " Ich habe vielleicht eine etwas schwammige Nachfrage.",
      "tokens": [
        50396,
        3141,
        6015,
        12547,
        3018,
        9569,
        17932,
        5136,
        3969,
        11815,
        40449,
        13,
        50696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28787878155708313,
      "compression_ratio": 1.7777777910232544,
      "no_speech_prob": 0.11869023740291595
    },
    {
      "id": 164,
      "seek": 68600,
      "start": 2074.250014648437,
      "end": 2077.9300073242184,
      "text": " Du hast ganz am Anfang gesagt, dass du, dass es Leute gibt, die sich gerade mit dem",
      "tokens": [
        50696,
        5153,
        6581,
        6312,
        669,
        25856,
        12260,
        11,
        2658,
        1581,
        11,
        2658,
        785,
        13495,
        6089,
        11,
        978,
        3041,
        12117,
        2194,
        1371,
        50880
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28787878155708313,
      "compression_ratio": 1.7777777910232544,
      "no_speech_prob": 0.11869023740291595
    },
    {
      "id": 165,
      "seek": 68600,
      "start": 2077.9300073242184,
      "end": 2081.4099877929684,
      "text": " Thema nicht auseinandersetzen wollen und dass, dass sie sich quasi,",
      "tokens": [
        50880,
        16306,
        1979,
        257,
        438,
        259,
        41430,
        24797,
        11253,
        674,
        2658,
        11,
        2658,
        2804,
        3041,
        20954,
        11,
        51054
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28787878155708313,
      "compression_ratio": 1.7777777910232544,
      "no_speech_prob": 0.11869023740291595
    },
    {
      "id": 166,
      "seek": 68600,
      "start": 2082.969985351562,
      "end": 2084.729995117187,
      "text": " dass sie dagegen sind, mehr oder weniger.",
      "tokens": [
        51132,
        2658,
        2804,
        45387,
        3290,
        11,
        5417,
        4513,
        23224,
        13,
        51220
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28787878155708313,
      "compression_ratio": 1.7777777910232544,
      "no_speech_prob": 0.11869023740291595
    },
    {
      "id": 167,
      "seek": 68600,
      "start": 2085.529982910156,
      "end": 2088.729995117187,
      "text": " Ist es denn so schlimm, wenn ich mich jetzt heute noch nicht damit auseinandersetzen",
      "tokens": [
        51260,
        12810,
        785,
        10471,
        370,
        48821,
        11,
        4797,
        1893,
        6031,
        4354,
        9801,
        3514,
        1979,
        9479,
        257,
        438,
        259,
        41430,
        24797,
        51420
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28787878155708313,
      "compression_ratio": 1.7777777910232544,
      "no_speech_prob": 0.11869023740291595
    },
    {
      "id": 168,
      "seek": 68600,
      "start": 2088.729995117187,
      "end": 2092.4500268554684,
      "text": " möchte, wenn ich quasi noch ein bisschen warte, bis der Hype etwas abgeebbt ist?",
      "tokens": [
        51420,
        14570,
        11,
        4797,
        1893,
        20954,
        3514,
        1343,
        10763,
        261,
        11026,
        11,
        7393,
        1163,
        5701,
        494,
        9569,
        37301,
        68,
        6692,
        83,
        1418,
        30,
        51606
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28787878155708313,
      "compression_ratio": 1.7777777910232544,
      "no_speech_prob": 0.11869023740291595
    },
    {
      "id": 169,
      "seek": 68600,
      "start": 2092.570021972656,
      "end": 2097.4099877929684,
      "text": " Also ich sehe noch nicht so ganz den, dieses, was daran schlimm ist, wenn ich mich",
      "tokens": [
        51612,
        2743,
        1893,
        35995,
        3514,
        1979,
        370,
        6312,
        1441,
        11,
        12113,
        11,
        390,
        24520,
        48821,
        1418,
        11,
        4797,
        1893,
        6031,
        51854
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28787878155708313,
      "compression_ratio": 1.7777777910232544,
      "no_speech_prob": 0.11869023740291595
    },
    {
      "id": 170,
      "seek": 71580,
      "start": 2097.4099877929684,
      "end": 2099.729995117187,
      "text": " jetzt heute noch nicht damit auseinandersetzen möchte.",
      "tokens": [
        50364,
        4354,
        9801,
        3514,
        1979,
        9479,
        257,
        438,
        259,
        41430,
        24797,
        14570,
        13,
        50480
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3058106005191803,
      "compression_ratio": 1.7575757503509521,
      "no_speech_prob": 0.0017272455152124166
    },
    {
      "id": 171,
      "seek": 71580,
      "start": 2102.050002441406,
      "end": 2105.6099999999997,
      "text": " Was wir noch nicht angesprochen haben, ist, wir haben einen massiven Energieverbrauch und",
      "tokens": [
        50596,
        3027,
        1987,
        3514,
        1979,
        31138,
        23902,
        3084,
        11,
        1418,
        11,
        1987,
        3084,
        4891,
        2758,
        5709,
        35309,
        331,
        6198,
        625,
        674,
        50774
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3058106005191803,
      "compression_ratio": 1.7575757503509521,
      "no_speech_prob": 0.0017272455152124166
    },
    {
      "id": 172,
      "seek": 71580,
      "start": 2105.6099999999997,
      "end": 2107.3700097656247,
      "text": " das wird ein großes Problem werden.",
      "tokens": [
        50774,
        1482,
        4578,
        1343,
        48875,
        11676,
        4604,
        13,
        50862
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3058106005191803,
      "compression_ratio": 1.7575757503509521,
      "no_speech_prob": 0.0017272455152124166
    },
    {
      "id": 173,
      "seek": 71580,
      "start": 2107.3700097656247,
      "end": 2111.8499902343747,
      "text": " Das ist jetzt schon ein großes Problem und das ist sicherlich ein Nachteil von",
      "tokens": [
        50862,
        2846,
        1418,
        4354,
        4981,
        1343,
        48875,
        11676,
        674,
        1482,
        1418,
        18623,
        1739,
        1343,
        426,
        26136,
        388,
        2957,
        51086
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3058106005191803,
      "compression_ratio": 1.7575757503509521,
      "no_speech_prob": 0.0017272455152124166
    },
    {
      "id": 174,
      "seek": 71580,
      "start": 2111.8499902343747,
      "end": 2115.0899804687497,
      "text": " AI und AI wie alle Technologien hat irgendwie Nachteile.",
      "tokens": [
        51086,
        7318,
        674,
        7318,
        3355,
        5430,
        8337,
        1132,
        1053,
        2385,
        20759,
        426,
        26136,
        794,
        13,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3058106005191803,
      "compression_ratio": 1.7575757503509521,
      "no_speech_prob": 0.0017272455152124166
    },
    {
      "id": 175,
      "seek": 71580,
      "start": 2115.729995117187,
      "end": 2120.6900170898434,
      "text": " Ich glaube, das, wo ich ein bisschen Angst habe, ist, dass",
      "tokens": [
        51280,
        3141,
        13756,
        11,
        1482,
        11,
        6020,
        1893,
        1343,
        10763,
        28622,
        6015,
        11,
        1418,
        11,
        2658,
        51528
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3058106005191803,
      "compression_ratio": 1.7575757503509521,
      "no_speech_prob": 0.0017272455152124166
    },
    {
      "id": 176,
      "seek": 71580,
      "start": 2120.9300073242184,
      "end": 2124.4099877929684,
      "text": " wenn man sich damit auseinandersetzt, dass man dann wirklich irgendwann obsolet wird.",
      "tokens": [
        51540,
        4797,
        587,
        3041,
        9479,
        257,
        438,
        259,
        41430,
        3524,
        11,
        2658,
        587,
        3594,
        9696,
        3418,
        432,
        273,
        86,
        969,
        43053,
        302,
        4578,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3058106005191803,
      "compression_ratio": 1.7575757503509521,
      "no_speech_prob": 0.0017272455152124166
    },
    {
      "id": 177,
      "seek": 74280,
      "start": 2125.4099877929684,
      "end": 2130.529982910156,
      "text": " Und ich bin da, glaube ich, nicht so wie Stefan, der ja sagt,",
      "tokens": [
        50414,
        2719,
        1893,
        5171,
        1120,
        11,
        13756,
        1893,
        11,
        1979,
        370,
        3355,
        32158,
        11,
        1163,
        2784,
        15764,
        11,
        50670
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3594890534877777,
      "compression_ratio": 1.73308265209198,
      "no_speech_prob": 0.005818464793264866
    },
    {
      "id": 178,
      "seek": 74280,
      "start": 2130.529982910156,
      "end": 2133.529982910156,
      "text": " nicht. Also das ist halt das neue Ding, was mich hat total begeistert.",
      "tokens": [
        50670,
        1979,
        13,
        2743,
        1482,
        1418,
        12479,
        1482,
        16842,
        20558,
        11,
        390,
        6031,
        2385,
        3217,
        41832,
        1964,
        83,
        13,
        50820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3594890534877777,
      "compression_ratio": 1.73308265209198,
      "no_speech_prob": 0.005818464793264866
    },
    {
      "id": 179,
      "seek": 74280,
      "start": 2134.729995117187,
      "end": 2139.250014648437,
      "text": " Aber ich warne, glaube ich, vor der Fehlwahrnehmung, dass das halt",
      "tokens": [
        50880,
        5992,
        1893,
        1516,
        716,
        11,
        13756,
        1893,
        11,
        4245,
        1163,
        3697,
        22950,
        86,
        5398,
        716,
        8587,
        1063,
        11,
        2658,
        1482,
        12479,
        51106
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3594890534877777,
      "compression_ratio": 1.73308265209198,
      "no_speech_prob": 0.005818464793264866
    },
    {
      "id": 180,
      "seek": 74280,
      "start": 2139.570021972656,
      "end": 2141.209975585937,
      "text": " nur ein Hype ist.",
      "tokens": [
        51122,
        4343,
        1343,
        5701,
        494,
        1418,
        13,
        51204
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3594890534877777,
      "compression_ratio": 1.73308265209198,
      "no_speech_prob": 0.005818464793264866
    },
    {
      "id": 181,
      "seek": 74280,
      "start": 2141.6499780273434,
      "end": 2143.570021972656,
      "text": " Also nicht Shichibiti ausprobieren.",
      "tokens": [
        51226,
        2743,
        1979,
        1160,
        480,
        897,
        270,
        72,
        3437,
        41990,
        5695,
        13,
        51322
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3594890534877777,
      "compression_ratio": 1.73308265209198,
      "no_speech_prob": 0.005818464793264866
    },
    {
      "id": 182,
      "seek": 74280,
      "start": 2143.570021972656,
      "end": 2147.6099999999997,
      "text": " Ich habe es halt irgendwie eine Stunde ausprobiert, als es halt rauskam.",
      "tokens": [
        51322,
        3141,
        6015,
        785,
        12479,
        20759,
        3018,
        42781,
        3437,
        41990,
        4859,
        11,
        3907,
        785,
        12479,
        17202,
        39917,
        13,
        51524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3594890534877777,
      "compression_ratio": 1.73308265209198,
      "no_speech_prob": 0.005818464793264866
    },
    {
      "id": 183,
      "seek": 74280,
      "start": 2147.6099999999997,
      "end": 2151.969985351562,
      "text": " Und ich war nach einer Stunde überzeugt, dass das halt ziemlich krasse Sachen sind, die",
      "tokens": [
        51524,
        2719,
        1893,
        1516,
        5168,
        6850,
        42781,
        48598,
        83,
        11,
        2658,
        1482,
        12479,
        28901,
        350,
        3906,
        405,
        26074,
        3290,
        11,
        978,
        51742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3594890534877777,
      "compression_ratio": 1.73308265209198,
      "no_speech_prob": 0.005818464793264866
    },
    {
      "id": 184,
      "seek": 74280,
      "start": 2151.969985351562,
      "end": 2153.769973144531,
      "text": " ich vorher nicht für möglich gehalten habe.",
      "tokens": [
        51742,
        1893,
        29195,
        1979,
        2959,
        16294,
        1519,
        15022,
        6015,
        13,
        51832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3594890534877777,
      "compression_ratio": 1.73308265209198,
      "no_speech_prob": 0.005818464793264866
    },
    {
      "id": 185,
      "seek": 77216,
      "start": 2154.289992675781,
      "end": 2156.1300195312497,
      "text": " Und ich habe ein Problem.",
      "tokens": [
        50390,
        2719,
        1893,
        6015,
        1343,
        11676,
        13,
        50482
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3407233953475952,
      "compression_ratio": 1.6760563850402832,
      "no_speech_prob": 0.0033762261737138033
    },
    {
      "id": 186,
      "seek": 77216,
      "start": 2156.250014648437,
      "end": 2158.810012207031,
      "text": " Also wie soll ich sagen, wenn man jetzt irgendwie sagt Nein, es ist halt nur ein Hype,",
      "tokens": [
        50488,
        2743,
        3355,
        7114,
        1893,
        8360,
        11,
        4797,
        587,
        4354,
        20759,
        15764,
        18878,
        11,
        785,
        1418,
        12479,
        4343,
        1343,
        5701,
        494,
        11,
        50616
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3407233953475952,
      "compression_ratio": 1.6760563850402832,
      "no_speech_prob": 0.0033762261737138033
    },
    {
      "id": 187,
      "seek": 77216,
      "start": 2160.490004882812,
      "end": 2162.969985351562,
      "text": " dann ist das halt meiner Ansicht nach objektiv einfach falsch.",
      "tokens": [
        50700,
        3594,
        1418,
        1482,
        12479,
        20529,
        14590,
        1405,
        5168,
        1111,
        14930,
        592,
        7281,
        43340,
        13,
        50824
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3407233953475952,
      "compression_ratio": 1.6760563850402832,
      "no_speech_prob": 0.0033762261737138033
    },
    {
      "id": 188,
      "seek": 77216,
      "start": 2163.729995117187,
      "end": 2168.289992675781,
      "text": " Und das bedeutet, dass man sich möglicherweise in seiner Karriere und halt in dem, was man",
      "tokens": [
        50862,
        2719,
        1482,
        27018,
        11,
        2658,
        587,
        3041,
        16294,
        44071,
        294,
        23114,
        8009,
        470,
        323,
        674,
        12479,
        294,
        1371,
        11,
        390,
        587,
        51090
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3407233953475952,
      "compression_ratio": 1.6760563850402832,
      "no_speech_prob": 0.0033762261737138033
    },
    {
      "id": 189,
      "seek": 77216,
      "start": 2168.289992675781,
      "end": 2172.8900292968747,
      "text": " halt so tut, in Anführungsstrichen falsch verhält, also nicht Dinge halt irgendwie falsch",
      "tokens": [
        51090,
        12479,
        370,
        3672,
        11,
        294,
        1107,
        29189,
        5846,
        9733,
        18613,
        43340,
        1306,
        28068,
        11,
        611,
        1979,
        25102,
        12479,
        20759,
        43340,
        51320
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3407233953475952,
      "compression_ratio": 1.6760563850402832,
      "no_speech_prob": 0.0033762261737138033
    },
    {
      "id": 190,
      "seek": 77216,
      "start": 2172.8900292968747,
      "end": 2175.3299707031247,
      "text": " einschätzt. Und das finde ich halt einfach schwierig.",
      "tokens": [
        51320,
        21889,
        339,
        3628,
        2682,
        13,
        2719,
        1482,
        17841,
        1893,
        12479,
        7281,
        37845,
        13,
        51442
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3407233953475952,
      "compression_ratio": 1.6760563850402832,
      "no_speech_prob": 0.0033762261737138033
    },
    {
      "id": 191,
      "seek": 77216,
      "start": 2176.209975585937,
      "end": 2180.769973144531,
      "text": " Also ich glaube halt nicht grundloser, grundloses Hype-Wissen",
      "tokens": [
        51486,
        2743,
        1893,
        13756,
        12479,
        1979,
        30886,
        9389,
        260,
        11,
        30886,
        75,
        4201,
        5701,
        494,
        12,
        54,
        10987,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3407233953475952,
      "compression_ratio": 1.6760563850402832,
      "no_speech_prob": 0.0033762261737138033
    },
    {
      "id": 192,
      "seek": 79916,
      "start": 2181.3299707031247,
      "end": 2185.3299707031247,
      "text": " ist halt ein Problem oder Grundloser, ein Hypes-Glauben ist halt ein echtes Problem.",
      "tokens": [
        50392,
        1418,
        12479,
        1343,
        11676,
        4513,
        13941,
        9389,
        260,
        11,
        1343,
        5701,
        5190,
        12,
        38,
        20798,
        268,
        1418,
        12479,
        1343,
        13972,
        279,
        11676,
        13,
        50592
      ],
      "temperature": 0.0,
      "avg_logprob": -0.338990181684494,
      "compression_ratio": 1.67399263381958,
      "no_speech_prob": 0.002216026419773698
    },
    {
      "id": 193,
      "seek": 79916,
      "start": 2186.050002441406,
      "end": 2190.8499902343747,
      "text": " Und da ist halt Blockchain für mich halt ein, da gab es halt Leute, die",
      "tokens": [
        50628,
        2719,
        1120,
        1418,
        12479,
        48916,
        2959,
        6031,
        12479,
        1343,
        11,
        1120,
        17964,
        785,
        12479,
        13495,
        11,
        978,
        50868
      ],
      "temperature": 0.0,
      "avg_logprob": -0.338990181684494,
      "compression_ratio": 1.67399263381958,
      "no_speech_prob": 0.002216026419773698
    },
    {
      "id": 194,
      "seek": 79916,
      "start": 2190.8499902343747,
      "end": 2192.1699975585934,
      "text": " irgendwie an diesen Hype geglaubt haben.",
      "tokens": [
        50868,
        20759,
        364,
        12862,
        5701,
        494,
        23982,
        20798,
        83,
        3084,
        13,
        50934
      ],
      "temperature": 0.0,
      "avg_logprob": -0.338990181684494,
      "compression_ratio": 1.67399263381958,
      "no_speech_prob": 0.002216026419773698
    },
    {
      "id": 195,
      "seek": 79916,
      "start": 2192.1699975585934,
      "end": 2196.8499902343747,
      "text": " Und das war eben grundlos. Das andere, wo ich irgendwie mich mit",
      "tokens": [
        50934,
        2719,
        1482,
        1516,
        11375,
        30886,
        9389,
        13,
        2846,
        10490,
        11,
        6020,
        1893,
        20759,
        6031,
        2194,
        51168
      ],
      "temperature": 0.0,
      "avg_logprob": -0.338990181684494,
      "compression_ratio": 1.67399263381958,
      "no_speech_prob": 0.002216026419773698
    },
    {
      "id": 196,
      "seek": 79916,
      "start": 2196.8499902343747,
      "end": 2200.1699975585934,
      "text": " der Technologie nicht ausreichend kritisch beschäftigt habe, war EGB am Anfang.",
      "tokens": [
        51168,
        1163,
        8337,
        20121,
        1979,
        3437,
        12594,
        521,
        42825,
        5494,
        38768,
        5828,
        6015,
        11,
        1516,
        462,
        8769,
        669,
        25856,
        13,
        51334
      ],
      "temperature": 0.0,
      "avg_logprob": -0.338990181684494,
      "compression_ratio": 1.67399263381958,
      "no_speech_prob": 0.002216026419773698
    },
    {
      "id": 197,
      "seek": 79916,
      "start": 2200.1699975585934,
      "end": 2204.3700097656247,
      "text": " Ganz anderes Thema. Und ich glaube, bei KI ist es umgekehrt.",
      "tokens": [
        51334,
        32496,
        31426,
        16306,
        13,
        2719,
        1893,
        13756,
        11,
        4643,
        47261,
        1418,
        785,
        1105,
        432,
        22833,
        83,
        13,
        51544
      ],
      "temperature": 0.0,
      "avg_logprob": -0.338990181684494,
      "compression_ratio": 1.67399263381958,
      "no_speech_prob": 0.002216026419773698
    },
    {
      "id": 198,
      "seek": 79916,
      "start": 2204.8499902343747,
      "end": 2206.810012207031,
      "text": " Also gibt es ein bisschen diese umgekehrte Tendenz.",
      "tokens": [
        51568,
        2743,
        6089,
        785,
        1343,
        10763,
        6705,
        1105,
        432,
        22833,
        975,
        314,
        8896,
        89,
        13,
        51666
      ],
      "temperature": 0.0,
      "avg_logprob": -0.338990181684494,
      "compression_ratio": 1.67399263381958,
      "no_speech_prob": 0.002216026419773698
    },
    {
      "id": 199,
      "seek": 82520,
      "start": 2206.810012207031,
      "end": 2213.810012207031,
      "text": " Für mich ist immer, ich würde noch, darf ich kurz, Stefan?",
      "tokens": [
        50364,
        14990,
        6031,
        1418,
        5578,
        11,
        1893,
        261,
        774,
        81,
        1479,
        3514,
        11,
        19374,
        1893,
        20465,
        11,
        318,
        975,
        20361,
        30,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5159336924552917,
      "compression_ratio": 1.408046007156372,
      "no_speech_prob": 0.03899870440363884
    },
    {
      "id": 200,
      "seek": 82520,
      "start": 2213.810012207031,
      "end": 2215.810012207031,
      "text": " Immer.",
      "tokens": [
        50714,
        42676,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5159336924552917,
      "compression_ratio": 1.408046007156372,
      "no_speech_prob": 0.03899870440363884
    },
    {
      "id": 201,
      "seek": 82520,
      "start": 2215.810012207031,
      "end": 2220.3700097656247,
      "text": " Okay, ansonsten schlägt das Alphabet, also Vorname.",
      "tokens": [
        50814,
        1033,
        11,
        1567,
        4068,
        268,
        956,
        22882,
        10463,
        1482,
        967,
        20890,
        11,
        611,
        691,
        1865,
        529,
        13,
        51042
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5159336924552917,
      "compression_ratio": 1.408046007156372,
      "no_speech_prob": 0.03899870440363884
    },
    {
      "id": 202,
      "seek": 82520,
      "start": 2223.4500268554684,
      "end": 2228.8900292968747,
      "text": " Was ich gerade mitbekomme oder was ich, was ich immer wieder höre, ist also gegen was",
      "tokens": [
        51196,
        3027,
        1893,
        12117,
        2194,
        25714,
        15117,
        4513,
        390,
        1893,
        11,
        390,
        1893,
        5578,
        6216,
        13531,
        265,
        11,
        1418,
        611,
        13953,
        390,
        51468
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5159336924552917,
      "compression_ratio": 1.408046007156372,
      "no_speech_prob": 0.03899870440363884
    },
    {
      "id": 203,
      "seek": 82520,
      "start": 2228.8900292968747,
      "end": 2231.050002441406,
      "text": " das halt antritt, also der Vergleich.",
      "tokens": [
        51468,
        1482,
        12479,
        2511,
        18579,
        11,
        611,
        1163,
        47998,
        13,
        51576
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5159336924552917,
      "compression_ratio": 1.408046007156372,
      "no_speech_prob": 0.03899870440363884
    },
    {
      "id": 204,
      "seek": 84944,
      "start": 2232.050002441406,
      "end": 2238.6099999999997,
      "text": " Und für mich ist die Frage, also tritt das gegen, also tritt Gen-AI gegen Cloud an oder",
      "tokens": [
        50414,
        2719,
        2959,
        6031,
        1418,
        978,
        13685,
        11,
        611,
        504,
        593,
        1482,
        13953,
        11,
        611,
        504,
        593,
        3632,
        12,
        48698,
        13953,
        8061,
        364,
        4513,
        50742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33325207233428955,
      "compression_ratio": 1.6090909242630005,
      "no_speech_prob": 0.002470164094120264
    },
    {
      "id": 205,
      "seek": 84944,
      "start": 2238.729995117187,
      "end": 2244.1699975585934,
      "text": " gegen Microservices oder tritt das gegen, ich mach jetzt mal das krasse Gegenteil,",
      "tokens": [
        50748,
        13953,
        5818,
        2635,
        47480,
        4513,
        504,
        593,
        1482,
        13953,
        11,
        1893,
        2246,
        4354,
        2806,
        1482,
        350,
        3906,
        405,
        27826,
        1576,
        388,
        11,
        51020
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33325207233428955,
      "compression_ratio": 1.6090909242630005,
      "no_speech_prob": 0.002470164094120264
    },
    {
      "id": 206,
      "seek": 84944,
      "start": 2244.1699975585934,
      "end": 2246.3700097656247,
      "text": " tritt das gegen Feuer und Elektrizität an?",
      "tokens": [
        51020,
        504,
        593,
        1482,
        13953,
        39972,
        674,
        40321,
        24959,
        14053,
        364,
        30,
        51130
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33325207233428955,
      "compression_ratio": 1.6090909242630005,
      "no_speech_prob": 0.002470164094120264
    },
    {
      "id": 207,
      "seek": 84944,
      "start": 2247.1699975585934,
      "end": 2251.9300073242184,
      "text": " Also quasi über, also wo ordnet sich das, das Thema halt ein?",
      "tokens": [
        51170,
        2743,
        20954,
        4502,
        11,
        611,
        6020,
        4792,
        7129,
        3041,
        1482,
        11,
        1482,
        16306,
        12479,
        1343,
        30,
        51408
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33325207233428955,
      "compression_ratio": 1.6090909242630005,
      "no_speech_prob": 0.002470164094120264
    },
    {
      "id": 208,
      "seek": 84944,
      "start": 2252.4099877929684,
      "end": 2258.6900170898434,
      "text": " Und das, ich glaube, wenn es sich eher quasi bei dem Cloud-Thema einordnet,",
      "tokens": [
        51432,
        2719,
        1482,
        11,
        1893,
        13756,
        11,
        4797,
        785,
        3041,
        24332,
        20954,
        4643,
        1371,
        8061,
        12,
        2434,
        5619,
        1343,
        765,
        7129,
        11,
        51746
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33325207233428955,
      "compression_ratio": 1.6090909242630005,
      "no_speech_prob": 0.002470164094120264
    },
    {
      "id": 209,
      "seek": 87708,
      "start": 2258.729995117187,
      "end": 2262.010024414062,
      "text": " dann würde ich sagen, okay, brauchst du dich im Zweifelsfall nicht beschäftigen.",
      "tokens": [
        50366,
        3594,
        11942,
        1893,
        8360,
        11,
        1392,
        11,
        45522,
        372,
        1581,
        10390,
        566,
        32475,
        351,
        1625,
        6691,
        1979,
        38768,
        3213,
        13,
        50530
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2975834608078003,
      "compression_ratio": 1.524647831916809,
      "no_speech_prob": 0.006090763956308365
    },
    {
      "id": 210,
      "seek": 87708,
      "start": 2262.209975585937,
      "end": 2267.3299707031247,
      "text": " Das wird auch wahrscheinlich noch in 100 Jahren quasi Data Center geben, die quasi",
      "tokens": [
        50540,
        2846,
        4578,
        2168,
        30957,
        3514,
        294,
        2319,
        13080,
        20954,
        11888,
        5169,
        17191,
        11,
        978,
        20954,
        50796
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2975834608078003,
      "compression_ratio": 1.524647831916809,
      "no_speech_prob": 0.006090763956308365
    },
    {
      "id": 211,
      "seek": 87708,
      "start": 2267.4099877929684,
      "end": 2271.9300073242184,
      "text": " anders funktionieren. Gibt immer gewisse Use Cases, die vielleicht eben nicht in der",
      "tokens": [
        50800,
        17999,
        20454,
        5695,
        13,
        460,
        13651,
        5578,
        6906,
        7746,
        8278,
        383,
        1957,
        11,
        978,
        12547,
        11375,
        1979,
        294,
        1163,
        51026
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2975834608078003,
      "compression_ratio": 1.524647831916809,
      "no_speech_prob": 0.006090763956308365
    },
    {
      "id": 212,
      "seek": 87708,
      "start": 2271.9300073242184,
      "end": 2277.1699975585934,
      "text": " Cloud laufen können oder wo man Infrastruktur nicht so weitestgehend automatisieren kann.",
      "tokens": [
        51026,
        8061,
        41647,
        6310,
        4513,
        6020,
        587,
        38425,
        31543,
        1979,
        370,
        15306,
        377,
        27057,
        521,
        28034,
        271,
        5695,
        4028,
        13,
        51288
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2975834608078003,
      "compression_ratio": 1.524647831916809,
      "no_speech_prob": 0.006090763956308365
    },
    {
      "id": 213,
      "seek": 87708,
      "start": 2278.0899804687497,
      "end": 2281.969985351562,
      "text": " Wenn das allerdings gegen die andere Seite antritt, dann ist es schon ziemlich fundamental.",
      "tokens": [
        51334,
        7899,
        1482,
        35489,
        13953,
        978,
        10490,
        19748,
        2511,
        18579,
        11,
        3594,
        1418,
        785,
        4981,
        28901,
        8088,
        13,
        51528
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2975834608078003,
      "compression_ratio": 1.524647831916809,
      "no_speech_prob": 0.006090763956308365
    },
    {
      "id": 214,
      "seek": 90036,
      "start": 2282.8499902343747,
      "end": 2289.289992675781,
      "text": " Und jetzt ist vielleicht Feuer und Elektrizität also sehr, sehr, sehr weit quasi auf der",
      "tokens": [
        50408,
        2719,
        4354,
        1418,
        12547,
        39972,
        674,
        40321,
        24959,
        14053,
        611,
        5499,
        11,
        5499,
        11,
        5499,
        15306,
        20954,
        2501,
        1163,
        50730
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2445128709077835,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.01689242012798786
    },
    {
      "id": 215,
      "seek": 90036,
      "start": 2289.289992675781,
      "end": 2293.9300073242184,
      "text": " anderen Seite. Aber man könnte auch sagen, es tritt quasi gegen die Dampfmaschine halt",
      "tokens": [
        50730,
        11122,
        19748,
        13,
        5992,
        587,
        17646,
        2168,
        8360,
        11,
        785,
        504,
        593,
        20954,
        13953,
        978,
        413,
        30157,
        3799,
        36675,
        12479,
        50962
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2445128709077835,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.01689242012798786
    },
    {
      "id": 216,
      "seek": 90036,
      "start": 2293.9300073242184,
      "end": 2298.769973144531,
      "text": " irgendwie an. Und das ist vielleicht dann schon irgendwas, was wir alle auch dann",
      "tokens": [
        50962,
        20759,
        364,
        13,
        2719,
        1482,
        1418,
        12547,
        3594,
        4981,
        47090,
        11,
        390,
        1987,
        5430,
        2168,
        3594,
        51204
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2445128709077835,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.01689242012798786
    },
    {
      "id": 217,
      "seek": 90036,
      "start": 2298.769973144531,
      "end": 2301.6099999999997,
      "text": " wiederum nicht kennen, aber was vielleicht ein bisschen greifbarer ist.",
      "tokens": [
        51204,
        6216,
        449,
        1979,
        28445,
        11,
        4340,
        390,
        12547,
        1343,
        10763,
        6066,
        351,
        5356,
        260,
        1418,
        13,
        51346
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2445128709077835,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.01689242012798786
    },
    {
      "id": 218,
      "seek": 90036,
      "start": 2302.289992675781,
      "end": 2305.010024414062,
      "text": " Und da muss man schon sagen, da sind ganze Industrien weggefallen.",
      "tokens": [
        51380,
        2719,
        1120,
        6425,
        587,
        4981,
        8360,
        11,
        1120,
        3290,
        18898,
        16018,
        27378,
        15565,
        432,
        24425,
        13,
        51516
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2445128709077835,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.01689242012798786
    },
    {
      "id": 219,
      "seek": 90036,
      "start": 2305.3700097656247,
      "end": 2308.1699975585934,
      "text": " Und vor dem Hintergrund und das vermisse ich so ein bisschen.",
      "tokens": [
        51534,
        2719,
        4245,
        1371,
        35006,
        23701,
        674,
        1482,
        26319,
        7746,
        1893,
        370,
        1343,
        10763,
        13,
        51674
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2445128709077835,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.01689242012798786
    },
    {
      "id": 220,
      "seek": 92656,
      "start": 2308.4099877929684,
      "end": 2313.490004882812,
      "text": " Meine Frage, die Frage war ja so ein bisschen, also was müsste man heute schon tun?",
      "tokens": [
        50376,
        22258,
        13685,
        11,
        978,
        13685,
        1516,
        2784,
        370,
        1343,
        10763,
        11,
        611,
        390,
        42962,
        587,
        9801,
        4981,
        4267,
        30,
        50630
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3011868894100189,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.015642045065760612
    },
    {
      "id": 221,
      "seek": 92656,
      "start": 2313.529982910156,
      "end": 2318.1300195312497,
      "text": " Und das verstehe ich nicht, weil ich quasi eigentlich unsere Disziplin immer als sehr",
      "tokens": [
        50632,
        2719,
        1482,
        22442,
        675,
        1893,
        1979,
        11,
        7689,
        1893,
        20954,
        10926,
        14339,
        4208,
        3992,
        48102,
        5578,
        3907,
        5499,
        50862
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3011868894100189,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.015642045065760612
    },
    {
      "id": 222,
      "seek": 92656,
      "start": 2318.1300195312497,
      "end": 2323.250014648437,
      "text": " neugierig wahrnehme. Quasi diese Offenheit gegenüber einer Sache.",
      "tokens": [
        50862,
        408,
        697,
        811,
        328,
        21628,
        30540,
        1398,
        13,
        2326,
        8483,
        6705,
        6318,
        268,
        8480,
        41830,
        6850,
        31452,
        13,
        51118
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3011868894100189,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.015642045065760612
    },
    {
      "id": 223,
      "seek": 92656,
      "start": 2323.6499780273434,
      "end": 2329.010024414062,
      "text": " Und was ich halt häufig wahrnehme, ist entweder so eine relativ theoretische Sache.",
      "tokens": [
        51138,
        2719,
        390,
        1893,
        12479,
        47543,
        21628,
        30540,
        1398,
        11,
        1418,
        948,
        49070,
        370,
        3018,
        21960,
        14308,
        7864,
        31452,
        13,
        51406
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3011868894100189,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.015642045065760612
    },
    {
      "id": 224,
      "seek": 92656,
      "start": 2329.050002441406,
      "end": 2332.4099877929684,
      "text": " Habe ich irgendwas gelesen? Aber ich habe es nie probiert.",
      "tokens": [
        51408,
        389,
        4488,
        1893,
        47090,
        4087,
        17403,
        30,
        5992,
        1893,
        6015,
        785,
        2838,
        1239,
        4859,
        13,
        51576
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3011868894100189,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.015642045065760612
    },
    {
      "id": 225,
      "seek": 92656,
      "start": 2333.6900170898434,
      "end": 2336.769973144531,
      "text": " Und eine der Sachen, die wir machen, um da mal ein konkretes Beispiel zu machen, wir haben",
      "tokens": [
        51640,
        2719,
        3018,
        1163,
        26074,
        11,
        978,
        1987,
        7069,
        11,
        1105,
        1120,
        2806,
        1343,
        36500,
        279,
        13772,
        2164,
        7069,
        11,
        1987,
        3084,
        51794
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3011868894100189,
      "compression_ratio": 1.616438388824463,
      "no_speech_prob": 0.015642045065760612
    },
    {
      "id": 226,
      "seek": 95516,
      "start": 2336.810012207031,
      "end": 2341.4500268554684,
      "text": " einmal im Monat so einen Self-Education-Friday und eigentlich versuchen wir da, die",
      "tokens": [
        50366,
        11078,
        566,
        4713,
        267,
        370,
        4891,
        16348,
        12,
        36,
        8117,
        399,
        12,
        40305,
        4708,
        674,
        10926,
        34749,
        1987,
        1120,
        11,
        978,
        50598
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38453248143196106,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.0026700706221163273
    },
    {
      "id": 227,
      "seek": 95516,
      "start": 2341.4500268554684,
      "end": 2344.570021972656,
      "text": " Leute zu ermutigen, mal Sachen auszuprobieren.",
      "tokens": [
        50598,
        13495,
        2164,
        25253,
        325,
        3213,
        11,
        2806,
        26074,
        3437,
        89,
        1010,
        16614,
        5695,
        13,
        50754
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38453248143196106,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.0026700706221163273
    },
    {
      "id": 228,
      "seek": 95516,
      "start": 2344.729995117187,
      "end": 2348.810012207031,
      "text": " Das heißt, es gibt verschiedene Themen, aber unter anderem halt auch Gen-AI, wo man",
      "tokens": [
        50762,
        2846,
        13139,
        11,
        785,
        6089,
        35411,
        39229,
        11,
        4340,
        8662,
        293,
        7333,
        12479,
        2168,
        3632,
        12,
        48698,
        11,
        6020,
        587,
        50966
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38453248143196106,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.0026700706221163273
    },
    {
      "id": 229,
      "seek": 95516,
      "start": 2349.1300195312497,
      "end": 2353.010024414062,
      "text": " mal sehen, guck mal, man kann halt auch einen LLM auf seinem Notebook laufen lassen, kann",
      "tokens": [
        50982,
        2806,
        11333,
        11,
        695,
        547,
        2806,
        11,
        587,
        4028,
        12479,
        2168,
        4891,
        441,
        43,
        44,
        2501,
        29187,
        11633,
        2939,
        41647,
        16168,
        11,
        4028,
        51176
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38453248143196106,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.0026700706221163273
    },
    {
      "id": 230,
      "seek": 95516,
      "start": 2353.010024414062,
      "end": 2355.050002441406,
      "text": " ja halt irgendwie deiner täglichen Arbeit halt irgendwie helfen.",
      "tokens": [
        51176,
        2784,
        12479,
        20759,
        368,
        4564,
        14619,
        8856,
        268,
        18604,
        12479,
        20759,
        29966,
        13,
        51278
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38453248143196106,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.0026700706221163273
    },
    {
      "id": 231,
      "seek": 95516,
      "start": 2356.3700097656247,
      "end": 2360.3700097656247,
      "text": " Ich will jetzt nicht sagen spielerische Herangehensweise, aber ich habe unsere",
      "tokens": [
        51344,
        3141,
        486,
        4354,
        1979,
        8360,
        637,
        1187,
        260,
        7864,
        3204,
        933,
        71,
        694,
        13109,
        11,
        4340,
        1893,
        6015,
        14339,
        51544
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38453248143196106,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.0026700706221163273
    },
    {
      "id": 232,
      "seek": 95516,
      "start": 2360.3700097656247,
      "end": 2364.8499902343747,
      "text": " Disziplin eigentlich immer so verstanden, Neugier, Automating all the things und halt",
      "tokens": [
        51544,
        4208,
        3992,
        48102,
        10926,
        5578,
        370,
        1306,
        33946,
        11,
        1734,
        697,
        811,
        11,
        24619,
        990,
        439,
        264,
        721,
        674,
        12479,
        51768
      ],
      "temperature": 0.0,
      "avg_logprob": -0.38453248143196106,
      "compression_ratio": 1.600000023841858,
      "no_speech_prob": 0.0026700706221163273
    },
    {
      "id": 233,
      "seek": 98324,
      "start": 2364.8499902343747,
      "end": 2369.490004882812,
      "text": " einfach Sachen auszuprobieren und natürlich quasi dieses",
      "tokens": [
        50364,
        7281,
        26074,
        3437,
        89,
        1010,
        16614,
        5695,
        674,
        8762,
        20954,
        12113,
        50596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2951425015926361,
      "compression_ratio": 1.724252462387085,
      "no_speech_prob": 0.006287492346018553
    },
    {
      "id": 234,
      "seek": 98324,
      "start": 2369.490004882812,
      "end": 2374.209975585937,
      "text": " ganz große Bild, was das Szenario, was Stefan gerade gezeichnet hat, also quasi",
      "tokens": [
        50596,
        6312,
        19691,
        15746,
        11,
        390,
        1482,
        318,
        2904,
        4912,
        11,
        390,
        318,
        975,
        20361,
        12117,
        1519,
        32338,
        7129,
        2385,
        11,
        611,
        20954,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2951425015926361,
      "compression_ratio": 1.724252462387085,
      "no_speech_prob": 0.006287492346018553
    },
    {
      "id": 235,
      "seek": 98324,
      "start": 2374.209975585937,
      "end": 2377.570021972656,
      "text": " vielleicht gibt es gar keine Software, das mag irgendwann mal kommen.",
      "tokens": [
        50832,
        12547,
        6089,
        785,
        3691,
        9252,
        27428,
        11,
        1482,
        2258,
        34313,
        2806,
        11729,
        13,
        51000
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2951425015926361,
      "compression_ratio": 1.724252462387085,
      "no_speech_prob": 0.006287492346018553
    },
    {
      "id": 236,
      "seek": 98324,
      "start": 2377.729995117187,
      "end": 2381.050002441406,
      "text": " Und da kann man sich natürlich jetzt vortrefflich drüber streiten, weil man es noch",
      "tokens": [
        51008,
        2719,
        1120,
        4028,
        587,
        3041,
        8762,
        4354,
        371,
        477,
        265,
        602,
        1739,
        1224,
        12670,
        2242,
        6009,
        11,
        7689,
        587,
        785,
        3514,
        51174
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2951425015926361,
      "compression_ratio": 1.724252462387085,
      "no_speech_prob": 0.006287492346018553
    },
    {
      "id": 237,
      "seek": 98324,
      "start": 2381.050002441406,
      "end": 2383.810012207031,
      "text": " nicht, weil es ist nicht around the corner wahrscheinlich.",
      "tokens": [
        51174,
        1979,
        11,
        7689,
        785,
        1418,
        1979,
        926,
        264,
        4538,
        30957,
        13,
        51312
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2951425015926361,
      "compression_ratio": 1.724252462387085,
      "no_speech_prob": 0.006287492346018553
    },
    {
      "id": 238,
      "seek": 98324,
      "start": 2384.3700097656247,
      "end": 2387.250014648437,
      "text": " Aber quasi was kann ich heute quasi einfacher machen?",
      "tokens": [
        51340,
        5992,
        20954,
        390,
        4028,
        1893,
        9801,
        20954,
        38627,
        4062,
        7069,
        30,
        51484
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2951425015926361,
      "compression_ratio": 1.724252462387085,
      "no_speech_prob": 0.006287492346018553
    },
    {
      "id": 239,
      "seek": 98324,
      "start": 2387.250014648437,
      "end": 2388.8499902343747,
      "text": " Womit kann ich mein Leben leichter machen?",
      "tokens": [
        51484,
        343,
        298,
        270,
        4028,
        1893,
        10777,
        15399,
        28333,
        260,
        7069,
        30,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2951425015926361,
      "compression_ratio": 1.724252462387085,
      "no_speech_prob": 0.006287492346018553
    },
    {
      "id": 240,
      "seek": 98324,
      "start": 2388.8499902343747,
      "end": 2393.570021972656,
      "text": " Wo kann ich halt irgendwie das wegautomatisieren oder auslagern, was",
      "tokens": [
        51564,
        6622,
        4028,
        1893,
        12479,
        20759,
        1482,
        15565,
        1375,
        34295,
        271,
        5695,
        4513,
        3437,
        27298,
        1248,
        11,
        390,
        51800
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2951425015926361,
      "compression_ratio": 1.724252462387085,
      "no_speech_prob": 0.006287492346018553
    },
    {
      "id": 241,
      "seek": 101196,
      "start": 2393.570021972656,
      "end": 2395.769973144531,
      "text": " mir vielleicht auch persönlich gar keinen Spaß macht?",
      "tokens": [
        50364,
        3149,
        12547,
        2168,
        42699,
        3691,
        20624,
        27460,
        10857,
        30,
        50474
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23470887541770935,
      "compression_ratio": 1.601265788078308,
      "no_speech_prob": 0.05747535824775696
    },
    {
      "id": 242,
      "seek": 101196,
      "start": 2396.1699975585934,
      "end": 2400.4099877929684,
      "text": " Das vermisse ich. Also für mich beginnt das tatsächlich individuell im Kopf.",
      "tokens": [
        50494,
        2846,
        26319,
        7746,
        1893,
        13,
        2743,
        2959,
        6031,
        1841,
        580,
        1482,
        20796,
        2461,
        13789,
        566,
        28231,
        13,
        50706
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23470887541770935,
      "compression_ratio": 1.601265788078308,
      "no_speech_prob": 0.05747535824775696
    },
    {
      "id": 243,
      "seek": 101196,
      "start": 2402.4500268554684,
      "end": 2407.1700585937497,
      "text": " Ich habe gerade kam im Chat ein Kommentar zu dem Dampfmaschinenvergleich und",
      "tokens": [
        50808,
        3141,
        6015,
        12117,
        9727,
        566,
        27503,
        1343,
        33708,
        289,
        2164,
        1371,
        413,
        30157,
        3799,
        339,
        5636,
        331,
        70,
        8445,
        674,
        51044
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23470887541770935,
      "compression_ratio": 1.601265788078308,
      "no_speech_prob": 0.05747535824775696
    },
    {
      "id": 244,
      "seek": 101196,
      "start": 2407.1700585937497,
      "end": 2410.689956054687,
      "text": " zwar schreibt Happy Tree, der Unterschied zu dem Dampfmaschinenvergleich ist, dass wenn",
      "tokens": [
        51044,
        19054,
        956,
        31174,
        8277,
        22291,
        11,
        1163,
        41414,
        2164,
        1371,
        413,
        30157,
        3799,
        339,
        5636,
        331,
        70,
        8445,
        1418,
        11,
        2658,
        4797,
        51220
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23470887541770935,
      "compression_ratio": 1.601265788078308,
      "no_speech_prob": 0.05747535824775696
    },
    {
      "id": 245,
      "seek": 101196,
      "start": 2410.689956054687,
      "end": 2413.3700097656247,
      "text": " das funktioniert, es keinen neuen Arbeitsbereich gibt.",
      "tokens": [
        51220,
        1482,
        26160,
        11,
        785,
        20624,
        21387,
        23262,
        48422,
        6089,
        13,
        51354
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23470887541770935,
      "compression_ratio": 1.601265788078308,
      "no_speech_prob": 0.05747535824775696
    },
    {
      "id": 246,
      "seek": 101196,
      "start": 2414.8900292968747,
      "end": 2418.1700585937497,
      "text": " Ich weiß nicht mehr, wer von euch es sagt, aber irgendjemand sagte schon vorhin, dass",
      "tokens": [
        51430,
        3141,
        13385,
        1979,
        5417,
        11,
        2612,
        2957,
        10403,
        785,
        15764,
        11,
        4340,
        11093,
        73,
        18941,
        36771,
        4981,
        4245,
        10876,
        11,
        2658,
        51594
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23470887541770935,
      "compression_ratio": 1.601265788078308,
      "no_speech_prob": 0.05747535824775696
    },
    {
      "id": 247,
      "seek": 101196,
      "start": 2418.1700585937497,
      "end": 2421.1700585937497,
      "text": " man die Beine von seinem eigenen Stühlchen abschneidet oder so.",
      "tokens": [
        51594,
        587,
        978,
        879,
        533,
        2957,
        29187,
        28702,
        745,
        21692,
        2470,
        1950,
        339,
        716,
        39741,
        4513,
        370,
        13,
        51744
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23470887541770935,
      "compression_ratio": 1.601265788078308,
      "no_speech_prob": 0.05747535824775696
    },
    {
      "id": 248,
      "seek": 103956,
      "start": 2421.689956054687,
      "end": 2422.6099999999997,
      "text": " Du warst das, genau.",
      "tokens": [
        50390,
        5153,
        1516,
        372,
        1482,
        11,
        12535,
        13,
        50436
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4451488256454468,
      "compression_ratio": 1.5220884084701538,
      "no_speech_prob": 0.008441057056188583
    },
    {
      "id": 249,
      "seek": 103956,
      "start": 2422.6099999999997,
      "end": 2425.5699609374997,
      "text": " Ich war beide, beide sogar, genau.",
      "tokens": [
        50436,
        3141,
        1516,
        35831,
        11,
        35831,
        19485,
        11,
        12535,
        13,
        50584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4451488256454468,
      "compression_ratio": 1.5220884084701538,
      "no_speech_prob": 0.008441057056188583
    },
    {
      "id": 250,
      "seek": 103956,
      "start": 2426.8900292968747,
      "end": 2428.1300195312497,
      "text": " Nee, ich wollte bloß kurz was sagen.",
      "tokens": [
        50650,
        22067,
        11,
        1893,
        24509,
        1749,
        2536,
        20465,
        390,
        8360,
        13,
        50712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4451488256454468,
      "compression_ratio": 1.5220884084701538,
      "no_speech_prob": 0.008441057056188583
    },
    {
      "id": 251,
      "seek": 103956,
      "start": 2428.1300195312497,
      "end": 2432.530043945312,
      "text": " Also ich, jetzt geht es ja um die Softwareentwicklung so ein bisschen weniger um",
      "tokens": [
        50712,
        2743,
        1893,
        11,
        4354,
        7095,
        785,
        2784,
        1105,
        978,
        27428,
        317,
        16038,
        17850,
        370,
        1343,
        10763,
        23224,
        1105,
        50932
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4451488256454468,
      "compression_ratio": 1.5220884084701538,
      "no_speech_prob": 0.008441057056188583
    },
    {
      "id": 252,
      "seek": 103956,
      "start": 2432.530043945312,
      "end": 2436.8499902343747,
      "text": " allgemeine. Aber für das Allgemeine glaube ich, dass",
      "tokens": [
        50932,
        439,
        31964,
        533,
        13,
        5992,
        2959,
        1482,
        1057,
        31964,
        533,
        13756,
        1893,
        11,
        2658,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4451488256454468,
      "compression_ratio": 1.5220884084701538,
      "no_speech_prob": 0.008441057056188583
    },
    {
      "id": 253,
      "seek": 103956,
      "start": 2437.530043945312,
      "end": 2442.010024414062,
      "text": " da würde ich dem Kommentar aus dem Chat sehr, sehr zustimmen.",
      "tokens": [
        51182,
        1120,
        11942,
        1893,
        1371,
        33708,
        289,
        3437,
        1371,
        27503,
        5499,
        11,
        5499,
        45034,
        32076,
        13,
        51406
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4451488256454468,
      "compression_ratio": 1.5220884084701538,
      "no_speech_prob": 0.008441057056188583
    },
    {
      "id": 254,
      "seek": 103956,
      "start": 2442.5699609374997,
      "end": 2447.8900292968747,
      "text": " Wir haben durch Rationalisierung alle Leute aus dem Agrarsektor in den Industriesektor,",
      "tokens": [
        51434,
        4347,
        3084,
        7131,
        497,
        1478,
        32531,
        5430,
        13495,
        3437,
        1371,
        24454,
        11668,
        28359,
        294,
        1441,
        16018,
        470,
        1130,
        28359,
        11,
        51700
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4451488256454468,
      "compression_ratio": 1.5220884084701538,
      "no_speech_prob": 0.008441057056188583
    },
    {
      "id": 255,
      "seek": 106628,
      "start": 2448.010024414062,
      "end": 2452.4100488281247,
      "text": " der glücklicherweise durch die gleichen Bedingungen zur gleichen Zeit entstanden ist,",
      "tokens": [
        50370,
        1163,
        1563,
        6536,
        25215,
        13109,
        7131,
        978,
        49069,
        363,
        9794,
        5084,
        7147,
        49069,
        9394,
        948,
        33946,
        1418,
        11,
        50590
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23290280997753143,
      "compression_ratio": 1.6640926599502563,
      "no_speech_prob": 0.002434177789837122
    },
    {
      "id": 256,
      "seek": 106628,
      "start": 2452.4100488281247,
      "end": 2455.689956054687,
      "text": " verfrachtet. Und dann haben wir alle Leute aus dem Industriesektor in den",
      "tokens": [
        50590,
        1306,
        5779,
        48833,
        13,
        2719,
        3594,
        3084,
        1987,
        5430,
        13495,
        3437,
        1371,
        16018,
        470,
        1130,
        28359,
        294,
        1441,
        50754
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23290280997753143,
      "compression_ratio": 1.6640926599502563,
      "no_speech_prob": 0.002434177789837122
    },
    {
      "id": 257,
      "seek": 106628,
      "start": 2455.689956054687,
      "end": 2457.209975585937,
      "text": " Dienstleistungssektor verfrachtet.",
      "tokens": [
        50754,
        43932,
        46820,
        5846,
        405,
        28359,
        1306,
        5779,
        48833,
        13,
        50830
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23290280997753143,
      "compression_ratio": 1.6640926599502563,
      "no_speech_prob": 0.002434177789837122
    },
    {
      "id": 258,
      "seek": 106628,
      "start": 2458.5699609374997,
      "end": 2463.5699609374997,
      "text": " Wenn man sich heute eine Fabrik anguckt, dann ist die quasi komplett leer, kaufen ganz",
      "tokens": [
        50898,
        7899,
        587,
        3041,
        9801,
        3018,
        17440,
        14456,
        2562,
        47800,
        11,
        3594,
        1418,
        978,
        20954,
        32261,
        34172,
        11,
        42083,
        6312,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23290280997753143,
      "compression_ratio": 1.6640926599502563,
      "no_speech_prob": 0.002434177789837122
    },
    {
      "id": 259,
      "seek": 106628,
      "start": 2463.5699609374997,
      "end": 2468.689956054687,
      "text": " wenig Leute rum. Und jetzt verlagern wir",
      "tokens": [
        51148,
        20911,
        13495,
        8347,
        13,
        2719,
        4354,
        19441,
        559,
        1248,
        1987,
        51404
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23290280997753143,
      "compression_ratio": 1.6640926599502563,
      "no_speech_prob": 0.002434177789837122
    },
    {
      "id": 260,
      "seek": 106628,
      "start": 2468.689956054687,
      "end": 2470.6099999999997,
      "text": " Leute aus dem Dienstleistungssektor irgendwo hin.",
      "tokens": [
        51404,
        13495,
        3437,
        1371,
        43932,
        46820,
        5846,
        405,
        28359,
        40865,
        14102,
        13,
        51500
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23290280997753143,
      "compression_ratio": 1.6640926599502563,
      "no_speech_prob": 0.002434177789837122
    },
    {
      "id": 261,
      "seek": 106628,
      "start": 2471.5699609374997,
      "end": 2474.530043945312,
      "text": " Aber leider ist halt, das war jetzt so der dritte Sektor.",
      "tokens": [
        51548,
        5992,
        29115,
        1418,
        12479,
        11,
        1482,
        1516,
        4354,
        370,
        1163,
        1224,
        9786,
        318,
        8192,
        284,
        13,
        51696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23290280997753143,
      "compression_ratio": 1.6640926599502563,
      "no_speech_prob": 0.002434177789837122
    },
    {
      "id": 262,
      "seek": 109292,
      "start": 2474.729995117187,
      "end": 2479.3299707031247,
      "text": " Aber es ist unklar, wo die Leute hinverlagert werden.",
      "tokens": [
        50374,
        5992,
        785,
        1418,
        517,
        74,
        2200,
        11,
        6020,
        978,
        13495,
        14102,
        331,
        27298,
        911,
        4604,
        13,
        50604
      ],
      "temperature": 0.0,
      "avg_logprob": -0.306167870759964,
      "compression_ratio": 1.4789272546768188,
      "no_speech_prob": 0.018536986783146858
    },
    {
      "id": 263,
      "seek": 109292,
      "start": 2479.4100488281247,
      "end": 2481.490004882812,
      "text": " Also das ist, glaube ich, der ganz große Unterschied zur Dampfmaschine.",
      "tokens": [
        50608,
        2743,
        1482,
        1418,
        11,
        13756,
        1893,
        11,
        1163,
        6312,
        19691,
        41414,
        7147,
        413,
        30157,
        3799,
        36675,
        13,
        50712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.306167870759964,
      "compression_ratio": 1.4789272546768188,
      "no_speech_prob": 0.018536986783146858
    },
    {
      "id": 264,
      "seek": 109292,
      "start": 2482.530043945312,
      "end": 2488.250014648437,
      "text": " Und da würde ich mich dem ein bisschen dem Kommentar anschließen.",
      "tokens": [
        50764,
        2719,
        1120,
        11942,
        1893,
        6031,
        1371,
        1343,
        10763,
        1371,
        33708,
        289,
        31508,
        38665,
        13,
        51050
      ],
      "temperature": 0.0,
      "avg_logprob": -0.306167870759964,
      "compression_ratio": 1.4789272546768188,
      "no_speech_prob": 0.018536986783146858
    },
    {
      "id": 265,
      "seek": 109292,
      "start": 2488.689956054687,
      "end": 2493.290053710937,
      "text": " Ich habe noch einen Punkt zu dem vorher zu dem Energiethema, was mir immer ein bisschen",
      "tokens": [
        51072,
        3141,
        6015,
        3514,
        4891,
        25487,
        2164,
        1371,
        29195,
        2164,
        1371,
        48195,
        1684,
        71,
        5619,
        11,
        390,
        3149,
        5578,
        1343,
        10763,
        51302
      ],
      "temperature": 0.0,
      "avg_logprob": -0.306167870759964,
      "compression_ratio": 1.4789272546768188,
      "no_speech_prob": 0.018536986783146858
    },
    {
      "id": 266,
      "seek": 109292,
      "start": 2493.290053710937,
      "end": 2496.3299707031247,
      "text": " aufstößt, also jetzt hier nur allgemein.",
      "tokens": [
        51302,
        2501,
        372,
        18595,
        83,
        11,
        611,
        4354,
        3296,
        4343,
        439,
        31964,
        259,
        13,
        51454
      ],
      "temperature": 0.0,
      "avg_logprob": -0.306167870759964,
      "compression_ratio": 1.4789272546768188,
      "no_speech_prob": 0.018536986783146858
    },
    {
      "id": 267,
      "seek": 109292,
      "start": 2497.209975585937,
      "end": 2502.3700097656247,
      "text": " Es wird sehr, sehr stark vermischt an Training und Inferenz.",
      "tokens": [
        51498,
        2313,
        4578,
        5499,
        11,
        5499,
        17417,
        26319,
        271,
        4701,
        364,
        20620,
        674,
        682,
        612,
        11368,
        13,
        51756
      ],
      "temperature": 0.0,
      "avg_logprob": -0.306167870759964,
      "compression_ratio": 1.4789272546768188,
      "no_speech_prob": 0.018536986783146858
    },
    {
      "id": 268,
      "seek": 112076,
      "start": 2502.4100488281247,
      "end": 2507.490004882812,
      "text": " Also das hat natürlich jetzt auch stark mit dem aktuell, wie wir arbeiten, zu tun, mit",
      "tokens": [
        50366,
        2743,
        1482,
        2385,
        8762,
        4354,
        2168,
        17417,
        2194,
        1371,
        36267,
        11,
        3355,
        1987,
        23162,
        11,
        2164,
        4267,
        11,
        2194,
        50620
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3145451247692108,
      "compression_ratio": 1.8805969953536987,
      "no_speech_prob": 0.0022513039875775576
    },
    {
      "id": 269,
      "seek": 112076,
      "start": 2507.490004882812,
      "end": 2511.530043945312,
      "text": " LLMs und mit dem quasi mit dem Batch arbeiten, also mit dem Trainieren und mit dem",
      "tokens": [
        50620,
        441,
        43,
        26386,
        674,
        2194,
        1371,
        20954,
        2194,
        1371,
        363,
        852,
        23162,
        11,
        611,
        2194,
        1371,
        28029,
        5695,
        674,
        2194,
        1371,
        50822
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3145451247692108,
      "compression_ratio": 1.8805969953536987,
      "no_speech_prob": 0.0022513039875775576
    },
    {
      "id": 270,
      "seek": 112076,
      "start": 2511.530043945312,
      "end": 2513.530043945312,
      "text": " Benutzen und mit dem Trainieren und mit dem Benutzen.",
      "tokens": [
        50822,
        3964,
        325,
        2904,
        674,
        2194,
        1371,
        28029,
        5695,
        674,
        2194,
        1371,
        3964,
        325,
        2904,
        13,
        50922
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3145451247692108,
      "compression_ratio": 1.8805969953536987,
      "no_speech_prob": 0.0022513039875775576
    },
    {
      "id": 271,
      "seek": 112076,
      "start": 2515.6099999999997,
      "end": 2519.8499902343747,
      "text": " Aber trotzdem, glaube ich, muss man schon auch immer ein bisschen unterscheiden, weil",
      "tokens": [
        51026,
        5992,
        28325,
        11,
        13756,
        1893,
        11,
        6425,
        587,
        4981,
        2168,
        5578,
        1343,
        10763,
        20983,
        1876,
        4380,
        11,
        7689,
        51238
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3145451247692108,
      "compression_ratio": 1.8805969953536987,
      "no_speech_prob": 0.0022513039875775576
    },
    {
      "id": 272,
      "seek": 112076,
      "start": 2519.8499902343747,
      "end": 2523.929946289062,
      "text": " Ralf vorher, glaube ich, es gesagt hat oder Andreas gesagt mit dem Laptop.",
      "tokens": [
        51238,
        497,
        1678,
        29195,
        11,
        13756,
        1893,
        11,
        785,
        12260,
        2385,
        4513,
        38785,
        12260,
        2194,
        1371,
        441,
        2796,
        404,
        13,
        51442
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3145451247692108,
      "compression_ratio": 1.8805969953536987,
      "no_speech_prob": 0.0022513039875775576
    },
    {
      "id": 273,
      "seek": 112076,
      "start": 2524.290053710937,
      "end": 2525.929946289062,
      "text": " Aber letztlich kann ich es auf meinem Handy haben.",
      "tokens": [
        51460,
        5992,
        35262,
        1739,
        4028,
        1893,
        785,
        2501,
        24171,
        47006,
        3084,
        13,
        51542
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3145451247692108,
      "compression_ratio": 1.8805969953536987,
      "no_speech_prob": 0.0022513039875775576
    },
    {
      "id": 274,
      "seek": 112076,
      "start": 2526.010024414062,
      "end": 2531.5699609374997,
      "text": " Also die Energienutzung von LLMs nutzen und von LLMs trainieren ist",
      "tokens": [
        51546,
        2743,
        978,
        48195,
        1053,
        12950,
        1063,
        2957,
        441,
        43,
        26386,
        36905,
        674,
        2957,
        441,
        43,
        26386,
        3847,
        5695,
        1418,
        51824
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3145451247692108,
      "compression_ratio": 1.8805969953536987,
      "no_speech_prob": 0.0022513039875775576
    },
    {
      "id": 275,
      "seek": 114996,
      "start": 2531.5699609374997,
      "end": 2533.250014648437,
      "text": " potenziell auch nochmal eine andere.",
      "tokens": [
        50364,
        1847,
        32203,
        285,
        2168,
        26509,
        3018,
        10490,
        13,
        50448
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34073811769485474,
      "compression_ratio": 1.6561086177825928,
      "no_speech_prob": 0.017689155414700508
    },
    {
      "id": 276,
      "seek": 114996,
      "start": 2534.8099511718747,
      "end": 2539.250014648437,
      "text": " Das wird auch bei Performance oft und bei Hardware Performance oft irgendwie alles in",
      "tokens": [
        50526,
        2846,
        4578,
        2168,
        4643,
        25047,
        11649,
        674,
        4643,
        11817,
        3039,
        25047,
        11649,
        20759,
        7874,
        294,
        50748
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34073811769485474,
      "compression_ratio": 1.6561086177825928,
      "no_speech_prob": 0.017689155414700508
    },
    {
      "id": 277,
      "seek": 114996,
      "start": 2539.250014648437,
      "end": 2540.8900292968747,
      "text": " einen in einen Topf geworfen.",
      "tokens": [
        50748,
        4891,
        294,
        4891,
        8840,
        69,
        6906,
        284,
        6570,
        13,
        50830
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34073811769485474,
      "compression_ratio": 1.6561086177825928,
      "no_speech_prob": 0.017689155414700508
    },
    {
      "id": 278,
      "seek": 114996,
      "start": 2541.490004882812,
      "end": 2546.490004882812,
      "text": " Für mich sind es aber aktuell durch den Batch Modus zwei fundamental unterschiedliche",
      "tokens": [
        50860,
        14990,
        6031,
        3290,
        785,
        4340,
        36267,
        7131,
        1441,
        363,
        852,
        6583,
        301,
        12002,
        8088,
        30058,
        10185,
        51110
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34073811769485474,
      "compression_ratio": 1.6561086177825928,
      "no_speech_prob": 0.017689155414700508
    },
    {
      "id": 279,
      "seek": 114996,
      "start": 2547.8099511718747,
      "end": 2552.449965820312,
      "text": " Operationsmodelle von von AIs mit unterschiedlicher Performance, unterschiedlichen",
      "tokens": [
        51176,
        36381,
        8014,
        4434,
        2957,
        2957,
        316,
        6802,
        2194,
        30058,
        25215,
        25047,
        11,
        30058,
        10193,
        51408
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34073811769485474,
      "compression_ratio": 1.6561086177825928,
      "no_speech_prob": 0.017689155414700508
    },
    {
      "id": 280,
      "seek": 114996,
      "start": 2552.449965820312,
      "end": 2553.729995117187,
      "text": " Energieverbrauch und so weiter und so fort.",
      "tokens": [
        51408,
        35309,
        331,
        6198,
        625,
        674,
        370,
        8988,
        674,
        370,
        5009,
        13,
        51472
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34073811769485474,
      "compression_ratio": 1.6561086177825928,
      "no_speech_prob": 0.017689155414700508
    },
    {
      "id": 281,
      "seek": 117212,
      "start": 2554.729995117187,
      "end": 2560.490004882812,
      "text": " Vielleicht ein paar Gedanken, also weil du es gerade sagtest, André, von wegen, wo",
      "tokens": [
        50414,
        29838,
        1343,
        16509,
        44612,
        11,
        611,
        7689,
        1581,
        785,
        12117,
        15764,
        377,
        11,
        400,
        10521,
        11,
        2957,
        32855,
        11,
        6020,
        50702
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 282,
      "seek": 117212,
      "start": 2560.490004882812,
      "end": 2561.490004882812,
      "text": " trägt das an?",
      "tokens": [
        50702,
        33367,
        10463,
        1482,
        364,
        30,
        50752
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 283,
      "seek": 117212,
      "start": 2562.0499414062497,
      "end": 2563.530043945312,
      "text": " Also wir haben das ja im Stream exerziert.",
      "tokens": [
        50780,
        2743,
        1987,
        3084,
        1482,
        2784,
        566,
        24904,
        454,
        260,
        89,
        4859,
        13,
        50854
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 284,
      "seek": 117212,
      "start": 2563.530043945312,
      "end": 2569.6500390624997,
      "text": " Das heißt, wir haben gesagt, liebes, liebes Chetchipi Team, mach doch mal die",
      "tokens": [
        50854,
        2846,
        13139,
        11,
        1987,
        3084,
        12260,
        11,
        4544,
        6446,
        11,
        287,
        414,
        6446,
        761,
        7858,
        647,
        72,
        7606,
        11,
        2246,
        9243,
        2806,
        978,
        51160
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 285,
      "seek": 117212,
      "start": 2569.6500390624997,
      "end": 2570.8900292968747,
      "text": " SAP Beispielaufgabe.",
      "tokens": [
        51160,
        27743,
        13772,
        9507,
        70,
        4488,
        13,
        51222
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 286,
      "seek": 117212,
      "start": 2571.5699609374997,
      "end": 2575.010024414062,
      "text": " So, damit lassen wir es halt in mir antreten gegen Software Architekten, die sich halt",
      "tokens": [
        51256,
        407,
        11,
        9479,
        16168,
        1987,
        785,
        12479,
        294,
        3149,
        2511,
        35383,
        13953,
        27428,
        10984,
        642,
        2320,
        268,
        11,
        978,
        3041,
        12479,
        51428
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 287,
      "seek": 117212,
      "start": 2575.010024414062,
      "end": 2576.010024414062,
      "text": " zertifizieren lassen.",
      "tokens": [
        51428,
        710,
        911,
        351,
        590,
        5695,
        16168,
        13,
        51478
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 288,
      "seek": 117212,
      "start": 2576.5699609374997,
      "end": 2582.0499414062497,
      "text": " Und das Schlimme dabei ist, dass die Ergebnisse zumindest scheinbar gar nicht so",
      "tokens": [
        51506,
        2719,
        1482,
        2065,
        4197,
        1398,
        14967,
        1418,
        11,
        2658,
        978,
        34657,
        31481,
        38082,
        25690,
        259,
        5356,
        3691,
        1979,
        370,
        51780
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 289,
      "seek": 117212,
      "start": 2582.0499414062497,
      "end": 2583.0499414062497,
      "text": " schlicht sind.",
      "tokens": [
        51780,
        956,
        20238,
        3290,
        13,
        51830
      ],
      "temperature": 0.0,
      "avg_logprob": -0.5089982151985168,
      "compression_ratio": 1.5326460599899292,
      "no_speech_prob": 0.11419718712568283
    },
    {
      "id": 290,
      "seek": 120212,
      "start": 2584.6500390624997,
      "end": 2589.6500390624997,
      "text": " Und das ist genau das, was wir jetzt vor 14 Tagen in dem in dem Stream nochmal gesehen",
      "tokens": [
        50410,
        2719,
        1482,
        1418,
        12535,
        1482,
        11,
        390,
        1987,
        4354,
        4245,
        3499,
        41721,
        294,
        1371,
        294,
        1371,
        24904,
        26509,
        21535,
        50660
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31181129813194275,
      "compression_ratio": 1.5838710069656372,
      "no_speech_prob": 0.0008166993502527475
    },
    {
      "id": 291,
      "seek": 120212,
      "start": 2589.6500390624997,
      "end": 2593.3700097656247,
      "text": " haben. Also wenn ich mir Qualitätsszenarien angucke, also was sind die Dinge, die dieses",
      "tokens": [
        50660,
        3084,
        13,
        2743,
        4797,
        1893,
        3149,
        13616,
        13187,
        1373,
        82,
        2904,
        289,
        1053,
        2562,
        1311,
        330,
        11,
        611,
        390,
        3290,
        978,
        25102,
        11,
        978,
        12113,
        50846
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31181129813194275,
      "compression_ratio": 1.5838710069656372,
      "no_speech_prob": 0.0008166993502527475
    },
    {
      "id": 292,
      "seek": 120212,
      "start": 2593.3700097656247,
      "end": 2594.4100488281247,
      "text": " Verfahren leisten soll?",
      "tokens": [
        50846,
        24583,
        7079,
        476,
        468,
        268,
        7114,
        30,
        50898
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31181129813194275,
      "compression_ratio": 1.5838710069656372,
      "no_speech_prob": 0.0008166993502527475
    },
    {
      "id": 293,
      "seek": 120212,
      "start": 2595.010024414062,
      "end": 2600.3299707031247,
      "text": " Da kommt halt eine ein über ein überzeugend klingender Text raus.",
      "tokens": [
        50928,
        3933,
        10047,
        12479,
        3018,
        1343,
        4502,
        1343,
        48598,
        521,
        350,
        1688,
        3216,
        18643,
        17202,
        13,
        51194
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31181129813194275,
      "compression_ratio": 1.5838710069656372,
      "no_speech_prob": 0.0008166993502527475
    },
    {
      "id": 294,
      "seek": 120212,
      "start": 2601.3700097656247,
      "end": 2606.0899804687497,
      "text": " Und wenn mir jetzt jemand fünf Sachen vorgeben würde und halt sagen würde Okay, guck dir",
      "tokens": [
        51246,
        2719,
        4797,
        3149,
        4354,
        21717,
        28723,
        26074,
        4245,
        16702,
        11942,
        674,
        12479,
        8360,
        11942,
        1033,
        11,
        695,
        547,
        4746,
        51482
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31181129813194275,
      "compression_ratio": 1.5838710069656372,
      "no_speech_prob": 0.0008166993502527475
    },
    {
      "id": 295,
      "seek": 120212,
      "start": 2606.0899804687497,
      "end": 2610.209975585937,
      "text": " mal die die an und sag mir mal, welches davon irgendwie von Chetchipi ist, bin ich mir",
      "tokens": [
        51482,
        2806,
        978,
        978,
        364,
        674,
        15274,
        3149,
        2806,
        11,
        2214,
        3781,
        18574,
        20759,
        2957,
        761,
        7858,
        647,
        72,
        1418,
        11,
        5171,
        1893,
        3149,
        51688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31181129813194275,
      "compression_ratio": 1.5838710069656372,
      "no_speech_prob": 0.0008166993502527475
    },
    {
      "id": 296,
      "seek": 120212,
      "start": 2610.209975585937,
      "end": 2611.6500390624997,
      "text": " nicht sicher, ob ich es rausfinden könnte.",
      "tokens": [
        51688,
        1979,
        18623,
        11,
        1111,
        1893,
        785,
        17202,
        43270,
        17646,
        13,
        51760
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31181129813194275,
      "compression_ratio": 1.5838710069656372,
      "no_speech_prob": 0.0008166993502527475
    },
    {
      "id": 297,
      "seek": 123004,
      "start": 2612.6500390624997,
      "end": 2619.0899804687497,
      "text": " Und das ist enttäuschend, weil das, was da irgendwie rauskommt, ist halt aus den",
      "tokens": [
        50414,
        2719,
        1482,
        1418,
        948,
        83,
        31611,
        339,
        521,
        11,
        7689,
        1482,
        11,
        390,
        1120,
        20759,
        17202,
        74,
        22230,
        11,
        1418,
        12479,
        3437,
        1441,
        50736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2718396782875061,
      "compression_ratio": 1.6192307472229004,
      "no_speech_prob": 0.00195690942928195
    },
    {
      "id": 298,
      "seek": 123004,
      "start": 2619.0899804687497,
      "end": 2623.290053710937,
      "text": " Fingern rausgesogen. Also kommen halt Performance Ideen raus, die halt irgendwie vom",
      "tokens": [
        50736,
        479,
        278,
        1248,
        17202,
        2880,
        8799,
        13,
        2743,
        11729,
        12479,
        25047,
        13090,
        268,
        17202,
        11,
        978,
        12479,
        20759,
        10135,
        50946
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2718396782875061,
      "compression_ratio": 1.6192307472229004,
      "no_speech_prob": 0.00195690942928195
    },
    {
      "id": 299,
      "seek": 123004,
      "start": 2623.290053710937,
      "end": 2628.969985351562,
      "text": " Himmel fallen. So und das ist eine nicht positive Aussage über",
      "tokens": [
        50946,
        5920,
        10909,
        11547,
        13,
        407,
        674,
        1482,
        1418,
        3018,
        1979,
        3353,
        21286,
        609,
        4502,
        51230
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2718396782875061,
      "compression_ratio": 1.6192307472229004,
      "no_speech_prob": 0.00195690942928195
    },
    {
      "id": 300,
      "seek": 123004,
      "start": 2629.250014648437,
      "end": 2630.8499902343747,
      "text": " unsere Branche.",
      "tokens": [
        51244,
        14339,
        1603,
        22806,
        13,
        51324
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2718396782875061,
      "compression_ratio": 1.6192307472229004,
      "no_speech_prob": 0.00195690942928195
    },
    {
      "id": 301,
      "seek": 123004,
      "start": 2631.250014648437,
      "end": 2635.490004882812,
      "text": " Wir müssen da besser werden. Wir müssen halt solche Dinge wie Qualitätsszenarien,",
      "tokens": [
        51344,
        4347,
        9013,
        1120,
        18021,
        4604,
        13,
        4347,
        9013,
        12479,
        29813,
        25102,
        3355,
        13616,
        13187,
        1373,
        82,
        2904,
        289,
        1053,
        11,
        51556
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2718396782875061,
      "compression_ratio": 1.6192307472229004,
      "no_speech_prob": 0.00195690942928195
    },
    {
      "id": 302,
      "seek": 123004,
      "start": 2635.490004882812,
      "end": 2640.0499414062497,
      "text": " Anforderungen, solche Sachen halt besser hinbekommen und deutlich unterscheidbar sein von",
      "tokens": [
        51556,
        1107,
        30943,
        5084,
        11,
        29813,
        26074,
        12479,
        18021,
        14102,
        650,
        13675,
        674,
        24344,
        20983,
        1876,
        327,
        5356,
        6195,
        2957,
        51784
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2718396782875061,
      "compression_ratio": 1.6192307472229004,
      "no_speech_prob": 0.00195690942928195
    },
    {
      "id": 303,
      "seek": 125844,
      "start": 2640.0899804687497,
      "end": 2643.729995117187,
      "text": " irgendwelchen AIs, sonst haben wir ein Problem.",
      "tokens": [
        50366,
        26455,
        338,
        2470,
        316,
        6802,
        11,
        26309,
        3084,
        1987,
        1343,
        11676,
        13,
        50548
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35099297761917114,
      "compression_ratio": 1.6895307302474976,
      "no_speech_prob": 0.0010469824774190784
    },
    {
      "id": 304,
      "seek": 125844,
      "start": 2643.8499902343747,
      "end": 2649.449965820312,
      "text": " Und das ist und das ist aber das, was also nicht, weil du sagtest, wo tritt das an?",
      "tokens": [
        50554,
        2719,
        1482,
        1418,
        674,
        1482,
        1418,
        4340,
        1482,
        11,
        390,
        611,
        1979,
        11,
        7689,
        1581,
        15764,
        377,
        11,
        6020,
        504,
        593,
        1482,
        364,
        30,
        50834
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35099297761917114,
      "compression_ratio": 1.6895307302474976,
      "no_speech_prob": 0.0010469824774190784
    },
    {
      "id": 305,
      "seek": 125844,
      "start": 2649.8099511718747,
      "end": 2653.290053710937,
      "text": " Also da draußen sind garantiert Leute, die genau das machen und die halt dann irgendwie",
      "tokens": [
        50852,
        2743,
        1120,
        44602,
        3290,
        22251,
        4859,
        13495,
        11,
        978,
        12535,
        1482,
        7069,
        674,
        978,
        12479,
        3594,
        20759,
        51026
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35099297761917114,
      "compression_ratio": 1.6895307302474976,
      "no_speech_prob": 0.0010469824774190784
    },
    {
      "id": 306,
      "seek": 125844,
      "start": 2653.290053710937,
      "end": 2655.6099999999997,
      "text": " genau dieses, die halt dann sagen Okay, reicht ja.",
      "tokens": [
        51026,
        12535,
        12113,
        11,
        978,
        12479,
        3594,
        8360,
        1033,
        11,
        47000,
        2784,
        13,
        51142
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35099297761917114,
      "compression_ratio": 1.6895307302474976,
      "no_speech_prob": 0.0010469824774190784
    },
    {
      "id": 307,
      "seek": 125844,
      "start": 2655.689956054687,
      "end": 2659.1700585937497,
      "text": " Also nicht haben wir ja. Und ich bin mal gespannt, wie das sozusagen rausgeht.",
      "tokens": [
        51146,
        2743,
        1979,
        3084,
        1987,
        2784,
        13,
        2719,
        1893,
        5171,
        2806,
        47355,
        11,
        3355,
        1482,
        33762,
        17202,
        46227,
        13,
        51320
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35099297761917114,
      "compression_ratio": 1.6895307302474976,
      "no_speech_prob": 0.0010469824774190784
    },
    {
      "id": 308,
      "seek": 125844,
      "start": 2659.1700585937497,
      "end": 2662.449965820312,
      "text": " Und wir müssen da irgendwie, glaube ich, qualifizierter werden und besser werden.",
      "tokens": [
        51320,
        2719,
        1987,
        9013,
        1120,
        20759,
        11,
        13756,
        1893,
        11,
        4101,
        351,
        590,
        811,
        391,
        4604,
        674,
        18021,
        4604,
        13,
        51484
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35099297761917114,
      "compression_ratio": 1.6895307302474976,
      "no_speech_prob": 0.0010469824774190784
    },
    {
      "id": 309,
      "seek": 125844,
      "start": 2663.3299707031247,
      "end": 2665.5699609374997,
      "text": " Und bezüglich der Produktivität.",
      "tokens": [
        51528,
        2719,
        10782,
        774,
        8856,
        1163,
        44599,
        592,
        14053,
        13,
        51640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35099297761917114,
      "compression_ratio": 1.6895307302474976,
      "no_speech_prob": 0.0010469824774190784
    },
    {
      "id": 0,
      "seek": 0,
      "start": 2668.12,
      "end": 2677.12,
      "text": " Ich finde das auch ein schwieriges Thema, weil nicht vor langer Zeit war es so, da gibt es immer wieder solche Schilde.",
      "tokens": [
        50414,
        3141,
        17841,
        1482,
        2168,
        1343,
        27546,
        20609,
        16306,
        11,
        7689,
        1979,
        4245,
        2265,
        260,
        9394,
        1516,
        785,
        370,
        11,
        1120,
        6089,
        785,
        5578,
        6216,
        29813,
        2065,
        388,
        1479,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3619424104690552,
      "compression_ratio": 1.4309391975402832,
      "no_speech_prob": 0.920031726360321
    },
    {
      "id": 1,
      "seek": 0,
      "start": 2677.12,
      "end": 2686.12,
      "text": " Wir haben schon diskutiert, die Fabriken sind mittlerweile leer. Trotzdem gibt es noch Facharbeiter, die Autos bauen oder solche Dinge tun.",
      "tokens": [
        50864,
        4347,
        3084,
        4981,
        36760,
        4859,
        11,
        978,
        17440,
        470,
        2653,
        3290,
        41999,
        34172,
        13,
        1765,
        23934,
        6089,
        785,
        3514,
        38213,
        289,
        29565,
        11,
        978,
        6049,
        329,
        43787,
        4513,
        29813,
        25102,
        4267,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3619424104690552,
      "compression_ratio": 1.4309391975402832,
      "no_speech_prob": 0.920031726360321
    },
    {
      "id": 2,
      "seek": 1900,
      "start": 2687.12,
      "end": 2698.12,
      "text": " Ich erinnere mich an die Zeit, wo ich Anfang der 2000er war, wo die Aussage war, wir machen Offshoring und Nearshoring und dann ist das Thema mit Softwareentwicklung in Deutschland tot.",
      "tokens": [
        50414,
        3141,
        1189,
        7729,
        323,
        6031,
        364,
        978,
        9394,
        11,
        6020,
        1893,
        25856,
        1163,
        8132,
        260,
        1516,
        11,
        6020,
        978,
        21286,
        609,
        1516,
        11,
        1987,
        7069,
        6318,
        82,
        2335,
        278,
        674,
        1734,
        7064,
        3662,
        674,
        3594,
        1418,
        1482,
        16306,
        2194,
        27428,
        317,
        16038,
        17850,
        294,
        14802,
        1993,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28441086411476135,
      "compression_ratio": 1.5151515007019043,
      "no_speech_prob": 0.7410221695899963
    },
    {
      "id": 3,
      "seek": 1900,
      "start": 2698.12,
      "end": 2708.12,
      "text": " Das ist einfach nicht so. Wir sind eine der größten Branchen, auch in Deutschland, auch in diesem Hochlohnland, obwohl das von der Autoindustrie so dominiert ist.",
      "tokens": [
        50964,
        2846,
        1418,
        7281,
        1979,
        370,
        13,
        4347,
        3290,
        3018,
        1163,
        20691,
        1147,
        45265,
        2470,
        11,
        2168,
        294,
        14802,
        11,
        2168,
        294,
        10975,
        29193,
        752,
        12071,
        1661,
        11,
        48428,
        1482,
        2957,
        1163,
        13738,
        50046,
        370,
        8859,
        4859,
        1418,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28441086411476135,
      "compression_ratio": 1.5151515007019043,
      "no_speech_prob": 0.7410221695899963
    },
    {
      "id": 4,
      "seek": 4100,
      "start": 2708.12,
      "end": 2714.12,
      "text": " Zitiert mich nicht, aber ich meine, wir sind auf Augenhöhe mit der Autoindustrie.",
      "tokens": [
        50364,
        1176,
        270,
        4859,
        6031,
        1979,
        11,
        4340,
        1893,
        10946,
        11,
        1987,
        3290,
        2501,
        29692,
        33195,
        675,
        2194,
        1163,
        13738,
        50046,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3233664333820343,
      "compression_ratio": 1.4054054021835327,
      "no_speech_prob": 0.3102119565010071
    },
    {
      "id": 5,
      "seek": 4100,
      "start": 2714.12,
      "end": 2729.12,
      "text": " Es gibt halt Produktivitätszuwächse und die führen scheinbar dazu, dass – oder nicht, wenn man das sozusagen extrapoliert von man hat – wir in diesem Bereich nicht mehr arbeiten.",
      "tokens": [
        50664,
        2313,
        6089,
        12479,
        44599,
        592,
        13187,
        1373,
        11728,
        86,
        10168,
        405,
        674,
        978,
        35498,
        25690,
        259,
        5356,
        13034,
        11,
        2658,
        1662,
        4513,
        1979,
        11,
        4797,
        587,
        1482,
        370,
        16236,
        64,
        1766,
        1279,
        4007,
        401,
        4859,
        2957,
        587,
        2385,
        1662,
        1987,
        294,
        10975,
        26489,
        1979,
        5417,
        23162,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3233664333820343,
      "compression_ratio": 1.4054054021835327,
      "no_speech_prob": 0.3102119565010071
    },
    {
      "id": 6,
      "seek": 4100,
      "start": 2729.12,
      "end": 2732.12,
      "text": " Ich habe nur das Gefühl, das ist nicht so.",
      "tokens": [
        51414,
        3141,
        6015,
        4343,
        1482,
        29715,
        11,
        1482,
        1418,
        1979,
        370,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3233664333820343,
      "compression_ratio": 1.4054054021835327,
      "no_speech_prob": 0.3102119565010071
    },
    {
      "id": 7,
      "seek": 6500,
      "start": 2733.12,
      "end": 2741.12,
      "text": " Es ist immer noch so, dass du als Facharbeiter in einem Automobilwerk gutes Geld verdienen kannst und du kannst immer noch als Entwickler hin gutes Geld verdienen,",
      "tokens": [
        50414,
        2313,
        1418,
        5578,
        3514,
        370,
        11,
        2658,
        1581,
        3907,
        38213,
        289,
        29565,
        294,
        6827,
        24619,
        21725,
        26833,
        45859,
        16535,
        6387,
        22461,
        20853,
        674,
        1581,
        20853,
        5578,
        3514,
        3907,
        29397,
        1918,
        14102,
        45859,
        16535,
        6387,
        22461,
        11,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2653818428516388,
      "compression_ratio": 1.5523465871810913,
      "no_speech_prob": 0.8317317962646484
    },
    {
      "id": 8,
      "seek": 6500,
      "start": 2741.12,
      "end": 2746.12,
      "text": " obwohl du im direkten Wettbewerb stehst zu irgendwelchen Leuten, die Offshoring machen.",
      "tokens": [
        50814,
        48428,
        1581,
        566,
        1264,
        47120,
        343,
        3093,
        650,
        1554,
        65,
        2126,
        38857,
        2164,
        26455,
        338,
        2470,
        42301,
        11,
        978,
        6318,
        82,
        2335,
        278,
        7069,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2653818428516388,
      "compression_ratio": 1.5523465871810913,
      "no_speech_prob": 0.8317317962646484
    },
    {
      "id": 9,
      "seek": 6500,
      "start": 2746.12,
      "end": 2753.12,
      "text": " Ich bin nicht sicher, ob das das Ding ist, was unser gesamten Brancheit das Licht ausbläst.",
      "tokens": [
        51064,
        3141,
        5171,
        1979,
        18623,
        11,
        1111,
        1482,
        1482,
        20558,
        1418,
        11,
        390,
        12977,
        39746,
        1147,
        40482,
        68,
        270,
        1482,
        32917,
        3437,
        5199,
        737,
        372,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2653818428516388,
      "compression_ratio": 1.5523465871810913,
      "no_speech_prob": 0.8317317962646484
    },
    {
      "id": 10,
      "seek": 6500,
      "start": 2753.12,
      "end": 2758.12,
      "text": " Wäre überraschend, weil die anderen Ansätze haben es im Jahr auch nicht geschafft.",
      "tokens": [
        51414,
        343,
        12277,
        4502,
        3906,
        339,
        521,
        11,
        7689,
        978,
        11122,
        14590,
        30179,
        3084,
        785,
        566,
        11674,
        2168,
        1979,
        45215,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2653818428516388,
      "compression_ratio": 1.5523465871810913,
      "no_speech_prob": 0.8317317962646484
    },
    {
      "id": 11,
      "seek": 9500,
      "start": 2763.12,
      "end": 2766.12,
      "text": " Das war jetzt ein langes Statement, wenn ich das mal so sagen darf.",
      "tokens": [
        50414,
        2846,
        1516,
        4354,
        1343,
        2265,
        279,
        16249,
        1712,
        11,
        4797,
        1893,
        1482,
        2806,
        370,
        8360,
        19374,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2223508507013321,
      "compression_ratio": 1.437837839126587,
      "no_speech_prob": 0.13816606998443604
    },
    {
      "id": 12,
      "seek": 9500,
      "start": 2766.12,
      "end": 2782.12,
      "text": " Du hast da sehr viele Punkte angesprochen, gerade jetzt zum Beispiel, die KI kann innerhalb kurzer Zeit eine Architektur erstellen und wir brauchen relativ lange, um diese Architektur zu validieren.",
      "tokens": [
        50564,
        5153,
        6581,
        1120,
        5499,
        9693,
        47352,
        31138,
        23902,
        11,
        12117,
        4354,
        5919,
        13772,
        11,
        978,
        47261,
        4028,
        48460,
        10072,
        4527,
        9394,
        3018,
        10984,
        642,
        2320,
        374,
        11301,
        8581,
        674,
        1987,
        19543,
        21960,
        18131,
        11,
        1105,
        6705,
        10984,
        642,
        2320,
        374,
        2164,
        7363,
        5695,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2223508507013321,
      "compression_ratio": 1.437837839126587,
      "no_speech_prob": 0.13816606998443604
    },
    {
      "id": 13,
      "seek": 11500,
      "start": 2783.12,
      "end": 2795.12,
      "text": " Zeit ist ein teures Gut und das merke ich eben auch, wenn die KI mir Code generiert innerhalb von Sekunden und ich brauche Minuten zum Verifizieren.",
      "tokens": [
        50414,
        9394,
        1418,
        1343,
        535,
        1303,
        24481,
        674,
        1482,
        3551,
        330,
        1893,
        11375,
        2168,
        11,
        4797,
        978,
        47261,
        3149,
        15549,
        1337,
        4859,
        48460,
        2957,
        24285,
        10028,
        674,
        1893,
        1548,
        17545,
        27593,
        5919,
        4281,
        351,
        590,
        5695,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23662929236888885,
      "compression_ratio": 1.4433497190475464,
      "no_speech_prob": 0.33068662881851196
    },
    {
      "id": 14,
      "seek": 11500,
      "start": 2795.12,
      "end": 2806.12,
      "text": " Gerade wenn ich jetzt sowas wie den ASCII-Doc Linter so nebenbei erstellen lasse, die Zeit, das alles zu verifizieren, ist teuer, ist aufwendig.",
      "tokens": [
        51014,
        48175,
        4797,
        1893,
        4354,
        19766,
        296,
        3355,
        1441,
        7469,
        34,
        9503,
        12,
        35,
        905,
        441,
        5106,
        370,
        36098,
        21845,
        11301,
        8581,
        2439,
        405,
        11,
        978,
        9394,
        11,
        1482,
        7874,
        2164,
        1306,
        351,
        590,
        5695,
        11,
        1418,
        535,
        5486,
        11,
        1418,
        2501,
        20128,
        328,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23662929236888885,
      "compression_ratio": 1.4433497190475464,
      "no_speech_prob": 0.33068662881851196
    },
    {
      "id": 15,
      "seek": 13900,
      "start": 2806.12,
      "end": 2813.12,
      "text": " Und sobald ich da einen gewissen Trust in die Maschine habe, werde ich das bleiben lassen.",
      "tokens": [
        50364,
        2719,
        370,
        2645,
        67,
        1893,
        1120,
        4891,
        6906,
        10987,
        11580,
        294,
        978,
        5224,
        36675,
        6015,
        11,
        24866,
        1893,
        1482,
        24912,
        16168,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15791578590869904,
      "compression_ratio": 1.5739644765853882,
      "no_speech_prob": 0.12240078300237656
    },
    {
      "id": 16,
      "seek": 13900,
      "start": 2813.12,
      "end": 2817.12,
      "text": " Aber interessant ist eben auch bei der Arbeit mit der Maschine.",
      "tokens": [
        50714,
        5992,
        37748,
        1418,
        11375,
        2168,
        4643,
        1163,
        18604,
        2194,
        1163,
        5224,
        36675,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15791578590869904,
      "compression_ratio": 1.5739644765853882,
      "no_speech_prob": 0.12240078300237656
    },
    {
      "id": 17,
      "seek": 13900,
      "start": 2819.12,
      "end": 2829.12,
      "text": " Das, was ich kontrollieren kann, das kann ich ganz gut mit der Maschine bearbeiten, weil sie erzeugt mir etwas.",
      "tokens": [
        51014,
        2846,
        11,
        390,
        1893,
        47107,
        5695,
        4028,
        11,
        1482,
        4028,
        1893,
        6312,
        5228,
        2194,
        1163,
        5224,
        36675,
        6155,
        16779,
        11,
        7689,
        2804,
        1189,
        19303,
        83,
        3149,
        9569,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15791578590869904,
      "compression_ratio": 1.5739644765853882,
      "no_speech_prob": 0.12240078300237656
    },
    {
      "id": 18,
      "seek": 16200,
      "start": 2829.12,
      "end": 2833.12,
      "text": " Ich gucke drüber, sage Ja, passt, die Qualität stimmt.",
      "tokens": [
        50364,
        3141,
        695,
        18627,
        1224,
        12670,
        11,
        19721,
        3530,
        11,
        37154,
        11,
        978,
        13616,
        14053,
        37799,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1872534155845642,
      "compression_ratio": 1.4148471355438232,
      "no_speech_prob": 0.14596740901470184
    },
    {
      "id": 19,
      "seek": 16200,
      "start": 2833.12,
      "end": 2838.12,
      "text": " Noch besser ist es, wenn die Maschine das selbst kontrollieren kann.",
      "tokens": [
        50564,
        38116,
        18021,
        1418,
        785,
        11,
        4797,
        978,
        5224,
        36675,
        1482,
        13053,
        47107,
        5695,
        4028,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1872534155845642,
      "compression_ratio": 1.4148471355438232,
      "no_speech_prob": 0.14596740901470184
    },
    {
      "id": 20,
      "seek": 16200,
      "start": 2838.12,
      "end": 2844.12,
      "text": " Und da sind wir halt mit der Softwareentwicklung in einer Situation, die einfach toll ist.",
      "tokens": [
        50814,
        2719,
        1120,
        3290,
        1987,
        12479,
        2194,
        1163,
        27428,
        317,
        16038,
        17850,
        294,
        6850,
        22247,
        11,
        978,
        7281,
        16629,
        1418,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1872534155845642,
      "compression_ratio": 1.4148471355438232,
      "no_speech_prob": 0.14596740901470184
    },
    {
      "id": 21,
      "seek": 16200,
      "start": 2844.12,
      "end": 2850.12,
      "text": " Dadurch, dass wir automatisierte Tests machen können, sind gerade diese Bereiche wie der ASCII-Doc Linter.",
      "tokens": [
        51114,
        5639,
        2476,
        11,
        2658,
        1987,
        28034,
        271,
        23123,
        314,
        4409,
        7069,
        6310,
        11,
        3290,
        12117,
        6705,
        17684,
        9304,
        3355,
        1163,
        7469,
        34,
        9503,
        12,
        35,
        905,
        441,
        5106,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1872534155845642,
      "compression_ratio": 1.4148471355438232,
      "no_speech_prob": 0.14596740901470184
    },
    {
      "id": 22,
      "seek": 18300,
      "start": 2850.12,
      "end": 2862.12,
      "text": " Ich habe ein einfaches Interface, ich kann Tests schreiben und die Maschine kann selbst gegen die Tests ausprobieren, ob ihre Software funktioniert, kann selbst iterieren.",
      "tokens": [
        50364,
        3141,
        6015,
        1343,
        7281,
        279,
        5751,
        2868,
        11,
        1893,
        4028,
        314,
        4409,
        48546,
        674,
        978,
        5224,
        36675,
        4028,
        13053,
        13953,
        978,
        314,
        4409,
        3437,
        41990,
        5695,
        11,
        1111,
        14280,
        27428,
        26160,
        11,
        4028,
        13053,
        17138,
        5695,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21155503392219543,
      "compression_ratio": 1.6758893728256226,
      "no_speech_prob": 0.08029671013355255
    },
    {
      "id": 23,
      "seek": 18300,
      "start": 2862.12,
      "end": 2868.12,
      "text": " Jetzt bin ich natürlich noch einen Schritt weiter gegangen und habe gesagt, Maschine, schreibt dir selbst die Tests.",
      "tokens": [
        50964,
        12592,
        5171,
        1893,
        8762,
        3514,
        4891,
        33062,
        8988,
        44415,
        674,
        6015,
        12260,
        11,
        5224,
        36675,
        11,
        956,
        31174,
        4746,
        13053,
        978,
        314,
        4409,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21155503392219543,
      "compression_ratio": 1.6758893728256226,
      "no_speech_prob": 0.08029671013355255
    },
    {
      "id": 24,
      "seek": 18300,
      "start": 2868.12,
      "end": 2878.12,
      "text": " Und da brauche ich dann wieder das Vertrauen oder ich brauche den Aufwand, um es zu überprüfen, was die Maschine da geschrieben hat.",
      "tokens": [
        51264,
        2719,
        1120,
        1548,
        17545,
        1893,
        3594,
        6216,
        1482,
        21044,
        46640,
        4513,
        1893,
        1548,
        17545,
        1441,
        9462,
        33114,
        11,
        1105,
        785,
        2164,
        4502,
        48715,
        6570,
        11,
        390,
        978,
        5224,
        36675,
        1120,
        47397,
        2385,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21155503392219543,
      "compression_ratio": 1.6758893728256226,
      "no_speech_prob": 0.08029671013355255
    },
    {
      "id": 25,
      "seek": 21100,
      "start": 2878.12,
      "end": 2893.12,
      "text": " Und das geht halt in den Bereich, der wirklich viel mit mit Trust in die Maschine zu tun hat und wo wir eben auch mit der Zeit sehen werden, wie sich das entwickelt.",
      "tokens": [
        50364,
        2719,
        1482,
        7095,
        12479,
        294,
        1441,
        26489,
        11,
        1163,
        9696,
        5891,
        2194,
        2194,
        11580,
        294,
        978,
        5224,
        36675,
        2164,
        4267,
        2385,
        674,
        6020,
        1987,
        11375,
        2168,
        2194,
        1163,
        9394,
        11333,
        4604,
        11,
        3355,
        3041,
        1482,
        43208,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1858474463224411,
      "compression_ratio": 1.5582524538040161,
      "no_speech_prob": 0.058315955102443695
    },
    {
      "id": 26,
      "seek": 21100,
      "start": 2893.12,
      "end": 2905.12,
      "text": " Und auch weil die Maschine so schnell manche Sachen machen kann, Aufgaben, die wir nicht gerne machen, laufen wir teilweise Gefahr, sie falsch einzusetzen.",
      "tokens": [
        51114,
        2719,
        2168,
        7689,
        978,
        5224,
        36675,
        370,
        17589,
        587,
        1876,
        26074,
        7069,
        4028,
        11,
        29648,
        25071,
        11,
        978,
        1987,
        1979,
        15689,
        7069,
        11,
        41647,
        1987,
        46748,
        17873,
        5398,
        11,
        2804,
        43340,
        1343,
        16236,
        24797,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1858474463224411,
      "compression_ratio": 1.5582524538040161,
      "no_speech_prob": 0.058315955102443695
    },
    {
      "id": 27,
      "seek": 23800,
      "start": 2905.12,
      "end": 2914.12,
      "text": " Also neue Features entwickeln, macht Spaß. Die Tests dafür zu schreiben und die Dokumentation zu erstellen, macht keinen Spaß.",
      "tokens": [
        50364,
        2743,
        16842,
        3697,
        3377,
        28449,
        32099,
        11,
        10857,
        27460,
        13,
        3229,
        314,
        4409,
        13747,
        2164,
        48546,
        674,
        978,
        29768,
        2206,
        399,
        2164,
        11301,
        8581,
        11,
        10857,
        20624,
        27460,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23405466973781586,
      "compression_ratio": 1.6271929740905762,
      "no_speech_prob": 0.13649429380893707
    },
    {
      "id": 28,
      "seek": 23800,
      "start": 2914.12,
      "end": 2923.12,
      "text": " Also übergeben wir das der Maschine. Wir schreiben erst den Code, erst die Features, sagen dann der Maschine, du dokumentiere das mal und schreib mir die Tests.",
      "tokens": [
        50814,
        2743,
        4502,
        16702,
        1987,
        1482,
        1163,
        5224,
        36675,
        13,
        4347,
        48546,
        11301,
        1441,
        15549,
        11,
        11301,
        978,
        3697,
        3377,
        11,
        8360,
        3594,
        1163,
        5224,
        36675,
        11,
        1581,
        40858,
        14412,
        1482,
        2806,
        674,
        956,
        38606,
        3149,
        978,
        314,
        4409,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23405466973781586,
      "compression_ratio": 1.6271929740905762,
      "no_speech_prob": 0.13649429380893707
    },
    {
      "id": 29,
      "seek": 23800,
      "start": 2923.12,
      "end": 2929.12,
      "text": " Also hatten wir ja vorhin schon Test-Driven geht anders, ist das erste Problem.",
      "tokens": [
        51264,
        2743,
        20441,
        1987,
        2784,
        4245,
        10876,
        4981,
        9279,
        12,
        35,
        470,
        553,
        7095,
        17999,
        11,
        1418,
        1482,
        20951,
        11676,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23405466973781586,
      "compression_ratio": 1.6271929740905762,
      "no_speech_prob": 0.13649429380893707
    },
    {
      "id": 30,
      "seek": 26200,
      "start": 2929.12,
      "end": 2938.12,
      "text": " Mit der Dokumentation ist das zweite Problem, weil die Maschine sieht den Code, kann nur sagen, was der Code macht, also das was dokumentieren.",
      "tokens": [
        50364,
        10821,
        1163,
        29768,
        2206,
        399,
        1418,
        1482,
        37456,
        11676,
        11,
        7689,
        978,
        5224,
        36675,
        14289,
        1441,
        15549,
        11,
        4028,
        4343,
        8360,
        11,
        390,
        1163,
        15549,
        10857,
        11,
        611,
        1482,
        390,
        40858,
        5695,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20238009095191956,
      "compression_ratio": 1.514285683631897,
      "no_speech_prob": 0.4447857141494751
    },
    {
      "id": 31,
      "seek": 26200,
      "start": 2938.12,
      "end": 2946.12,
      "text": " Warum ich das Ganze so aufgebaut habe, warum ich folgende Libraries benutzt habe, das kann sie in dem Moment nicht sagen.",
      "tokens": [
        50814,
        25541,
        1893,
        1482,
        35206,
        370,
        2501,
        42858,
        6015,
        11,
        24331,
        1893,
        3339,
        27429,
        12006,
        4889,
        38424,
        2682,
        6015,
        11,
        1482,
        4028,
        2804,
        294,
        1371,
        19093,
        1979,
        8360,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20238009095191956,
      "compression_ratio": 1.514285683631897,
      "no_speech_prob": 0.4447857141494751
    },
    {
      "id": 32,
      "seek": 27900,
      "start": 2946.12,
      "end": 2960.12,
      "text": " Es sei denn, ich mache den Kontext wieder groß und dieses Warum, das ist teilweise eben in der Architektur, dass wir uns für Technologien entscheiden aufgrund der Qualitätskriterien.",
      "tokens": [
        50364,
        2313,
        10842,
        10471,
        11,
        1893,
        28289,
        1441,
        20629,
        3828,
        6216,
        17253,
        674,
        12113,
        25541,
        11,
        1482,
        1418,
        46748,
        11375,
        294,
        1163,
        10984,
        642,
        2320,
        374,
        11,
        2658,
        1987,
        2693,
        2959,
        8337,
        1132,
        1053,
        44560,
        2501,
        23701,
        1163,
        13616,
        13187,
        1373,
        38553,
        1681,
        1053,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2163255363702774,
      "compression_ratio": 1.39130437374115,
      "no_speech_prob": 0.32379865646362305
    },
    {
      "id": 33,
      "seek": 27900,
      "start": 2960.12,
      "end": 2964.12,
      "text": " Und das wird dann eben spannend, wenn wir das mit der Maschine machen.",
      "tokens": [
        51064,
        2719,
        1482,
        4578,
        3594,
        11375,
        49027,
        11,
        4797,
        1987,
        1482,
        2194,
        1163,
        5224,
        36675,
        7069,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2163255363702774,
      "compression_ratio": 1.39130437374115,
      "no_speech_prob": 0.32379865646362305
    },
    {
      "id": 34,
      "seek": 29700,
      "start": 2964.12,
      "end": 2978.12,
      "text": " Aber diese diese Vorgänge sind wieder zeitlich aufwendig und es wird längere Zeit dauern, bis wir eben verschiedene Experimente gemacht haben, wie wir mit den Maschinen arbeiten können.",
      "tokens": [
        50364,
        5992,
        6705,
        6705,
        691,
        4646,
        29091,
        3290,
        6216,
        49367,
        1739,
        2501,
        20128,
        328,
        674,
        785,
        4578,
        22566,
        323,
        9394,
        37359,
        1248,
        11,
        7393,
        1987,
        11375,
        35411,
        12522,
        332,
        1576,
        12293,
        3084,
        11,
        3355,
        1987,
        2194,
        1441,
        5224,
        339,
        5636,
        23162,
        6310,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18845926225185394,
      "compression_ratio": 1.5339806079864502,
      "no_speech_prob": 0.39563608169555664
    },
    {
      "id": 35,
      "seek": 29700,
      "start": 2978.12,
      "end": 2986.12,
      "text": " Und dann eben diese Experimente auch entsprechend bewerten können, um zu sagen, das war der richtige Weg oder eben auch nicht.",
      "tokens": [
        51064,
        2719,
        3594,
        11375,
        6705,
        12522,
        332,
        1576,
        2168,
        47823,
        17897,
        39990,
        6310,
        11,
        1105,
        2164,
        8360,
        11,
        1482,
        1516,
        1163,
        41569,
        18919,
        4513,
        11375,
        2168,
        1979,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18845926225185394,
      "compression_ratio": 1.5339806079864502,
      "no_speech_prob": 0.39563608169555664
    },
    {
      "id": 36,
      "seek": 31900,
      "start": 2987.12,
      "end": 3000.12,
      "text": " Zum Thema Verifikation hat HappyTree gerade noch auf YouTube geschrieben, die Verifikation ist auch der Game Changer. Bis wir das haben, wird zum Beispiel im Medizinbereich und so weiter auch nicht KI zum Einsatz kommen.",
      "tokens": [
        50414,
        23906,
        16306,
        4281,
        45475,
        399,
        2385,
        8277,
        51,
        701,
        12117,
        3514,
        2501,
        3088,
        47397,
        11,
        978,
        4281,
        45475,
        399,
        1418,
        2168,
        1163,
        7522,
        761,
        3176,
        13,
        25271,
        1987,
        1482,
        3084,
        11,
        4578,
        5919,
        13772,
        566,
        3982,
        36367,
        48422,
        674,
        370,
        8988,
        2168,
        1979,
        47261,
        5919,
        38474,
        11729,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25838926434516907,
      "compression_ratio": 1.6701388359069824,
      "no_speech_prob": 0.47126296162605286
    },
    {
      "id": 37,
      "seek": 31900,
      "start": 3000.12,
      "end": 3014.12,
      "text": " Aber das wird auf jeden Fall kommen, denkt er. Und er hat gerade noch geschrieben, man muss sich ja nun mal angucken, was im Modellchecker-Bereich im RE, wahrscheinlich Requirements Engineering Bereich geht, wie viele der Requirements am Ende unvereinbar sind.",
      "tokens": [
        51064,
        5992,
        1482,
        4578,
        2501,
        12906,
        7465,
        11729,
        11,
        38658,
        1189,
        13,
        2719,
        1189,
        2385,
        12117,
        3514,
        47397,
        11,
        587,
        6425,
        3041,
        2784,
        8905,
        2806,
        2562,
        49720,
        11,
        390,
        566,
        6583,
        898,
        1876,
        9178,
        12,
        33,
        323,
        480,
        566,
        10869,
        11,
        30957,
        42029,
        621,
        1117,
        16215,
        26489,
        7095,
        11,
        3355,
        9693,
        1163,
        42029,
        621,
        1117,
        669,
        15152,
        517,
        5887,
        259,
        5356,
        3290,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25838926434516907,
      "compression_ratio": 1.6701388359069824,
      "no_speech_prob": 0.47126296162605286
    },
    {
      "id": 38,
      "seek": 34700,
      "start": 3015.12,
      "end": 3016.12,
      "text": " Hat er noch geschrieben.",
      "tokens": [
        50414,
        15867,
        1189,
        3514,
        47397,
        13,
        50464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3401537537574768,
      "compression_ratio": 1.1111111640930176,
      "no_speech_prob": 0.03357413411140442
    },
    {
      "id": 39,
      "seek": 34700,
      "start": 3019.12,
      "end": 3022.12,
      "text": " Stefan, du hast, glaube ich, noch gar nichts gesagt jetzt hier zu der Frage.",
      "tokens": [
        50614,
        32158,
        11,
        1581,
        6581,
        11,
        13756,
        1893,
        11,
        3514,
        3691,
        13004,
        12260,
        4354,
        3296,
        2164,
        1163,
        13685,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3401537537574768,
      "compression_ratio": 1.1111111640930176,
      "no_speech_prob": 0.03357413411140442
    },
    {
      "id": 40,
      "seek": 34700,
      "start": 3023.12,
      "end": 3024.12,
      "text": " Wie ungewöhnlich.",
      "tokens": [
        50814,
        9233,
        517,
        21306,
        973,
        35646,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3401537537574768,
      "compression_ratio": 1.1111111640930176,
      "no_speech_prob": 0.03357413411140442
    },
    {
      "id": 41,
      "seek": 35700,
      "start": 3024.12,
      "end": 3047.12,
      "text": " Also vielleicht noch mal ganz kurz zum Hintergrund. Ich habe zwar in größeren Firmen auch ein bisschen gearbeitet, aber mein Hintergrund ist eigentlich so bis 50, 60 Entwicklern in meistens stark wachsenden Startups, wo ich entweder Cyber Engineering Manager war oder jetzt meine Kunden sind.",
      "tokens": [
        50364,
        2743,
        12547,
        3514,
        2806,
        6312,
        20465,
        5919,
        35006,
        23701,
        13,
        3141,
        6015,
        19054,
        294,
        20691,
        5170,
        28164,
        2558,
        2168,
        1343,
        10763,
        7394,
        32401,
        11,
        4340,
        10777,
        35006,
        23701,
        1418,
        10926,
        370,
        7393,
        2625,
        11,
        4060,
        29397,
        75,
        1248,
        294,
        36894,
        694,
        17417,
        261,
        608,
        82,
        8896,
        6481,
        7528,
        11,
        6020,
        1893,
        948,
        49070,
        22935,
        16215,
        13821,
        1516,
        4513,
        4354,
        10946,
        38192,
        3290,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2358824610710144,
      "compression_ratio": 1.3933649063110352,
      "no_speech_prob": 0.4830124080181122
    },
    {
      "id": 42,
      "seek": 38000,
      "start": 3047.12,
      "end": 3064.12,
      "text": " Das heißt, das sind wahrscheinlich auch so ein bisschen unterschiedliche Welten, vielleicht auch vom Programmier-Niveau, vom Herangehensniveau von anderen. Da unterscheiden wir uns sicherlich in dem Hintergrund oder unterscheide ich mich vielleicht ein bisschen im Hintergrund von anderen Leuten hier.",
      "tokens": [
        50364,
        2846,
        13139,
        11,
        1482,
        3290,
        30957,
        2168,
        370,
        1343,
        10763,
        30058,
        10185,
        3778,
        1147,
        11,
        12547,
        2168,
        10135,
        48244,
        811,
        12,
        45,
        488,
        1459,
        11,
        10135,
        3204,
        933,
        71,
        694,
        77,
        488,
        1459,
        2957,
        11122,
        13,
        3933,
        20983,
        1876,
        4380,
        1987,
        2693,
        18623,
        1739,
        294,
        1371,
        35006,
        23701,
        4513,
        20983,
        1876,
        482,
        1893,
        6031,
        12547,
        1343,
        10763,
        566,
        35006,
        23701,
        2957,
        11122,
        42301,
        3296,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2369261234998703,
      "compression_ratio": 1.6871508359909058,
      "no_speech_prob": 0.46011266112327576
    },
    {
      "id": 43,
      "seek": 39700,
      "start": 3064.12,
      "end": 3080.12,
      "text": " Und da muss ich sagen, beim Thema Test und AI, immer wenn, da hatte ich mich auch mit jemandem vor kurzem drüber unterhalten. Wir waren der Meinung, in dem Kontext, in dem wir uns bewegen, schreibt AI bessere Tests.",
      "tokens": [
        50364,
        2719,
        1120,
        6425,
        1893,
        8360,
        11,
        13922,
        16306,
        9279,
        674,
        7318,
        11,
        5578,
        4797,
        11,
        1120,
        13299,
        1893,
        6031,
        2168,
        2194,
        21717,
        443,
        4245,
        20465,
        443,
        1224,
        12670,
        8662,
        15022,
        13,
        4347,
        11931,
        1163,
        36519,
        11,
        294,
        1371,
        20629,
        3828,
        11,
        294,
        1371,
        1987,
        2693,
        312,
        13683,
        11,
        956,
        31174,
        7318,
        42410,
        323,
        314,
        4409,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26762622594833374,
      "compression_ratio": 1.3670885562896729,
      "no_speech_prob": 0.4789152145385742
    },
    {
      "id": 44,
      "seek": 41300,
      "start": 3080.12,
      "end": 3105.12,
      "text": " Also besser, weil mehr. Also besser durch Quantität und besser durch Qualität. Das heißt, da muss man natürlich immer noch mal gucken, was will man mit den Tests? Will man mit den Tests Requirements abdecken? Will man Regressionen vermeiden?",
      "tokens": [
        50364,
        2743,
        18021,
        11,
        7689,
        5417,
        13,
        2743,
        18021,
        7131,
        26968,
        14053,
        674,
        18021,
        7131,
        13616,
        14053,
        13,
        2846,
        13139,
        11,
        1120,
        6425,
        587,
        8762,
        5578,
        3514,
        2806,
        33135,
        11,
        390,
        486,
        587,
        2194,
        1441,
        314,
        4409,
        30,
        3099,
        587,
        2194,
        1441,
        314,
        4409,
        42029,
        621,
        1117,
        410,
        1479,
        13029,
        30,
        3099,
        587,
        4791,
        2775,
        268,
        40064,
        4380,
        30,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26703980565071106,
      "compression_ratio": 1.5123456716537476,
      "no_speech_prob": 0.8139222264289856
    },
    {
      "id": 45,
      "seek": 43800,
      "start": 3105.12,
      "end": 3129.12,
      "text": " Also was ist denn vielleicht die Aufgabe? Und dann ist auch innerhalb von dem Test, wo die AI manche Sachen besser kann und manche Sachen schlechter kann. Aber irgendwie scheint die AI meine Code-Basis und andere Klassen mehr zu verstehen und durch Anbindung potenziell an größere LLMs die Welt besser zu verstehen und an Sachen zu denken, an die ich nicht denke.",
      "tokens": [
        50364,
        2743,
        390,
        1418,
        10471,
        12547,
        978,
        40070,
        30,
        2719,
        3594,
        1418,
        2168,
        48460,
        2957,
        1371,
        9279,
        11,
        6020,
        978,
        7318,
        587,
        1876,
        26074,
        18021,
        4028,
        674,
        587,
        1876,
        26074,
        22664,
        26690,
        4028,
        13,
        5992,
        20759,
        47906,
        978,
        7318,
        10946,
        383,
        378,
        68,
        12,
        33,
        26632,
        674,
        10490,
        16053,
        8356,
        5417,
        2164,
        37352,
        674,
        7131,
        1107,
        65,
        41442,
        1847,
        32203,
        285,
        364,
        20691,
        323,
        441,
        43,
        26386,
        978,
        14761,
        18021,
        2164,
        37352,
        674,
        364,
        26074,
        2164,
        28780,
        11,
        364,
        978,
        1893,
        1979,
        27245,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20302990078926086,
      "compression_ratio": 1.580086588859558,
      "no_speech_prob": 0.9371387362480164
    },
    {
      "id": 46,
      "seek": 46200,
      "start": 3130.12,
      "end": 3152.12,
      "text": " Und von dem Hintergrund, wenn ich vielleicht nicht sehr stark, jetzt mal eben kein Medizinbereich, vielleicht auch kein Rüstungsbereich oder keine kritischen Bereiche, würde ich eher dazu gehen, dass die AI bessere Tests schreibt als der Durchschnittsentwickler und meistens bessere Tests schreibt als ich, vielleicht eben als Durchschnittsentwickler.",
      "tokens": [
        50414,
        2719,
        2957,
        1371,
        35006,
        23701,
        11,
        4797,
        1893,
        12547,
        1979,
        5499,
        17417,
        11,
        4354,
        2806,
        11375,
        13424,
        3982,
        36367,
        48422,
        11,
        12547,
        2168,
        13424,
        497,
        44244,
        5846,
        48422,
        4513,
        9252,
        42825,
        6282,
        17684,
        9304,
        11,
        11942,
        1893,
        24332,
        13034,
        13230,
        11,
        2658,
        978,
        7318,
        42410,
        323,
        314,
        4409,
        956,
        31174,
        3907,
        1163,
        28557,
        82,
        32064,
        49315,
        16038,
        1918,
        674,
        36894,
        694,
        42410,
        323,
        314,
        4409,
        956,
        31174,
        3907,
        1893,
        11,
        12547,
        11375,
        3907,
        28557,
        82,
        32064,
        49315,
        16038,
        1918,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25239014625549316,
      "compression_ratio": 1.713592290878296,
      "no_speech_prob": 0.9595449566841125
    },
    {
      "id": 47,
      "seek": 48500,
      "start": 3153.12,
      "end": 3159.12,
      "text": " Durch den größeren Hintergrund an Wissen und an was kann denn eigentlich schief gehen?",
      "tokens": [
        50414,
        28557,
        1441,
        20691,
        5170,
        35006,
        23701,
        364,
        343,
        10987,
        674,
        364,
        390,
        4028,
        10471,
        10926,
        956,
        2521,
        13230,
        30,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.303519606590271,
      "compression_ratio": 1.0232558250427246,
      "no_speech_prob": 0.7840656638145447
    },
    {
      "id": 48,
      "seek": 49200,
      "start": 3160.12,
      "end": 3182.12,
      "text": " Du hast da zwei interessante Punkte genannt. Also zum einen, die AI schreibt halt schnell viele Tests. Dadurch habe ich schnell eine hohe Testabdeckung. Meistens kriegen die Entwickler aufgrund von Zeitdruck gar nicht die Möglichkeit, sich mal über die Tests groß Gedanken zu machen.",
      "tokens": [
        50414,
        5153,
        6581,
        1120,
        12002,
        24372,
        47352,
        1049,
        39878,
        13,
        2743,
        5919,
        4891,
        11,
        978,
        7318,
        956,
        31174,
        12479,
        17589,
        9693,
        314,
        4409,
        13,
        5639,
        2476,
        6015,
        1893,
        17589,
        3018,
        1106,
        675,
        9279,
        455,
        1479,
        547,
        1063,
        13,
        1923,
        468,
        694,
        46882,
        978,
        29397,
        1918,
        2501,
        23701,
        2957,
        9394,
        67,
        8161,
        3691,
        1979,
        978,
        30662,
        11,
        3041,
        2806,
        4502,
        978,
        314,
        4409,
        17253,
        44612,
        2164,
        7069,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.185016930103302,
      "compression_ratio": 1.375,
      "no_speech_prob": 0.8291488289833069
    },
    {
      "id": 49,
      "seek": 51500,
      "start": 3182.12,
      "end": 3198.12,
      "text": " Zumindest ist das so meine Erfahrung. Wir haben die Theorie, wie es laufen sollte und wir haben die Praxis. Das andere, was du gesagt hast, ist, dass du das Gefühl hast, dass die AI dein Repository gut versteht und ein gutes Weltbild hat.",
      "tokens": [
        50364,
        23906,
        33155,
        1418,
        1482,
        370,
        10946,
        49318,
        13,
        4347,
        3084,
        978,
        440,
        17473,
        11,
        3355,
        785,
        41647,
        18042,
        674,
        1987,
        3084,
        978,
        12133,
        39637,
        13,
        2846,
        10490,
        11,
        390,
        1581,
        12260,
        6581,
        11,
        1418,
        11,
        2658,
        1581,
        1482,
        29715,
        6581,
        11,
        2658,
        978,
        7318,
        25641,
        3696,
        9598,
        827,
        5228,
        22442,
        357,
        674,
        1343,
        45859,
        14761,
        16248,
        2385,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21334324777126312,
      "compression_ratio": 1.389534831047058,
      "no_speech_prob": 0.8656074404716492
    },
    {
      "id": 50,
      "seek": 53100,
      "start": 3198.12,
      "end": 3220.12,
      "text": " Da finde ich es interessant, dass es dazwischen noch etwas gibt, dass ich zum Beispiel mit der AI in einem Repository arbeite, welches zu einem größeren Projekt gehört. Und das kennt die KI momentan meistens nicht, weil sie sich auf ein Repository konzentriert. Das heißt, sie kennt gar nicht die Schnittstellen zu der anderen Software.",
      "tokens": [
        50364,
        3933,
        17841,
        1893,
        785,
        37748,
        11,
        2658,
        785,
        274,
        921,
        86,
        6282,
        3514,
        9569,
        6089,
        11,
        2658,
        1893,
        5919,
        13772,
        2194,
        1163,
        7318,
        294,
        6827,
        3696,
        9598,
        827,
        40476,
        642,
        11,
        2214,
        3781,
        2164,
        6827,
        20691,
        5170,
        34804,
        21544,
        13,
        2719,
        1482,
        37682,
        978,
        47261,
        1623,
        282,
        36894,
        694,
        1979,
        11,
        7689,
        2804,
        3041,
        2501,
        1343,
        3696,
        9598,
        827,
        5897,
        14185,
        470,
        911,
        13,
        2846,
        13139,
        11,
        2804,
        37682,
        3691,
        1979,
        978,
        318,
        32064,
        17538,
        2164,
        1163,
        11122,
        27428,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17032299935817719,
      "compression_ratio": 1.4847161769866943,
      "no_speech_prob": 0.870675265789032
    },
    {
      "id": 51,
      "seek": 55300,
      "start": 3220.12,
      "end": 3249.12,
      "text": " Oder wenn wir jetzt die Architektur Dokumentation in einem Wiki haben, kommt die KI nicht dran. Wenn wir aber den DOCSIS Code Ansatz verwenden, dann kann es passieren, dass unsere Architektur oder andere Dokumentation im Repository liegt und die KI, die aufgreift, die Qualitätskriterien vielleicht findet und deswegen sagt, oh, dann nehme ich lieber die Library anstatt jener.",
      "tokens": [
        50414,
        20988,
        4797,
        1987,
        4354,
        978,
        10984,
        642,
        2320,
        374,
        29768,
        2206,
        399,
        294,
        6827,
        35892,
        3084,
        11,
        10047,
        978,
        47261,
        1979,
        32801,
        13,
        7899,
        1987,
        4340,
        1441,
        10699,
        26283,
        2343,
        15549,
        14590,
        10300,
        24615,
        8896,
        11,
        3594,
        4028,
        785,
        46223,
        11,
        2658,
        14339,
        10984,
        642,
        2320,
        374,
        4513,
        10490,
        29768,
        2206,
        399,
        566,
        3696,
        9598,
        827,
        22421,
        674,
        978,
        47261,
        11,
        978,
        2501,
        33248,
        2008,
        11,
        978,
        13616,
        13187,
        1373,
        38553,
        1681,
        1053,
        12547,
        27752,
        674,
        26482,
        15764,
        11,
        1954,
        11,
        3594,
        48276,
        1893,
        38252,
        978,
        12806,
        364,
        372,
        1591,
        361,
        7971,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2070091813802719,
      "compression_ratio": 1.5619834661483765,
      "no_speech_prob": 0.9300104975700378
    },
    {
      "id": 52,
      "seek": 58300,
      "start": 3251.12,
      "end": 3255.12,
      "text": " Oder Zugriff auf Bug Reports hat und Critical Incidents und Postmortems.",
      "tokens": [
        50414,
        20988,
        34722,
        81,
        3661,
        2501,
        23821,
        45910,
        2385,
        674,
        39482,
        7779,
        6026,
        674,
        10223,
        76,
        477,
        9097,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29084938764572144,
      "compression_ratio": 0.9599999785423279,
      "no_speech_prob": 0.4713634252548218
    },
    {
      "id": 53,
      "seek": 58800,
      "start": 3255.12,
      "end": 3284.12,
      "text": " Und da fällt mir auch gerade wieder ein, ich habe gerade wieder was entwickeln lassen und die KI hat ja einen alten Stand von Libraries benutzt, weil sie eben ein Knowledge Cutoff von vor einem Jahr hat. Und dann haben wir da auch wieder ein Problem. Wie gehen wir mit neueren Versionen um? Das sind alles Sachen, auf die sich heutige Entwickler einstellen müssen. Wir brauchen Antworten auf diese Fragen.",
      "tokens": [
        50414,
        2719,
        1120,
        42870,
        3149,
        2168,
        12117,
        6216,
        1343,
        11,
        1893,
        6015,
        12117,
        6216,
        390,
        28449,
        32099,
        16168,
        674,
        978,
        47261,
        2385,
        2784,
        4891,
        41217,
        9133,
        2957,
        12006,
        4889,
        38424,
        2682,
        11,
        7689,
        2804,
        11375,
        1343,
        32906,
        9431,
        4506,
        2957,
        4245,
        6827,
        11674,
        2385,
        13,
        2719,
        3594,
        3084,
        1987,
        1120,
        2168,
        6216,
        1343,
        11676,
        13,
        9233,
        13230,
        1987,
        2194,
        22510,
        5170,
        12226,
        17068,
        1105,
        30,
        2846,
        3290,
        7874,
        26074,
        11,
        2501,
        978,
        3041,
        42793,
        3969,
        29397,
        1918,
        1343,
        17538,
        9013,
        13,
        4347,
        19543,
        34693,
        268,
        2501,
        6705,
        25588,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21757672727108002,
      "compression_ratio": 1.5534350872039795,
      "no_speech_prob": 0.8787214756011963
    },
    {
      "id": 54,
      "seek": 61800,
      "start": 3286.12,
      "end": 3297.12,
      "text": " Was aber by the way bedeutet, und das ist auch etwas, was mir auffällt, dass die Anforderungen an Menschen in diesem ganzen Bereich eigentlich eher steigen.",
      "tokens": [
        50414,
        3027,
        4340,
        538,
        264,
        636,
        27018,
        11,
        674,
        1482,
        1418,
        2168,
        9569,
        11,
        390,
        3149,
        257,
        1245,
        25333,
        11,
        2658,
        978,
        1107,
        30943,
        5084,
        364,
        8397,
        294,
        10975,
        23966,
        26489,
        10926,
        24332,
        2126,
        3213,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.255760133266449,
      "compression_ratio": 1.2460317611694336,
      "no_speech_prob": 0.7555833458900452
    },
    {
      "id": 55,
      "seek": 63000,
      "start": 3298.12,
      "end": 3315.12,
      "text": " Das, was du jetzt zum Beispiel als Beispiel gesagt hast, ist, hat eine alte Library benutzt, suboptimale Idee, muss ich halt irgendwie korrigieren. Das heißt, ich muss nicht das Problem lösen, sondern ich muss erkennen, dass das Problem nicht vernünftig gelöst ist und es noch besser lösen.",
      "tokens": [
        50414,
        2846,
        11,
        390,
        1581,
        4354,
        5919,
        13772,
        3907,
        13772,
        12260,
        6581,
        11,
        1418,
        11,
        2385,
        3018,
        38973,
        12806,
        38424,
        2682,
        11,
        1422,
        5747,
        332,
        1220,
        32651,
        11,
        6425,
        1893,
        12479,
        20759,
        14784,
        7065,
        5695,
        13,
        2846,
        13139,
        11,
        1893,
        6425,
        1979,
        1482,
        11676,
        25209,
        6748,
        11,
        11465,
        1893,
        6425,
        45720,
        11,
        2658,
        1482,
        11676,
        1979,
        35793,
        3412,
        34765,
        4087,
        36995,
        1418,
        674,
        785,
        3514,
        18021,
        25209,
        6748,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2624766528606415,
      "compression_ratio": 1.475000023841858,
      "no_speech_prob": 0.8187320828437805
    },
    {
      "id": 56,
      "seek": 64800,
      "start": 3316.12,
      "end": 3326.12,
      "text": " Und das ist halt anspruchsvoll, wobei man auch natürlich argumentieren kann, aber es funktioniert ja und dann ist ja gut. Und vielleicht ist das auch ein Teil der Komponente.",
      "tokens": [
        50414,
        2719,
        1482,
        1418,
        12479,
        1567,
        1424,
        37503,
        20654,
        11,
        6020,
        21845,
        587,
        2168,
        8762,
        6770,
        5695,
        4028,
        11,
        4340,
        785,
        26160,
        2784,
        674,
        3594,
        1418,
        2784,
        5228,
        13,
        2719,
        12547,
        1418,
        1482,
        2168,
        1343,
        16357,
        1163,
        591,
        8586,
        266,
        1576,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2639525830745697,
      "compression_ratio": 1.325757622718811,
      "no_speech_prob": 0.9005775451660156
    },
    {
      "id": 57,
      "seek": 65900,
      "start": 3327.12,
      "end": 3344.12,
      "text": " Also wenn man sagt, ich will am Ende nur etwas haben, was funktioniert und mein konkretes Problem löst, vielleicht definiert durch Tests, dann kriege ich es halt. Und das ist vielleicht intern gar nicht so wahnsinnig schön.",
      "tokens": [
        50414,
        2743,
        4797,
        587,
        15764,
        11,
        1893,
        486,
        669,
        15152,
        4343,
        9569,
        3084,
        11,
        390,
        26160,
        674,
        10777,
        36500,
        279,
        11676,
        25209,
        372,
        11,
        12547,
        1561,
        4859,
        7131,
        314,
        4409,
        11,
        3594,
        25766,
        432,
        1893,
        785,
        12479,
        13,
        2719,
        1482,
        1418,
        12547,
        2154,
        3691,
        1979,
        370,
        31979,
        46134,
        328,
        13527,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3036799132823944,
      "compression_ratio": 1.3803681135177612,
      "no_speech_prob": 0.9074351191520691
    },
    {
      "id": 58,
      "seek": 67700,
      "start": 3344.12,
      "end": 3357.12,
      "text": " Du hattest noch einen Kommentar bekommen, der bezog sich eventuell nur auf deinen Kommentar, den du in YouTube geschrieben hast, aber er passt, glaube ich, generell noch ganz gut von Christian Beuthmüller.",
      "tokens": [
        50364,
        5153,
        276,
        1591,
        377,
        3514,
        4891,
        33708,
        289,
        19256,
        11,
        1163,
        10782,
        664,
        3041,
        2280,
        13789,
        4343,
        2501,
        49362,
        33708,
        289,
        11,
        1441,
        1581,
        294,
        3088,
        47397,
        6581,
        11,
        4340,
        1189,
        37154,
        11,
        13756,
        1893,
        11,
        41553,
        285,
        3514,
        6312,
        5228,
        2957,
        5778,
        879,
        2910,
        76,
        774,
        4658,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25953489542007446,
      "compression_ratio": 1.4912281036376953,
      "no_speech_prob": 0.9035674333572388
    },
    {
      "id": 59,
      "seek": 67700,
      "start": 3357.12,
      "end": 3362.12,
      "text": " Er hat nämlich geschrieben, KI generierter Code basiert auf GitHub und Stackoverflows wird nicht besser werden als der Durchschnitt.",
      "tokens": [
        51014,
        3300,
        2385,
        21219,
        47397,
        11,
        47261,
        1337,
        811,
        391,
        15549,
        987,
        4859,
        2501,
        23331,
        674,
        37649,
        3570,
        33229,
        4578,
        1979,
        18021,
        4604,
        3907,
        1163,
        28557,
        82,
        32064,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25953489542007446,
      "compression_ratio": 1.4912281036376953,
      "no_speech_prob": 0.9035674333572388
    },
    {
      "id": 60,
      "seek": 69500,
      "start": 3363.12,
      "end": 3375.12,
      "text": " Und ich finde, der Kommentar passt auch nochmal gut zu den generierten Tests, die Stefan erwähnt hat. Ich bin nämlich immer noch, also in meinem Kopf bin ich immer noch bei diesen Tests, die besser sind als die menschgeschriebenen Tests.",
      "tokens": [
        50414,
        2719,
        1893,
        17841,
        11,
        1163,
        33708,
        289,
        37154,
        2168,
        26509,
        5228,
        2164,
        1441,
        1337,
        29632,
        314,
        4409,
        11,
        978,
        32158,
        21715,
        6860,
        580,
        2385,
        13,
        3141,
        5171,
        21219,
        5578,
        3514,
        11,
        611,
        294,
        24171,
        28231,
        5171,
        1893,
        5578,
        3514,
        4643,
        12862,
        314,
        4409,
        11,
        978,
        18021,
        3290,
        3907,
        978,
        10923,
        339,
        23378,
        24027,
        268,
        314,
        4409,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20881105959415436,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.6642442941665649
    },
    {
      "id": 61,
      "seek": 69500,
      "start": 3375.12,
      "end": 3385.12,
      "text": " Und ich frage mich, wie das dazu kommen kann, dass sie besser sind. Also ich hänge da, glaube ich, im Kopf noch ein bisschen dran.",
      "tokens": [
        51014,
        2719,
        1893,
        6600,
        432,
        6031,
        11,
        3355,
        1482,
        13034,
        11729,
        4028,
        11,
        2658,
        2804,
        18021,
        3290,
        13,
        2743,
        1893,
        276,
        29091,
        1120,
        11,
        13756,
        1893,
        11,
        566,
        28231,
        3514,
        1343,
        10763,
        32801,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20881105959415436,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.6642442941665649
    },
    {
      "id": 62,
      "seek": 71800,
      "start": 3386.12,
      "end": 3395.12,
      "text": " Genau, und was ich jetzt noch gerade kam, noch ein von Happy Tree auch sehr interessant, finde ich auch die Frage, was mit Problemen wie dem XZ-Exploit passieren wird in diesem Kontext.",
      "tokens": [
        50414,
        22340,
        11,
        674,
        390,
        1893,
        4354,
        3514,
        12117,
        9727,
        11,
        3514,
        1343,
        2957,
        8277,
        22291,
        2168,
        5499,
        37748,
        11,
        17841,
        1893,
        2168,
        978,
        13685,
        11,
        390,
        2194,
        11676,
        268,
        3355,
        1371,
        1783,
        57,
        12,
        11149,
        21132,
        270,
        46223,
        4578,
        294,
        10975,
        20629,
        3828,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3274502158164978,
      "compression_ratio": 1.3421052694320679,
      "no_speech_prob": 0.47206807136535645
    },
    {
      "id": 63,
      "seek": 71800,
      "start": 3398.12,
      "end": 3404.12,
      "text": " Da habt ihr da eine Meinung zu? Also Richtung Exploits, Security, KI?",
      "tokens": [
        51014,
        3933,
        23660,
        5553,
        1120,
        3018,
        36519,
        2164,
        30,
        2743,
        33023,
        12514,
        78,
        1208,
        11,
        11164,
        11,
        47261,
        30,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3274502158164978,
      "compression_ratio": 1.3421052694320679,
      "no_speech_prob": 0.47206807136535645
    },
    {
      "id": 64,
      "seek": 73700,
      "start": 3405.12,
      "end": 3428.12,
      "text": " Also da sind jetzt verschiedene Punkte. Der eine Punkt ist halt, also weil du bei den Tests sozusagen noch hängst, ich glaube, also wie soll ich sagen, ich bin nicht sicher, ob mich das überrascht, weil das bedeutet, dass halt viele Dinge, die wir halt in unserer Branche machen, eben qualitativ nicht so gut sind.",
      "tokens": [
        50414,
        2743,
        1120,
        3290,
        4354,
        35411,
        47352,
        13,
        5618,
        3018,
        25487,
        1418,
        12479,
        11,
        611,
        7689,
        1581,
        4643,
        1441,
        314,
        4409,
        33762,
        3514,
        276,
        9935,
        372,
        11,
        1893,
        13756,
        11,
        611,
        3355,
        7114,
        1893,
        8360,
        11,
        1893,
        5171,
        1979,
        18623,
        11,
        1111,
        6031,
        1482,
        4502,
        3906,
        4701,
        11,
        7689,
        1482,
        27018,
        11,
        2658,
        12479,
        9693,
        25102,
        11,
        978,
        1987,
        12479,
        294,
        20965,
        1603,
        22806,
        7069,
        11,
        11375,
        4101,
        270,
        10662,
        1979,
        370,
        5228,
        3290,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23563511669635773,
      "compression_ratio": 1.5192307233810425,
      "no_speech_prob": 0.32034891843795776
    },
    {
      "id": 65,
      "seek": 76100,
      "start": 3428.12,
      "end": 3438.12,
      "text": " Und das führt irgendwie dazu, dass halt eine AI über diese Hürde drüber springt und halt besser ist. Und ich weiß nicht, ob mich das überrascht.",
      "tokens": [
        50364,
        2719,
        1482,
        39671,
        20759,
        13034,
        11,
        2658,
        12479,
        3018,
        7318,
        4502,
        6705,
        389,
        1655,
        1479,
        1224,
        12670,
        5587,
        83,
        674,
        12479,
        18021,
        1418,
        13,
        2719,
        1893,
        13385,
        1979,
        11,
        1111,
        6031,
        1482,
        4502,
        3906,
        4701,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2523769736289978,
      "compression_ratio": 1.2195122241973877,
      "no_speech_prob": 0.250676691532135
    },
    {
      "id": 66,
      "seek": 77100,
      "start": 3439.12,
      "end": 3450.12,
      "text": " Ich frage mich nur, was das bedeutet. Also wenn jetzt einer von diesen generierten Tests fehlschlägt, also sind die, kann man die gut verstehen? Versteht ihr dann gut, wo jetzt das Problem liegt?",
      "tokens": [
        50414,
        3141,
        6600,
        432,
        6031,
        4343,
        11,
        390,
        1482,
        27018,
        13,
        2743,
        4797,
        4354,
        6850,
        2957,
        12862,
        1337,
        29632,
        314,
        4409,
        579,
        22950,
        6145,
        22882,
        10463,
        11,
        611,
        3290,
        978,
        11,
        4028,
        587,
        978,
        5228,
        37352,
        30,
        4281,
        2941,
        357,
        5553,
        3594,
        5228,
        11,
        6020,
        4354,
        1482,
        11676,
        22421,
        30,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1989198923110962,
      "compression_ratio": 1.558823585510254,
      "no_speech_prob": 0.8092426061630249
    },
    {
      "id": 67,
      "seek": 77100,
      "start": 3450.12,
      "end": 3458.12,
      "text": " Also das sind ja auch nochmal Sachen, also ich schreibe ja auch einen Test, der mir nochmal sagt, okay, ich habe das Problem verstanden, ich weiß, was die Lösung sein soll.",
      "tokens": [
        50964,
        2743,
        1482,
        3290,
        2784,
        2168,
        26509,
        26074,
        11,
        611,
        1893,
        956,
        10271,
        650,
        2784,
        2168,
        4891,
        9279,
        11,
        1163,
        3149,
        26509,
        15764,
        11,
        1392,
        11,
        1893,
        6015,
        1482,
        11676,
        1306,
        33946,
        11,
        1893,
        13385,
        11,
        390,
        978,
        46934,
        6195,
        7114,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1989198923110962,
      "compression_ratio": 1.558823585510254,
      "no_speech_prob": 0.8092426061630249
    },
    {
      "id": 68,
      "seek": 79100,
      "start": 3458.12,
      "end": 3468.12,
      "text": " Also das Schreiben des Tests hat ja einen positiven Effekt auf das Verständnis des Gesamtcodes. Und wenn ich dann sehe, dieser Test schlägt fehl, weiß ich ja warum.",
      "tokens": [
        50364,
        2743,
        1482,
        2065,
        25946,
        730,
        314,
        4409,
        2385,
        2784,
        4891,
        11218,
        5709,
        34192,
        8192,
        2501,
        1482,
        4281,
        16913,
        10661,
        730,
        6761,
        16887,
        66,
        4789,
        13,
        2719,
        4797,
        1893,
        3594,
        35995,
        11,
        9053,
        9279,
        956,
        22882,
        10463,
        579,
        22950,
        11,
        13385,
        1893,
        2784,
        24331,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18539336323738098,
      "compression_ratio": 1.6653225421905518,
      "no_speech_prob": 0.8381085991859436
    },
    {
      "id": 69,
      "seek": 79100,
      "start": 3468.12,
      "end": 3481.12,
      "text": " Und wenn ich mich jetzt so daran zurückerinnere, auch nur wenn ein anderer den Test geschrieben hat und genau dieser Test schlägt dann fehl, je nachdem, wie komplex oder unumständlich der geschrieben ist, hampel ich ja dann daran länger rum.",
      "tokens": [
        50864,
        2719,
        4797,
        1893,
        6031,
        4354,
        370,
        24520,
        15089,
        260,
        7729,
        323,
        11,
        2168,
        4343,
        4797,
        1343,
        48108,
        1441,
        9279,
        47397,
        2385,
        674,
        12535,
        9053,
        9279,
        956,
        22882,
        10463,
        3594,
        579,
        22950,
        11,
        1506,
        5168,
        10730,
        11,
        3355,
        5207,
        18945,
        4513,
        517,
        449,
        16913,
        1739,
        1163,
        47397,
        1418,
        11,
        324,
        2455,
        338,
        1893,
        2784,
        3594,
        24520,
        40935,
        8347,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18539336323738098,
      "compression_ratio": 1.6653225421905518,
      "no_speech_prob": 0.8381085991859436
    },
    {
      "id": 70,
      "seek": 81400,
      "start": 3481.12,
      "end": 3491.12,
      "text": " Und ich frage mich nur gerade, wie gut das noch so für die restliche Nachvollziehbarkeit der Problemlösung ist, wenn dann mal ein Test fehlschlägt, weil wir einen Change gemacht haben.",
      "tokens": [
        50364,
        2719,
        1893,
        6600,
        432,
        6031,
        4343,
        12117,
        11,
        3355,
        5228,
        1482,
        3514,
        370,
        2959,
        978,
        1472,
        10185,
        11815,
        20654,
        28213,
        5356,
        9238,
        1163,
        11676,
        75,
        11310,
        1063,
        1418,
        11,
        4797,
        3594,
        2806,
        1343,
        9279,
        579,
        22950,
        6145,
        22882,
        10463,
        11,
        7689,
        1987,
        4891,
        15060,
        12293,
        3084,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23715893924236298,
      "compression_ratio": 1.5528454780578613,
      "no_speech_prob": 0.6282134056091309
    },
    {
      "id": 71,
      "seek": 81400,
      "start": 3491.12,
      "end": 3495.12,
      "text": " Stefan, du hättest dich gemeldet.",
      "tokens": [
        50864,
        32158,
        11,
        1581,
        276,
        11567,
        377,
        10390,
        7173,
        5957,
        302,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23715893924236298,
      "compression_ratio": 1.5528454780578613,
      "no_speech_prob": 0.6282134056091309
    },
    {
      "id": 72,
      "seek": 81400,
      "start": 3495.12,
      "end": 3502.12,
      "text": " Das mit dem Fehlschlagen ist, denke ich, ja üblich jetzt schon ein Problem, wenn jemand einen anderen Test geschrieben hat und ich habe den nicht geschrieben.",
      "tokens": [
        51064,
        2846,
        2194,
        1371,
        3697,
        22950,
        6145,
        29177,
        1418,
        11,
        27245,
        1893,
        11,
        2784,
        3304,
        65,
        1739,
        4354,
        4981,
        1343,
        11676,
        11,
        4797,
        21717,
        4891,
        11122,
        9279,
        47397,
        2385,
        674,
        1893,
        6015,
        1441,
        1979,
        47397,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23715893924236298,
      "compression_ratio": 1.5528454780578613,
      "no_speech_prob": 0.6282134056091309
    },
    {
      "id": 73,
      "seek": 83500,
      "start": 3503.12,
      "end": 3509.12,
      "text": " Das passiert ja öfter mal in Firmen bei Integrationstests oder sowas und dann hält das auf, weil ich nicht verstehe.",
      "tokens": [
        50414,
        2846,
        21671,
        2784,
        4044,
        828,
        2806,
        294,
        28164,
        2558,
        4643,
        47713,
        372,
        4409,
        4513,
        19766,
        296,
        674,
        3594,
        40751,
        1482,
        2501,
        11,
        7689,
        1893,
        1979,
        22442,
        675,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2751695513725281,
      "compression_ratio": 1.5215686559677124,
      "no_speech_prob": 0.8546069264411926
    },
    {
      "id": 74,
      "seek": 83500,
      "start": 3509.12,
      "end": 3512.12,
      "text": " Also ich glaube, das kann ich nachvollziehen.",
      "tokens": [
        50714,
        2743,
        1893,
        13756,
        11,
        1482,
        4028,
        1893,
        5168,
        20654,
        28768,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2751695513725281,
      "compression_ratio": 1.5215686559677124,
      "no_speech_prob": 0.8546069264411926
    },
    {
      "id": 75,
      "seek": 83500,
      "start": 3512.12,
      "end": 3517.12,
      "text": " Ich wollte aber zu dem, wie kann es besser sein, als ich einwerfen.",
      "tokens": [
        50864,
        3141,
        24509,
        4340,
        2164,
        1371,
        11,
        3355,
        4028,
        785,
        18021,
        6195,
        11,
        3907,
        1893,
        1343,
        1554,
        6570,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2751695513725281,
      "compression_ratio": 1.5215686559677124,
      "no_speech_prob": 0.8546069264411926
    },
    {
      "id": 76,
      "seek": 83500,
      "start": 3517.12,
      "end": 3527.12,
      "text": " Es gab mal diese Blogposts, wo dort stand, 100 Mythen, die Entwickler über Adressen glauben oder über Namen glauben oder sowas.",
      "tokens": [
        51114,
        2313,
        17964,
        2806,
        6705,
        46693,
        23744,
        82,
        11,
        6020,
        15775,
        1463,
        11,
        2319,
        1222,
        19096,
        11,
        978,
        29397,
        1918,
        4502,
        1999,
        735,
        268,
        47139,
        4513,
        4502,
        38771,
        47139,
        4513,
        19766,
        296,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2751695513725281,
      "compression_ratio": 1.5215686559677124,
      "no_speech_prob": 0.8546069264411926
    },
    {
      "id": 77,
      "seek": 83500,
      "start": 3527.12,
      "end": 3529.12,
      "text": " Also das weiß ich nicht.",
      "tokens": [
        51614,
        2743,
        1482,
        13385,
        1893,
        1979,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2751695513725281,
      "compression_ratio": 1.5215686559677124,
      "no_speech_prob": 0.8546069264411926
    },
    {
      "id": 78,
      "seek": 86200,
      "start": 3530.12,
      "end": 3538.12,
      "text": " Und vor dem Hintergrund glaube ich schon, dass eine KI bessere Tests schreiben kann als der Entwickler.",
      "tokens": [
        50414,
        2719,
        4245,
        1371,
        35006,
        23701,
        13756,
        1893,
        4981,
        11,
        2658,
        3018,
        47261,
        42410,
        323,
        314,
        4409,
        48546,
        4028,
        3907,
        1163,
        29397,
        1918,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27335259318351746,
      "compression_ratio": 1.5582524538040161,
      "no_speech_prob": 0.7829767465591431
    },
    {
      "id": 79,
      "seek": 86200,
      "start": 3538.12,
      "end": 3546.12,
      "text": " Ich glaube, ich bin bei besser eher so ins Stocken geraten.",
      "tokens": [
        50814,
        3141,
        13756,
        11,
        1893,
        5171,
        4643,
        18021,
        24332,
        370,
        1028,
        17857,
        268,
        5713,
        7186,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27335259318351746,
      "compression_ratio": 1.5582524538040161,
      "no_speech_prob": 0.7829767465591431
    },
    {
      "id": 80,
      "seek": 86200,
      "start": 3546.12,
      "end": 3554.12,
      "text": " Nicht ob der Vielfältigkeit der Tests und gerade bei Validierung glaube ich sofort, dass Entwickler nicht anders denken, was wirklich validiert werden soll,",
      "tokens": [
        51214,
        22629,
        1111,
        1163,
        35931,
        69,
        15510,
        16626,
        1163,
        314,
        4409,
        674,
        12117,
        4643,
        7188,
        327,
        11651,
        13756,
        1893,
        33168,
        11,
        2658,
        29397,
        1918,
        1979,
        17999,
        28780,
        11,
        390,
        9696,
        7363,
        4859,
        4604,
        7114,
        11,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27335259318351746,
      "compression_ratio": 1.5582524538040161,
      "no_speech_prob": 0.7829767465591431
    },
    {
      "id": 81,
      "seek": 88700,
      "start": 3554.12,
      "end": 3567.12,
      "text": " sondern eher in Richtung der Lesbarkeit der Ergebnisse, eben weil die Dinge über Stack Overflow und GitHub gezogen werden und nicht alle EntwicklerInnen bisher groß auf Lesbarkeit von Dingen geachtet haben.",
      "tokens": [
        50364,
        11465,
        24332,
        294,
        33023,
        1163,
        6965,
        5356,
        9238,
        1163,
        34657,
        31481,
        11,
        11375,
        7689,
        978,
        25102,
        4502,
        37649,
        4886,
        10565,
        674,
        23331,
        18110,
        8799,
        4604,
        674,
        1979,
        5430,
        29397,
        1918,
        4575,
        2866,
        33598,
        17253,
        2501,
        6965,
        5356,
        9238,
        2957,
        49351,
        1519,
        48833,
        3084,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1877683699131012,
      "compression_ratio": 1.4485596418380737,
      "no_speech_prob": 0.2332434505224228
    },
    {
      "id": 82,
      "seek": 88700,
      "start": 3567.12,
      "end": 3575.12,
      "text": " Also ich hatte gerade jetzt das Problem, dass die Tests, die die Maschine mir erzeugt hat, tatsächlich für mich nicht so schön lesbar waren.",
      "tokens": [
        51014,
        2743,
        1893,
        13299,
        12117,
        4354,
        1482,
        11676,
        11,
        2658,
        978,
        314,
        4409,
        11,
        978,
        978,
        5224,
        36675,
        3149,
        1189,
        19303,
        83,
        2385,
        11,
        20796,
        2959,
        6031,
        1979,
        370,
        13527,
        1512,
        5356,
        11931,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1877683699131012,
      "compression_ratio": 1.4485596418380737,
      "no_speech_prob": 0.2332434505224228
    },
    {
      "id": 83,
      "seek": 90800,
      "start": 3575.12,
      "end": 3585.12,
      "text": " Ich habe der Maschine dann gesagt, sie soll sie überarbeiten und bitte mit Behavior Driven Development arbeiten, dass ich eben sehe, gegeben das und das.",
      "tokens": [
        50364,
        3141,
        6015,
        1163,
        5224,
        36675,
        3594,
        12260,
        11,
        2804,
        7114,
        2804,
        4502,
        43918,
        674,
        23231,
        2194,
        45807,
        19150,
        553,
        15041,
        23162,
        11,
        2658,
        1893,
        11375,
        35995,
        11,
        32572,
        1482,
        674,
        1482,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20892785489559174,
      "compression_ratio": 1.6160000562667847,
      "no_speech_prob": 0.21968942880630493
    },
    {
      "id": 84,
      "seek": 90800,
      "start": 3585.12,
      "end": 3588.12,
      "text": " Wenn das passiert, dann erwarte ich Folgendes.",
      "tokens": [
        50864,
        7899,
        1482,
        21671,
        11,
        3594,
        21715,
        11026,
        1893,
        15255,
        9395,
        279,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20892785489559174,
      "compression_ratio": 1.6160000562667847,
      "no_speech_prob": 0.21968942880630493
    },
    {
      "id": 85,
      "seek": 90800,
      "start": 3588.12,
      "end": 3592.12,
      "text": " Und dadurch wurde es wieder lesbarer.",
      "tokens": [
        51014,
        2719,
        35472,
        11191,
        785,
        6216,
        1512,
        5356,
        260,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20892785489559174,
      "compression_ratio": 1.6160000562667847,
      "no_speech_prob": 0.21968942880630493
    },
    {
      "id": 86,
      "seek": 90800,
      "start": 3592.12,
      "end": 3602.12,
      "text": " Also ich meine, das ist dann eben jetzt auch so eine Entwicklung, dass ich der Maschine Verhalten beibringe, dass ich ihr sage, wie ich Software entwickeln möchte.",
      "tokens": [
        51214,
        2743,
        1893,
        10946,
        11,
        1482,
        1418,
        3594,
        11375,
        4354,
        2168,
        370,
        3018,
        39654,
        11,
        2658,
        1893,
        1163,
        5224,
        36675,
        4281,
        15022,
        312,
        897,
        38895,
        11,
        2658,
        1893,
        5553,
        19721,
        11,
        3355,
        1893,
        27428,
        28449,
        32099,
        14570,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20892785489559174,
      "compression_ratio": 1.6160000562667847,
      "no_speech_prob": 0.21968942880630493
    },
    {
      "id": 87,
      "seek": 93500,
      "start": 3602.12,
      "end": 3608.12,
      "text": " Und das landet dann im System prompt und das nächste Mal macht sie es dann eben genauso, wie ich das brauche.",
      "tokens": [
        50364,
        2719,
        1482,
        2117,
        302,
        3594,
        566,
        8910,
        12391,
        674,
        1482,
        30661,
        5746,
        10857,
        2804,
        785,
        3594,
        11375,
        37694,
        11,
        3355,
        1893,
        1482,
        1548,
        17545,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.253767728805542,
      "compression_ratio": 1.5658363103866577,
      "no_speech_prob": 0.3805375397205353
    },
    {
      "id": 88,
      "seek": 93500,
      "start": 3608.12,
      "end": 3613.12,
      "text": " Das spricht so ein bisschen für die These, dass man quasi prompt Engineering auf jeden Fall besser werden soll.",
      "tokens": [
        50664,
        2846,
        42088,
        370,
        1343,
        10763,
        2959,
        978,
        1981,
        11,
        2658,
        587,
        20954,
        12391,
        16215,
        2501,
        12906,
        7465,
        18021,
        4604,
        7114,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.253767728805542,
      "compression_ratio": 1.5658363103866577,
      "no_speech_prob": 0.3805375397205353
    },
    {
      "id": 89,
      "seek": 93500,
      "start": 3613.12,
      "end": 3619.12,
      "text": " Wer hat das vorhin gesagt? Shit in, shit out. Das ist ja mit fast allem eh so.",
      "tokens": [
        50914,
        14255,
        2385,
        1482,
        4245,
        10876,
        12260,
        30,
        19593,
        294,
        11,
        4611,
        484,
        13,
        2846,
        1418,
        2784,
        2194,
        2370,
        17585,
        7670,
        370,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.253767728805542,
      "compression_ratio": 1.5658363103866577,
      "no_speech_prob": 0.3805375397205353
    },
    {
      "id": 90,
      "seek": 93500,
      "start": 3619.12,
      "end": 3625.12,
      "text": " Also wenn ich schlechte Requirements habe, dann kriege ich ja auch eine schlechte, eine unpassende Software. Das ist ja nicht nur bei KI.",
      "tokens": [
        51214,
        2743,
        4797,
        1893,
        22664,
        10553,
        42029,
        621,
        1117,
        6015,
        11,
        3594,
        25766,
        432,
        1893,
        2784,
        2168,
        3018,
        22664,
        10553,
        11,
        3018,
        517,
        9216,
        5445,
        27428,
        13,
        2846,
        1418,
        2784,
        1979,
        4343,
        4643,
        47261,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.253767728805542,
      "compression_ratio": 1.5658363103866577,
      "no_speech_prob": 0.3805375397205353
    },
    {
      "id": 91,
      "seek": 95800,
      "start": 3625.12,
      "end": 3632.12,
      "text": " Ja, wir brauchen halt vor allem Mechanismen, wie wir die KI uns unseren Bedürfnissen anpassen können.",
      "tokens": [
        50364,
        3530,
        11,
        1987,
        19543,
        12479,
        4245,
        17585,
        30175,
        1434,
        268,
        11,
        3355,
        1987,
        978,
        47261,
        2693,
        25305,
        19893,
        1655,
        69,
        77,
        10987,
        364,
        44270,
        6310,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21824310719966888,
      "compression_ratio": 1.3406593799591064,
      "no_speech_prob": 0.26539286971092224
    },
    {
      "id": 92,
      "seek": 95800,
      "start": 3632.12,
      "end": 3637.12,
      "text": " Und André hatte vorhin dieses Feintuning genannt.",
      "tokens": [
        50714,
        2719,
        400,
        10521,
        13299,
        4245,
        10876,
        12113,
        3697,
        686,
        37726,
        1049,
        39878,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21824310719966888,
      "compression_ratio": 1.3406593799591064,
      "no_speech_prob": 0.26539286971092224
    },
    {
      "id": 93,
      "seek": 95800,
      "start": 3637.12,
      "end": 3642.12,
      "text": " Also zumindest André, du sagtest, dass die ganzen Modelle in den Benchmarks getunt sind.",
      "tokens": [
        50964,
        2743,
        38082,
        400,
        10521,
        11,
        1581,
        15764,
        377,
        11,
        2658,
        978,
        23966,
        6583,
        4434,
        294,
        1441,
        3964,
        339,
        37307,
        483,
        2760,
        3290,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21824310719966888,
      "compression_ratio": 1.3406593799591064,
      "no_speech_prob": 0.26539286971092224
    },
    {
      "id": 94,
      "seek": 97500,
      "start": 3643.12,
      "end": 3650.12,
      "text": " Und ja, ist das dieses Feintuning, was man in den in der Dokumentation liest?",
      "tokens": [
        50414,
        2719,
        2784,
        11,
        1418,
        1482,
        12113,
        3697,
        686,
        37726,
        11,
        390,
        587,
        294,
        1441,
        294,
        1163,
        29768,
        2206,
        399,
        375,
        377,
        30,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2344110757112503,
      "compression_ratio": 1.4793814420700073,
      "no_speech_prob": 0.23325051367282867
    },
    {
      "id": 95,
      "seek": 97500,
      "start": 3650.12,
      "end": 3659.12,
      "text": " Oder meinst du, das sind quasi auf spezialisierte, mit spezialisierten Trainingsdaten gefütterte Modelle?",
      "tokens": [
        50764,
        20988,
        10777,
        372,
        1581,
        11,
        1482,
        3290,
        20954,
        2501,
        768,
        17787,
        271,
        23123,
        11,
        2194,
        768,
        17787,
        271,
        29632,
        28029,
        1109,
        67,
        7186,
        11271,
        7695,
        391,
        975,
        6583,
        4434,
        30,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2344110757112503,
      "compression_ratio": 1.4793814420700073,
      "no_speech_prob": 0.23325051367282867
    },
    {
      "id": 96,
      "seek": 97500,
      "start": 3659.12,
      "end": 3664.12,
      "text": " Sowohl also sowohl das als halt auch die Gewichtung, die die Modelle halt irgendwie nützlich bringen.",
      "tokens": [
        51214,
        48644,
        12768,
        611,
        19766,
        12768,
        1482,
        3907,
        12479,
        2168,
        978,
        19063,
        1405,
        1063,
        11,
        978,
        978,
        6583,
        4434,
        12479,
        20759,
        297,
        7695,
        16813,
        27519,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2344110757112503,
      "compression_ratio": 1.4793814420700073,
      "no_speech_prob": 0.23325051367282867
    },
    {
      "id": 97,
      "seek": 99700,
      "start": 3665.12,
      "end": 3674.12,
      "text": " Also im Zweifelsfall ist ja aus so einem Large-Language-Modell nur ein Bruchteil der Informationen halt irrelevant und die werden einfach höher gewichtet.",
      "tokens": [
        50414,
        2743,
        566,
        32475,
        351,
        1625,
        6691,
        1418,
        2784,
        3437,
        370,
        6827,
        33092,
        12,
        43,
        656,
        20473,
        12,
        44,
        378,
        898,
        4343,
        1343,
        1603,
        625,
        30766,
        1163,
        46753,
        12479,
        28682,
        674,
        978,
        4604,
        7281,
        48045,
        6906,
        40387,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29717543721199036,
      "compression_ratio": 1.449056625366211,
      "no_speech_prob": 0.4208911061286926
    },
    {
      "id": 98,
      "seek": 99700,
      "start": 3674.12,
      "end": 3679.12,
      "text": " Das ist eine Sache. Da muss ich auch fairerweise zugeben, stehe ich jetzt nicht im Detail drin.",
      "tokens": [
        50864,
        2846,
        1418,
        3018,
        31452,
        13,
        3933,
        6425,
        1893,
        2168,
        3143,
        44071,
        2164,
        16702,
        11,
        2126,
        675,
        1893,
        4354,
        1979,
        566,
        4237,
        864,
        24534,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29717543721199036,
      "compression_ratio": 1.449056625366211,
      "no_speech_prob": 0.4208911061286926
    },
    {
      "id": 99,
      "seek": 99700,
      "start": 3679.12,
      "end": 3689.12,
      "text": " Man sieht bloß diesen Trend, dass es eben nicht ein JGPT 4.0 Mini ist, was da halt irgendwie antritt, sondern das sind alles quasi.",
      "tokens": [
        51114,
        2458,
        14289,
        1749,
        2536,
        12862,
        37417,
        11,
        2658,
        785,
        11375,
        1979,
        1343,
        508,
        38,
        47,
        51,
        1017,
        13,
        15,
        18239,
        1418,
        11,
        390,
        1120,
        12479,
        20759,
        2511,
        18579,
        11,
        11465,
        1482,
        3290,
        7874,
        20954,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29717543721199036,
      "compression_ratio": 1.449056625366211,
      "no_speech_prob": 0.4208911061286926
    },
    {
      "id": 100,
      "seek": 102200,
      "start": 3689.12,
      "end": 3698.12,
      "text": " Ich würde fast sogar soweit gehen, zu sagen, das sind alles Forschungsmodelle oder Forschungsansätze, wo halt wirklich sehr genau getweakt wird.",
      "tokens": [
        50364,
        3141,
        11942,
        2370,
        19485,
        262,
        6880,
        270,
        13230,
        11,
        2164,
        8360,
        11,
        1482,
        3290,
        7874,
        42938,
        5846,
        8014,
        4434,
        4513,
        42938,
        5846,
        599,
        30179,
        11,
        6020,
        12479,
        9696,
        5499,
        12535,
        483,
        826,
        5886,
        4578,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2321440577507019,
      "compression_ratio": 1.5233333110809326,
      "no_speech_prob": 0.1345711201429367
    },
    {
      "id": 101,
      "seek": 102200,
      "start": 3698.12,
      "end": 3704.12,
      "text": " Wie kann man jetzt diese Cases halt irgendwie lösen?",
      "tokens": [
        50814,
        9233,
        4028,
        587,
        4354,
        6705,
        383,
        1957,
        12479,
        20759,
        25209,
        6748,
        30,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2321440577507019,
      "compression_ratio": 1.5233333110809326,
      "no_speech_prob": 0.1345711201429367
    },
    {
      "id": 102,
      "seek": 102200,
      "start": 3704.12,
      "end": 3710.12,
      "text": " Im Chat kam gerade noch ein spannendes Statement. Das möchte ich jetzt noch eben loswerden, obwohl wir schon überziehen.",
      "tokens": [
        51114,
        4331,
        27503,
        9727,
        12117,
        3514,
        1343,
        33360,
        34533,
        16249,
        1712,
        13,
        2846,
        14570,
        1893,
        4354,
        3514,
        11375,
        1750,
        1554,
        1556,
        11,
        48428,
        1987,
        4981,
        4502,
        28768,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2321440577507019,
      "compression_ratio": 1.5233333110809326,
      "no_speech_prob": 0.1345711201429367
    },
    {
      "id": 103,
      "seek": 102200,
      "start": 3710.12,
      "end": 3717.12,
      "text": " Christian Beuthmüller hat geschrieben, meine aktuelle Erfahrung ist Senior Developer werden schneller und Junior eher das Gegenteil.",
      "tokens": [
        51414,
        5778,
        879,
        2910,
        76,
        774,
        4658,
        2385,
        47397,
        11,
        10946,
        13680,
        23635,
        49318,
        1418,
        18370,
        44915,
        4604,
        43865,
        674,
        21954,
        24332,
        1482,
        27826,
        1576,
        388,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2321440577507019,
      "compression_ratio": 1.5233333110809326,
      "no_speech_prob": 0.1345711201429367
    },
    {
      "id": 104,
      "seek": 105000,
      "start": 3717.12,
      "end": 3721.12,
      "text": " Habt ihr die Erfahrung auch gemacht in eurem beruflichen Kontext?",
      "tokens": [
        50364,
        14225,
        83,
        5553,
        978,
        49318,
        2168,
        12293,
        294,
        32845,
        76,
        5948,
        2947,
        10193,
        20629,
        3828,
        30,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26560670137405396,
      "compression_ratio": 1.5586854219436646,
      "no_speech_prob": 0.021605536341667175
    },
    {
      "id": 105,
      "seek": 105000,
      "start": 3721.12,
      "end": 3725.12,
      "text": " Ich habe ja mit 100 geantwortet, also insofern ja.",
      "tokens": [
        50564,
        3141,
        6015,
        2784,
        2194,
        2319,
        1519,
        21655,
        302,
        11,
        611,
        294,
        539,
        28958,
        2784,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26560670137405396,
      "compression_ratio": 1.5586854219436646,
      "no_speech_prob": 0.021605536341667175
    },
    {
      "id": 106,
      "seek": 105000,
      "start": 3725.12,
      "end": 3732.12,
      "text": " Ich glaube, es ist ein großes Problem oder nicht ein großes Problem, aber es ist ein Problem, was ich auch sehe.",
      "tokens": [
        50764,
        3141,
        13756,
        11,
        785,
        1418,
        1343,
        48875,
        11676,
        4513,
        1979,
        1343,
        48875,
        11676,
        11,
        4340,
        785,
        1418,
        1343,
        11676,
        11,
        390,
        1893,
        2168,
        35995,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26560670137405396,
      "compression_ratio": 1.5586854219436646,
      "no_speech_prob": 0.021605536341667175
    },
    {
      "id": 107,
      "seek": 105000,
      "start": 3732.12,
      "end": 3738.12,
      "text": " Es ist halt quasi, dass halt einfach du der Sache blind folgst und das nicht quasi einhornen kannst.",
      "tokens": [
        51114,
        2313,
        1418,
        12479,
        20954,
        11,
        2658,
        12479,
        7281,
        1581,
        1163,
        31452,
        6865,
        3339,
        70,
        372,
        674,
        1482,
        1979,
        20954,
        1343,
        31990,
        268,
        20853,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26560670137405396,
      "compression_ratio": 1.5586854219436646,
      "no_speech_prob": 0.021605536341667175
    },
    {
      "id": 108,
      "seek": 107100,
      "start": 3739.12,
      "end": 3743.12,
      "text": " Die Frage ist halt – das hatte ich auch geschrieben – so what?",
      "tokens": [
        50414,
        3229,
        13685,
        1418,
        12479,
        1662,
        1482,
        13299,
        1893,
        2168,
        47397,
        1662,
        370,
        437,
        30,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2659692168235779,
      "compression_ratio": 1.6363636255264282,
      "no_speech_prob": 0.33067786693573
    },
    {
      "id": 109,
      "seek": 107100,
      "start": 3743.12,
      "end": 3747.12,
      "text": " Das fügt ja trotzdem jetzt keinen Weg dran vorbei.",
      "tokens": [
        50614,
        2846,
        283,
        774,
        10463,
        2784,
        28325,
        4354,
        20624,
        18919,
        32801,
        38881,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2659692168235779,
      "compression_ratio": 1.6363636255264282,
      "no_speech_prob": 0.33067786693573
    },
    {
      "id": 110,
      "seek": 107100,
      "start": 3747.12,
      "end": 3750.12,
      "text": " Man sollte sich trotzdem mit dem Thema auseinandersetzen.",
      "tokens": [
        50814,
        2458,
        18042,
        3041,
        28325,
        2194,
        1371,
        16306,
        257,
        438,
        259,
        41430,
        24797,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2659692168235779,
      "compression_ratio": 1.6363636255264282,
      "no_speech_prob": 0.33067786693573
    },
    {
      "id": 111,
      "seek": 107100,
      "start": 3750.12,
      "end": 3753.12,
      "text": " Das ist für mich immer so eine Frage nach eines bewussten Handelns.",
      "tokens": [
        50964,
        2846,
        1418,
        2959,
        6031,
        5578,
        370,
        3018,
        13685,
        5168,
        18599,
        17897,
        301,
        6266,
        8854,
        338,
        3695,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2659692168235779,
      "compression_ratio": 1.6363636255264282,
      "no_speech_prob": 0.33067786693573
    },
    {
      "id": 112,
      "seek": 107100,
      "start": 3753.12,
      "end": 3755.12,
      "text": " So würde ich es wahrscheinlich einordnen.",
      "tokens": [
        51114,
        407,
        11942,
        1893,
        785,
        30957,
        1343,
        765,
        2866,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2659692168235779,
      "compression_ratio": 1.6363636255264282,
      "no_speech_prob": 0.33067786693573
    },
    {
      "id": 113,
      "seek": 107100,
      "start": 3755.12,
      "end": 3764.12,
      "text": " Ich muss mir dessen bewusst sein oder ich muss den Leuten bewusst machen, den Kollegen bewusst machen, dass es da etwas gibt.",
      "tokens": [
        51214,
        3141,
        6425,
        3149,
        6874,
        268,
        46221,
        6195,
        4513,
        1893,
        6425,
        1441,
        42301,
        46221,
        7069,
        11,
        1441,
        23713,
        46221,
        7069,
        11,
        2658,
        785,
        1120,
        9569,
        6089,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2659692168235779,
      "compression_ratio": 1.6363636255264282,
      "no_speech_prob": 0.33067786693573
    },
    {
      "id": 114,
      "seek": 109700,
      "start": 3764.12,
      "end": 3772.12,
      "text": " Aber dass man halt auch quasi mit einem gewissen – wie soll ich sagen – einfach nicht blind einsetzt.",
      "tokens": [
        50364,
        5992,
        2658,
        587,
        12479,
        2168,
        20954,
        2194,
        6827,
        6906,
        10987,
        1662,
        3355,
        7114,
        1893,
        8360,
        1662,
        7281,
        1979,
        6865,
        21889,
        3524,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26930540800094604,
      "compression_ratio": 1.6142321825027466,
      "no_speech_prob": 0.06184236705303192
    },
    {
      "id": 115,
      "seek": 109700,
      "start": 3772.12,
      "end": 3777.12,
      "text": " Ich glaube auch, dass der Christian hat darauf geantwortet und würde ich auch zustimmen.",
      "tokens": [
        50764,
        3141,
        13756,
        2168,
        11,
        2658,
        1163,
        5778,
        2385,
        18654,
        1519,
        21655,
        302,
        674,
        11942,
        1893,
        2168,
        45034,
        32076,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26930540800094604,
      "compression_ratio": 1.6142321825027466,
      "no_speech_prob": 0.06184236705303192
    },
    {
      "id": 116,
      "seek": 109700,
      "start": 3777.12,
      "end": 3779.12,
      "text": " Man muss trotzdem die Grundlagen halt irgendwie beherrschen.",
      "tokens": [
        51014,
        2458,
        6425,
        28325,
        978,
        13941,
        29177,
        12479,
        20759,
        312,
        511,
        22943,
        2470,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26930540800094604,
      "compression_ratio": 1.6142321825027466,
      "no_speech_prob": 0.06184236705303192
    },
    {
      "id": 117,
      "seek": 109700,
      "start": 3779.12,
      "end": 3787.12,
      "text": " Solange wir in einem Szenario sind, wo wir auch eher das in einem Co-Pilot oder auch irgendwann mal in einem Pilotenmodus haben,",
      "tokens": [
        51114,
        7026,
        933,
        1987,
        294,
        6827,
        318,
        2904,
        4912,
        3290,
        11,
        6020,
        1987,
        2168,
        24332,
        1482,
        294,
        6827,
        3066,
        12,
        47,
        31516,
        4513,
        2168,
        34313,
        2806,
        294,
        6827,
        18026,
        21990,
        8014,
        301,
        3084,
        11,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26930540800094604,
      "compression_ratio": 1.6142321825027466,
      "no_speech_prob": 0.06184236705303192
    },
    {
      "id": 118,
      "seek": 109700,
      "start": 3787.12,
      "end": 3791.12,
      "text": " muss man trotzdem verstehen, was da passiert.",
      "tokens": [
        51514,
        6425,
        587,
        28325,
        37352,
        11,
        390,
        1120,
        21671,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26930540800094604,
      "compression_ratio": 1.6142321825027466,
      "no_speech_prob": 0.06184236705303192
    },
    {
      "id": 119,
      "seek": 112400,
      "start": 3791.12,
      "end": 3800.12,
      "text": " Ich glaube spannend wird dann erst quasi so dieses Szenario, was Stefan ganz am Anfang aufgemacht hat, quasi Software verschwingelt.",
      "tokens": [
        50364,
        3141,
        13756,
        49027,
        4578,
        3594,
        11301,
        20954,
        370,
        12113,
        318,
        2904,
        4912,
        11,
        390,
        32158,
        6312,
        669,
        25856,
        2501,
        26322,
        3589,
        2385,
        11,
        20954,
        27428,
        20563,
        7904,
        2018,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23135560750961304,
      "compression_ratio": 1.5296610593795776,
      "no_speech_prob": 0.024388257414102554
    },
    {
      "id": 120,
      "seek": 112400,
      "start": 3800.12,
      "end": 3804.12,
      "text": " Und da gibt es noch ein paar Episoden vorher.",
      "tokens": [
        50814,
        2719,
        1120,
        6089,
        785,
        3514,
        1343,
        16509,
        9970,
        271,
        33482,
        29195,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23135560750961304,
      "compression_ratio": 1.5296610593795776,
      "no_speech_prob": 0.024388257414102554
    },
    {
      "id": 121,
      "seek": 112400,
      "start": 3804.12,
      "end": 3810.12,
      "text": " Das Thema mit den Grundlagen ist aus meiner Sicht ganz wichtig, weil wenn ich die Grundlagen verstanden habe,",
      "tokens": [
        51014,
        2846,
        16306,
        2194,
        1441,
        13941,
        29177,
        1418,
        3437,
        20529,
        36615,
        6312,
        13621,
        11,
        7689,
        4797,
        1893,
        978,
        13941,
        29177,
        1306,
        33946,
        6015,
        11,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23135560750961304,
      "compression_ratio": 1.5296610593795776,
      "no_speech_prob": 0.024388257414102554
    },
    {
      "id": 122,
      "seek": 112400,
      "start": 3810.12,
      "end": 3815.12,
      "text": " dann kann ich GenAI sehr gut verwenden, um mir neue Sachen beizubringen.",
      "tokens": [
        51314,
        3594,
        4028,
        1893,
        3632,
        48698,
        5499,
        5228,
        24615,
        8896,
        11,
        1105,
        3149,
        16842,
        26074,
        312,
        590,
        836,
        2937,
        268,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23135560750961304,
      "compression_ratio": 1.5296610593795776,
      "no_speech_prob": 0.024388257414102554
    },
    {
      "id": 123,
      "seek": 114800,
      "start": 3815.12,
      "end": 3820.12,
      "text": " Also ich habe jetzt Python verwendet, weil ich sage, GenAI kann am besten Python.",
      "tokens": [
        50364,
        2743,
        1893,
        6015,
        4354,
        15329,
        1306,
        20128,
        302,
        11,
        7689,
        1893,
        19721,
        11,
        3632,
        48698,
        4028,
        669,
        30930,
        15329,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23040871322155,
      "compression_ratio": 1.6206896305084229,
      "no_speech_prob": 0.17975302040576935
    },
    {
      "id": 124,
      "seek": 114800,
      "start": 3820.12,
      "end": 3825.12,
      "text": " Und deswegen habe ich in meinem Zeitprojekt eben Python verwendet, obwohl ich überhaupt keine Ahnung davon hatte.",
      "tokens": [
        50614,
        2719,
        26482,
        6015,
        1893,
        294,
        24171,
        9394,
        4318,
        14930,
        11375,
        15329,
        1306,
        20128,
        302,
        11,
        48428,
        1893,
        20023,
        9252,
        2438,
        15539,
        18574,
        13299,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23040871322155,
      "compression_ratio": 1.6206896305084229,
      "no_speech_prob": 0.17975302040576935
    },
    {
      "id": 125,
      "seek": 114800,
      "start": 3825.12,
      "end": 3827.12,
      "text": " Aber ich habe mir einiges erklären lassen.",
      "tokens": [
        50864,
        5992,
        1893,
        6015,
        3149,
        1343,
        20609,
        46528,
        16168,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23040871322155,
      "compression_ratio": 1.6206896305084229,
      "no_speech_prob": 0.17975302040576935
    },
    {
      "id": 126,
      "seek": 114800,
      "start": 3827.12,
      "end": 3838.12,
      "text": " Die Maschine weiß, dass ich eigentlich aus der Java-Welt komme und erklärt mir dann, wie eben Vererbung oder Klassen oder Entkapselierung eben in Python funktionieren.",
      "tokens": [
        50964,
        3229,
        5224,
        36675,
        13385,
        11,
        2658,
        1893,
        10926,
        3437,
        1163,
        10745,
        12,
        54,
        2018,
        31194,
        674,
        27570,
        24802,
        3149,
        3594,
        11,
        3355,
        11375,
        4281,
        260,
        36776,
        4513,
        16053,
        8356,
        4513,
        3951,
        34334,
        790,
        11651,
        11375,
        294,
        15329,
        20454,
        5695,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23040871322155,
      "compression_ratio": 1.6206896305084229,
      "no_speech_prob": 0.17975302040576935
    },
    {
      "id": 127,
      "seek": 114800,
      "start": 3838.12,
      "end": 3841.12,
      "text": " Und wow, es hilft mir.",
      "tokens": [
        51514,
        2719,
        6076,
        11,
        785,
        42493,
        3149,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23040871322155,
      "compression_ratio": 1.6206896305084229,
      "no_speech_prob": 0.17975302040576935
    },
    {
      "id": 128,
      "seek": 114800,
      "start": 3841.12,
      "end": 3844.12,
      "text": " Aber die Grundlagen muss ich wissen.",
      "tokens": [
        51664,
        5992,
        978,
        13941,
        29177,
        6425,
        1893,
        16331,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23040871322155,
      "compression_ratio": 1.6206896305084229,
      "no_speech_prob": 0.17975302040576935
    },
    {
      "id": 129,
      "seek": 117700,
      "start": 3845.12,
      "end": 3849.12,
      "text": " Lustiger Kommentar im Chat von HappyTree.",
      "tokens": [
        50414,
        45834,
        4810,
        33708,
        289,
        566,
        27503,
        2957,
        8277,
        51,
        701,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21354395151138306,
      "compression_ratio": 1.481327772140503,
      "no_speech_prob": 0.24433663487434387
    },
    {
      "id": 130,
      "seek": 117700,
      "start": 3849.12,
      "end": 3853.12,
      "text": " KI macht dumme dümmer und schlau schlauer wie das Fernsehen.",
      "tokens": [
        50614,
        47261,
        10857,
        16784,
        1398,
        274,
        8966,
        936,
        674,
        956,
        875,
        84,
        956,
        875,
        5486,
        3355,
        1482,
        16675,
        27750,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21354395151138306,
      "compression_ratio": 1.481327772140503,
      "no_speech_prob": 0.24433663487434387
    },
    {
      "id": 131,
      "seek": 117700,
      "start": 3853.12,
      "end": 3855.12,
      "text": " Deswegen musste ich lachen.",
      "tokens": [
        50814,
        24864,
        34497,
        1893,
        287,
        11646,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21354395151138306,
      "compression_ratio": 1.481327772140503,
      "no_speech_prob": 0.24433663487434387
    },
    {
      "id": 132,
      "seek": 117700,
      "start": 3855.12,
      "end": 3857.12,
      "text": " Wir haben schon leicht überzogen.",
      "tokens": [
        50914,
        4347,
        3084,
        4981,
        28333,
        4502,
        42120,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21354395151138306,
      "compression_ratio": 1.481327772140503,
      "no_speech_prob": 0.24433663487434387
    },
    {
      "id": 133,
      "seek": 117700,
      "start": 3857.12,
      "end": 3862.12,
      "text": " Hat noch jemand von euch ein Schlusswort, was er noch loswerden möchte?",
      "tokens": [
        51014,
        15867,
        3514,
        21717,
        2957,
        10403,
        1343,
        36573,
        13802,
        11,
        390,
        1189,
        3514,
        1750,
        1554,
        1556,
        14570,
        30,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21354395151138306,
      "compression_ratio": 1.481327772140503,
      "no_speech_prob": 0.24433663487434387
    },
    {
      "id": 134,
      "seek": 117700,
      "start": 3864.12,
      "end": 3869.12,
      "text": " Also Eberhard sehen wir jetzt nicht, ob er reden möchte, aber ich glaube, wir würden ihn hören, wenn er es wollte.",
      "tokens": [
        51364,
        2743,
        462,
        607,
        21491,
        11333,
        1987,
        4354,
        1979,
        11,
        1111,
        1189,
        26447,
        14570,
        11,
        4340,
        1893,
        13756,
        11,
        1987,
        27621,
        14534,
        38681,
        11,
        4797,
        1189,
        785,
        24509,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21354395151138306,
      "compression_ratio": 1.481327772140503,
      "no_speech_prob": 0.24433663487434387
    },
    {
      "id": 135,
      "seek": 120200,
      "start": 3869.12,
      "end": 3871.12,
      "text": " Stefan hatte länger nichts gesagt.",
      "tokens": [
        50364,
        32158,
        13299,
        40935,
        13004,
        12260,
        13,
        50464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19769422709941864,
      "compression_ratio": 1.4801980257034302,
      "no_speech_prob": 0.059015966951847076
    },
    {
      "id": 136,
      "seek": 120200,
      "start": 3871.12,
      "end": 3875.12,
      "text": " Möchtest du noch was loswerden, was dir auf der Zunge brennt?",
      "tokens": [
        50464,
        376,
        973,
        4701,
        377,
        1581,
        3514,
        390,
        1750,
        1554,
        1556,
        11,
        390,
        4746,
        2501,
        1163,
        1176,
        27588,
        272,
        1095,
        580,
        30,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19769422709941864,
      "compression_ratio": 1.4801980257034302,
      "no_speech_prob": 0.059015966951847076
    },
    {
      "id": 137,
      "seek": 120200,
      "start": 3875.12,
      "end": 3877.12,
      "text": " Ja, ich würde noch zwei Sachen kurz loswerden.",
      "tokens": [
        50664,
        3530,
        11,
        1893,
        11942,
        3514,
        12002,
        26074,
        20465,
        1750,
        1554,
        1556,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19769422709941864,
      "compression_ratio": 1.4801980257034302,
      "no_speech_prob": 0.059015966951847076
    },
    {
      "id": 138,
      "seek": 120200,
      "start": 3877.12,
      "end": 3882.12,
      "text": " Das eine ist die Diskussion um Taschenrechner ab der ersten Klasse.",
      "tokens": [
        50764,
        2846,
        3018,
        1418,
        978,
        45963,
        313,
        1105,
        27293,
        2470,
        265,
        339,
        1193,
        410,
        1163,
        17324,
        591,
        40255,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19769422709941864,
      "compression_ratio": 1.4801980257034302,
      "no_speech_prob": 0.059015966951847076
    },
    {
      "id": 139,
      "seek": 120200,
      "start": 3882.12,
      "end": 3886.12,
      "text": " Das ist so die gleiche Diskussion.",
      "tokens": [
        51014,
        2846,
        1418,
        370,
        978,
        11699,
        68,
        45963,
        313,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19769422709941864,
      "compression_ratio": 1.4801980257034302,
      "no_speech_prob": 0.059015966951847076
    },
    {
      "id": 140,
      "seek": 120200,
      "start": 3886.12,
      "end": 3889.12,
      "text": " Und das andere ist Punkt, den ich loswerden will.",
      "tokens": [
        51214,
        2719,
        1482,
        10490,
        1418,
        25487,
        11,
        1441,
        1893,
        1750,
        1554,
        1556,
        486,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19769422709941864,
      "compression_ratio": 1.4801980257034302,
      "no_speech_prob": 0.059015966951847076
    },
    {
      "id": 141,
      "seek": 122200,
      "start": 3889.12,
      "end": 3902.12,
      "text": " Alle Engineering Manager, deren Entwickler heute nicht State of the Art AI und Gen AI benutzen, würde ich gleich am Montag irgendwie alle einladen, Pizza kaufen und zeigen, was geht.",
      "tokens": [
        50364,
        25318,
        16215,
        13821,
        11,
        48300,
        29397,
        1918,
        9801,
        1979,
        4533,
        295,
        264,
        5735,
        7318,
        674,
        3632,
        7318,
        38424,
        2904,
        11,
        11942,
        1893,
        11699,
        669,
        7947,
        559,
        20759,
        5430,
        1343,
        9290,
        268,
        11,
        24469,
        42083,
        674,
        24687,
        11,
        390,
        7095,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689262330532074,
      "compression_ratio": 1.5229681730270386,
      "no_speech_prob": 0.7203987240791321
    },
    {
      "id": 142,
      "seek": 122200,
      "start": 3902.12,
      "end": 3905.12,
      "text": " Also das wäre schon mein dringender Appell.",
      "tokens": [
        51014,
        2743,
        1482,
        14558,
        4981,
        10777,
        1224,
        278,
        3216,
        3132,
        898,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689262330532074,
      "compression_ratio": 1.5229681730270386,
      "no_speech_prob": 0.7203987240791321
    },
    {
      "id": 143,
      "seek": 122200,
      "start": 3905.12,
      "end": 3908.12,
      "text": " Ich hoffe, Montag sind nicht so viele Menschen im Büro.",
      "tokens": [
        51164,
        3141,
        34903,
        11,
        7947,
        559,
        3290,
        1979,
        370,
        9693,
        8397,
        566,
        37186,
        340,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689262330532074,
      "compression_ratio": 1.5229681730270386,
      "no_speech_prob": 0.7203987240791321
    },
    {
      "id": 144,
      "seek": 122200,
      "start": 3908.12,
      "end": 3909.12,
      "text": " Vielleicht lieber nächstes Jahr.",
      "tokens": [
        51314,
        29838,
        38252,
        13201,
        45355,
        11674,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689262330532074,
      "compression_ratio": 1.5229681730270386,
      "no_speech_prob": 0.7203987240791321
    },
    {
      "id": 145,
      "seek": 122200,
      "start": 3909.12,
      "end": 3911.12,
      "text": " Am ersten Montag an dem Leute wieder überhaupt im Büro.",
      "tokens": [
        51364,
        2012,
        17324,
        7947,
        559,
        364,
        1371,
        13495,
        6216,
        20023,
        566,
        37186,
        340,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689262330532074,
      "compression_ratio": 1.5229681730270386,
      "no_speech_prob": 0.7203987240791321
    },
    {
      "id": 146,
      "seek": 122200,
      "start": 3911.12,
      "end": 3914.12,
      "text": " Ich weiß ja nicht, ob jemand ins Büro geht oder so.",
      "tokens": [
        51464,
        3141,
        13385,
        2784,
        1979,
        11,
        1111,
        21717,
        1028,
        37186,
        340,
        7095,
        4513,
        370,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689262330532074,
      "compression_ratio": 1.5229681730270386,
      "no_speech_prob": 0.7203987240791321
    },
    {
      "id": 147,
      "seek": 124700,
      "start": 3914.12,
      "end": 3924.12,
      "text": " Dann würde ich sagen, wir haben jetzt hier eine Stunde spannende KI-Diskussion gehabt.",
      "tokens": [
        50364,
        7455,
        11942,
        1893,
        8360,
        11,
        1987,
        3084,
        4354,
        3296,
        3018,
        42781,
        33360,
        5445,
        47261,
        12,
        35,
        7797,
        2023,
        313,
        37092,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17379403114318848,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.5060305595397949
    },
    {
      "id": 148,
      "seek": 124700,
      "start": 3924.12,
      "end": 3931.12,
      "text": " Ich bin immer noch fasziniert, dass es im Chat auch die ganze Zeit so gut abging und dass ihr Ralf und André auch noch nebenbei im Chat dabei wart.",
      "tokens": [
        50864,
        3141,
        5171,
        5578,
        3514,
        283,
        19601,
        259,
        4859,
        11,
        2658,
        785,
        566,
        27503,
        2168,
        978,
        18898,
        9394,
        370,
        5228,
        410,
        3249,
        674,
        2658,
        5553,
        497,
        1678,
        674,
        400,
        10521,
        2168,
        3514,
        36098,
        21845,
        566,
        27503,
        14967,
        45124,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17379403114318848,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.5060305595397949
    },
    {
      "id": 149,
      "seek": 124700,
      "start": 3931.12,
      "end": 3935.12,
      "text": " Respekt, dass ihr zwei Diskussionen verfolgt habt oder sogar drei.",
      "tokens": [
        51214,
        5015,
        23533,
        11,
        2658,
        5553,
        12002,
        45963,
        17068,
        1306,
        7082,
        10463,
        23660,
        4513,
        19485,
        16809,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17379403114318848,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.5060305595397949
    },
    {
      "id": 150,
      "seek": 124700,
      "start": 3935.12,
      "end": 3939.12,
      "text": " Ich bin sehr dankbar, dass ich heute dabei sein durfte und das moderieren durfte.",
      "tokens": [
        51414,
        3141,
        5171,
        5499,
        35121,
        5356,
        11,
        2658,
        1893,
        9801,
        14967,
        6195,
        4861,
        16268,
        674,
        1482,
        10494,
        5695,
        4861,
        16268,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17379403114318848,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.5060305595397949
    },
    {
      "id": 151,
      "seek": 127200,
      "start": 3939.12,
      "end": 3943.12,
      "text": " Ich weiß, dass Ralf gleich noch ankündigen will, womit es weitergeht im nächsten Jahr.",
      "tokens": [
        50364,
        3141,
        13385,
        11,
        2658,
        497,
        1678,
        11699,
        3514,
        30890,
        9541,
        3213,
        486,
        11,
        1579,
        270,
        785,
        8988,
        46227,
        566,
        19101,
        11674,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383075833320618,
      "compression_ratio": 1.6217228174209595,
      "no_speech_prob": 0.40620383620262146
    },
    {
      "id": 152,
      "seek": 127200,
      "start": 3943.12,
      "end": 3947.12,
      "text": " Erstmal aber danke, André, dass du da warst und danke, Stefan, dass du dabei warst.",
      "tokens": [
        50564,
        31183,
        5579,
        4340,
        46434,
        11,
        400,
        10521,
        11,
        2658,
        1581,
        1120,
        1516,
        372,
        674,
        46434,
        11,
        32158,
        11,
        2658,
        1581,
        14967,
        1516,
        372,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383075833320618,
      "compression_ratio": 1.6217228174209595,
      "no_speech_prob": 0.40620383620262146
    },
    {
      "id": 153,
      "seek": 127200,
      "start": 3947.12,
      "end": 3950.12,
      "text": " Danke euch allen im Chat, dass ihr dabei wart.",
      "tokens": [
        50764,
        26508,
        10403,
        18440,
        566,
        27503,
        11,
        2658,
        5553,
        14967,
        45124,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383075833320618,
      "compression_ratio": 1.6217228174209595,
      "no_speech_prob": 0.40620383620262146
    },
    {
      "id": 154,
      "seek": 127200,
      "start": 3950.12,
      "end": 3954.12,
      "text": " Ralf, wie startet Software Architektur im Stream im neuen Jahr?",
      "tokens": [
        50914,
        497,
        1678,
        11,
        3355,
        722,
        302,
        27428,
        10984,
        642,
        2320,
        374,
        566,
        24904,
        566,
        21387,
        11674,
        30,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383075833320618,
      "compression_ratio": 1.6217228174209595,
      "no_speech_prob": 0.40620383620262146
    },
    {
      "id": 155,
      "seek": 127200,
      "start": 3954.12,
      "end": 3962.12,
      "text": " Ja, wir werden den als ersten Gast den Christian Weiher haben und da wird es wieder um KI in der Software Architektur gehen.",
      "tokens": [
        51114,
        3530,
        11,
        1987,
        4604,
        1441,
        3907,
        17324,
        31988,
        1441,
        5778,
        21174,
        511,
        3084,
        674,
        1120,
        4578,
        785,
        6216,
        1105,
        47261,
        294,
        1163,
        27428,
        10984,
        642,
        2320,
        374,
        13230,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383075833320618,
      "compression_ratio": 1.6217228174209595,
      "no_speech_prob": 0.40620383620262146
    },
    {
      "id": 156,
      "seek": 127200,
      "start": 3962.12,
      "end": 3965.12,
      "text": " Das wird am 9.1. sein.",
      "tokens": [
        51514,
        2846,
        4578,
        669,
        1722,
        13,
        16,
        13,
        6195,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19383075833320618,
      "compression_ratio": 1.6217228174209595,
      "no_speech_prob": 0.40620383620262146
    },
    {
      "id": 157,
      "seek": 129800,
      "start": 3965.12,
      "end": 3971.12,
      "text": " Mit der Uhrzeit sind wir noch ein bisschen am Gucken, was da am besten passt, aber es wird auf jeden Fall spannend.",
      "tokens": [
        50364,
        10821,
        1163,
        30084,
        13712,
        3290,
        1987,
        3514,
        1343,
        10763,
        669,
        460,
        49720,
        11,
        390,
        1120,
        669,
        30930,
        37154,
        11,
        4340,
        785,
        4578,
        2501,
        12906,
        7465,
        49027,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2455490082502365,
      "compression_ratio": 1.4950979948043823,
      "no_speech_prob": 0.2620261609554291
    },
    {
      "id": 158,
      "seek": 129800,
      "start": 3971.12,
      "end": 3973.12,
      "text": " Schaltet wieder ein.",
      "tokens": [
        50664,
        2065,
        39931,
        6216,
        1343,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2455490082502365,
      "compression_ratio": 1.4950979948043823,
      "no_speech_prob": 0.2620261609554291
    },
    {
      "id": 159,
      "seek": 129800,
      "start": 3973.12,
      "end": 3974.12,
      "text": " Genau.",
      "tokens": [
        50764,
        22340,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2455490082502365,
      "compression_ratio": 1.4950979948043823,
      "no_speech_prob": 0.2620261609554291
    },
    {
      "id": 160,
      "seek": 129800,
      "start": 3974.12,
      "end": 3984.12,
      "text": " Ich packe den Chat auf jeden Fall nochmal zum Download in dieses Internet, also auf diese Webseite, die wir haben, dass man das nachlesen kann.",
      "tokens": [
        50814,
        3141,
        15165,
        330,
        1441,
        27503,
        2501,
        12906,
        7465,
        26509,
        5919,
        32282,
        294,
        12113,
        7703,
        11,
        611,
        2501,
        6705,
        9573,
        405,
        642,
        11,
        978,
        1987,
        3084,
        11,
        2658,
        587,
        1482,
        5168,
        904,
        268,
        4028,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2455490082502365,
      "compression_ratio": 1.4950979948043823,
      "no_speech_prob": 0.2620261609554291
    },
    {
      "id": 161,
      "seek": 129800,
      "start": 3984.12,
      "end": 3986.12,
      "text": " Super, gute Idee.",
      "tokens": [
        51314,
        4548,
        11,
        21476,
        32651,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2455490082502365,
      "compression_ratio": 1.4950979948043823,
      "no_speech_prob": 0.2620261609554291
    },
    {
      "id": 162,
      "seek": 131900,
      "start": 3986.12,
      "end": 3995.12,
      "text": " Ja, dann danke euch allen, danke euch ZuschauerInnen und schöne Feiertage, erholsame Feiertage und einen guten Rutsch ins neue Jahr.",
      "tokens": [
        50364,
        3530,
        11,
        3594,
        46434,
        10403,
        18440,
        11,
        46434,
        10403,
        48333,
        18120,
        4575,
        2866,
        674,
        41152,
        3697,
        4859,
        609,
        11,
        1189,
        5449,
        82,
        529,
        3697,
        4859,
        609,
        674,
        4891,
        31277,
        497,
        3648,
        339,
        1028,
        16842,
        11674,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19615770876407623,
      "compression_ratio": 1.4128440618515015,
      "no_speech_prob": 0.40189531445503235
    },
    {
      "id": 163,
      "seek": 131900,
      "start": 3995.12,
      "end": 3997.12,
      "text": " Genau, guten Rutsch.",
      "tokens": [
        50814,
        22340,
        11,
        31277,
        497,
        3648,
        339,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19615770876407623,
      "compression_ratio": 1.4128440618515015,
      "no_speech_prob": 0.40189531445503235
    }
  ],
  "language": "german"
}