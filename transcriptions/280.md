# Folge 280 - Monorepos bei Uber: üëç oder üëé? mit den Monorepos.

Aufh√§nger ist so ein Artikel, den Uber vor einiger Zeit oft nicht hat, wo sie halt ein Tooling rund um Monorepos kurz erl√§utern.

Und also stellt sich erst mal die Frage, was ist denn eigentlich Monorepo?

## Was ist ein Monorepo?

Ein Monorepo ist ein gro√ües Repository, in dem mehrere Dinge, mehrere verschiedene Projekte untergebracht sind, also zum Beispiel alle Microservices eines Unternehmens oder halt zumindest sehr viele Microservices, was auch immer Teil deines Systems.

Und jetzt ist ja die Frage, warum w√ºrde man das halt tun wollen?

### Vorteile von Monorepos

Also ein Vorteil davon ist, dass man leichter Code teilen kann.

Ein anderer Vorteil davon ist, dass es halt leicht m√∂glich ist, √Ñnderungen √ºber Komponentengrenzen weg zu vorzunehmen und dann halt auch irgendwie automat zu versionieren.

Das hei√üt, ich kann jetzt sagen, ich habe eine bestimmte √Ñnderung.

Diese √Ñnderung beeinflusst eben ganz viele Microservices und die ist eben eine √Ñnderung in einem Projekt, in diesem Monorepo, obwohl sie halt mehrere Microservices beeinflusst.

Das ist auch gleich f√ºr mich so ein bisschen der Punkt, weswegen ich das so auf einer abstrakten Ebene ein bisschen problematisch finde, weil ich w√ºrde behaupten, dass wir eigentlich lose Kopplung haben wollen und eigentlich √Ñnderungen haben wollen, die jetzt jeweils in einem Repo f√ºr einen Microservice relevant sind.

Und man kann jetzt irgendwie sagen, wenn ich also √ºbergreifende √Ñnderungen erleichtere, dann ist es so, dass ich im Prinzip sage, meine Modularisierung hat nicht funktioniert.

Ich muss eben modul√ºbergreifende √Ñnderungen erleichtern und ich habe eben keine lose Kopplung.

Also √Ñnderungen erwischen eben mehr als nur ein Microservice, sondern eben viele Sachen.

Das muss sich erleichtern und das finde ich ein bisschen problematisch.

Die andere Sache ist, und das ist glaube ich das, was uns auch den Rest der Session so ein bisschen verfolgen wird, wir haben halt ein Repository, das ist also das Source Code, aber der Source Code muss ja dann irgendwie deployed werden.

Wenn ich jetzt irgendwie sage, ich habe irgendwie Microservices, mehrere Microservices in einem Repo, dann bedeutet das eben immer noch, dass ich mehrere Microservices deployen muss.

Und das ist ja eigentlich eher sogar ein Feature.

Also die Idee ist ja, dass ich nicht mehr gro√üe monolithische Deployments machen und irgendwie alles auf einmal deploye, sondern dass ich eben kleinere Einheiten deploye, Microservices.

Das ist gerade der Unterschied zwischen Microservices und Deploymentmonolithen, was also bedeutet, dass ich dann zwar immer noch eine gro√üe √Ñnderung machen kann, √ºber viele Microservices hinweg, aber das Deployment ist halt dann schwierig, weil ich eben mehrere Sachen deployen muss.

Ich brauche dann halt irgendwie auch noch eine Infrastruktur, die irgendwie daf√ºr sorgt, dass das irgendwie effizient umsetzbar ist.

Ich werde gleich nochmal das Beispiel von Google genauer erl√§utern.

Will hei√üen, wenn ich jetzt irgendwie ganz Google auschecke und halt irgendwie bearbeiten will, das kann ich nicht einfach so tun.

Da brauche ich also ein Studium drumherum und das ist eben das, was uns insbesondere dann nachher bei dem Uber-Thema auch nochmal verfolgen wird.

Ich habe nochmal das Google-Paper rausgesucht.

Das verlinke ich auch sp√§ter nochmal.

## Google's Monorepo-Ansatz

Das ist halt ein ACM-Paper von 2016, also mittlerweile fast zehn Jahre alt und dort diskutieren sie eben, dass sie im Prinzip fast ganz Google in einem gro√üen Repository haben.

Die beiden popul√§ren Ausnahmen sind Android und Chrome, die zu dem damaligen Zeitpunkt zumindest eben nicht in diesem Repository drin sind.

Die ganzen Daten sind nat√ºrlich eben auch zehn Jahre alt.

Es ist ja wahrscheinlich so, dass Google da halt mittlerweile noch viel gr√∂√üer und umf√§nglicher geworden ist.

### Gr√∂√üenordnung des Google Monorepos

Zu dem Zeitpunkt war es so, dass sie dort neun Millionen Source-Code-Dateien hatten, mit zwei Milliarden Zeilen Code, so sagen sie, 35 Millionen commits insgesamt, 18 Jahre Code-Basis.

Das kann man sich √ºberlegen, seit 1998 und insgesamt 86 Terabyte Daten.

Im Vergleich dazu, Linux hat 15 Millionen Zeilen Code, das ist also nicht zwei Milliarden, 15 Millionen.

Das ist also deutlich weniger und hat 40.000 Dateien statt eben neun Millionen bei Google.

Das hei√üt also, wir reden √ºber ein wirklich, wirklich, wirklich gro√ües Ding in Bezug auf die Menge an Source-Code und es ist eben tats√§chlich zu dem damaligen Zeitpunkt ganz Google-exklusive Android und Chrome, also die Suche-Engine und nicht Gmail und was auch immer da halt irgendwie sonst alles rumfliegt.

Sie haben 800.000 Queries pro Sekunde Peak, also in der Spitze, 500.000 Queries pro Sekunde unter normaler Last und insgesamt 25.000 EntwicklerInnen.

Das hei√üt also, sie sind dort auf einer Ebene, die weit jenseits dessen ist, was KundInnen von mir typischerweise halt irgendwie machen und sowohl in Bezug darauf die Anzahl EntwicklerInnen, die dort irgendwie arbeiten, wie auch die Menge an Code, die produziert wird, wie auch die Menge Code, die dort irgendwie sozusagen unter Kontrolle ist.

Und das ist so ein bisschen die erste Sache, also wenn es sozusagen nicht Google w√§re, von dem wir ja wissen, dass sie ja technisch sehr, sehr gut sind, dann ist irgendwie die Frage, ob wir das nicht irgendwie also sozusagen als historisch gewachsen klassifizieren w√ºrden und als au√üer Kontrolle geraten.

Und bei Google ist es jetzt so, dass sie eben in dieser Ebene rangieren, wo sie halt selber Werkzeuge bauen f√ºr solche Sachen.

Sie haben also Piper gebaut, um diesem ganzen Zeug irgendwie herzuwerden.

Und das Piper, was Google dort ver√∂ffentlicht hat, finde ich pers√∂nlich eigentlich sehr sch√∂n, weil es halt irgendwie sagt, was so die Vorteile von diesem Ansatz sind, aber auch eben die Nachteile.

Also eben nicht so eine Geschichte, wo man jetzt sagt, das ist alles supi und wir haben halt die gesamte Weisheit und machen es halt sozusagen richtig.

### Trunk-Based Development bei Google

Die Vorteile, die sie nennen, sind einmal Trank-Based Development.

Das hei√üt also, Trank-Based bedeutet, dass eben die aktuelle Version tats√§chlich die Version ist, die halt in Produktion ist und dass es halt nicht ernsthaft Branches gibt, die eine l√§ngere Zeit irgendwie offen sind.

Also klassisches Gegenbeispiel sind Feature-Branches, wo ich dann irgendwie sage, f√ºr ein Feature mache ich einen Branch auf und irgendwann merche ich das halt irgendwie zur√ºck.

Was halt bedeutet, dass sie halt als Vorteile haben, dass sie halt Branches nicht mehr merken m√ºssen.

Was halt sonst bei Branches, insbesondere wenn sie l√§ngere Zeit offen sind, halt zu Problemen f√ºhren kann.

Und das ist so ein bisschen das erste Interessante.

Also Trank-Based Development, was ja h√§ufig so ein bisschen kritisch sozusagen gesehen wird, geht halt auch in der Gr√∂√üenordnung von Google und ist dort auch das, was eben Google selber als die bessere Variante ansieht.

Bedeutet, sie haben eine konsistente Code-Basis, also sie wissen, was das ist, was in Produktion ist und das ist irgendwie auch trivial MWC-Bahn.

Ich kann irgendwie auschecken und es ist halt irgendwie da.

Sie haben logischerweise Features ausgetrockelt.

Das hei√üt, wenn ich Trank-Based arbeite und halt st√§ndig Sachen kommite, dann ist nat√ºrlich die Frage, wenn ich jetzt irgendwie an einem Feature arbeite und das dauert l√§nger als ein Kommit, was es wahrscheinlich tut, dann muss ich halt irgendwie daf√ºr sorgen, dass das nicht aus Versehen in Produktion pl√∂tzlich live ist.

Daf√ºr haben sie Features ausgetrockelt und bedeutet halt, dass Dinge, die halt noch nicht durch Kundinnen gesehen werden sollen, dann eben noch nicht aktiviert sind und erst mal in Produktion ausgeschaltet sind.

Und was ich auch interessant fand, sie machen Cherry-Picking f√ºr Releases.

Das hei√üt also, sie nehmen bestimmte Kommits raus aus dem Trunk und darauf basierend werden dann Releases gemacht.

Also es ist nicht Continuous Deployment in dem Sinne, dass halt Sachen sofort automatisch in Produktion gehen, wenn jemand etwas √§ndert, sondern da gibt es irgendwie nochmal einen Cherry-Picking-Prozess.

Sie m√ºssen auch irgendwie Rollbacks k√∂nnen.

Das hei√üt, wenn irgendwas in Produktion schief geht, muss es zur√ºckgerollt werden.

Mittlerweile ja etwas, was glaube ich unstrittig ist, dass das irgendwie eine gute Sache ist und mit Kubernetes auch umgesetzt wird, wo ich eben dann sage, ich mache halt ein Kommit.

Wenn es schief geht, wird eben der Container nicht ausgerollt.

Jetzt haben wir ja dieses Thema mit, wie gehe ich damit um, dass ich gro√üe √Ñnderungen habe, die halt √ºber mehrere Microservices, Dienste, whatever, halt hinweggehen.

Und auf Google Scale ist es eben tats√§chlich so, dass wir zum Beispiel Basis-Libraries haben, also irgendwelche technischen Dinge, die dort gebaut werden.

Sie sprachen auch sp√§ter davon, dass es eine Compiler-Gruppe gibt, die sich so darum k√ºmmert, dass der Compiler vern√ºnftig funktioniert.

Und √Ñnderungen, die jetzt dort vorgenommen werden, wenn ich sage, es gibt eine neue Compiler-Version, gehen Cross-Cutting √ºber alle Services hinweg.

Was eben zu der Frage f√ºhrt, wie kann ich solche √Ñnderungen abhandeln.

Ich hatte schon gesagt, wenn ich viele von diesen √Ñnderungen habe, dann w√ºrde ich sagen, ist eigentlich irgendwie etwas doof.

Sprich, dann ist vielleicht mein Schnitt nicht in Ordnung.

Mindestens w√ºrde ich das sagen f√ºr die Gr√∂√üenordnung, in der ich typischerweise operiere.

Bei Google ist es halt, glaube ich, erstmal ein anderes Setting, weil ich habe eben diese Teile der Organisation, die technische Infrastruktur bauen.

Die haben Leute, die sich um das Linux-Betriebssystem k√ºmmern, um Compiler und was auch immer da ist.

Was bedeutet, dass sie einfach eine gr√∂√üere technische Tiefe haben, als das der Fall ist bei den meisten Unternehmen, mit denen ich arbeite.

Die benutzen Open-Source-Projekte und benutzen Betriebssysteme von der Stange und benutzen ganz viele Libraries von der Stange, weil sie nicht springen oder was auch immer die Libraries sind.

Das ist bei Google nochmal etwas anderes.

Was dazu f√ºhrt, dass vielleicht diese querschnittlichen Sachen, wo jetzt jemand an der technischen Infrastruktur was dreht, dort h√§ufiger sind.

Deswegen ist das vielleicht eher ein Thema.

Sie haben daf√ºr eine L√∂sung, die sich ROSI nennt.

Das ist offensichtlich ein Werkzeug.

Sie arbeiten dort au√üerdem mit R√ºckw√§rtskompatibilit√§t.

Das hei√üt, was sie jetzt machen, wenn zum Beispiel eine Library sich √§ndert, ist, dass sie sagen, ich √§ndere diese Library.

Das hat irgendwelche Dinge ge√§ndert werden m√ºssen, in welchen Projekten.

Dann deprecate ich nur den alten Weg.

Der neue Weg ist nicht der einzige.

Der alte bleibt erstmal bestehen.

Dann teile ich halt diese gro√üe √Ñnderung.

Alle Projekte, die diese Library benutzen, sind potenziell beeinflusst von dieser √Ñnderung dieser Library.

Das teile ich jetzt mit ROSI in lauter kleine √Ñnderungen auf pro Projekt.

Die werden dann reviewed von, wer auch immer daf√ºr zust√§ndig ist, dieses Projekt unter Kontrolle zu haben.

Dann gehe ich weiter vor.

Ich pers√∂nlich finde das jetzt erstmal gar keinen schlechten Ansatz.

Es wirkt f√ºr mich so, als w√ºrde ich jetzt eine Serviceeinheit haben, die f√ºr mich irgendwelche Dinge macht.

Hier ist √ºbrigens eine neue Library.

Ich fixe das mal f√ºr dich, was sich √§ndert.

Ich kann jetzt sehen, das machst du.

Cool, das gef√§llt mir oder eben auch nicht.

FS schreibt, wenn tausende Devs an Monorepo arbeiten, kommt es dann nicht √∂fter zu Merge-Konflikten.

Das ist ja das, weswegen dieses Joint-Based-Development so entscheidend ist.

Es bedeutet, es gibt eine aktuelle Version von jeder Datei.

Dass parallel Menschen daran arbeiten, sollte dann eben relativ wenig h√§ufig der Fall sein, weil es eben keine Branches gibt.

Das ist so ein bisschen der Hinweis.

Kann ich eigentlich in dieser Gr√∂√üenordnung noch mit Branches zumindest arbeiten?

Bin ich mir nicht sicher.

Meine ausgecheckte Version ist nat√ºrlich in gewisser Weise ein Branch.

Das bedeutet, dass ich regelm√§√üig vom Trunk mir die √Ñnderungen holen muss.

Die Frage, die man sich stellen kann, ist, ob Google eine ernsthafte Alternative hat.

Man kann die in gewisser Weise beantworten, weil Google selber bei Chrome und Android, was ja auch gro√üe Projekte sind, anders vorgeht.

Es gibt also eine Alternative, um dem sozusagen vorzugreifen.

Das, was Sie meiner Ansicht nach in dem Paper schreiben und f√ºr mich auch nachvollziehbar ist, ist, dass es der Weg ist, wie Sie arbeiten.

Wenn Sie jetzt fundamental diesen Weg √§ndern, warum sollten die das tun?

Ich glaube, es ist nicht so sehr die Aussage, dass es der Weg ist, sondern es ist eher die Aussage, wir haben uns daran gew√∂hnt.

Wir haben uns darum herum gestrickt.

Wenn wir das jetzt fix √§ndern, dann gibt es ein Problem.

Vorteile laut Google.

Die Vorteile sind, einheitliche Versionierung.

Es gibt eine Quelle der Wahrheit, was die aktuelle Version ist.

Es gibt die M√∂glichkeit, Code zu teilen und wiederzuverwenden.

Nicht-Code-Wiederverwendung ist letztendlich das Einf√ºhren von Abh√§ngigkeiten.

Dar√ºber kann man jetzt diskutieren.

Das ist in gewisser Weise nat√ºrlich eine gute Sache, aber es f√ºhrt eben zu diesem Problem, dass ich dann viele Abh√§ngigkeiten habe.

Es gibt ein Detailproblem, was Sie als Vorteil nennen.

Interessanterweise ist das Dependency Management im Allgemeinen und dann insbesondere Diamanten Dependency Management.

Also ich habe oben eine Komponente, eine Komponente A.

A h√§ngt ab von B und C, also von zwei anderen Komponenten.

Und B und C h√§ngt beides ab von D.

Ich habe also ein Diamant.

Oben ist A.

A h√§ngt von B und C ab und B und C h√§ngen beide von D ab.

Jetzt ist halt das Problem, wenn ich D √§ndere, dann habe ich eine neue Version von D.

Richtig unsch√∂n w√§re es, wenn ich jetzt B und C unterschiedliche Versionen von D ben√∂tigen.

Das kann ich hiermit jetzt irgendwie verhindern.

Also ich sage, ich habe einen Atom-Organ-Commit, der √§ndert halt D.

Ich √§ndere in dem Zug B und C gleich mit und dann habe ich eben einen einheitlichen Stand.

B und C benutzen beide dieselbe neue Version von D.

Das kann ich atomar dann √§ndern.

Also atomare √Ñnderungen sind ein Vorteil.

Ich kann gro√üe Refactorings machen.

Das hei√üt, ich kann jetzt irgendwie sagen, ich mache tats√§chlich eine nicht r√ºckw√§rts kompatible √Ñnderung einer API, sehe halt alle, die es benutzen und kann das √ºberhaupt erstmal einigerma√üen vern√ºnftig durchf√ºhren.

Ich kann √ºber Teamgrenzen hinweg kollaborieren.

N√§mlich Team-Co-Teile.

Ich habe flexible Teamgrenzen.

Ich kann also ohne gr√∂√üere Probleme sagen, ich bin jetzt f√ºr einen anderen gr√∂√üeren Teil verantwortlich und sie k√∂nnen eben durch da entsprechend auch Sichtbarkeiten einf√ºhren, dass sie bestimmte Teile des Teams bestimmte Dinge halt sehen.

Gro√üe √Ñnderungen, die halt unterst√ºtzt werden, da hatten sie als Beispiel zum Beispiel, das ist zehn Jahre her, nicht x86-Code, also ARM-Code oder sowas.

Ich kann da jetzt mehr als √ºberlegen, ob ich eben das ganze Zeug auch auf ARM kompiliert bekomme.

So und das ist eben etwas, was inherent eine gro√üe √Ñnderung ist.

C++11 hatten sie genannt als ein Beispiel, wo also neuer Compiler, dann neuer Standard, neue Sprachstandard unterst√ºtzt wird und auch irgendwelche Performance-Optimierung m√∂glicherweise, die halt vielleicht auf Libraries irgendwie durchgreifen.

Die flie√üenden Grenzen zwischen Teams hatte ich schon erw√§hnt und das sind so die wesentlichen Vorteile.

Und das ist nachvollziehbar auf Google-Ebene.

W√§re ich jetzt eine kleinere Organisation, h√§tte ich halt bestimmte Dinge vielleicht nicht.

Vielleicht w√§re es dann so, dass ich eben nur Open-Source-Frameworks benutze, die halt da drau√üen existieren und dann ist es vielleicht so, dass ich besser damit fahre, dass ich eben jedem Team √ºberlasse, wann sie halt eine neue Version einer Library verwenden.

Nachteile.

Also Tooling, sagen sie, ist notwendig.

Kann man sich vorstellen.

Ich kann nicht auf meinem Rechner 85 Terabyte einfach auschecken und das Ganze mal kurz durchkompilieren, sondern da muss ich halt irgendwie anders vorgehen.

Dann fand ich halt sehr spannend, die Menschen lesen eher den Code und versuchen den zu verstehen, statt der Dokumentation.

Es schleichen sich eher Abh√§ngigkeiten ein.

Die Interna, die ich halt lerne durch Code lesen, werden eher ausgenutzt.

Und damit sind wir eigentlich bei dem Information-Hiding-Thema, was der Pranas schon diskutiert hat.

Also ich kann jetzt eben, ich habe mehr Information, ich sehe den gesamten Code.

Ich nutze also alles, was irgendwie da ist und dadurch kriege ich tiefere Abh√§ngigkeiten, was halt dazu f√ºhrt, dass das ganze Ding dazu neigt, st√§rker unwartbar zu sein, weil eben alles irgendwie mit allem abh√§ngt.

Und das erleichtere ich ja dadurch, dass ich halt Monorepo habe, weil ich eben dann tats√§chlich eben alles erst mal sehe.

Und die Gegenma√ünahme ist dann eben explizite APIs einzuf√ºhren, wo ich eben sage, das sind Dinge, die man tats√§chlich benutzen darf.

Andere Dinge darf man nicht benutzen und das halt irgendwie explizit zu machen.

Das ist Information-Hiding.

Das ist das, was der Pranas schon vorher gesagt hat.

Ich muss also definiert sagen, welche Dinge genutzt werden k√∂nnen und nur die sollten dann noch genutzt werden.

Ja und das Paper sagt eben auch sehr offen, wenn man was anderes einf√ºhren w√ºrde, dann gibt es eine aufwendige Migration und das w√ºrde die Arbeitsweise √§ndern.

Und da ist tats√§chlich die Frage, also wenn 25.000 EntwicklerInnen auf diese Art und Weise arbeiten, kann ich mich jetzt hinstellen und sagen, ich mache es kurz mal anders.

Und dass Android und Chrome nicht so verwalten, sondern da tats√§chlich einzelne Repositories haben f√ºr die einzelnen Bestandteile des Browsers beziehungsweise des Betriebssystems, sagt halt, dass es nicht ein universelles Konzept ist.

Bedeutet eigentlich, dass es eben eine bestimmte Art von Kollaboration ist.

Ich habe also jetzt irgendein Team, was irgendwelche Libraries baut und durch das Monorepo habe ich eben eine M√∂glichkeit erreicht, mit denen eben dieses Team mit anderen Teams, die es benutzen, kollaborieren kann.

Und ich glaube, dass ich halt irgendwie insbesondere, also wie soll ich sagen, ich k√∂nnte mir vorstellen, wenn ich jetzt in der Umgebung arbeiten w√ºrde von dem, was in dem Paper steht, dass das etwas ist, was tats√§chlich vorteilhaft ist.

Ich komme also morgens zur Arbeit, habe einen Pull-Request.

Der Pull-Request sagt halt, ich habe f√ºr dich halt mal die neue Library da irgendwie reingebaut.

Und das ist etwas, was auch im Minimumsrost-Projekten passiert.

So, jetzt geht es um das eigentliche Paper oder das andere Paper von Uber.

Uber ist dieser Fahrdienst.

Und da sind erstmal verschiedene Disclaimer.

Den einen habe ich schon mal gebracht.

Es ist eben so, dass wir jetzt nicht √ºber eine Gr√∂√üenordnung reden, die ich typischerweise bei meinen Kundinnen so sehe, sondern das ist eigentlich eher dr√ºber.

Und das Paper von Uber ist anders als das Google-Paper.

Das Google-Paper sagt halt im Prinzip Monorepos.

Hier ist die Art und Weise, wie wir es halt machen.

Wir haben beobachtet, dass es eben hilfreich ist.

Uber sagt halt, wir haben Monorepos.

Hier ist ein Tooling rund um Monorepos.

## Uber's Monorepo-Ansatz

Was halt bedeutet, dass wir eigentlich nur die Frage stellen k√∂nnen, was bringt uns eigentlich dieses Tooling?

Und es geht um das Thema mit den Rollouts.

Also es geht irgendwie darum, wie ich jetzt √Ñnderungen, die viele im Fall von Uber sind es tats√§chlich die Microservices, betreffen, wie ich daf√ºr sorge, dass die halt irgendwie koordiniert ausgerollt werden. offensichtlich Go-Code und anderer Code, die jeweils in anderen Repositories sind.

Jeder von denen hat hunderte oder tausende Microservices.

Und es gibt ein anderes Paper √ºber Continuous Delivery bei Uber, das verlinke ich auch noch, bei dem halt steht, dass eben dieses Go-Repository 1.000 Commits pro Tag bekommt und 3.000 Microservices enth√§lt.

So, und um jetzt zu verstehen, was der Impact eines Commits ist, muss ich den Abh√§ngigkeitsgrafen des Bildsystems, das ist bei dem Bazel, verstehen.

Das hei√üt also, durch das Bild wird irgendwie angesto√üen, welche abh√§ngigen Sachen auch gebaut werden sollen.

Wenn ich also eine Library baue, dann sagt er mir dieses Bildsystem, okay, ich baue irgendwie auch alle anderen Dinge, die von dieser Library abh√§ngig sind.

Und jeder Commit, der jetzt also das Binary eines Services √§ndert, also auch ein Commit, der nur auf eine Library geht, z√§hlt dann sozusagen als ein Commit f√ºr den Service, in dem Sinne, dass es eben den Service √§ndert.

Und die haben halt im Rahmen von diesem Continuous Delivery Paper beschrieben, sie hat eine Art, wie sie halt ihr Continuous Delivery System standardisiert und vereinheitlicht haben.

Also eben nicht gesagt haben, sorgt halt irgendwie daf√ºr, dass das Zeug in Produktion geht, euer Problem, sondern gesagt haben, hier ist halt ein Set an Dingen, die ihr benutzen sollt oder vielleicht sogar m√ºsst.

Und die sollen irgendwie diese Continuous Delivery Funktionalit√§ten erm√∂glichen, was ja auch schon eine gewisse Zentralisierung ist.

Also es sagt halt irgendwie, ihr als Teams seid dann ein bisschen befreiter von euch, um dieses Thema selber zu k√ºmmern.

Da gibt es eine zentrale Einheit, die das f√ºr euch anbietet.

Und sie sagen dann, dass man mit einem Commit tats√§chlich tausende von Services √§ndern kann, beispielsweise mit der Anpassung einer RPC Library, also Remote Procedure Codes, also f√ºr die verteilte Kommunikation.

Nachvollziehbar, die wird halt wahrscheinlich √ºberall benutzt, weil eben alle Services √ºber Netzwerk miteinander reden.

Und dann sorgt irgendwie so ein Commit daf√ºr, dass ich halt irgendwie ganz viele Services habe, die davon beeinflusst sind.

Und das bedeutet, dass eben eine √Ñnderung potenziell viele Services beeinflussen und auch brechen kann.

Und da haben die jetzt in dem Paper eine Statistik ver√∂ffentlicht.

Und zwar sind das Perzentile, das hei√üt also 50 Prozent der Commits √§ndern einen Service.

95 √§ndern bis zu 14 Services.

Das hei√üt, ich habe 95 Prozent der Commits, haben h√∂chstens 14 Services, die sie beeinflussen.

Bei 99 Prozent sind es h√∂chstens 182 Services und bei 99,9 Prozent sind es 2178 Services.

Das hei√üt, wir k√∂nnen jetzt daraus ableiten, dass halt vier Prozent der Commits zwischen 14 und 182 Services erwischen und 0,9 Prozent erwischen 182 bis 2178 Services.

So und das bedeutet, dass halt so ungef√§hr gr√∂√üenordnungsm√§√üig ein Prozent der Commits wahnsinnig viele Services √§ndern.

So und das ist jetzt irgendwie eine Geschichte, wo man ja sagen w√ºrde How hard can it possibly be?

Also das wirkt ja jetzt erst mal nicht so schlimm.

Man muss sie aber vergegenw√§rtigen, dass sie halt irgendwie so das andere Paper 1000 Commits pro Tag haben.

Und das bedeutet halt, ein Prozent von 1000 sind irgendwie 10.

Das hei√üt, sie haben bis zu zehn √Ñnderungen, die halt in diesen so gro√üen Impact irgendwie reingehen.

Wenn man dem anderen Paper vertraut, was sie jetzt in dem Paper zu dem Studium geschrieben haben, ist, dass es halt eine √Ñnderung pro Tag gibt, die mehr als 2000 Services erwischt.

Das ist ein bisschen, also da, wie soll ich sagen, sagen die Paper ein bisschen unterschiedliche Dinge.

Aber wir nehmen also eigentlich aber einen Commit pro Tag, der mindestens 2000 Services erwischt.

Und also ich wei√ü nicht, wie es euch geht, aber ich empfinde es jetzt erst mal als erschreckend.

Also das bedeutet halt, dass ich eine √Ñnderung pro Tag habe, die im Prinzip praktisch fast das gesamte System erwischt.

Also ich habe 3000 Go-Services insgesamt.

Zwei Drittel des Systems werden also durch ein Commit ge√§ndert und das passiert jeden Tag.

Und das kann ja wieder zu, also das bedeutet halt, dass da halt sozusagen ein Problem auf der Hand ist.

Genau, wir k√∂nnen noch mal kurz zur√ºckspringen.

Jules Schulz hat gerade geschrieben, Merge-Konflikts w√ºrden entstehen, wenn tausende Devs an einem gleichen Base-Code arbeiten w√ºrden.

Meiner Ansicht nach, Devs, die an derselben Basis gleichzeitig arbeiten, sind weniger und kommunizieren miteinander.

W√ºrde ich halt auch so erwarten.

Ich w√ºrde halt erwarten, insbesondere in dem Google-Modell, dass ich halt sozusagen ein Team habe, was f√ºr einen Teil verantwortlich ist, was zum Beispiel dort √Ñnderungen auch reviewt.

Und es w√§re jetzt irgendwie extrem √ºberraschend, wenn halt an einem Teil sehr viele Leute √§ndern, nicht?

Aber wie dem auch sei.

Also die Annahme ist halt, dass die √Ñnderungen im Wesentlichen gleich verteilt sind.

Das, was halt dann ein Problem ist, das hier in der Tabelle ist, dass eben bestimmte √Ñnderungen am Basis Sachen halt irgendwie sozusagen gro√üe Auswirkungen hat.

So, was Sie jetzt eben weiterschreiben, ist, dass also ein schlechter Komit durchkommen kann.

Damit meinen Sie einen Komit, der jetzt daf√ºr sorgt, dass sozusagen beobachtbar durch Menschen, das System irgendwie schlecht performt.

Und das wollen Sie jetzt, dieses Risiko wollen Sie managen.

Das hei√üt, Sie wollen jetzt irgendwie herausfinden, ob ein Service √ºberhaupt ein Problem haben wird.

Und dazu sind Sie halt auf die Idee gekommen, das Deployment in Kohorten aufzuteilen.

### Kohortenbasiertes Deployment

Das hei√üt also, ich mache eine √Ñnderung an so einer OPC-Library beispielsweise.

Es erwischt 2000 Microservices.

Ich deploye die nicht alle auf einmal, sondern ich deploye eine Kohorte, dann die n√§chste und die n√§chste.

Und ich kann die nach Kritikalit√§t, die haben sie nach Kritikalit√§t aufgeteilt.

Das hei√üt, ich deploye erst mal die Kohorte mit den Services, die halt nicht so wichtig sind, weil sie eben vielleicht eh nicht so stark verf√ºgbar sein sollen, sein m√ºssen.

Und dann arbeite ich mir weiter durch die Kohorten fort.

Und das Deployment geht eben nur dann weiter, wenn die √Ñnderung keine Fehler erzeugt hat.

Beispiele f√ºr Fehler, die ich jetzt haben kann, ist zum Beispiel, dass der Ressourcenverbrauch pl√∂tzlich zu hoch ist.

Das also spricht, das ist irgendwie etwas, was aus Monitoring herausf√§llt.

Das ist noch nicht ein Ausfall, sondern bedeutet nur Irgendetwas ist komisch.

Der Ressourcenverbrauch sei pl√∂tzlich hoch.

Speicher geht hoch.

CPU geht hoch.

Ich habe vielleicht einen ineffizienten Algorithmus.

Ich habe vielleicht irgendwo Memory Leak.

Und das hatte ich eben vorher im Testing nicht gefunden, weil eben eine bestimmte Benutzung das erst erm√∂glicht.

Ich habe irgendwelche SLA's, die verletzt sind, also Service Level Agreements, sprich Antwortzeiten sind zu lang oder was auch immer.

Oder im Worst Case der Container startet nicht.

Und das f√ºhrt jetzt zu einer.

Also.

Da gibt es jetzt ein Problem.

Das Problem ist halt, ich habe also eine √Ñnderung an einem Microservice und diese √Ñnderung ist noch nicht deployed, weil der ist in einer Kohorte, die jetzt sp√§ter deployed wird.

Und das ist irgendwie doof, weil das bedeutet, ich habe jetzt irgendwie etwas, was eben noch nicht in Produktion ist.

Und ich will ja eigentlich idealerweise Trunk Based arbeiten.

Und das bedeutet, dass ich jetzt irgendwie Dinge habe, die sozusagen in der Schwebe sind und dem muss ich jetzt irgendwie begegnen.

Und die begegnen dem, indem sie sagen, dass sie halt die Container f√ºr das automatische Deployment sperren.

Das hei√üt also, so verstehe ich das zumindest, wenn ich jetzt eine √Ñnderung mache, w√§hrend ich in einer Kohorte bin, die noch nicht deployed ist, dann kann ich die halt im Moment nicht deployen.

Was bedeutet, dass die sich halt sozusagen auft√ºrmt.

Also irgendjemand √§ndert diese RPC Library.

Mein Service ist betroffen.

Ich m√∂chte jetzt gerne mit dem Service live gehen.

Geht nicht, weil der ist in einer Kohorte, die eben noch nicht durch die RPC Library, durch diese √Ñnderung halt durch ist und halt deployed worden ist.

Und das ist eben etwas, was zwei Drittel der Services einmal pro Tag sozusagen durchgehen.

So der Uwe Patel hat gefragt, welchen zeitlichen Abstand haben die Kohorten, denn die Kohortendeployments, dazu kommen wir gleich.

Da gibt es eine Grafik, die ich zeigen m√∂chte.

Dann schreibt der Tobi Wan Kenobi, ab welcher Metrik Gr√∂√üen, Devs, Teams, Anzahl der Projekte w√ºrde man √ºberhaupt beginnen, √ºber Monorepos nachzudenken bzw. umgekehrt.

Wann sind Monorepos √ºberhaupt nicht sinnvoll?

Ja, das ist halt die Frage, die ich halt am Ende nochmal gerne beantworten m√∂chte.

Zwischenstand ist halt, dass Monorepos nach dem Google Paper eine bestimmte Art erm√∂glichen, zu kollaborieren zwischen Teams und dass das wahrscheinlich hilfreich ist, wenn ich ein hohes Ma√ü an Investment, insbesondere an gemeinsamen Libraries habe.

Was das bedeutet, das ist wahrscheinlich aus dieser Motivation f√ºr die meisten Unternehmen, mit denen ich arbeite, die halt Standard Open Source Systeme benutzen als Basis und nicht die Basis, wenn wir komplett selber bauen, was vielleicht nicht so sinnvoll ist.

Aber dazu kommen wir noch.

Genau, so.

Ich muss mal kurz schauen.

Ach so, genau.

Also diese Sperren, √ºber die ich sprach, m√∂chte ich nat√ºrlich m√∂glichst schnell aufhalten, also daf√ºr sorgen, dass die m√∂glichst schnell wieder freigegeben werden.

Daf√ºr haben die jetzt gesagt, wir bauen einen Simulator, der sich √ºberlegt, wenn wir so ein koh√§urantenbasiertes Deployment machen, wie lange denn das dauert, bis es gesperrt worden ist.

Und da kann man halt so Parameter einstellen wie Kohortengr√∂√üe, also wie viel ich auf einmal deploye, Progress Threshold, also ab wie viel erfolgreich deployten Services ich halt die n√§chste Kohorte nehme und halt weitere Policies f√ºr die CD-Pipeline.

Und da gibt es zwei Abbildungen in dem Paper.

Da gibt es also einmal die Abbildung, die zeigt, wie das halt sozusagen unoptimiert ist und dann haben sie die Abbildung mit optimiert.

Und die m√∂chte ich jetzt eigentlich ganz gerne zeigen.

Das ist die da, nee, das ist nicht die da.

Das ist die da.

Und die m√∂chte ich eigentlich nochmal h√ºbsch machen.

So sieht es besser aus.

So, was wir da jetzt sehen, ist folgendes.

Wir sehen also eine Woche unten.

Die Woche f√§ngt an bei Sonntag, 18 Uhr, UTC und geht bis Donnerstag, 18 Uhr, UTC.

Und wir haben hier die Zeit, die es halt ben√∂tigt, bis eine Kohorte entsperrt ist und die Zeit, die sie sich halt dort geben sind oder wie sie anstreben, sind 24 Stunden.

Das ist die gestrichelte horizontale Linie.

Dann gibt es hier irgendwie den Tier 0, 1, 2.

Das ist die Business-Kritikalit√§t und dann irgendwie das vollst√§ndige.

Und was das jetzt bedeutet, ist folgendes.

Wenn ich halt am Sonntag um 18 Uhr UTC in die Programme anwerfe, dann kann ich damit rechnen, dass ich halt innerhalb von, was sind das, 15 Stunden Tier 2 entblockt habe, innerhalb von ungef√§hr 20 Stunden Tier 1, innerhalb von etwas √ºber 24 Stunden Tier 0 und komplett ist es halt irgendwie nach 41 Stunden.

Wenn ich dasselbe am Mittwoch mache, dann Mittwoch um 18 Uhr, dann dauert es halt 120 Stunden, bis alles entblockt ist.

Die Tier 0, 1, 2 Sachen sind halt unterhalb von 24 Stunden entblockt.

Und wenn ich das Donnerstag um 18 Uhr mache, dann dauert halt auch der Tier 0 irgendwie, was sind das, 85 Stunden oder so.

Dazu muss man jetzt sagen, dass ein wesentlicher Einfluss die Konfiguration ist, die zum Beispiel erlaubte Zeitfenster definiert.

Also das bedeutet, dass die Teams jetzt sagen k√∂nnen, wir m√∂chten halt gerne nur deployen zu den Zeiten, zu denen wir halt vor Ort sind, damit wir da mehr eingreifen k√∂nnen.

Ich glaube, das ist das, was die ja auch typischerweise tun.

Ich bin ehrlich gesagt nicht sicher.

Das Paper ist meiner Ansicht nach nicht ganz eindeutig.

Man kann es entweder so lesen, wie ich es gerade gesagt habe, dass sie also die Zeiten so einschr√§nken, dass zu den Zeiten, wo sie selber arbeiten, Deployments stattfinden sollen.

Das ist wahrscheinlich auch logisch, weil man dann eingreifen kann.

Ich hatte halt so ein bisschen oder das Optimum w√§re ja eigentlich, dass es passiert, w√§hrend sie nicht arbeiten, damit sie halt in ihrer Arbeit nicht gehemmt werden, also dann einfach sozusagen ihre t√§gliche Arbeit mit.

Ich mache eine √Ñnderung und ich deploye es halt, dass sie die irgendwie machen k√∂nnen.

Ich glaube, am Ende ist es halt so, dass sie eigentlich eher sicher spielen wollen.

Also zu den Zeitpunkten, wo es deployt wird, sie eben dann arbeiten wollen.

Und das erkl√§rt halt, glaube ich, warum irgendwie nicht das Donnerstag halt so in die H√∂he geht, weil sie dann nicht am Freitag noch schauen wollen, dass sie das halt irgendwie deployt wird.

Was bedeutet, dass da irgendwie das Wochenende inzwischen steckt?

Dann geht es halt in die H√∂he.

Also es ist nicht so, dass das jetzt die reine Zeit ist, die das technisch ben√∂tigt, sondern dass das eben eher offensichtlich die Constraints sind, die halt die Teams einf√ºhren.

So verstehe ich das halt zumindest.

Und das ist da, glaube ich, so im Wesentlichen die Aussage, also eher Erfolg.

Und sie haben dieses Werkzeug dann, also Uwe Bartel hat gerade geschrieben, oder es gibt Spitzenzeiten, wo sie unkritische Deployments vermeiden.

Genau, sowas kann irgendwie auch sein.

Wei√ües Uber, Wochenende, unkritisch.

Also ich glaube nicht, dass das Wochenende da ein Peak ist, aber keine Ahnung.

Achso, und sie benutzen das Werkzeug halt auch zum Koordinieren von Deployments mit nur Konfigurationsunterschieden.

Also es gibt halt offensichtlich Machine Learning Systeme, wo sie dann eben nur unterschiedliche Modelle haben, die unterschiedliche Daten nehmen, aber die halt am Ende dann letztendlich denselben Code haben.

Und daf√ºr nutzt sie halt auch dieses Koordinierungswerkzeug.

Soweit, so gut.

Jetzt ist die Frage, wie wir das bewerten.

Und da ist erstmal mein Disclaimer.

Also das ist halt die Papierform.

Und wenn ich irgendwie Architekturreviews mache, lassen wir uns typischerweise irgendwie die Papiere alle zuschicken, die man irgendwie so haben kann.

Und wir lesen die halt irgendwie durch.

Und weil halt die Hoffnung ist, dass man mit den Papieren irgendwie mehr wei√ü als ohne.

Und das ist eben auch so.

Aber die Sachen, die halt irgendwie in den Papieren drinstehen, sind halt h√§ufig missverst√§ndlich, falsch oder beantworten dann bestimmte Fragen nicht.

Das hei√üt also, ich kann hier halt einen ersten Eindruck schildern, nicht eine echte Bewertung.

Und es f√ºhrt irgendwie eher zu der Frage, okay, nehmen wir jetzt mal an, wir w√ºrden das halt bewerten wollen.

Was w√§ren irgendwie Sachen, die wir halt diesen Uber-Menschen fragen w√ºrden?

Und was w√§re also das, womit wir halt in die Termine mit ihnen reingehen w√ºrden?

Das Erste ist so eine, ich sag mal so eine Art Geschmacksfrage.

Also will hei√üen, das ist eben tats√§chlich super subjektiv, mein allererster Eindruck.

Und ich wei√ü nicht, wie es euch geht.

F√ºr mich war das auch einer der Gr√ºnde, warum ich das Paper jetzt irgendwie diskutieren wollte, insbesondere das Uber-Paper.

## Kritische Analyse des Uber-Ansatzes

Mir widerstrebt halt die Komplexit√§t dieser L√∂sung und dieser Koordination.

Also wir wollen doch am Ende, h√§tte ich jetzt erwartet, Teams haben, die halt idealerweise f√ºr ihre Microservices zust√§ndig sind oder f√ºr einen Teil des Systems und halt unabh√§ngig von allen anderen irgendetwas tun k√∂nnen.

Und das sollte halt hoffentlich einfach sein.

Also ich sollte einen Knopf dr√ºcken und dann sollte halt in die Probleme losgehen.

Und das ist es halt.

Und das konterkarieren wir hier gerade, weil wir halt den Teams, sie davon abhalten, die Deployments zu machen, weil wir halt irgendwie ein zentralisiertes Deployment von einem anderen Team h√∂her priorisieren.

Das bedeutet also, mein Eindruck, so lese ich das aus dem Paper raus, irgendjemand √§ndert eine RPC-Library und ich stehe da und sage, cool, ich kann halt jetzt irgendwie mein Zeug nicht deployen in Produktion.

Und das ist halt insbesondere deswegen schmerzhaft, weil eben ein h√§ufiges Produktions-Deployment zahlreiche Vorteile mit sich bringt.

Es bringt halt letztendlich h√∂her Produktivit√§t.

Und f√ºhrt eben auch dazu, nicht nur ein besseres Time-to-Market, sondern eben auch h√∂here Produktivit√§t.

Und da gibt es irgendwie die DevOps-Studie, die belegt das.

Und da gehe ich jetzt gerade im Kompromiss rein und das finde ich halt irgendwie schwierig.

Insbesondere, weil wir ja so eine, also das ist das andere, was ich halt so ein bisschen paradox finde.

Also teilweise ist Deployment ist ja irgendwie offensichtlich akzeptabel.

Das hei√üt also ich kann, mein Service ist noch nicht deployed, der Service von den Leuten um mich herum ist deployed mit der neuen RPC-Library.

Warum m√ºssen wir das dann √ºberhaupt koordinieren?

Also warum kann ich dann nicht irgendwie sagen, ich deploy halt, wenn ich Bock habe, irgendwann.

Und wenn ich das n√§chste Mal eine Feature-√Ñnderung mache, dann deploy ich halt einfach.

Und wenn es halt irgendwie schief geht, dann wei√ü ich halt, dass ich da ein Problem habe.

Und dann kann ich halt irgendwie zur√ºckrollen.

Und ich bin halt der Herr √ºber den Service und entscheide das halt.

Das ist nach meinem Empfinden das, was halt dieses Konzept von Google und ROSI halt eher sagt, wo ich eben tats√§chlich dann selber die Person bin, die sich halt k√ºmmern muss oder kann.

Und ich dann eben die Person bin, die halt die Herrschaft √ºber den Server hat, √ºber den Service hat und eben sagen kann, nee, ich rolle das jetzt irgendwie zur√ºck oder die √Ñnderung ist irgendwie doof oder nicht, geht halt nicht durch den Test oder was auch immer.

Und ja genau, also diese Geschichte mit dem Sperren von Containern f√ºr Deployments finde ich schwierig.

Und irgendwie f√ºhrt das zu so einer komischen Zust√§ndigkeit.

Also es f√ºhrt irgendwie dazu, dass diese Menschen, die halt den RPC-Service √§ndern, den ich halt irgendwie als Beispiel nur nehme, weil das das einzige Beispiel ist, dass ich da halt irgendwie, was halt in dem Paper halt direkt drinsteht, die √ºbernehmen jetzt eine Verantwortung daf√ºr indirekt, dass mein Service nicht bricht.

Finde ich ja nett von denen, aber eigentlich sollte ich verantwortlich sein f√ºr meinen Service.

Und da ist irgendwie dieses Pool-Prinzip, dass ich also eher sage, okay, ich nehme halt die √Ñnderung rein, ich review die, liegt mir pers√∂nlich n√§her.

### Probleme mit Deployment-Sperren

Und diese Summe aus dem, der Anzahl, der Dauer der Sperren, nicht Ziel unter 24 Stunden und der Frequenz der √ºbergreifenden √Ñnderung, eine √Ñnderung mit mehr als 2000 Microservices jeden Tag, da liegt halt sozusagen die Frage so ein bisschen auf dem Tisch, die man jetzt irgendwie stellen muss, wie stark behindert das die Teams bei der Arbeit?

Also so aus der Papierform h√∂rt sich das an, wie das.

Und da kann halt kein Team arbeiten.

Das kann aber eigentlich nicht sein.

Und deswegen verstehe ich da vielleicht irgendwas nicht.

M√ºssen wir halt schieben.

Genau.

Uwe schreibt halt gerade, das Verschwimmen der Verantwortlichkeiten finde ich auch schwierig.

Und das f√ºhrt dazu, das ist das, was ich bei dem Google-Modell schon sagte, dass wir eigentlich ein Tooling und einen Ansatz haben, um halt eine bestimmte Art von Kollaboration zu unterst√ºtzen.

Da schreibt er irgendwie weiter.

Das klingt so, dass ohne Not die Probleme erschaffen werden, wo irgendwie alles deployt werden muss.

Genau.

Das ist so ein bisschen das Problem.

Stimmt nicht.

Es sind nicht die Probleme mit Monolithen.

Es ist eben so, dass seit 50 Prozent der √Ñnderungen mindestens, wahrscheinlich ja noch mehr.

Ich kann noch mal die Grafik zeigen.

Nee, das war nicht die Grafik, die ich zeigen wollte.

Die da.

50 Prozent der √Ñnderungen erwischen nur einen Service.

So irgendwo zwischen 95 und 50 Prozent w√§chst diese Zahl auf zwei.

Bei 95 sind es 14 Services.

Das hei√üt also im Wesentlichen sind Deployments unabh√§ngig, mindestens die H√§lfte.

Also die Mehrheit ist unabh√§ngig.

Und deswegen ist es nicht ein Deployment Monolith, aber es gibt nicht dieses eine Prozent, was halt irgendwie so extrem krass ist und hat das gesamte System nahezu beeinflusst.

Und bei nicht 2000 Microservices ist es halt irgendwie schon offensichtlich problematisch.

So was hat der Urs Enzler geschrieben?

Ich war nicht von Anfang an mit dabei, darum fehlen mir wahrscheinlich etwas Informationen.

K√∂nnte es sein, dass das Problem darin liegt, dass die Deployment-Units nicht direkt zu den logischen Service-Einheiten passen?

Das ist eine von den Sachen, die ich eigentlich noch mal diskutieren wollen w√ºrde.

Kann ich jetzt eigentlich auch machen.

Das, was wir hier eigentlich diskutieren ist, also mir ist das Problemstatement nicht ganz klar.

Und das l√§uft halt letztendlich in diese Richtung rein.

Das einzige konkrete Beispiel f√ºr eine √ºbergreifende √Ñnderung ist eine RPC Service √Ñnderung.

Und dass die √ºbergreifen, ist halt irgendwie trivial.

So was mich jetzt interessieren w√ºrde, und das w√§re eine von den Fragen, die ich stellen w√ºrde, was sind diese √Ñnderungen, die zu viele Microservices erwischen?

Wie viele sind das und welche Art von √Ñnderungen ist das genau und welche Constraints gibt es?

Also wenn man jetzt sagt, ich mache eine √Ñnderung auf eine RPC Service, muss ich das auf die RPC Library zeitnah deployen.

Warum muss ich das?

Also bei einem Security Update ist das was anderes.

Wenn ich jetzt sage, ich habe ein Security Problem erwischt, dann muss ich eben sehr schnell alles M√∂gliche deployen.

Und das ist auch wichtiger als Features.

Aber nur eine RPC Library, die vielleicht irgendwo irgendeinen Corner Case jetzt irgendwie anders l√∂st, muss ich das halt wirklich deployen.

So und das ist eben eine von den Fragen und etwas, was das Paper nicht beantwortet.

Also was sind denn eigentlich die Dinge, die halt diese vielen √Ñnderungen triggern?

Und da sind noch ein paar andere Sachen, die ich halt irgendwie erstaunlich finde.

### Deployment-Koordination

Also die haben halt eine Modularisierung.

Ich behaupte, die Aufteilung in mehrere Monorepos ist eine Modularisierung.

Ich sage n√§mlich, dass ich halt mein System habe und ich habe es jetzt aufgeteilt in Monorepos und das sind Module in dem Sinne, dass ich eben meine gesamte Software aufgeteilt habe, so wie ich halt eben in einem Unternehmen auch irgendwie Systeme habe.

Ich habe ein ERP, Enterprise Resource Planning System, CRM f√ºr Customer Relationship Management, ich habe eine Webseite und hier habe ich jetzt irgendwie entschieden, dass ich auf dieser sehr grobgranularen Ebene Monorepos habe.

So und bei Google ist halt die Aussage, die gesamte Webseite ist halt Monorepo, also nicht Gmail und Google Mail und was da drauf was.

Android und Chrome sind extra.

So und bei Uber ist halt die Aussage, die Monorepos sind aufgeteilt nach Programmiersprache.

Das ist keine fachliche Aufteilung.

Die Aufteilung bei Google ist eine fachliche Aufteilung und das ist halt ein bisschen komisch und das zieht sich ja jetzt weiter durch.

Das hei√üt, also letztendlich ist das eine rein technische Aufteilung.

Also ob ich was in Go schreibe oder nicht, ist halt nicht abh√§ngig von der Fachlichkeit, behaupte ich jetzt mal.

Und das bedeutet, diese Aufteilung in diese Monorepos, die grobgranulare Aufteilung, die sie halt haben, die ist halt technisch beeinflusst.

Und das liegt dem Verdacht nahe, dass wir ja auch √ºber eine technische Optimierung sprechen.

Also das hat bestimmte technische Commits, zum Beispiel √Ñnderungen an einer RPC Library.

Das sind die, weswegen ich irgendwie dieses System baue.

Und da ist die Frage, warum muss ich das eben schnell zeitnah alles deployen, wenn es eben kein Sicherheitsupdate ist.

Und das ist halt insbesondere deswegen ein Thema, weil ich eben immer noch behaupten w√ºrde, fachliche √Ñnderungen sind das, was eigentlich wichtig ist.

Also die neue RPC Library interessiert halt niemanden.

Was mich interessiert, ist, dass halt Uber aus irgendwelchen Gr√ºnden mir das Auto schneller anbieten kann, mir mehr Informationen gibt √ºber das Auto oder was auch immer.

Also irgendwelche Fachlichkeiten, das sind halt die Sachen, die Kunden typischerweise beeindrucken und nicht die RPC Library, wenn sie nicht irgendwie kundenwirksame Features hat.

So und das bedeutet halt, dass irgendwie dieses ganze Ding halt technisch wirkt.

Und das f√ºhrt irgendwie dazu, dass ich mir da nicht sicher bin, was denn eigentlich der Grund ist, dass man eben diesen Ansatz w√§hlt und dass man halt insbesondere diese Art von Deployment w√§hlt.

Also letztendlich bedeutet es halt, wenn die da dr√ºben die RPC Library √§ndern, dann ist mein Service blockiert und ich kann meine eigenen Features halt erst mal nicht ausliefern.

Und das finde ich komisch.

Genauso fehlt halt die Frage, was denn eigentlich Alternativen sind.

### Fehlende Alternativen

Also ich w√ºrde behaupten, die offensichtliche Alternative ist, diese √Ñnderung halt irgendwie zu pullen.

Also das, was eben Rosie bei Google √ºber solche, f√ºr mich wirkt es wie Pull-Requests, halt da irgendwie macht, kann man ja auch irgendwie anders machen.

Also man k√∂nnte sagen, es gibt halt die neue Library oder was auch immer die Abh√§ngigkeit halt ist und ich mache gar keinen Pull-Request, sondern ich kann mir das irgendwie selber reinziehen.

Und diese Alternative, warum die nicht gew√§hlt worden ist, ist hier nicht diskutiert worden, was jetzt bei dem Uber-Paper nicht erstaunlich ist, weil sie ja nur √ºber das Tooling sprechen.

Sie sagen nicht, warum sie √ºberhaupt so vorgehen.

Also sie sagen, im Prinzip sagt dieses Paper Pr√§misse, wir machen eben Monorepos, wir machen es so, wie wir es halt bei Uber machen.

Konklusion, wir bauen ein System, um eben Deployments √ºber viele Microservices zu koordinieren.

Aber da fehlt halt was.

Und ich w√ºrde halt bei dieser Diskussion eher das konkrete Problem mit m√∂glichen Corner Cases diskutieren und analysieren und Alternativen betrachten.

Dann w√ºrde ich es halt bewerten und das ist auch der Grund, warum wir halt in Architectural Decision Records ja genau das machen, dass wir halt irgendwie schauen, was ist eigentlich die Entscheidung, was sind die Alternativen, warum haben wir diese Entscheidung halt gef√§llt.

Und das fehlt eben in diesem Paper, was, wie gesagt, man dem nicht anlasten kann, weil es eben ein Paper ist √ºber Tooling.

Die Ziele sind mir auch nicht so klar.

Ich finde es, also was ich dadurch erreiche, ist halt eine st√§rkere Konsolidierung und Einheitlichkeit.

Also ich kann jetzt irgendwie Leute dazu zwingen, eine bestimmte Version der Library zum Nutzen in allen Services.

Und das ist eben einheitlich und konsolidiert.

Und die Gr√ºnde sind jetzt irgendwie auch nicht so klar.

Also was wir ja jetzt erreichen, ist eine h√∂here Zuverl√§ssigkeit von √Ñnderungen.

Und zwar erreichen wir das deswegen, weil wir jetzt eben eine Kohorte erst mal loslaufen lassen und die √Ñnderung sozusagen Beta testen lassen.

Und dann lassen wir halt die n√§chste Kohorte, die n√§chste Kohorte laufen und dann kriegen wir halt irgendwann alles deployed.

So das hei√üt, die Zuverl√§ssigkeit, die Deployments w√§chst.

Aber es steht ja jetzt ein wenig drin, wir hatten √ºbrigens andere Produktionsprobleme, weil uns halt irgendwie Commits durchgerutscht sind, durch √Ñnderungen von Basis-Libraries oder Basis-Dingen.

Das w√ºrde da aber auch nicht stehen, weil nicht, also wir werden nicht ein Paper schreiben, wo sie irgendwie reinschreiben.

Wir bei Uber hatten √ºbrigens ein Zuverl√§ssigkeitsproblem, halte ich f√ºr eher unwahrscheinlich.

Es f√ºhrt aber auch noch zu anderen Fragen.

Es f√ºhrt eben zum Beispiel zu der Frage, warum die lokalen Tests solche Probleme nicht gefunden haben.

Also deploy jetzt irgendeinen Service, da ist der neue RPC-Library.

Die RPC-Library hat ein Problem, das eben dazu f√ºhrt, dass in der Produktion irgendetwas nicht funktioniert.

Wieso merkt mein Test das nicht?

Das w√§re eine Frage, die man halt irgendwie stellen muss.

Und da, ja das ist auch noch eine gute Frage, der Uwe fragt gerade die Frage, die sich mir stellt, werden alle Kohorten bei einem Fehler zur√ºckgerollt und nur die Kohorte, wo der Fehler aufgetreten ist, das ist eine gute Frage.

Und das ist auch die Frage, was mache ich denn, wenn in der Kohorte, die bereits ausgerollt ist, noch √Ñnderungen danach gekommen sind und die neuen √Ñnderungen auch ausgerollt worden sind.

Also RPC-Libraries ge√§ndert, ich habe das Ding in Produktion, es ist also entsperrt, also mache ich jetzt eine √Ñnderung.

Die √Ñnderung umfasst halt irgendwelche Dinge.

Jetzt habe ich also zwei √Ñnderungen, die RPC-Library und meine √Ñnderungen vielleicht an den Features.

Jetzt m√∂chte ich ja zur√ºckrollen, nehme ich dann irgendwie diese RPC-√Ñnderung raus, irgendwie auch doof, nicht?

Also dann torpediere ich ein bisschen diese Trunk-basierte Entwicklung, weil ich eben nicht immer nur vorw√§rtsgehe, sondern ich nehme irgendwo hinten einen Kuh mit raus und lasse das dann nicht live gehen.

Ja, dann die Frage grunds√§tzlich, also es gibt jetzt vier Repros mit 100 oder 1000 Microservices und meine These, also bei dem Go-Repro sind es offensichtlich deutlich √ºber 2000.

Ich behaupte, das kann niemand √ºberblicken.

Also es ist nicht menschenm√∂glich, 2000 Microservices zu verstehen und im Kopf zu behalten.

Das hei√üt, da muss meine Behauptung irgendetwas sein, was das noch st√§rker strukturiert.

Also ich w√ºrde erwarten, dass man halt, wenn man jemanden fragt, hey, diese 2000 Microservices oder vielleicht sind es auch 3000, sind ja genau genommen nur etwas √ºber 2000, die halt durch einen Kommitt einmal ge√§ndert werden, sind die irgendwie noch unterstrukturiert.

Und da w√ºrde ich jetzt irgendwie erwarten, dass halt rauskommt, ja, da gibt es irgendwelche Strukturen, nicht?

Also vielleicht ist ein Teil davon die Kommunikation mit dem Uber-Fahrer, ein anderer Teil davon ist irgendwie die Buchung oder was auch immer.

So und der Hintergrund ist f√ºr mich die Frage, will ich vielleicht eine andere L√∂sung haben, bei der nicht Kommitts von 1000 Microservices passieren oder 2000?

So und dann ist die Frage, ob ich halt diese explizite √Ñnderung halt irgendwie abbilden m√∂chte oder dieses implizite Clustering, dass ich also sage, okay, ich habe also jetzt ein Cluster von Services, die sind halt verantwortlich f√ºr die Kommunikation mit dem Uber-Fahrer und diese Services baue ich halt in ein eigenes Repro.

Und dadurch habe ich jetzt vielleicht sowas wie eine Art Kohorte, nur irgendwie fachlich und eine √Ñnderung in der RPC-Library lasse ich eben dort als erstes oder als letztes durchlaufen, weil das eben besonders businesskritisch ist oder besonders wenig businesskritisch.

Der Urs hat geschrieben, was hat er geschrieben?

Um so eine √Ñnderung in einer Serviceeinheit zu √Ñnderungen in mehreren Diplomate-Units f√ºhren.

Genau, also das ist ja so ein bisschen die Frage.

Und letztendlich optimieren wir nur in Anf√ºhrungsstrichen auf, glaube ich, irgendwie wenige Prozent.

Also wir k√∂nnen es uns doch mal angucken.

Nicht die Sachen, die wirklich schwer handelbar sind, sind halt so die letzten f√ºnf Prozent.

Also ab 14 Microservices oder mehr, die beeinflusst sind, sind halt f√ºnf Prozent von den Kommitts.

Da wird es halt irgendwie schwierig.

Das ist vom Anteil nicht viel, aber wenn ich halt irgendwie 1000 Kommitts pro Tag habe, sind das irgendwie 15 und das hat irgendwie dann doch vier.

So, die fundamentale Frage ist ja, w√ºrde ich ein Monorepo einf√ºhren?

Das erleuchtet halt die Kommitts √ºber Grenzen von Services hinweg.

Ist das eher ein Feature oder eher ein Bug?

Ich finde das schwer zu beantworten.

Was wir hier auf jeden Fall sehen, ist, dass es halt zu einem Risiko im Deployment f√ºhrt.

Also ja, ich kann jetzt eine Library √§ndern, aber diese √Ñnderung geht irgendwie quer √ºber alles.

Und ich muss das sozusagen sp√§ter beim Deployment irgendwie im Kompromiss eingehen, damit eben nicht die √Ñnderung wirklich √ºber alles geht.

Sondern ich will dann irgendwie erst mal Kohorten deployen und ich bin mir nicht sicher, ob es nicht dann besser w√§re, das irgendwie aufzuteilen, aber das m√ºsste man halt schauen.

Und die andere Frage, die sich halt stellt, ist, warum benutzt man denn √ºberhaupt Microservices?

Die sollten ja eigentlich unabh√§ngig sein und insbesondere das unabh√§ngige Deployment ist halt f√ºr mich ein wesentliches Feature.

Ich will also einem Team die M√∂glichkeit geben, ein Microservice zu deployen, m√∂glichst ohne Koordination.

So, und da fehlt mir die Frage, sind die eigentlich unabh√§ngig?

Und ich finde die Frage schwer zu beantworten.

Was wir wissen ist, dass die Mehrheit der Commits nur ein Microservice erwischt.

Aber wir haben also eine Grauzone, also zwischen 95 Prozent der √Ñnderungen beeinflussen 14 oder weniger Microservices, 50 Prozent ein Microservice.

Das hei√üt also zwischen 50 Prozent der Commits und 95 Prozent der Commits w√§chst die Anzahl der beeinflussten Microservices von 1 auf 14.

So, und da w√§re jetzt die Frage, also da f√§nde ich halt interessant zu sehen, wie genau dort die Aufteilung ist.

Wenn also, sagen wir mal, 90 Prozent der Commits immer noch nur zwei Microservices erwischen und 80 Prozent nur ein Microservice, dann w√ºrde ich sagen, sind die Microservices im Wesentlichen unabh√§ngig und es ist vielleicht auf so einer Architekturebene gar nicht so schlimm, weil die halt im Wesentlichen unabh√§ngig sind.

Aber wenn das halt irgendwie anders aussieht, wenn also 60 Prozent der √Ñnderungen schon f√ºnf Microservices erwischen, dann habe ich vielleicht ein anderes Thema.

So, da ist noch eine Meldung von dem Uwe Bartelt, der schreibt, wenn es √ñkosystem ohne vern√ºnftige Dependency Management Systeme gibt, kann ein Monorepro interessant sein.

Ich stelle mir aber auch die Frage, ob dann der Service-Schnitt stimmt.

Ja, also nicht ein vern√ºnftiges Dependency Management kann ja Monorepro sein.

So, was bedeutet das jetzt?

Also, das war eine von den Fragen, die wir vorhin hier irgendwie rumgesp√ºrt sind.

Das war die von dem Tobi Van Kenobi.

Ab welcher Metrikgr√∂√üe Devs w√ºrde man √ºberhaupt beginnen, √ºber Monorepros nachzudenken, beziehungsweise umgekehrt, waren Monorepros √ºberhaupt nicht sinnvoll?

### Wann sind Monorepos sinnvoll?

Das finde ich ist eine spannende Frage und ist auch ein bisschen der Motivator.

Ich w√ºrde behaupten, Monorepros helfen halt dabei, in einer bestimmten Art und Weise zu arbeiten.

Also, ich kann dadurch eben querschnittliche √Ñnderungen machen, durch beispielsweise irgendwelche Einheiten, die halt tief unten im Dependency grafen sind.

Aber genau genommen k√∂nnte ich das ja auch anders machen.

### Alternative Ans√§tze

Also, wenn ich mir Android angucke, die halt offensichtlich kein Monorepro fahren, dann kann ich dort eben das trotzdem √Ñnderungen machen, die mehrere Teile umfasst, indem ich eben mehrere Repositories auschecke und die halt irgendwie √§ndere.

Daf√ºr brauche ich auch Tooling und Ma√ünahmen, wie f√ºr Monorepros halt auch, so dass irgendwie dieser Unterschied zunehmend f√ºr mich verschwimmt, ob das nun wirklich, wirklich notwendig ist.

Aber irgendwas brauche ich halt f√ºr solche √ºbergreifenden √Ñnderungen.

## Fazit und Empfehlungen

Ich pers√∂nlich w√ºrde halt diese Code Ownership beibehalten, so wie Google sie f√§hrt.

Ich w√ºrde also sagen, irgendjemand reviewt halt alle √Ñnderungen, auch wenn es halt √Ñnderungen sind von irgendwelchen Basis-Libraries, die halt eigentlich nur problemlos sein sollten.

Aber dass da halt jemand noch mal drauf guckt, der die Komponente sozusagen kennt, jemand, der verantwortlich ist f√ºr die Komponente, finde ich grunds√§tzlich gut.

Deswegen hat mir halt dieser Ansatz von MitRosy eher geholfen oder ich den eher gut finde.

Und dann brauche ich das Tooling von Uber halt nicht, weil irgendjemand guckt sich das halt an, irgendjemand √ºbernimmt die √Ñnderung, dann wird es halt deployed und ich muss nicht das sozusagen generalstabsm√§√üig √ºber alle durchdeployen.

Und da kann ich jetzt irgendwie auch eine Abw√§gung treffen und kann halt sagen, okay, es ist jetzt erstmal wichtiger, dass wir dieses Feature schippen und das halt irgendwie rausbekommen und dann k√ºmmere ich mich sp√§ter um diesen √Ñnderungen, wo halt irgendwie die RPC-Library ge√§ndert werden muss.

Und dann ist das mit dem Sperren nicht so problematisch.

Aber es ist irgendwie schwierig zu verstehen, was genau Uber's Problem ist und was die Alternativen sind, die sie zu diesem Ansatz untersucht haben und was die Nachteile sind.

Uber's Paper ist da deutlich besser, hat aber auch einen anderen Scope.

Da geht es nicht um Tooling, sondern um das grunds√§tzliche Vorgehen.

Ich gucke nochmal, was also insgesamt bedeutet, dass eigentlich dahinter so ein bisschen die Frage steht, wie gehe ich mit √ºbergreifenden √Ñnderungen um und wie arbeite ich damit?

Und offensichtlich ist eben eine M√∂glichkeit, ein Monorepo zu bauen, aber es ist auf jeden Fall nicht die einzige M√∂glichkeit, die ich da habe.

Gut, schaue ich nochmal, ob es irgendwo Fragen gab, zus√§tzliche Fragen, das scheint nicht der Fall zu sein.

Dann w√ºrde ich sagen, erstmal vielen Dank soweit f√ºr die Diskussion, f√ºr die Fragen, sch√∂nes Wochenende.

Und wir m√ºssen mal sehen, was wir n√§chste Woche machen.

Der dritte ist ja mit dem Feiertag, glaube ich nicht, dass wir am dritten direkt was machen werden, aber eigentlich sollte es schon eine Episode geben, w√ºrde ich mich nochmal sozusagen melden, guckt auf die Webseite und genie√üt das Wochenende und bis dahin, vielen Dank.
