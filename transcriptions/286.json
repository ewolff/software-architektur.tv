{
  "text": "Vielleicht erinnerst du dich noch, dann habe ich gesagt, ja gut, dann lassen wir es bleiben, weil mehr Aufwand wollte ich nicht erzeugen. Und das finde ich so spannend, weil momentan sucht jeder in der Wirtschaft nach dem KI-Use-Case, mit dem man Einsparungen erzielen kann. Und hier war das für mich so wichtig, weil ja, wir wollten ja eigentlich nicht mehr Aufwand haben. Aber du hast dann gesagt, dass der Mehrwert so hoch ist, dass es sich eben lohnt, diesen Mehraufwand reinzusetzen. Das heißt, die KI hat uns hier enabled, etwas zu erzeugen, was wir vorher nicht konnten. Und es hat diesen Wert, dass wir bereit sind, den Mehraufwand für das Laufenlassen, den Review und sowas zu investieren. Und das fand ich für mich auch nochmal ein Schlüsselmoment, weil es muss nicht unbedingt irgendwie sein, dass ich das gegenüber dem Status Quo was spare, sondern dieses Enablement durch die KI finde ich wichtig. Ja, also zwei Sachen dazu. Die eine Sache ist halt das Reviewen dieser Episoden. Also wahrscheinlich kann man das auch an den GitHub-Historien sich anschauen. Das sind halt eine Zusammenfassung und Stichworte. Fünf Minuten, zehn Minuten, das ist halt kein echter Aufwand. Dann kommt halt das Transkript dazu. Das ist halt diese Geschichte mit dem Anfang abschneiden. Das ist halt das MP3 und das MP3 hat ein Intro. Da haben wir auch nur zwei Runden drüber gedreht, wo erst die Idee war, das Intro abzuschneiden in der Pipeline. Wo wir dann gesagt haben, wir lassen das halt drin und benotieren es halt raus. Und dann halt irgendwie noch in dem Transkript Sachen ändern, falls da Dinge sind, die aufgefallen sind und das halt irgendwie zu lösen. So, das heißt der Aufwand ist gering. Und der Grund, warum ich das… Also die Transkripte sind einfach für Gehörlose, glaube ich, offensichtlich wichtig. Die brauchen halt sowas. Und deswegen fand ich das eben wichtig, sowas auf die Reihe zu bekommen. Und wir hätten es sonst nicht hinbekommen. Ich hatte ja das Vergnügen, aus der Nähe zu sehen, wie der InnoCube-Podcast entsteht. Da sind halt Menschen gewesen, die das Transkriptieren. Dann ist halt noch jemand drüber gegangen und hat es irgendwie nacheditiert, weil die übrigens auch ähnliche Schwierigkeiten haben. Also wenn da Begriffe sind, die die Menschen nicht kennen, dann ist das halt ein Problem und das muss im Manuell nachgearbeitet werden. Und den Aufwand können wir halt nicht leisten. Niemand von uns kann sich hinsetzen und das Ding anhören und darunter schreiben. Und wir können eigentlich auch nicht ernsthaft jemanden dafür bezahlen, weil wir im Prinzip kein Budget haben. Und deswegen ist das die einzige Möglichkeit, Transkripte zu bekommen. Und das ist eben auch tatsächlich wichtig. Und bei den Zusammenfassungen habe ich eben selber gemerkt, dass die… Also ich empfinde sie zumindest als nützlich, weil ich da nochmal eine Idee davon habe, was in der Episode passiert ist. Und deswegen fand ich das halt gut. Und das ist ja auch vielleicht das Allgemeine. Ich habe darüber auch bei Heise mal so einen Artikel geschrieben, eben auch in Bezug auf AI. Wenn wir jetzt also mehr Produktivität haben, was auch immer das bedeutet. Also wir haben jetzt ja die Möglichkeit, mit wenig Aufwand eben diese Sachen zu erzeugen mit AI. Und dadurch sind wir halt in dem Bereich produktiver. Also wenn ich jetzt die Zusammenfassung schreiben würde oder die Stichworte, wenn wir die schreiben würden, das würde halt viel länger dauern. Und dadurch machen wir halt mehr. Und da gibt es einen Begriff für Rebound-Effekt. Ich glaube, so heißt der. Rebound. Genau. Wo du halt sagst, wenn etwas billiger wird, einfacher wird, dann wird es irgendwie für neue Sachen genutzt und halt mehr genutzt. Und wenn also Sachen eben stromeffizienter werden, dann nutzt man halt mehr davon. Und das ist glaube ich das, was wir hier beobachten in gewisser Weise. Und das ist auch das, was ich in diesem heißen Blogpost geschrieben habe, weil ich eben nicht erwarten würde, dass wenn wir tatsächlich höher Produktivität durch AI haben. Das ist meiner Ansicht nach eine offene Frage, aber das ist eine andere Diskussion. Dann ist die nächste Frage, ob das durchschlägt zu weniger Menschen, die halt arbeiten. Mindestens in der Softwareentwicklung ist es so, dass wir hoffentlich, glaube ich, die ganze Zeit Produktivitätsvorteile haben. Und ich würde behaupten, jetzt oder vor einiger Zeit, also bevor dieser Downturn bei uns war in unserer Branche, zu dem Zeitpunkt gab es halt die meisten EntwicklerInnen. Trotzdem, dass die viel produktiver sind als die vor zehn Jahren oder so. Und deswegen bin ich halt sehr unsicher, ob wir dann tatsächlich in einer sozusagen Massenarbeitslosigkeit haben werden. Ich würde eher erwarten, dass wir halt andere Sachen dann noch produzieren. Hier haben wir im Prinzip genau das, wie du gerade sagtest, als kleines Beispiel. Ja, definitiv. Bei mir schwingt das Pendel hin und her. Und ich sehe, dass manche Sachen die KI gut machen kann. Sie kann mich enablen, sie kann mich unterstützen. Und andere Sachen, die klappen irgendwie ganz schlecht. Also die Transcriptions, die lagen eine Weile dann so rum, weil das Problem war, ja, ich hatte das Skript, aber ich musste dann immer es laufen lassen. Er hat irgendwie acht bis zehn Minuten gebraucht. Dann musste ich ein Pull-Request erstellen und habe mir gedacht, eigentlich müsste ich es automatisieren. Und das war dann nämlich so der nächste Aha-Effekt, weil wir kennen das ja mit den Pipelines, die dann irgendwie nur auf dem Server laufen, die man irgendwie schlecht lokal ausprobieren kann und so. Und ich glaube, ich hatte da einen gemischten Ansatz, dass ich einmal den GitHub Copilot mit Issues gefüttert habe, direkt auf GitHub.com und einmal, dass ich es lokal gemacht habe und meiner KI Zugriff über das Command-Line-Tool GH auf die Workflows gegeben habe, dass es eben gucken konnte, wie sie laufen. Und ehrlich gesagt, das war eine Katastrophe. Da hat er sich total schwer getan. Ich habe auch das Gefühl, ich habe den Code nicht reviewed. Das lief alles irgendwie nebenbei. Mach mal und teste mal und schau mal, ob es funktioniert. Und ich habe die Befürchtung, dass er viel Code dupliziert hat. Also er hat angefangen. Es war irgendwie so eine Zeit, wo die Modelle sehr gern Back-Files angelegt haben, Backup-Files oder neue Versionen von den Files, obwohl ich immer gesagt habe, du bist hier unter Versionskontrolle, musst du nicht machen. Und er hatte eben auch Probleme mit den Berechtigungen, dass er halt von meinem Repository in das Software-Architektur im Stream Repository das Ganze pushen kann. Und da war ich sehr froh, als es dann irgendwann lief. Aber es kam mir fragil vor. Das Code weiß nicht. Also die Code-Basis ist da, glaube ich, nicht mehr so gut. Läuft, aber fragil. Und da bin ich dann noch zu einem Moment gekommen, wo ich gemerkt habe, geht so gar nicht. Weil irgendwann, ich wollte die Transkription anschmeißen und er lief auf einen Berechtigungsfehler und nichts ging. Ich habe die KI angeschmissen, habe gesagt, guck mal da Fehler, fix das. Ich war mir nicht so sicher, ob ich da noch irgendwie mit der KI was geändert hatte, das dadurch broken war. Und die KI so, ach ja, klar, warum hast du hier ein Personal Access Token? Können wir doch rausschmeißen, weil du hast ja hier Berechtigung in der Action und hat das rausgeschmissen. Und dann haben wir es ausprobiert und dann ist weiter unten ein Berechtigungsfehler passiert. Und da ist mir auf einmal ein Licht aufgegangen. Er wollte dann schon anfangen und weiter unten den Berechtigungsfehler auch beheben. Aber das eigentliche Problem war ein ganz anderes, denn ich hatte ein temporäres Personal Access Token gesetzt und diese Berechtigung war ausgelaufen. Das hätte er eigentlich erkennen müssen. Aber er hat eben da rumgefummelt am Code, hat nicht den eigentlichen Grund gefunden und hätte den Code jetzt komplett umgeschmissen und sich in komischen Sachen verrannt. Das Ding ist, im ersten Stück reichen die normalen Berechtigungen aus, Open Source Repository Pullen. Im zweiten Bereich braucht es Personal Access Token, um zu pushen oder den Pull Request zu stellen. Und das war ihm nicht aufgefallen. Und da habe ich gemerkt, dass er eben in diesem relativ kleinen Stück Code anscheinend den Überblick verloren hat, das mentale Modell über seinen eigenen Code nicht mehr bewahrt hat und deswegen das nicht editieren oder fixen konnte. Genau, also nochmal ein Reminder. Nicht die Dinger sind Textgeneratoren, sodass sie auf Basis von dem, was sie reinbekommen haben, einen Text generieren. Ich würde behaupten, es gibt da kein mentales Modell, aber nicht die Type. Das andere ist, ich finde, das ist halt ein, also wie soll ich sagen, wir kommen ja noch dazu, wie wir es auf die Webseite tun und was ich mittlerweile bei mir, wenn ich halt irgendwelche Sachen mache, wie zum Beispiel jetzt die Webseite editieren, was mir da zugutekommt, ist, dass ich mit Chitchipiti die Möglichkeit habe, irgendwie zu sagen, okay, sagt mir mal, wie ich dieses oder jenes auf die Reihe bekomme. Und da kommen halt gute Vorschläge. Das bedeutet aber, dass ich halt im Prinzip mir Webrecherche nur spare. Das, was du ja beschreibst, ist eigentlich, mach mal und implementier mal. Und das, was du jetzt gerade beschreibst, ist ein gutes Beispiel, um es platt zu sagen, das funktioniert eigentlich nicht. Denn was an irgendeiner Stelle dann immer wieder passiert, ist genau das, was wir jetzt hier gerade sehen. Du fängst irgendwie an und sagst, naja, das funktioniert ja so und so und so. Und da sind folgende Themen, also ein Security-Thema an dieser Stelle, das müssen wir irgendwie fixen. Und dann musst du halt in diese Abstraktion reingreifen. Du bist jetzt irgendwie nicht mehr auf dieser Ebene, dass du sagst, mach mal und löse mal. Mir egal. Ich will nicht verstehen, wie es funktioniert, sondern du musst es ja mal verstehen. Es gibt halt GitHub. GitHub hat irgendwelche Security-Tokens. Da gibt es offensichtlich Lokale und andere. Und irgendwie ist da etwas schief. Und deswegen muss ich das halt fixen. Was halt bedeutet, ich komme eben nicht auf die höhere Abstraktionsschicht, sondern an bestimmten Stellen bricht es eben und ich muss irgendwie reingreifen. Das beobachte ich an extrem vielen Stellen, was eben dazu führt, dass ich mir überhaupt gar nicht vorstellen kann, wie nicht EntwicklerInnen damit irgendetwas auf die Reihe bekommen sollen. Weil an der Stelle werden sie halt gescheitert. Und dann ist halt Schluss. Weil eben solche Menschen dann da nicht reingreifen können und sagen können, ach so, ja klar, das funktioniert ja folgendermaßen. Eben nicht Security-Token. So funktioniert die Security. Das ist halt GitHub. Sondern die können das dann eben auf der Ebene nicht mehr verfolgen und dann ist eben Schluss. Absolut. Und du hattest gerade eben schon gesagt, ja das Modell kann ja kein mentales Modell aufbauen. Und wir hatten vorher die Vermenschlichung. Aber ich habe mich jetzt seit ein paar Wochen mit dem mentalen Modell nach Peter Nauer beschäftigt. 85 war das, glaube ich, als er das so beschrieben hat. Und das Witzige ist, also das mentale Modell beschreibt halt, was man als Entwickler so aufbaut beim Programmieren, um eben auch die Frage nach dem Warum im Code beantworten zu können. Warum wird da ein Personal Access-Token und nicht einfach der Access-Key, den man in der Action hat, verwendet, zum Beispiel. Und das Faszinierende ist, dass die Modelle dieses Konzept, mentales Modell nach Nauer kennen und dann wissen, was sie zu tun haben, also was es bedeutet. Und die ganzen Tool-Hersteller haben schon angefangen. Man kennt das ja, diese Cloud.md oder Agent.md-Files, die immer im Root liegen, wo die KI mal über das Repository rübergegangen ist und zumindest sich wichtige Sachen wie Technology-Stack und die File-Struktur und sowas rausgeschrieben hat. Gehört auch zum mentalen Modell. Ich behaupte, dass wenn man eben auch so ein bisschen an dieses Warum geht, wenn man jetzt eben zum Beispiel da das hinterlassen würde, wir benutzen hier Personal Access-Token, weil erster Weg ohne Personal Access-Token hat nicht funktioniert. Deswegen, wenn man das hinterlässt, dann könnte die KI da besser werden. Und das zeigt eben auch, dass, ja, du sagst ja selbst, wenn man jetzt das Programmieren noch nicht gewöhnt ist, nicht jahrelang trainiert hat, dann fallen einem diese Sachen nicht auf. Und dann ist die Frage, ob man es schafft, das Modell richtig zu besprechen, also die Prompts richtig zu wählen. Und das macht einen großen Unterschied aus. Genau, also vielleicht noch zwei Worte dazu. Es ist halt ganz spannend, weil der Sebastian Hans hat mich vor fünf Tagen, so sagt Mastodon, auf dieses Paper hingewiesen, Programming as Theory Building. Und ich hatte mir irgendwie sozusagen vorgenommen, das nochmal genauer durchzulesen, weil ich halt vermutet hatte, dass das halt im Prinzip bedeutet, dass man eben als Menschenteam ein gemeinsames mentales Modell entwickelt und dass sich im Code das eben nur ausdrückt. Ich hatte die ersten Seiten überflogen und da war so eine Geschichte, von wegen irgendein Team hat einen Compiler gebaut oder irgendwas. Dann hat ein anderes Team versucht, den zu erweitern. Und daraufhin hat man das dem ursprünglichen Team gegeben und hat gesagt, das ursprüngliche Team, was den Compiler ursprünglich gebaut hat, hat gesagt, das ist ein netter Versuch, aber das zerstört die ganze Struktur des Systems. Und hier ist ein viel einfacherer Weg. Und das hängt eben damit zusammen, dass das ursprüngliche Team diese Theorie verstanden hat und das neue Team nicht. So jedenfalls meine Wahrnehmung. Das, was du beschreibst, bedeutet ja nur, dass man in den Texten etwas hinschreibt. Das ist kein mentales Modell. Und das ist was anderes. Darüber muss man offensichtlich, weil das wäre dann sozusagen die nächste Episode, die man nochmal planen könnte und machen könnte. Absolut. Der Begriff mentales Modell ist da vielleicht auch ein bisschen schwierig. Aber ich habe halt gemerkt, da ist was dran an diesem mentalen Modell. Und wie du ja gesagt hast, die KI nimmt Text und produziert Text. Das heißt, man muss irgendwo, wenn man es schaffen will, im Text abbilden. Aber das Faszinierende daran ist auch, wenn man dieses Konzept betrachtet und jetzt zum Beispiel Legacy Modernisation machen möchte mit der KI und sagt, warum ist dieser Code eigentlich Legacy? Weil die Entwickler sagen alle, muss neu geschrieben werden. Warum muss er neu geschrieben werden? Weil das mentale Modell fehlt, weil das Warum fehlt. Und wenn ich dann mit der KI es umschreiben lasse, in eine moderne Sprache, habe ich dann das mentale Modell. Kann dann der Entwickler weiterarbeiten? Das finde ich so faszinierend. Eine interessante Erkenntnis. Meiner Ansicht nach ist das Grundproblem dabei, dass man an der Stelle nicht wahrhaben will, dass Schriftverentwicklung ein sozialer Prozess ist. Und das ist das, was ich glaubte, was man aus diesem Paper, was ich eben nicht gelesen habe, vielleicht rauslesen kann. Dass dieses soziale Modell im Code Ausdruck findet. Und das ist dann halt trivial, wenn ich eine AI ansetze, dass das dieses mentale Modell und den sozialen Prozess nicht abbildet. Und dann ist halt Schluss. Das ist eine Vielkonzeption. Und das ist eine von den Sachen, die mich so ärgert. In diesem AI-Bereich ist eine Vielkonzeption über Softwareentwicklung. Softwareentwicklung ist ein sozialer Prozess. Und die fundamentalen Schwierigkeiten sind meiner Ansicht nach sozial. Und das wird nicht dadurch besser, dass ich eine Maschine da reinsetze. Aber anderes Thema. Wir sollten offensichtlich noch eine Episode mindestens planen. Wollen wir sprechen, wie du es auf die Webseite bekommen hast? Ich würde ganz gern jetzt schon mal auf die andere Idee eingehen. Wegen dem sozialen Prozess. Weil ich das so faszinierend fand. Ich kam dann irgendwie auf die Idee, dass man ja auch mal die Folgen nach Gast sortieren könnte und auf die Seite bringen könnte. War irgendwie so eine Idee. Könnte man doch mal machen. Und die Idee dabei war halt, also ich finde diese verschiedenen Ebenen, wie man die KI verwendet. Und wir haben jetzt zum Beispiel bei dem Transkriptionsprozess habe ich mit KI gekodet. Wir haben mit KI machen wir ein Review. Und in dem Prozess, die Zusammenfassung, läuft ja selbst auch mit KI. Müssen wir gleich auch dran denken. Da gibt es ja jetzt ein aktuelles Problem, was wir haben. Und so habe ich dann eben auch gedacht, Mensch, wir haben jetzt diese ganzen 180 Folgen. Wow, da hat sich was angesammelt. Und da ist überall irgendwo unstrukturiert der Gast mit genannt. Und das könnte man ja jetzt mit der KI rausziehen. Und ich habe einfach mal die KI drauf angesetzt. Habe gesagt, guck mal, hier ist das Repository und mach dir mal Gedanken, wie könnte man das rausziehen. Iterier mal drüber. Da war es schon mal faszinierend, weil ich gesagt habe, du KI kannst du bitte drüber iterieren. Und die KI hat gesagt, ich mache mir mal einen Plan. Und der Plan sind eigentlich maximal fünf Schritte bei Claude. Das heißt, erster Schritt, erste Episode, Gast rausziehen. Zweite Episode, Gast rausziehen. Dritte Episode, Gast rausziehen. Vierte Episode, Gast rausziehen. Aus den anderen Episoden, Gast rausziehen. Und genau so hat das Modell dann gearbeitet. Mit den ersten vier Episoden ist gut klargekommen. Und dann ist es abgedriftet und konnte nicht mehr iterieren. Da habe ich dann gemerkt, es könnte ja ein Programm schreiben, ein Skript, was iteriert. Und dann in dem Skript habe ich aber die KI wieder nicht mehr zur Verfügung, um das zu extrahieren. Das war so ein Ding, was ich dann da angegangen bin. Aber viel faszinierender fand ich es, als das Ganze irgendwo stand, mehr oder weniger fertig war. Ja, da waren viele Fehler drin. Aber du dann drüber geguckt hast und ich irgendwie gemerkt habe, dass du mir freundlich sagen wolltest, Ralf, das was da rausgekommen ist, das ist totaler Käse, weil das passt überhaupt nicht zur bestehenden Architektur. War so, richtig? Also genau, soll ich kurz sozusagen ausholen. Also die eine Sache war halt, wir haben ja eine Jacke-basierte Webseite. Das heißt, wir haben im Prinzip Markdown-Files, die hat irgendwie gerendert werden mit Ruby-Skripten. Und das macht eben, in Produktion macht das halt GitHub-Pages. Und im Prinzip ist das ja ein CMS. Das heißt, ich habe Content, der ist als MD-Files da und dann wird er irgendwie gerendert. Und was jetzt irgendwie rausgekommen ist, in deinem Fall war eine MD-Seite, die aber in Wirklichkeit HTML hatte. Also in Markdown kann ich HTML einbetten und JavaScript-Code. Und wo dann im Prinzip alle Gäste rausgesammelt wurden aus, ich weiß nicht, irgendeiner Datenquelle, YAML-File oder so. Und wenn ich dann etwas gesucht habe, hat das halt der JavaScript-Code auf dem Client gemacht. Das ist wahrscheinlich die einzige Möglichkeit, wie man das hinbekommen kann. Also meine, wie soll ich sagen, meine Intuition wäre halt, ich will eigentlich irgendwo einen Server haben, der halt sucht. Das können wir nicht, weil wir GitHub-Pages nur haben. Und deswegen ist das wahrscheinlich der einzige Weg. Was sich da in meiner Erinnerung aber so gezeigt hat, ist, dass das so aufgepfropft wird. Also wir haben CSS-Files, wie sich das gehört. Und ich würde jetzt erwarten, dass diese CSS-Sachen wiederverwendet werden. Werden halt nicht wiederverwendet, da ist halt irgendwie eigener Kram drin. Solche Sachen halt nicht. Es ist halt was Eigenes, was halt getrennt ist von dem Rest, zum Beispiel in Bezug auf CSS oder nicht eigene JavaScript-Dateien oder so. Und das andere ist halt, also das hat mich dazu gerade sozusagen angestiftet, das sozusagen deutlich zu sagen.",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 1315.41,
      "end": 1320.8500000572205,
      "text": " Vielleicht erinnerst du dich noch, dann habe ich gesagt, ja gut, dann lassen wir es bleiben,",
      "tokens": [
        50364,
        29838,
        1189,
        19166,
        372,
        1581,
        10390,
        3514,
        11,
        3594,
        6015,
        1893,
        12260,
        11,
        2784,
        5228,
        11,
        3594,
        16168,
        1987,
        785,
        24912,
        11,
        50636
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25992056727409363,
      "compression_ratio": 1.6118143796920776,
      "no_speech_prob": 0.9101332426071167
    },
    {
      "id": 1,
      "seek": 0,
      "start": 1320.8500000572205,
      "end": 1327.5900003051759,
      "text": " weil mehr Aufwand wollte ich nicht erzeugen. Und das finde ich so spannend, weil momentan",
      "tokens": [
        50636,
        7689,
        5417,
        9462,
        33114,
        24509,
        1893,
        1979,
        1189,
        19303,
        268,
        13,
        2719,
        1482,
        17841,
        1893,
        370,
        49027,
        11,
        7689,
        1623,
        282,
        50973
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25992056727409363,
      "compression_ratio": 1.6118143796920776,
      "no_speech_prob": 0.9101332426071167
    },
    {
      "id": 2,
      "seek": 0,
      "start": 1327.5900003051759,
      "end": 1334.6100007629395,
      "text": " sucht jeder in der Wirtschaft nach dem KI-Use-Case, mit dem man Einsparungen erzielen kann. Und hier",
      "tokens": [
        50973,
        1270,
        83,
        19610,
        294,
        1163,
        29412,
        5168,
        1371,
        47261,
        12,
        52,
        405,
        12,
        34,
        651,
        11,
        2194,
        1371,
        587,
        22790,
        2181,
        5084,
        1189,
        89,
        12844,
        4028,
        13,
        2719,
        3296,
        51324
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25992056727409363,
      "compression_ratio": 1.6118143796920776,
      "no_speech_prob": 0.9101332426071167
    },
    {
      "id": 3,
      "seek": 0,
      "start": 1334.6100007629395,
      "end": 1342.2899991607667,
      "text": " war das für mich so wichtig, weil ja, wir wollten ja eigentlich nicht mehr Aufwand haben. Aber du",
      "tokens": [
        51324,
        1516,
        1482,
        2959,
        6031,
        370,
        13621,
        11,
        7689,
        2784,
        11,
        1987,
        46019,
        2784,
        10926,
        1979,
        5417,
        9462,
        33114,
        3084,
        13,
        5992,
        1581,
        51708
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25992056727409363,
      "compression_ratio": 1.6118143796920776,
      "no_speech_prob": 0.9101332426071167
    },
    {
      "id": 4,
      "seek": 2688,
      "start": 1342.2899991607667,
      "end": 1348.1699983215333,
      "text": " hast dann gesagt, dass der Mehrwert so hoch ist, dass es sich eben lohnt, diesen Mehraufwand",
      "tokens": [
        50364,
        6581,
        3594,
        12260,
        11,
        2658,
        1163,
        30782,
        26521,
        370,
        19783,
        1418,
        11,
        2658,
        785,
        3041,
        11375,
        46957,
        580,
        11,
        12862,
        30782,
        9507,
        33114,
        50658
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24825184047222137,
      "compression_ratio": 1.5833333730697632,
      "no_speech_prob": 0.028412505984306335
    },
    {
      "id": 5,
      "seek": 2688,
      "start": 1348.1699983215333,
      "end": 1354.7299996948243,
      "text": " reinzusetzen. Das heißt, die KI hat uns hier enabled, etwas zu erzeugen, was wir vorher nicht",
      "tokens": [
        50658,
        6561,
        16236,
        24797,
        13,
        2846,
        13139,
        11,
        978,
        47261,
        2385,
        2693,
        3296,
        15172,
        11,
        9569,
        2164,
        1189,
        19303,
        268,
        11,
        390,
        1987,
        29195,
        1979,
        50986
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24825184047222137,
      "compression_ratio": 1.5833333730697632,
      "no_speech_prob": 0.028412505984306335
    },
    {
      "id": 6,
      "seek": 2688,
      "start": 1354.7299996948243,
      "end": 1361.9300004577638,
      "text": " konnten. Und es hat diesen Wert, dass wir bereit sind, den Mehraufwand für das Laufenlassen,",
      "tokens": [
        50986,
        38216,
        13,
        2719,
        785,
        2385,
        12862,
        37205,
        11,
        2658,
        1987,
        38758,
        3290,
        11,
        1441,
        29337,
        424,
        2947,
        33114,
        2959,
        1482,
        441,
        20748,
        44898,
        11,
        51346
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24825184047222137,
      "compression_ratio": 1.5833333730697632,
      "no_speech_prob": 0.028412505984306335
    },
    {
      "id": 7,
      "seek": 2688,
      "start": 1361.9300004577638,
      "end": 1367.0499993896485,
      "text": " den Review und sowas zu investieren. Und das fand ich für mich auch nochmal ein Schlüsselmoment,",
      "tokens": [
        51346,
        1441,
        19954,
        674,
        19766,
        296,
        2164,
        1963,
        5695,
        13,
        2719,
        1482,
        38138,
        1893,
        2959,
        6031,
        2168,
        26509,
        1343,
        16420,
        37838,
        42544,
        317,
        11,
        51602
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24825184047222137,
      "compression_ratio": 1.5833333730697632,
      "no_speech_prob": 0.028412505984306335
    },
    {
      "id": 8,
      "seek": 5164,
      "start": 1367.0499993896485,
      "end": 1373.2099992370606,
      "text": " weil es muss nicht unbedingt irgendwie sein, dass ich das gegenüber dem Status Quo was spare,",
      "tokens": [
        50364,
        7689,
        785,
        6425,
        1979,
        41211,
        20759,
        6195,
        11,
        2658,
        1893,
        1482,
        41830,
        1371,
        47409,
        2326,
        78,
        390,
        13798,
        11,
        50672
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3279867172241211,
      "compression_ratio": 1.563829779624939,
      "no_speech_prob": 0.050282612442970276
    },
    {
      "id": 9,
      "seek": 5164,
      "start": 1373.2099992370606,
      "end": 1376.689998779297,
      "text": " sondern dieses Enablement durch die KI finde ich wichtig.",
      "tokens": [
        50672,
        11465,
        12113,
        2193,
        712,
        518,
        7131,
        978,
        47261,
        17841,
        1893,
        13621,
        13,
        50846
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3279867172241211,
      "compression_ratio": 1.563829779624939,
      "no_speech_prob": 0.050282612442970276
    },
    {
      "id": 10,
      "seek": 5164,
      "start": 1376.689998779297,
      "end": 1383.41,
      "text": " Ja, also zwei Sachen dazu. Die eine Sache ist halt das Reviewen dieser Episoden. Also wahrscheinlich",
      "tokens": [
        50846,
        3530,
        11,
        611,
        12002,
        26074,
        13034,
        13,
        3229,
        3018,
        31452,
        1418,
        12479,
        1482,
        19954,
        268,
        9053,
        9970,
        271,
        33482,
        13,
        2743,
        30957,
        51182
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3279867172241211,
      "compression_ratio": 1.563829779624939,
      "no_speech_prob": 0.050282612442970276
    },
    {
      "id": 11,
      "seek": 5164,
      "start": 1383.41,
      "end": 1388.7299996948243,
      "text": " kann man das auch an den GitHub-Historien sich anschauen. Das sind halt eine Zusammenfassung",
      "tokens": [
        51182,
        4028,
        587,
        1482,
        2168,
        364,
        1441,
        23331,
        12,
        39,
        468,
        7386,
        268,
        3041,
        31508,
        11715,
        13,
        2846,
        3290,
        12479,
        3018,
        29442,
        69,
        40828,
        51448
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3279867172241211,
      "compression_ratio": 1.563829779624939,
      "no_speech_prob": 0.050282612442970276
    },
    {
      "id": 12,
      "seek": 5164,
      "start": 1388.7299996948243,
      "end": 1394.3299981689454,
      "text": " und Stichworte. Fünf Minuten, zehn Minuten, das ist halt kein echter Aufwand. Dann kommt halt",
      "tokens": [
        51448,
        674,
        745,
        480,
        86,
        12752,
        13,
        479,
        26292,
        27593,
        11,
        33975,
        27593,
        11,
        1482,
        1418,
        12479,
        13424,
        308,
        26690,
        9462,
        33114,
        13,
        7455,
        10047,
        12479,
        51728
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3279867172241211,
      "compression_ratio": 1.563829779624939,
      "no_speech_prob": 0.050282612442970276
    },
    {
      "id": 13,
      "seek": 7892,
      "start": 1394.6099969482423,
      "end": 1400.810001525879,
      "text": " das Transkript dazu. Das ist halt diese Geschichte mit dem Anfang abschneiden. Das ist halt das MP3",
      "tokens": [
        50378,
        1482,
        6531,
        74,
        470,
        662,
        13034,
        13,
        2846,
        1418,
        12479,
        6705,
        28896,
        2194,
        1371,
        25856,
        1950,
        339,
        716,
        4380,
        13,
        2846,
        1418,
        12479,
        1482,
        14146,
        18,
        50688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37960007786750793,
      "compression_ratio": 1.4974619150161743,
      "no_speech_prob": 0.00040447042556479573
    },
    {
      "id": 14,
      "seek": 7892,
      "start": 1400.810001525879,
      "end": 1410.0099984741212,
      "text": " und das MP3 hat ein Intro. Da haben wir auch nur zwei Runden drüber gedreht, wo erst die Idee war,",
      "tokens": [
        50688,
        674,
        1482,
        14146,
        18,
        2385,
        1343,
        47406,
        13,
        3933,
        3084,
        1987,
        2168,
        4343,
        12002,
        497,
        10028,
        1224,
        12670,
        19238,
        265,
        357,
        11,
        6020,
        11301,
        978,
        32651,
        1516,
        11,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37960007786750793,
      "compression_ratio": 1.4974619150161743,
      "no_speech_prob": 0.00040447042556479573
    },
    {
      "id": 15,
      "seek": 7892,
      "start": 1410.0099984741212,
      "end": 1416.7700006103516,
      "text": " das Intro abzuschneiden in der Pipeline. Wo wir dann gesagt haben, wir lassen das halt drin und",
      "tokens": [
        51148,
        1482,
        47406,
        410,
        16236,
        339,
        716,
        4380,
        294,
        1163,
        35396,
        5440,
        13,
        6622,
        1987,
        3594,
        12260,
        3084,
        11,
        1987,
        16168,
        1482,
        12479,
        24534,
        674,
        51486
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37960007786750793,
      "compression_ratio": 1.4974619150161743,
      "no_speech_prob": 0.00040447042556479573
    },
    {
      "id": 16,
      "seek": 10136,
      "start": 1416.7700006103516,
      "end": 1423.3299981689454,
      "text": " benotieren es halt raus. Und dann halt irgendwie noch in dem Transkript Sachen ändern, falls da",
      "tokens": [
        50364,
        3271,
        310,
        5695,
        785,
        12479,
        17202,
        13,
        2719,
        3594,
        12479,
        20759,
        3514,
        294,
        1371,
        6531,
        74,
        470,
        662,
        26074,
        47775,
        11,
        8804,
        1120,
        50692
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35506418347358704,
      "compression_ratio": 1.5440000295639038,
      "no_speech_prob": 0.3412027060985565
    },
    {
      "id": 17,
      "seek": 10136,
      "start": 1423.3299981689454,
      "end": 1429.289997253418,
      "text": " Dinge sind, die aufgefallen sind und das halt irgendwie zu lösen. So, das heißt der Aufwand",
      "tokens": [
        50692,
        25102,
        3290,
        11,
        978,
        35031,
        24425,
        3290,
        674,
        1482,
        12479,
        20759,
        2164,
        25209,
        6748,
        13,
        407,
        11,
        1482,
        13139,
        1163,
        9462,
        33114,
        50990
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35506418347358704,
      "compression_ratio": 1.5440000295639038,
      "no_speech_prob": 0.3412027060985565
    },
    {
      "id": 18,
      "seek": 10136,
      "start": 1429.289997253418,
      "end": 1435.5700036621095,
      "text": " ist gering. Und der Grund, warum ich das… Also die Transkripte sind einfach für Gehörlose,",
      "tokens": [
        50990,
        1418,
        290,
        1794,
        13,
        2719,
        1163,
        13941,
        11,
        24331,
        1893,
        1482,
        1260,
        2743,
        978,
        6531,
        74,
        470,
        662,
        68,
        3290,
        7281,
        2959,
        2876,
        71,
        2311,
        75,
        541,
        11,
        51304
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35506418347358704,
      "compression_ratio": 1.5440000295639038,
      "no_speech_prob": 0.3412027060985565
    },
    {
      "id": 19,
      "seek": 10136,
      "start": 1435.5700036621095,
      "end": 1443.210003051758,
      "text": " glaube ich, offensichtlich wichtig. Die brauchen halt sowas. Und deswegen fand ich das eben wichtig,",
      "tokens": [
        51304,
        13756,
        1893,
        11,
        766,
        694,
        41971,
        13621,
        13,
        3229,
        19543,
        12479,
        19766,
        296,
        13,
        2719,
        26482,
        38138,
        1893,
        1482,
        11375,
        13621,
        11,
        51686
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35506418347358704,
      "compression_ratio": 1.5440000295639038,
      "no_speech_prob": 0.3412027060985565
    },
    {
      "id": 20,
      "seek": 12780,
      "start": 1443.210003051758,
      "end": 1453.5299951171876,
      "text": " sowas auf die Reihe zu bekommen. Und wir hätten es sonst nicht hinbekommen. Ich hatte ja das",
      "tokens": [
        50364,
        19766,
        296,
        2501,
        978,
        34549,
        675,
        2164,
        19256,
        13,
        2719,
        1987,
        33278,
        785,
        26309,
        1979,
        14102,
        650,
        13675,
        13,
        3141,
        13299,
        2784,
        1482,
        50880
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31204530596733093,
      "compression_ratio": 1.5993376970291138,
      "no_speech_prob": 0.005467881448566914
    },
    {
      "id": 21,
      "seek": 12780,
      "start": 1453.5299951171876,
      "end": 1457.3700067138673,
      "text": " Vergnügen, aus der Nähe zu sehen, wie der InnoCube-Podcast entsteht. Da sind halt Menschen",
      "tokens": [
        50880,
        4281,
        4568,
        45336,
        11,
        3437,
        1163,
        32731,
        675,
        2164,
        11333,
        11,
        3355,
        1163,
        682,
        1771,
        34,
        1977,
        12,
        40742,
        3734,
        35955,
        357,
        13,
        3933,
        3290,
        12479,
        8397,
        51072
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31204530596733093,
      "compression_ratio": 1.5993376970291138,
      "no_speech_prob": 0.005467881448566914
    },
    {
      "id": 22,
      "seek": 12780,
      "start": 1457.3700067138673,
      "end": 1460.8099938964845,
      "text": " gewesen, die das Transkriptieren. Dann ist halt noch jemand drüber gegangen und hat es irgendwie",
      "tokens": [
        51072,
        27653,
        11,
        978,
        1482,
        6531,
        74,
        470,
        662,
        5695,
        13,
        7455,
        1418,
        12479,
        3514,
        21717,
        1224,
        12670,
        44415,
        674,
        2385,
        785,
        20759,
        51244
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31204530596733093,
      "compression_ratio": 1.5993376970291138,
      "no_speech_prob": 0.005467881448566914
    },
    {
      "id": 23,
      "seek": 12780,
      "start": 1460.8099938964845,
      "end": 1464.8899957275391,
      "text": " nacheditiert, weil die übrigens auch ähnliche Schwierigkeiten haben. Also wenn da Begriffe sind,",
      "tokens": [
        51244,
        5168,
        292,
        270,
        4859,
        11,
        7689,
        978,
        38215,
        2168,
        3078,
        12071,
        10185,
        17576,
        811,
        37545,
        3084,
        13,
        2743,
        4797,
        1120,
        879,
        861,
        31387,
        3290,
        11,
        51448
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31204530596733093,
      "compression_ratio": 1.5993376970291138,
      "no_speech_prob": 0.005467881448566914
    },
    {
      "id": 24,
      "seek": 12780,
      "start": 1464.8899957275391,
      "end": 1469.8899957275391,
      "text": " die die Menschen nicht kennen, dann ist das halt ein Problem und das muss im Manuell nachgearbeitet",
      "tokens": [
        51448,
        978,
        978,
        8397,
        1979,
        28445,
        11,
        3594,
        1418,
        1482,
        12479,
        1343,
        11676,
        674,
        1482,
        6425,
        566,
        2458,
        13789,
        5168,
        432,
        24024,
        302,
        51698
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31204530596733093,
      "compression_ratio": 1.5993376970291138,
      "no_speech_prob": 0.005467881448566914
    },
    {
      "id": 25,
      "seek": 15448,
      "start": 1469.8899957275391,
      "end": 1476.7700006103516,
      "text": " werden. Und den Aufwand können wir halt nicht leisten. Niemand von uns kann sich hinsetzen und",
      "tokens": [
        50364,
        4604,
        13,
        2719,
        1441,
        9462,
        33114,
        6310,
        1987,
        12479,
        1979,
        47013,
        13,
        426,
        39362,
        2957,
        2693,
        4028,
        3041,
        276,
        1292,
        24797,
        674,
        50708
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28213027119636536,
      "compression_ratio": 1.6789772510528564,
      "no_speech_prob": 0.012428474612534046
    },
    {
      "id": 26,
      "seek": 15448,
      "start": 1476.7700006103516,
      "end": 1481.0499993896485,
      "text": " das Ding anhören und darunter schreiben. Und wir können eigentlich auch nicht ernsthaft jemanden",
      "tokens": [
        50708,
        1482,
        20558,
        18931,
        26377,
        674,
        4072,
        21777,
        48546,
        13,
        2719,
        1987,
        6310,
        10926,
        2168,
        1979,
        43412,
        25127,
        21717,
        268,
        50922
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28213027119636536,
      "compression_ratio": 1.6789772510528564,
      "no_speech_prob": 0.012428474612534046
    },
    {
      "id": 27,
      "seek": 15448,
      "start": 1481.0499993896485,
      "end": 1486.3299981689454,
      "text": " dafür bezahlen, weil wir im Prinzip kein Budget haben. Und deswegen ist das die einzige Möglichkeit,",
      "tokens": [
        50922,
        13747,
        10782,
        21128,
        11,
        7689,
        1987,
        566,
        47572,
        13424,
        33751,
        3084,
        13,
        2719,
        26482,
        1418,
        1482,
        978,
        47743,
        30662,
        11,
        51186
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28213027119636536,
      "compression_ratio": 1.6789772510528564,
      "no_speech_prob": 0.012428474612534046
    },
    {
      "id": 28,
      "seek": 15448,
      "start": 1486.3299981689454,
      "end": 1490.6500054931641,
      "text": " Transkripte zu bekommen. Und das ist eben auch tatsächlich wichtig. Und bei den Zusammenfassungen",
      "tokens": [
        51186,
        6531,
        74,
        470,
        662,
        68,
        2164,
        19256,
        13,
        2719,
        1482,
        1418,
        11375,
        2168,
        20796,
        13621,
        13,
        2719,
        4643,
        1441,
        29442,
        69,
        640,
        5084,
        51402
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28213027119636536,
      "compression_ratio": 1.6789772510528564,
      "no_speech_prob": 0.012428474612534046
    },
    {
      "id": 29,
      "seek": 15448,
      "start": 1490.6500054931641,
      "end": 1494.6500054931641,
      "text": " habe ich eben selber gemerkt, dass die… Also ich empfinde sie zumindest als nützlich, weil ich da",
      "tokens": [
        51402,
        6015,
        1893,
        11375,
        23888,
        7173,
        49015,
        11,
        2658,
        978,
        1260,
        2743,
        1893,
        4012,
        69,
        8274,
        2804,
        38082,
        3907,
        297,
        7695,
        16813,
        11,
        7689,
        1893,
        1120,
        51602
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28213027119636536,
      "compression_ratio": 1.6789772510528564,
      "no_speech_prob": 0.012428474612534046
    },
    {
      "id": 30,
      "seek": 15448,
      "start": 1494.6500054931641,
      "end": 1498.7700006103516,
      "text": " nochmal eine Idee davon habe, was in der Episode passiert ist. Und deswegen fand ich das halt",
      "tokens": [
        51602,
        26509,
        3018,
        32651,
        18574,
        6015,
        11,
        390,
        294,
        1163,
        19882,
        21671,
        1418,
        13,
        2719,
        26482,
        38138,
        1893,
        1482,
        12479,
        51808
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28213027119636536,
      "compression_ratio": 1.6789772510528564,
      "no_speech_prob": 0.012428474612534046
    },
    {
      "id": 31,
      "seek": 18336,
      "start": 1498.930004272461,
      "end": 1503.6500054931641,
      "text": " gut. Und das ist ja auch vielleicht das Allgemeine. Ich habe darüber auch bei Heise mal so einen",
      "tokens": [
        50372,
        5228,
        13,
        2719,
        1482,
        1418,
        2784,
        2168,
        12547,
        1482,
        1057,
        31964,
        533,
        13,
        3141,
        6015,
        21737,
        2168,
        4643,
        634,
        908,
        2806,
        370,
        4891,
        50608
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2970145046710968,
      "compression_ratio": 1.6208053827285767,
      "no_speech_prob": 0.007342595141381025
    },
    {
      "id": 32,
      "seek": 18336,
      "start": 1503.6500054931641,
      "end": 1510.930004272461,
      "text": " Artikel geschrieben, eben auch in Bezug auf AI. Wenn wir jetzt also mehr Produktivität haben,",
      "tokens": [
        50608,
        5735,
        41486,
        47397,
        11,
        11375,
        2168,
        294,
        879,
        29742,
        2501,
        7318,
        13,
        7899,
        1987,
        4354,
        611,
        5417,
        44599,
        592,
        14053,
        3084,
        11,
        50972
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2970145046710968,
      "compression_ratio": 1.6208053827285767,
      "no_speech_prob": 0.007342595141381025
    },
    {
      "id": 33,
      "seek": 18336,
      "start": 1510.930004272461,
      "end": 1517.2900048828126,
      "text": " was auch immer das bedeutet. Also wir haben jetzt ja die Möglichkeit, mit wenig Aufwand eben diese",
      "tokens": [
        50972,
        390,
        2168,
        5578,
        1482,
        27018,
        13,
        2743,
        1987,
        3084,
        4354,
        2784,
        978,
        30662,
        11,
        2194,
        20911,
        9462,
        33114,
        11375,
        6705,
        51290
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2970145046710968,
      "compression_ratio": 1.6208053827285767,
      "no_speech_prob": 0.007342595141381025
    },
    {
      "id": 34,
      "seek": 18336,
      "start": 1517.2900048828126,
      "end": 1522.4900018310548,
      "text": " Sachen zu erzeugen mit AI. Und dadurch sind wir halt in dem Bereich produktiver. Also wenn ich",
      "tokens": [
        51290,
        26074,
        2164,
        1189,
        19303,
        268,
        2194,
        7318,
        13,
        2719,
        35472,
        3290,
        1987,
        12479,
        294,
        1371,
        26489,
        42816,
        1837,
        13,
        2743,
        4797,
        1893,
        51550
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2970145046710968,
      "compression_ratio": 1.6208053827285767,
      "no_speech_prob": 0.007342595141381025
    },
    {
      "id": 35,
      "seek": 18336,
      "start": 1522.4900018310548,
      "end": 1525.8099938964845,
      "text": " jetzt die Zusammenfassung schreiben würde oder die Stichworte, wenn wir die schreiben würden,",
      "tokens": [
        51550,
        4354,
        978,
        29442,
        69,
        40828,
        48546,
        11942,
        4513,
        978,
        745,
        480,
        86,
        12752,
        11,
        4797,
        1987,
        978,
        48546,
        27621,
        11,
        51716
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2970145046710968,
      "compression_ratio": 1.6208053827285767,
      "no_speech_prob": 0.007342595141381025
    },
    {
      "id": 36,
      "seek": 21040,
      "start": 1525.8099938964845,
      "end": 1532.41,
      "text": " das würde halt viel länger dauern. Und dadurch machen wir halt mehr. Und da gibt es einen Begriff",
      "tokens": [
        50364,
        1482,
        11942,
        12479,
        5891,
        40935,
        37359,
        1248,
        13,
        2719,
        35472,
        7069,
        1987,
        12479,
        5417,
        13,
        2719,
        1120,
        6089,
        785,
        4891,
        879,
        32783,
        50694
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272490531206131,
      "compression_ratio": 1.7028985023498535,
      "no_speech_prob": 0.04461732134222984
    },
    {
      "id": 37,
      "seek": 21040,
      "start": 1532.41,
      "end": 1540.3299981689454,
      "text": " für Rebound-Effekt. Ich glaube, so heißt der. Rebound. Genau. Wo du halt sagst, wenn etwas",
      "tokens": [
        50694,
        2959,
        1300,
        18767,
        12,
        36,
        602,
        8192,
        13,
        3141,
        13756,
        11,
        370,
        13139,
        1163,
        13,
        1300,
        18767,
        13,
        22340,
        13,
        6622,
        1581,
        12479,
        15274,
        372,
        11,
        4797,
        9569,
        51090
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272490531206131,
      "compression_ratio": 1.7028985023498535,
      "no_speech_prob": 0.04461732134222984
    },
    {
      "id": 38,
      "seek": 21040,
      "start": 1540.3299981689454,
      "end": 1544.169994506836,
      "text": " billiger wird, einfacher wird, dann wird es irgendwie für neue Sachen genutzt und halt mehr",
      "tokens": [
        51090,
        2961,
        4810,
        4578,
        11,
        38627,
        4062,
        4578,
        11,
        3594,
        4578,
        785,
        20759,
        2959,
        16842,
        26074,
        1049,
        325,
        2682,
        674,
        12479,
        5417,
        51282
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272490531206131,
      "compression_ratio": 1.7028985023498535,
      "no_speech_prob": 0.04461732134222984
    },
    {
      "id": 39,
      "seek": 21040,
      "start": 1544.169994506836,
      "end": 1551.2499963378907,
      "text": " genutzt. Und wenn also Sachen eben stromeffizienter werden, dann nutzt man halt mehr davon. Und das",
      "tokens": [
        51282,
        1049,
        325,
        2682,
        13,
        2719,
        4797,
        611,
        26074,
        11375,
        1056,
        423,
        602,
        590,
        1196,
        260,
        4604,
        11,
        3594,
        5393,
        2682,
        587,
        12479,
        5417,
        18574,
        13,
        2719,
        1482,
        51636
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272490531206131,
      "compression_ratio": 1.7028985023498535,
      "no_speech_prob": 0.04461732134222984
    },
    {
      "id": 40,
      "seek": 21040,
      "start": 1551.2499963378907,
      "end": 1554.2900048828126,
      "text": " ist glaube ich das, was wir hier beobachten in gewisser Weise. Und das ist auch das,",
      "tokens": [
        51636,
        1418,
        13756,
        1893,
        1482,
        11,
        390,
        1987,
        3296,
        312,
        996,
        20806,
        294,
        6906,
        23714,
        41947,
        13,
        2719,
        1482,
        1418,
        2168,
        1482,
        11,
        51788
      ],
      "temperature": 0.0,
      "avg_logprob": -0.272490531206131,
      "compression_ratio": 1.7028985023498535,
      "no_speech_prob": 0.04461732134222984
    },
    {
      "id": 41,
      "seek": 23888,
      "start": 1554.2900048828126,
      "end": 1559.1300012207032,
      "text": " was ich in diesem heißen Blogpost geschrieben habe, weil ich eben nicht erwarten würde,",
      "tokens": [
        50364,
        390,
        1893,
        294,
        10975,
        39124,
        268,
        46693,
        23744,
        47397,
        6015,
        11,
        7689,
        1893,
        11375,
        1979,
        21715,
        11719,
        11942,
        11,
        50606
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3325669467449188,
      "compression_ratio": 1.5378485918045044,
      "no_speech_prob": 0.022259807214140892
    },
    {
      "id": 42,
      "seek": 23888,
      "start": 1559.1300012207032,
      "end": 1569.930004272461,
      "text": " dass wenn wir tatsächlich höher Produktivität durch AI haben. Das ist meiner Ansicht nach eine",
      "tokens": [
        50606,
        2658,
        4797,
        1987,
        20796,
        48045,
        44599,
        592,
        14053,
        7131,
        7318,
        3084,
        13,
        2846,
        1418,
        20529,
        14590,
        1405,
        5168,
        3018,
        51146
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3325669467449188,
      "compression_ratio": 1.5378485918045044,
      "no_speech_prob": 0.022259807214140892
    },
    {
      "id": 43,
      "seek": 23888,
      "start": 1569.930004272461,
      "end": 1574.4899865722657,
      "text": " offene Frage, aber das ist eine andere Diskussion. Dann ist die nächste Frage, ob das durchschlägt",
      "tokens": [
        51146,
        766,
        1450,
        13685,
        11,
        4340,
        1482,
        1418,
        3018,
        10490,
        45963,
        313,
        13,
        7455,
        1418,
        978,
        30661,
        13685,
        11,
        1111,
        1482,
        7131,
        6145,
        22882,
        10463,
        51374
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3325669467449188,
      "compression_ratio": 1.5378485918045044,
      "no_speech_prob": 0.022259807214140892
    },
    {
      "id": 44,
      "seek": 23888,
      "start": 1574.4899865722657,
      "end": 1580.1300012207032,
      "text": " zu weniger Menschen, die halt arbeiten. Mindestens in der Softwareentwicklung ist es so, dass wir",
      "tokens": [
        51374,
        2164,
        23224,
        8397,
        11,
        978,
        12479,
        23162,
        13,
        13719,
        42624,
        294,
        1163,
        27428,
        317,
        16038,
        17850,
        1418,
        785,
        370,
        11,
        2658,
        1987,
        51656
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3325669467449188,
      "compression_ratio": 1.5378485918045044,
      "no_speech_prob": 0.022259807214140892
    },
    {
      "id": 45,
      "seek": 26472,
      "start": 1580.41,
      "end": 1585.6100122070313,
      "text": " hoffentlich, glaube ich, die ganze Zeit Produktivitätsvorteile haben. Und ich würde",
      "tokens": [
        50378,
        1106,
        22805,
        11,
        13756,
        1893,
        11,
        978,
        18898,
        9394,
        44599,
        592,
        13187,
        1373,
        85,
        12752,
        794,
        3084,
        13,
        2719,
        1893,
        11942,
        50638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3129999041557312,
      "compression_ratio": 1.5723683834075928,
      "no_speech_prob": 0.11088122427463531
    },
    {
      "id": 46,
      "seek": 26472,
      "start": 1585.6100122070313,
      "end": 1590.7300073242188,
      "text": " behaupten, jetzt oder vor einiger Zeit, also bevor dieser Downturn bei uns war in unserer",
      "tokens": [
        50638,
        1540,
        13343,
        268,
        11,
        4354,
        4513,
        4245,
        1343,
        4810,
        9394,
        11,
        611,
        37591,
        9053,
        44386,
        925,
        4643,
        2693,
        1516,
        294,
        20965,
        50894
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3129999041557312,
      "compression_ratio": 1.5723683834075928,
      "no_speech_prob": 0.11088122427463531
    },
    {
      "id": 47,
      "seek": 26472,
      "start": 1590.7300073242188,
      "end": 1595.8099938964845,
      "text": " Branche, zu dem Zeitpunkt gab es halt die meisten EntwicklerInnen. Trotzdem, dass die viel produktiver",
      "tokens": [
        50894,
        1603,
        22806,
        11,
        2164,
        1371,
        9394,
        31744,
        17964,
        785,
        12479,
        978,
        29708,
        29397,
        1918,
        4575,
        2866,
        13,
        1765,
        23934,
        11,
        2658,
        978,
        5891,
        42816,
        1837,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3129999041557312,
      "compression_ratio": 1.5723683834075928,
      "no_speech_prob": 0.11088122427463531
    },
    {
      "id": 48,
      "seek": 26472,
      "start": 1595.8099938964845,
      "end": 1601.3699914550782,
      "text": " sind als die vor zehn Jahren oder so. Und deswegen bin ich halt sehr unsicher, ob wir dann tatsächlich",
      "tokens": [
        51148,
        3290,
        3907,
        978,
        4245,
        33975,
        13080,
        4513,
        370,
        13,
        2719,
        26482,
        5171,
        1893,
        12479,
        5499,
        2693,
        14934,
        11,
        1111,
        1987,
        3594,
        256,
        1720,
        10168,
        2081,
        339,
        51426
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3129999041557312,
      "compression_ratio": 1.5723683834075928,
      "no_speech_prob": 0.11088122427463531
    },
    {
      "id": 49,
      "seek": 26472,
      "start": 1601.3699914550782,
      "end": 1607.8500024414063,
      "text": " in einer sozusagen Massenarbeitslosigkeit haben werden. Ich würde eher erwarten, dass wir halt",
      "tokens": [
        51426,
        294,
        6850,
        33762,
        376,
        8356,
        289,
        21604,
        9389,
        16626,
        3084,
        4604,
        13,
        3141,
        11942,
        24332,
        21715,
        11719,
        11,
        2658,
        1987,
        12479,
        51750
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3129999041557312,
      "compression_ratio": 1.5723683834075928,
      "no_speech_prob": 0.11088122427463531
    },
    {
      "id": 50,
      "seek": 29244,
      "start": 1607.8500024414063,
      "end": 1612.5700036621095,
      "text": " andere Sachen dann noch produzieren. Hier haben wir im Prinzip genau das, wie du gerade sagtest,",
      "tokens": [
        50364,
        10490,
        26074,
        3594,
        3514,
        28093,
        5695,
        13,
        10886,
        3084,
        1987,
        566,
        47572,
        12535,
        1482,
        11,
        3355,
        1581,
        12117,
        15764,
        377,
        11,
        50600
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30676519870758057,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.02367512322962284
    },
    {
      "id": 51,
      "seek": 29244,
      "start": 1612.5700036621095,
      "end": 1621.450008544922,
      "text": " als kleines Beispiel. Ja, definitiv. Bei mir schwingt das Pendel hin und her. Und ich sehe,",
      "tokens": [
        50600,
        3907,
        9318,
        1652,
        13772,
        13,
        3530,
        11,
        28781,
        592,
        13,
        16188,
        3149,
        956,
        7904,
        83,
        1482,
        38048,
        338,
        14102,
        674,
        720,
        13,
        2719,
        1893,
        35995,
        11,
        51044
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30676519870758057,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.02367512322962284
    },
    {
      "id": 52,
      "seek": 29244,
      "start": 1621.450008544922,
      "end": 1626.6100122070313,
      "text": " dass manche Sachen die KI gut machen kann. Sie kann mich enablen, sie kann mich unterstützen.",
      "tokens": [
        51044,
        2658,
        587,
        1876,
        26074,
        978,
        47261,
        5228,
        7069,
        4028,
        13,
        3559,
        4028,
        6031,
        9528,
        77,
        11,
        2804,
        4028,
        6031,
        43081,
        13,
        51302
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30676519870758057,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.02367512322962284
    },
    {
      "id": 53,
      "seek": 29244,
      "start": 1626.6100122070313,
      "end": 1633.3300134277345,
      "text": " Und andere Sachen, die klappen irgendwie ganz schlecht. Also die Transcriptions, die lagen eine",
      "tokens": [
        51302,
        2719,
        10490,
        26074,
        11,
        978,
        33337,
        21278,
        20759,
        6312,
        32427,
        13,
        2743,
        978,
        6531,
        34173,
        11,
        978,
        287,
        4698,
        3018,
        51638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30676519870758057,
      "compression_ratio": 1.5469387769699097,
      "no_speech_prob": 0.02367512322962284
    },
    {
      "id": 54,
      "seek": 31792,
      "start": 1633.3300134277345,
      "end": 1640.0899926757813,
      "text": " Weile dann so rum, weil das Problem war, ja, ich hatte das Skript, aber ich musste dann immer es",
      "tokens": [
        50364,
        492,
        794,
        3594,
        370,
        8347,
        11,
        7689,
        1482,
        11676,
        1516,
        11,
        2784,
        11,
        1893,
        13299,
        1482,
        7324,
        470,
        662,
        11,
        4340,
        1893,
        34497,
        3594,
        5578,
        785,
        50702
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2596939206123352,
      "compression_ratio": 1.545454502105713,
      "no_speech_prob": 0.0171625018119812
    },
    {
      "id": 55,
      "seek": 31792,
      "start": 1640.0899926757813,
      "end": 1647.4899865722657,
      "text": " laufen lassen. Er hat irgendwie acht bis zehn Minuten gebraucht. Dann musste ich ein Pull-Request",
      "tokens": [
        50702,
        41647,
        16168,
        13,
        3300,
        2385,
        20759,
        43048,
        7393,
        33975,
        27593,
        1519,
        6198,
        10084,
        13,
        7455,
        34497,
        1893,
        1343,
        15074,
        12,
        8524,
        20343,
        51072
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2596939206123352,
      "compression_ratio": 1.545454502105713,
      "no_speech_prob": 0.0171625018119812
    },
    {
      "id": 56,
      "seek": 31792,
      "start": 1647.4899865722657,
      "end": 1652.7300073242188,
      "text": " erstellen und habe mir gedacht, eigentlich müsste ich es automatisieren. Und das war dann nämlich",
      "tokens": [
        51072,
        11301,
        8581,
        674,
        6015,
        3149,
        33296,
        11,
        10926,
        42962,
        1893,
        785,
        28034,
        271,
        5695,
        13,
        2719,
        1482,
        1516,
        3594,
        21219,
        51334
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2596939206123352,
      "compression_ratio": 1.545454502105713,
      "no_speech_prob": 0.0171625018119812
    },
    {
      "id": 57,
      "seek": 31792,
      "start": 1652.7300073242188,
      "end": 1659.5700036621095,
      "text": " so der nächste Aha-Effekt, weil wir kennen das ja mit den Pipelines, die dann irgendwie nur auf",
      "tokens": [
        51334,
        370,
        1163,
        30661,
        27448,
        12,
        36,
        602,
        8192,
        11,
        7689,
        1987,
        28445,
        1482,
        2784,
        2194,
        1441,
        35396,
        9173,
        11,
        978,
        3594,
        20759,
        4343,
        2501,
        51676
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2596939206123352,
      "compression_ratio": 1.545454502105713,
      "no_speech_prob": 0.0171625018119812
    },
    {
      "id": 58,
      "seek": 34416,
      "start": 1659.5700036621095,
      "end": 1664.4899865722657,
      "text": " dem Server laufen, die man irgendwie schlecht lokal ausprobieren kann und so. Und ich glaube,",
      "tokens": [
        50364,
        1371,
        25684,
        41647,
        11,
        978,
        587,
        20759,
        32427,
        450,
        19990,
        3437,
        41990,
        5695,
        4028,
        674,
        370,
        13,
        2719,
        1893,
        13756,
        11,
        50610
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2506045401096344,
      "compression_ratio": 1.5275590419769287,
      "no_speech_prob": 0.05024169012904167
    },
    {
      "id": 59,
      "seek": 34416,
      "start": 1664.4899865722657,
      "end": 1670.5299951171876,
      "text": " ich hatte da einen gemischten Ansatz, dass ich einmal den GitHub Copilot mit Issues gefüttert",
      "tokens": [
        50610,
        1893,
        13299,
        1120,
        4891,
        7173,
        5494,
        1147,
        14590,
        10300,
        11,
        2658,
        1893,
        11078,
        1441,
        23331,
        11579,
        31516,
        2194,
        38195,
        1247,
        11271,
        7695,
        391,
        83,
        50912
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2506045401096344,
      "compression_ratio": 1.5275590419769287,
      "no_speech_prob": 0.05024169012904167
    },
    {
      "id": 60,
      "seek": 34416,
      "start": 1670.5299951171876,
      "end": 1678.8900109863282,
      "text": " habe, direkt auf GitHub.com und einmal, dass ich es lokal gemacht habe und meiner KI Zugriff über",
      "tokens": [
        50912,
        6015,
        11,
        20315,
        2501,
        23331,
        13,
        1112,
        674,
        11078,
        11,
        2658,
        1893,
        785,
        450,
        19990,
        12293,
        6015,
        674,
        20529,
        47261,
        34722,
        81,
        3661,
        4502,
        51330
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2506045401096344,
      "compression_ratio": 1.5275590419769287,
      "no_speech_prob": 0.05024169012904167
    },
    {
      "id": 61,
      "seek": 34416,
      "start": 1678.8900109863282,
      "end": 1686.3699914550782,
      "text": " das Command-Line-Tool GH auf die Workflows gegeben habe, dass es eben gucken konnte, wie sie laufen.",
      "tokens": [
        51330,
        1482,
        17901,
        12,
        43,
        533,
        12,
        51,
        1092,
        40690,
        2501,
        978,
        6603,
        33229,
        32572,
        6015,
        11,
        2658,
        785,
        11375,
        33135,
        24058,
        11,
        3355,
        2804,
        41647,
        13,
        51704
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2506045401096344,
      "compression_ratio": 1.5275590419769287,
      "no_speech_prob": 0.05024169012904167
    },
    {
      "id": 62,
      "seek": 37096,
      "start": 1686.5700036621095,
      "end": 1697.0500146484376,
      "text": " Und ehrlich gesagt, das war eine Katastrophe. Da hat er sich total schwer getan. Ich habe auch",
      "tokens": [
        50374,
        2719,
        40872,
        12260,
        11,
        1482,
        1516,
        3018,
        8365,
        525,
        27194,
        13,
        3933,
        2385,
        1189,
        3041,
        3217,
        23809,
        45599,
        13,
        3141,
        6015,
        2168,
        50898
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2946491837501526,
      "compression_ratio": 1.4739583730697632,
      "no_speech_prob": 0.012616154737770557
    },
    {
      "id": 63,
      "seek": 37096,
      "start": 1697.0500146484376,
      "end": 1703.4899865722657,
      "text": " das Gefühl, ich habe den Code nicht reviewed. Das lief alles irgendwie nebenbei. Mach mal und",
      "tokens": [
        50898,
        1482,
        29715,
        11,
        1893,
        6015,
        1441,
        15549,
        1979,
        3698,
        1093,
        292,
        13,
        2846,
        4544,
        69,
        7874,
        20759,
        36098,
        21845,
        13,
        12089,
        2806,
        674,
        51220
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2946491837501526,
      "compression_ratio": 1.4739583730697632,
      "no_speech_prob": 0.012616154737770557
    },
    {
      "id": 64,
      "seek": 37096,
      "start": 1703.4899865722657,
      "end": 1710.929989013672,
      "text": " teste mal und schau mal, ob es funktioniert. Und ich habe die Befürchtung, dass er viel Code",
      "tokens": [
        51220,
        49586,
        2806,
        674,
        956,
        1459,
        2806,
        11,
        1111,
        785,
        26160,
        13,
        2719,
        1893,
        6015,
        978,
        879,
        12474,
        4701,
        1063,
        11,
        2658,
        1189,
        5891,
        15549,
        51592
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2946491837501526,
      "compression_ratio": 1.4739583730697632,
      "no_speech_prob": 0.012616154737770557
    },
    {
      "id": 65,
      "seek": 39552,
      "start": 1711.0500146484376,
      "end": 1719.689998779297,
      "text": " dupliziert hat. Also er hat angefangen. Es war irgendwie so eine Zeit, wo die Modelle sehr gern",
      "tokens": [
        50370,
        1581,
        564,
        43590,
        2385,
        13,
        2743,
        1189,
        2385,
        43907,
        10784,
        13,
        2313,
        1516,
        20759,
        370,
        3018,
        9394,
        11,
        6020,
        978,
        6583,
        4434,
        5499,
        38531,
        50802
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28268349170684814,
      "compression_ratio": 1.5537848472595215,
      "no_speech_prob": 0.12731695175170898
    },
    {
      "id": 66,
      "seek": 39552,
      "start": 1719.689998779297,
      "end": 1726.0899926757813,
      "text": " Back-Files angelegt haben, Backup-Files oder neue Versionen von den Files, obwohl ich immer gesagt",
      "tokens": [
        50802,
        5833,
        12,
        37,
        4680,
        15495,
        22745,
        3084,
        11,
        5833,
        1010,
        12,
        37,
        4680,
        4513,
        16842,
        12226,
        17068,
        2957,
        1441,
        479,
        4680,
        11,
        48428,
        1893,
        5578,
        12260,
        51122
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28268349170684814,
      "compression_ratio": 1.5537848472595215,
      "no_speech_prob": 0.12731695175170898
    },
    {
      "id": 67,
      "seek": 39552,
      "start": 1726.0899926757813,
      "end": 1732.2499963378907,
      "text": " habe, du bist hier unter Versionskontrolle, musst du nicht machen. Und er hatte eben auch Probleme",
      "tokens": [
        51122,
        6015,
        11,
        1581,
        18209,
        3296,
        8662,
        12226,
        626,
        74,
        896,
        45344,
        11,
        31716,
        1581,
        1979,
        7069,
        13,
        2719,
        1189,
        13299,
        11375,
        2168,
        32891,
        51430
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28268349170684814,
      "compression_ratio": 1.5537848472595215,
      "no_speech_prob": 0.12731695175170898
    },
    {
      "id": 68,
      "seek": 39552,
      "start": 1732.2499963378907,
      "end": 1740.3699914550782,
      "text": " mit den Berechtigungen, dass er halt von meinem Repository in das Software-Architektur im Stream",
      "tokens": [
        51430,
        2194,
        1441,
        17684,
        4701,
        328,
        5084,
        11,
        2658,
        1189,
        12479,
        2957,
        24171,
        3696,
        9598,
        827,
        294,
        1482,
        27428,
        12,
        10683,
        339,
        642,
        2320,
        374,
        566,
        24904,
        51836
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28268349170684814,
      "compression_ratio": 1.5537848472595215,
      "no_speech_prob": 0.12731695175170898
    },
    {
      "id": 69,
      "seek": 42496,
      "start": 1740.450008544922,
      "end": 1751.0100061035157,
      "text": " Repository das Ganze pushen kann. Und da war ich sehr froh, als es dann irgendwann lief. Aber es",
      "tokens": [
        50368,
        3696,
        9598,
        827,
        1482,
        35206,
        2944,
        268,
        4028,
        13,
        2719,
        1120,
        1516,
        1893,
        5499,
        9795,
        71,
        11,
        3907,
        785,
        3594,
        34313,
        4544,
        69,
        13,
        5992,
        785,
        50896
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2530587613582611,
      "compression_ratio": 1.4285714626312256,
      "no_speech_prob": 0.0026725761126726866
    },
    {
      "id": 70,
      "seek": 42496,
      "start": 1751.0100061035157,
      "end": 1759.2499963378907,
      "text": " kam mir fragil vor. Das Code weiß nicht. Also die Code-Basis ist da, glaube ich, nicht mehr so gut.",
      "tokens": [
        50896,
        9727,
        3149,
        9241,
        388,
        4245,
        13,
        2846,
        15549,
        13385,
        1979,
        13,
        2743,
        978,
        15549,
        12,
        33,
        26632,
        1418,
        1120,
        11,
        13756,
        1893,
        11,
        1979,
        5417,
        370,
        5228,
        13,
        51308
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2530587613582611,
      "compression_ratio": 1.4285714626312256,
      "no_speech_prob": 0.0026725761126726866
    },
    {
      "id": 71,
      "seek": 42496,
      "start": 1759.2499963378907,
      "end": 1767.3300134277345,
      "text": " Läuft, aber fragil. Und da bin ich dann noch zu einem Moment gekommen, wo ich gemerkt habe,",
      "tokens": [
        51308,
        441,
        737,
        25005,
        11,
        4340,
        9241,
        388,
        13,
        2719,
        1120,
        5171,
        1893,
        3594,
        3514,
        2164,
        6827,
        19093,
        32732,
        11,
        6020,
        1893,
        7173,
        49015,
        6015,
        11,
        51712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2530587613582611,
      "compression_ratio": 1.4285714626312256,
      "no_speech_prob": 0.0026725761126726866
    },
    {
      "id": 72,
      "seek": 45192,
      "start": 1767.3300134277345,
      "end": 1775.7300073242188,
      "text": " geht so gar nicht. Weil irgendwann, ich wollte die Transkription anschmeißen und er lief auf",
      "tokens": [
        50364,
        7095,
        370,
        3691,
        1979,
        13,
        18665,
        34313,
        11,
        1893,
        24509,
        978,
        6531,
        74,
        470,
        1695,
        31508,
        1398,
        6230,
        268,
        674,
        1189,
        4544,
        69,
        2501,
        50784
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30991971492767334,
      "compression_ratio": 1.5236220359802246,
      "no_speech_prob": 0.002934615360572934
    },
    {
      "id": 73,
      "seek": 45192,
      "start": 1775.7300073242188,
      "end": 1783.0500146484376,
      "text": " einen Berechtigungsfehler und nichts ging. Ich habe die KI angeschmissen, habe gesagt, guck mal",
      "tokens": [
        50784,
        4891,
        17684,
        4701,
        328,
        5846,
        33865,
        1918,
        674,
        13004,
        21924,
        13,
        3141,
        6015,
        978,
        47261,
        2562,
        22320,
        76,
        10987,
        11,
        6015,
        12260,
        11,
        695,
        547,
        2806,
        51150
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30991971492767334,
      "compression_ratio": 1.5236220359802246,
      "no_speech_prob": 0.002934615360572934
    },
    {
      "id": 74,
      "seek": 45192,
      "start": 1783.0500146484376,
      "end": 1789.2099877929688,
      "text": " da Fehler, fix das. Ich war mir nicht so sicher, ob ich da noch irgendwie mit der KI was geändert",
      "tokens": [
        51150,
        1120,
        48101,
        11,
        3191,
        1482,
        13,
        3141,
        1516,
        3149,
        1979,
        370,
        18623,
        11,
        1111,
        1893,
        1120,
        3514,
        20759,
        2194,
        1163,
        47261,
        390,
        1519,
        34945,
        51458
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30991971492767334,
      "compression_ratio": 1.5236220359802246,
      "no_speech_prob": 0.002934615360572934
    },
    {
      "id": 75,
      "seek": 45192,
      "start": 1789.2099877929688,
      "end": 1795.450008544922,
      "text": " hatte, das dadurch broken war. Und die KI so, ach ja, klar, warum hast du hier ein Personal Access",
      "tokens": [
        51458,
        13299,
        11,
        1482,
        35472,
        738,
        8406,
        1516,
        13,
        2719,
        978,
        47261,
        370,
        11,
        2800,
        2784,
        11,
        14743,
        11,
        24331,
        6581,
        1581,
        3296,
        1343,
        25317,
        17166,
        51770
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30991971492767334,
      "compression_ratio": 1.5236220359802246,
      "no_speech_prob": 0.002934615360572934
    },
    {
      "id": 76,
      "seek": 48004,
      "start": 1795.5299951171876,
      "end": 1803.2099877929688,
      "text": " Token? Können wir doch rausschmeißen, weil du hast ja hier Berechtigung in der Action und hat das",
      "tokens": [
        50368,
        314,
        8406,
        30,
        29077,
        2866,
        1987,
        9243,
        3342,
        2023,
        339,
        1398,
        6230,
        268,
        11,
        7689,
        1581,
        6581,
        2784,
        3296,
        17684,
        4701,
        21034,
        294,
        1163,
        16261,
        674,
        2385,
        1482,
        50752
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2595146894454956,
      "compression_ratio": 1.6458333730697632,
      "no_speech_prob": 0.011683915741741657
    },
    {
      "id": 77,
      "seek": 48004,
      "start": 1803.2099877929688,
      "end": 1808.41,
      "text": " rausgeschmissen. Und dann haben wir es ausprobiert und dann ist weiter unten ein Berechtigungsfehler",
      "tokens": [
        50752,
        17202,
        23378,
        76,
        10987,
        13,
        2719,
        3594,
        3084,
        1987,
        785,
        3437,
        41990,
        4859,
        674,
        3594,
        1418,
        8988,
        25693,
        1343,
        17684,
        4701,
        328,
        5846,
        33865,
        1918,
        51012
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2595146894454956,
      "compression_ratio": 1.6458333730697632,
      "no_speech_prob": 0.011683915741741657
    },
    {
      "id": 78,
      "seek": 48004,
      "start": 1808.41,
      "end": 1813.0899926757813,
      "text": " passiert. Und da ist mir auf einmal ein Licht aufgegangen. Er wollte dann schon anfangen und",
      "tokens": [
        51012,
        21671,
        13,
        2719,
        1120,
        1418,
        3149,
        2501,
        11078,
        1343,
        32917,
        35031,
        47152,
        13,
        3300,
        24509,
        3594,
        4981,
        33709,
        10784,
        674,
        51246
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2595146894454956,
      "compression_ratio": 1.6458333730697632,
      "no_speech_prob": 0.011683915741741657
    },
    {
      "id": 79,
      "seek": 48004,
      "start": 1813.0899926757813,
      "end": 1819.2900048828126,
      "text": " weiter unten den Berechtigungsfehler auch beheben. Aber das eigentliche Problem war ein ganz anderes,",
      "tokens": [
        51246,
        8988,
        25693,
        1441,
        17684,
        4701,
        328,
        5846,
        33865,
        1918,
        2168,
        312,
        675,
        1799,
        13,
        5992,
        1482,
        10926,
        68,
        11676,
        1516,
        1343,
        6312,
        31426,
        11,
        51556
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2595146894454956,
      "compression_ratio": 1.6458333730697632,
      "no_speech_prob": 0.011683915741741657
    },
    {
      "id": 80,
      "seek": 50388,
      "start": 1819.2900048828126,
      "end": 1827.0899926757813,
      "text": " denn ich hatte ein temporäres Personal Access Token gesetzt und diese Berechtigung war ausgelaufen.",
      "tokens": [
        50364,
        10471,
        1893,
        13299,
        1343,
        8219,
        737,
        495,
        25317,
        17166,
        314,
        8406,
        5019,
        3524,
        674,
        6705,
        17684,
        4701,
        21034,
        1516,
        3437,
        10345,
        20748,
        13,
        50754
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24410076439380646,
      "compression_ratio": 1.580645203590393,
      "no_speech_prob": 0.05494769662618637
    },
    {
      "id": 81,
      "seek": 50388,
      "start": 1827.0899926757813,
      "end": 1833.0099755859376,
      "text": " Das hätte er eigentlich erkennen müssen. Aber er hat eben da rumgefummelt am Code, hat nicht den",
      "tokens": [
        50754,
        2846,
        20041,
        1189,
        10926,
        45720,
        9013,
        13,
        5992,
        1189,
        2385,
        11375,
        1120,
        8347,
        13529,
        40879,
        2018,
        669,
        15549,
        11,
        2385,
        1979,
        1441,
        51050
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24410076439380646,
      "compression_ratio": 1.580645203590393,
      "no_speech_prob": 0.05494769662618637
    },
    {
      "id": 82,
      "seek": 50388,
      "start": 1833.0099755859376,
      "end": 1840.0899926757813,
      "text": " eigentlichen Grund gefunden und hätte den Code jetzt komplett umgeschmissen und sich in komischen",
      "tokens": [
        51050,
        10926,
        268,
        13941,
        36923,
        674,
        20041,
        1441,
        15549,
        4354,
        32261,
        1105,
        23378,
        76,
        10987,
        674,
        3041,
        294,
        5207,
        6282,
        51404
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24410076439380646,
      "compression_ratio": 1.580645203590393,
      "no_speech_prob": 0.05494769662618637
    },
    {
      "id": 83,
      "seek": 50388,
      "start": 1840.0899926757813,
      "end": 1847.2500268554688,
      "text": " Sachen verrannt. Das Ding ist, im ersten Stück reichen die normalen Berechtigungen aus, Open",
      "tokens": [
        51404,
        26074,
        1306,
        4257,
        580,
        13,
        2846,
        20558,
        1418,
        11,
        566,
        17324,
        31146,
        319,
        18613,
        978,
        2710,
        268,
        17684,
        4701,
        328,
        5084,
        3437,
        11,
        7238,
        51762
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24410076439380646,
      "compression_ratio": 1.580645203590393,
      "no_speech_prob": 0.05494769662618637
    },
    {
      "id": 84,
      "seek": 53184,
      "start": 1847.2900048828126,
      "end": 1855.0899926757813,
      "text": " Source Repository Pullen. Im zweiten Bereich braucht es Personal Access Token, um zu pushen oder den",
      "tokens": [
        50366,
        29629,
        3696,
        9598,
        827,
        15074,
        268,
        13,
        4331,
        39943,
        26489,
        22623,
        785,
        25317,
        17166,
        314,
        8406,
        11,
        1105,
        2164,
        2944,
        268,
        4513,
        1441,
        50756
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2599579989910126,
      "compression_ratio": 1.538759708404541,
      "no_speech_prob": 0.08379007130861282
    },
    {
      "id": 85,
      "seek": 53184,
      "start": 1855.0899926757813,
      "end": 1860.5299951171876,
      "text": " Pull Request zu stellen. Und das war ihm nicht aufgefallen. Und da habe ich gemerkt, dass er eben",
      "tokens": [
        50756,
        15074,
        1300,
        20343,
        2164,
        24407,
        13,
        2719,
        1482,
        1516,
        16021,
        1979,
        35031,
        24425,
        13,
        2719,
        1120,
        6015,
        1893,
        7173,
        49015,
        11,
        2658,
        1189,
        11375,
        51028
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2599579989910126,
      "compression_ratio": 1.538759708404541,
      "no_speech_prob": 0.08379007130861282
    },
    {
      "id": 86,
      "seek": 53184,
      "start": 1860.5299951171876,
      "end": 1868.2099877929688,
      "text": " in diesem relativ kleinen Stück Code anscheinend den Überblick verloren hat, das mentale Modell",
      "tokens": [
        51028,
        294,
        10975,
        21960,
        26512,
        31146,
        15549,
        1567,
        1876,
        259,
        521,
        1441,
        18086,
        38263,
        44884,
        2385,
        11,
        1482,
        3074,
        1220,
        6583,
        898,
        51412
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2599579989910126,
      "compression_ratio": 1.538759708404541,
      "no_speech_prob": 0.08379007130861282
    },
    {
      "id": 87,
      "seek": 53184,
      "start": 1868.2099877929688,
      "end": 1874.9699975585938,
      "text": " über seinen eigenen Code nicht mehr bewahrt hat und deswegen das nicht editieren oder fixen konnte.",
      "tokens": [
        51412,
        4502,
        24427,
        28702,
        15549,
        1979,
        5417,
        17897,
        5398,
        83,
        2385,
        674,
        26482,
        1482,
        1979,
        8129,
        5695,
        4513,
        3191,
        268,
        24058,
        13,
        51750
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2599579989910126,
      "compression_ratio": 1.538759708404541,
      "no_speech_prob": 0.08379007130861282
    },
    {
      "id": 88,
      "seek": 55956,
      "start": 1875.9699975585938,
      "end": 1880.9699975585938,
      "text": " Genau, also nochmal ein Reminder. Nicht die Dinger sind Textgeneratoren,",
      "tokens": [
        50414,
        22340,
        11,
        611,
        26509,
        1343,
        4080,
        5669,
        13,
        22629,
        978,
        413,
        6911,
        3290,
        18643,
        21848,
        267,
        10948,
        11,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.42541274428367615,
      "compression_ratio": 1.5269709825515747,
      "no_speech_prob": 0.09387650340795517
    },
    {
      "id": 89,
      "seek": 55956,
      "start": 1880.9699975585938,
      "end": 1885.7300073242188,
      "text": " sodass sie auf Basis von dem, was sie reinbekommen haben, einen Text generieren. Ich würde behaupten,",
      "tokens": [
        50664,
        262,
        378,
        640,
        2804,
        2501,
        5859,
        271,
        2957,
        1371,
        11,
        390,
        2804,
        6561,
        650,
        13675,
        3084,
        11,
        4891,
        18643,
        1337,
        5695,
        13,
        3141,
        11942,
        1540,
        13343,
        268,
        11,
        50902
      ],
      "temperature": 0.0,
      "avg_logprob": -0.42541274428367615,
      "compression_ratio": 1.5269709825515747,
      "no_speech_prob": 0.09387650340795517
    },
    {
      "id": 90,
      "seek": 55956,
      "start": 1885.7300073242188,
      "end": 1892.3700219726563,
      "text": " es gibt da kein mentales Modell, aber nicht die Type. Das andere ist, ich finde, das ist halt ein,",
      "tokens": [
        50902,
        785,
        6089,
        1120,
        13424,
        4973,
        279,
        6583,
        898,
        11,
        4340,
        1979,
        978,
        15576,
        13,
        2846,
        10490,
        1418,
        11,
        1893,
        17841,
        11,
        1482,
        1418,
        12479,
        1343,
        11,
        51234
      ],
      "temperature": 0.0,
      "avg_logprob": -0.42541274428367615,
      "compression_ratio": 1.5269709825515747,
      "no_speech_prob": 0.09387650340795517
    },
    {
      "id": 91,
      "seek": 55956,
      "start": 1892.3700219726563,
      "end": 1904.0099755859376,
      "text": " also wie soll ich sagen, wir kommen ja noch dazu, wie wir es auf die Webseite tun und was ich",
      "tokens": [
        51234,
        611,
        3355,
        7114,
        1893,
        8360,
        11,
        1987,
        11729,
        2784,
        3514,
        13034,
        11,
        3355,
        1987,
        785,
        2501,
        978,
        9573,
        405,
        642,
        4267,
        674,
        390,
        1893,
        51816
      ],
      "temperature": 0.0,
      "avg_logprob": -0.42541274428367615,
      "compression_ratio": 1.5269709825515747,
      "no_speech_prob": 0.09387650340795517
    },
    {
      "id": 92,
      "seek": 58860,
      "start": 1904.0099755859376,
      "end": 1908.7300073242188,
      "text": " mittlerweile bei mir, wenn ich halt irgendwelche Sachen mache, wie zum Beispiel jetzt die Webseite",
      "tokens": [
        50364,
        41999,
        4643,
        3149,
        11,
        4797,
        1893,
        12479,
        26455,
        338,
        1876,
        26074,
        28289,
        11,
        3355,
        5919,
        13772,
        4354,
        978,
        9573,
        405,
        642,
        50600
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3237230181694031,
      "compression_ratio": 1.6550523042678833,
      "no_speech_prob": 0.008983722887933254
    },
    {
      "id": 93,
      "seek": 58860,
      "start": 1908.7300073242188,
      "end": 1912.7699853515626,
      "text": " editieren, was mir da zugutekommt, ist, dass ich mit Chitchipiti die Möglichkeit habe,",
      "tokens": [
        50600,
        8129,
        5695,
        11,
        390,
        3149,
        1120,
        33507,
        325,
        916,
        22230,
        11,
        1418,
        11,
        2658,
        1893,
        2194,
        761,
        1549,
        647,
        8707,
        978,
        30662,
        6015,
        11,
        50802
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3237230181694031,
      "compression_ratio": 1.6550523042678833,
      "no_speech_prob": 0.008983722887933254
    },
    {
      "id": 94,
      "seek": 58860,
      "start": 1912.7699853515626,
      "end": 1916.9699975585938,
      "text": " irgendwie zu sagen, okay, sagt mir mal, wie ich dieses oder jenes auf die Reihe bekomme. Und da",
      "tokens": [
        50802,
        20759,
        2164,
        8360,
        11,
        1392,
        11,
        15764,
        3149,
        2806,
        11,
        3355,
        1893,
        12113,
        4513,
        361,
        25973,
        2501,
        978,
        34549,
        675,
        9393,
        15117,
        13,
        2719,
        1120,
        51012
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3237230181694031,
      "compression_ratio": 1.6550523042678833,
      "no_speech_prob": 0.008983722887933254
    },
    {
      "id": 95,
      "seek": 58860,
      "start": 1916.9699975585938,
      "end": 1922.2500268554688,
      "text": " kommen halt gute Vorschläge. Das bedeutet aber, dass ich halt im Prinzip mir Webrecherche nur",
      "tokens": [
        51012,
        11729,
        12479,
        21476,
        31438,
        11439,
        737,
        432,
        13,
        2846,
        27018,
        4340,
        11,
        2658,
        1893,
        12479,
        566,
        47572,
        3149,
        9573,
        265,
        6759,
        1876,
        4343,
        51276
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3237230181694031,
      "compression_ratio": 1.6550523042678833,
      "no_speech_prob": 0.008983722887933254
    },
    {
      "id": 96,
      "seek": 58860,
      "start": 1922.2500268554688,
      "end": 1929.2500268554688,
      "text": " spare. Das, was du ja beschreibst, ist eigentlich, mach mal und implementier mal. Und das, was du",
      "tokens": [
        51276,
        13798,
        13,
        2846,
        11,
        390,
        1581,
        2784,
        17498,
        38606,
        372,
        11,
        1418,
        10926,
        11,
        2246,
        2806,
        674,
        4445,
        811,
        2806,
        13,
        2719,
        1482,
        11,
        390,
        1581,
        51626
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3237230181694031,
      "compression_ratio": 1.6550523042678833,
      "no_speech_prob": 0.008983722887933254
    },
    {
      "id": 97,
      "seek": 61384,
      "start": 1929.2500268554688,
      "end": 1936.170009765625,
      "text": " jetzt gerade beschreibst, ist ein gutes Beispiel, um es platt zu sagen, das funktioniert eigentlich",
      "tokens": [
        50364,
        4354,
        12117,
        17498,
        38606,
        372,
        11,
        1418,
        1343,
        45859,
        13772,
        11,
        1105,
        785,
        499,
        1591,
        2164,
        8360,
        11,
        1482,
        26160,
        10926,
        50710
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27000001072883606,
      "compression_ratio": 1.7491039037704468,
      "no_speech_prob": 0.15384343266487122
    },
    {
      "id": 98,
      "seek": 61384,
      "start": 1936.170009765625,
      "end": 1942.93001953125,
      "text": " nicht. Denn was an irgendeiner Stelle dann immer wieder passiert, ist genau das, was wir jetzt hier",
      "tokens": [
        50710,
        1979,
        13,
        19027,
        390,
        364,
        3418,
        27429,
        4564,
        26629,
        3594,
        5578,
        6216,
        21671,
        11,
        1418,
        12535,
        1482,
        11,
        390,
        1987,
        4354,
        3296,
        51048
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27000001072883606,
      "compression_ratio": 1.7491039037704468,
      "no_speech_prob": 0.15384343266487122
    },
    {
      "id": 99,
      "seek": 61384,
      "start": 1942.93001953125,
      "end": 1947.690029296875,
      "text": " gerade sehen. Du fängst irgendwie an und sagst, naja, das funktioniert ja so und so und so. Und",
      "tokens": [
        51048,
        12117,
        11333,
        13,
        5153,
        283,
        9935,
        372,
        20759,
        364,
        674,
        15274,
        372,
        11,
        1667,
        2938,
        11,
        1482,
        26160,
        2784,
        370,
        674,
        370,
        674,
        370,
        13,
        2719,
        51286
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27000001072883606,
      "compression_ratio": 1.7491039037704468,
      "no_speech_prob": 0.15384343266487122
    },
    {
      "id": 100,
      "seek": 61384,
      "start": 1947.690029296875,
      "end": 1952.0899926757813,
      "text": " da sind folgende Themen, also ein Security-Thema an dieser Stelle, das müssen wir irgendwie fixen.",
      "tokens": [
        51286,
        1120,
        3290,
        3339,
        27429,
        39229,
        11,
        611,
        1343,
        11164,
        12,
        2434,
        5619,
        364,
        9053,
        26629,
        11,
        1482,
        9013,
        1987,
        20759,
        3191,
        268,
        13,
        51506
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27000001072883606,
      "compression_ratio": 1.7491039037704468,
      "no_speech_prob": 0.15384343266487122
    },
    {
      "id": 101,
      "seek": 61384,
      "start": 1952.0899926757813,
      "end": 1957.8100244140626,
      "text": " Und dann musst du halt in diese Abstraktion reingreifen. Du bist jetzt irgendwie nicht mehr",
      "tokens": [
        51506,
        2719,
        3594,
        31716,
        1581,
        12479,
        294,
        6705,
        2847,
        19639,
        9780,
        319,
        278,
        265,
        25076,
        13,
        5153,
        18209,
        4354,
        20759,
        1979,
        5417,
        51792
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27000001072883606,
      "compression_ratio": 1.7491039037704468,
      "no_speech_prob": 0.15384343266487122
    },
    {
      "id": 102,
      "seek": 64240,
      "start": 1957.88998046875,
      "end": 1963.3299829101563,
      "text": " auf dieser Ebene, dass du sagst, mach mal und löse mal. Mir egal. Ich will nicht verstehen,",
      "tokens": [
        50368,
        2501,
        9053,
        20418,
        1450,
        11,
        2658,
        1581,
        15274,
        372,
        11,
        2246,
        2806,
        674,
        25209,
        405,
        2806,
        13,
        9421,
        31528,
        13,
        3141,
        486,
        1979,
        37352,
        11,
        50640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31870949268341064,
      "compression_ratio": 1.6236933469772339,
      "no_speech_prob": 0.0056402119807899
    },
    {
      "id": 103,
      "seek": 64240,
      "start": 1963.3299829101563,
      "end": 1967.2500268554688,
      "text": " wie es funktioniert, sondern du musst es ja mal verstehen. Es gibt halt GitHub. GitHub hat",
      "tokens": [
        50640,
        3355,
        785,
        26160,
        11,
        11465,
        1581,
        31716,
        785,
        2784,
        2806,
        37352,
        13,
        2313,
        6089,
        12479,
        23331,
        13,
        23331,
        2385,
        50836
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31870949268341064,
      "compression_ratio": 1.6236933469772339,
      "no_speech_prob": 0.0056402119807899
    },
    {
      "id": 104,
      "seek": 64240,
      "start": 1967.2500268554688,
      "end": 1972.4499780273438,
      "text": " irgendwelche Security-Tokens. Da gibt es offensichtlich Lokale und andere. Und irgendwie",
      "tokens": [
        50836,
        26455,
        338,
        1876,
        11164,
        12,
        18797,
        694,
        13,
        3933,
        6089,
        785,
        766,
        694,
        41971,
        46278,
        1220,
        674,
        10490,
        13,
        2719,
        20759,
        51096
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31870949268341064,
      "compression_ratio": 1.6236933469772339,
      "no_speech_prob": 0.0056402119807899
    },
    {
      "id": 105,
      "seek": 64240,
      "start": 1972.4499780273438,
      "end": 1978.2099877929688,
      "text": " ist da etwas schief. Und deswegen muss ich das halt fixen. Was halt bedeutet, ich komme eben",
      "tokens": [
        51096,
        1418,
        1120,
        9569,
        956,
        2521,
        13,
        2719,
        26482,
        6425,
        1893,
        1482,
        12479,
        3191,
        268,
        13,
        3027,
        12479,
        27018,
        11,
        1893,
        31194,
        11375,
        51384
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31870949268341064,
      "compression_ratio": 1.6236933469772339,
      "no_speech_prob": 0.0056402119807899
    },
    {
      "id": 106,
      "seek": 64240,
      "start": 1978.2099877929688,
      "end": 1982.8500024414063,
      "text": " nicht auf die höhere Abstraktionsschicht, sondern an bestimmten Stellen bricht es eben und ich muss",
      "tokens": [
        51384,
        1979,
        2501,
        978,
        13531,
        6703,
        2847,
        19639,
        2320,
        626,
        6145,
        1405,
        11,
        11465,
        364,
        35180,
        1147,
        41893,
        738,
        1405,
        785,
        11375,
        674,
        1893,
        6425,
        51616
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31870949268341064,
      "compression_ratio": 1.6236933469772339,
      "no_speech_prob": 0.0056402119807899
    },
    {
      "id": 107,
      "seek": 66744,
      "start": 1982.8500024414063,
      "end": 1990.8500024414063,
      "text": " irgendwie reingreifen. Das beobachte ich an extrem vielen Stellen, was eben dazu führt,",
      "tokens": [
        50364,
        20759,
        319,
        278,
        265,
        25076,
        13,
        2846,
        312,
        996,
        26136,
        1893,
        364,
        4040,
        19885,
        41893,
        11,
        390,
        11375,
        13034,
        39671,
        11,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32203802466392517,
      "compression_ratio": 1.6284722089767456,
      "no_speech_prob": 0.08873789757490158
    },
    {
      "id": 108,
      "seek": 66744,
      "start": 1990.8500024414063,
      "end": 1997.0099755859376,
      "text": " dass ich mir überhaupt gar nicht vorstellen kann, wie nicht EntwicklerInnen damit irgendetwas auf",
      "tokens": [
        50764,
        2658,
        1893,
        3149,
        20023,
        3691,
        1979,
        34346,
        4028,
        11,
        3355,
        1979,
        29397,
        1918,
        4575,
        2866,
        9479,
        11093,
        302,
        6569,
        2501,
        51072
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32203802466392517,
      "compression_ratio": 1.6284722089767456,
      "no_speech_prob": 0.08873789757490158
    },
    {
      "id": 109,
      "seek": 66744,
      "start": 1997.0099755859376,
      "end": 2000.7300073242188,
      "text": " die Reihe bekommen sollen. Weil an der Stelle werden sie halt gescheitert. Und dann ist halt",
      "tokens": [
        51072,
        978,
        34549,
        675,
        19256,
        24713,
        13,
        18665,
        364,
        1163,
        26629,
        4604,
        2804,
        12479,
        5019,
        1876,
        270,
        911,
        13,
        2719,
        3594,
        1418,
        12479,
        51258
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32203802466392517,
      "compression_ratio": 1.6284722089767456,
      "no_speech_prob": 0.08873789757490158
    },
    {
      "id": 110,
      "seek": 66744,
      "start": 2000.7300073242188,
      "end": 2006.41,
      "text": " Schluss. Weil eben solche Menschen dann da nicht reingreifen können und sagen können, ach so,",
      "tokens": [
        51258,
        36573,
        13,
        18665,
        11375,
        29813,
        8397,
        3594,
        1120,
        1979,
        319,
        278,
        265,
        25076,
        6310,
        674,
        8360,
        6310,
        11,
        2800,
        370,
        11,
        51542
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32203802466392517,
      "compression_ratio": 1.6284722089767456,
      "no_speech_prob": 0.08873789757490158
    },
    {
      "id": 111,
      "seek": 66744,
      "start": 2006.41,
      "end": 2011.2900048828126,
      "text": " ja klar, das funktioniert ja folgendermaßen. Eben nicht Security-Token. So funktioniert die",
      "tokens": [
        51542,
        2784,
        14743,
        11,
        1482,
        26160,
        2784,
        3339,
        9395,
        39994,
        8989,
        13,
        462,
        1799,
        1979,
        11164,
        12,
        51,
        8406,
        13,
        407,
        26160,
        978,
        51786
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32203802466392517,
      "compression_ratio": 1.6284722089767456,
      "no_speech_prob": 0.08873789757490158
    },
    {
      "id": 112,
      "seek": 69588,
      "start": 2011.2900048828126,
      "end": 2017.3299829101563,
      "text": " Security. Das ist halt GitHub. Sondern die können das dann eben auf der Ebene nicht mehr",
      "tokens": [
        50364,
        11164,
        13,
        2846,
        1418,
        12479,
        23331,
        13,
        318,
        10881,
        978,
        6310,
        1482,
        3594,
        11375,
        2501,
        1163,
        20418,
        1450,
        1979,
        5417,
        50666
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25992441177368164,
      "compression_ratio": 1.5203251838684082,
      "no_speech_prob": 0.023318827152252197
    },
    {
      "id": 113,
      "seek": 69588,
      "start": 2017.3299829101563,
      "end": 2023.170009765625,
      "text": " verfolgen und dann ist eben Schluss. Absolut. Und du hattest gerade eben schon gesagt,",
      "tokens": [
        50666,
        1306,
        7082,
        1766,
        674,
        3594,
        1418,
        11375,
        36573,
        13,
        5813,
        2308,
        13,
        2719,
        1581,
        276,
        1591,
        377,
        12117,
        11375,
        4981,
        12260,
        11,
        50958
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25992441177368164,
      "compression_ratio": 1.5203251838684082,
      "no_speech_prob": 0.023318827152252197
    },
    {
      "id": 114,
      "seek": 69588,
      "start": 2023.170009765625,
      "end": 2029.2099877929688,
      "text": " ja das Modell kann ja kein mentales Modell aufbauen. Und wir hatten vorher die Vermenschlichung. Aber",
      "tokens": [
        50958,
        2784,
        1482,
        6583,
        898,
        4028,
        2784,
        13424,
        4973,
        279,
        6583,
        898,
        2501,
        65,
        11715,
        13,
        2719,
        1987,
        20441,
        29195,
        978,
        20185,
        26590,
        1739,
        1063,
        13,
        5992,
        51260
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25992441177368164,
      "compression_ratio": 1.5203251838684082,
      "no_speech_prob": 0.023318827152252197
    },
    {
      "id": 115,
      "seek": 69588,
      "start": 2029.2099877929688,
      "end": 2034.9699975585938,
      "text": " ich habe mich jetzt seit ein paar Wochen mit dem mentalen Modell nach Peter Nauer beschäftigt.",
      "tokens": [
        51260,
        1893,
        6015,
        6031,
        4354,
        16452,
        1343,
        16509,
        23126,
        2194,
        1371,
        4973,
        268,
        6583,
        898,
        5168,
        6508,
        426,
        18120,
        38768,
        5828,
        13,
        51548
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25992441177368164,
      "compression_ratio": 1.5203251838684082,
      "no_speech_prob": 0.023318827152252197
    },
    {
      "id": 116,
      "seek": 71956,
      "start": 2035.5299951171876,
      "end": 2041.3299829101563,
      "text": " 85 war das, glaube ich, als er das so beschrieben hat. Und das Witzige ist,",
      "tokens": [
        50392,
        14695,
        1516,
        1482,
        11,
        13756,
        1893,
        11,
        3907,
        1189,
        1482,
        370,
        17498,
        24027,
        2385,
        13,
        2719,
        1482,
        343,
        6862,
        3969,
        1418,
        11,
        50682
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3029833137989044,
      "compression_ratio": 1.4278074502944946,
      "no_speech_prob": 0.04330383986234665
    },
    {
      "id": 117,
      "seek": 71956,
      "start": 2041.3299829101563,
      "end": 2051.170009765625,
      "text": " also das mentale Modell beschreibt halt, was man als Entwickler so aufbaut beim Programmieren,",
      "tokens": [
        50682,
        611,
        1482,
        3074,
        1220,
        6583,
        898,
        17498,
        31174,
        12479,
        11,
        390,
        587,
        3907,
        29397,
        1918,
        370,
        2501,
        65,
        1375,
        13922,
        48244,
        5695,
        11,
        51174
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3029833137989044,
      "compression_ratio": 1.4278074502944946,
      "no_speech_prob": 0.04330383986234665
    },
    {
      "id": 118,
      "seek": 71956,
      "start": 2051.170009765625,
      "end": 2058.4499780273436,
      "text": " um eben auch die Frage nach dem Warum im Code beantworten zu können. Warum wird da ein Personal",
      "tokens": [
        51174,
        1105,
        11375,
        2168,
        978,
        13685,
        5168,
        1371,
        25541,
        566,
        15549,
        312,
        21655,
        268,
        2164,
        6310,
        13,
        25541,
        4578,
        1120,
        1343,
        25317,
        51538
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3029833137989044,
      "compression_ratio": 1.4278074502944946,
      "no_speech_prob": 0.04330383986234665
    },
    {
      "id": 119,
      "seek": 74304,
      "start": 2058.4499780273436,
      "end": 2065.2099877929686,
      "text": " Access-Token und nicht einfach der Access-Key, den man in der Action hat, verwendet, zum Beispiel.",
      "tokens": [
        50364,
        17166,
        12,
        51,
        8406,
        674,
        1979,
        7281,
        1163,
        17166,
        12,
        42,
        2030,
        11,
        1441,
        587,
        294,
        1163,
        16261,
        2385,
        11,
        1306,
        20128,
        302,
        11,
        5919,
        13772,
        13,
        50702
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968924641609192,
      "compression_ratio": 1.4948453903198242,
      "no_speech_prob": 0.13627925515174866
    },
    {
      "id": 120,
      "seek": 74304,
      "start": 2065.2099877929686,
      "end": 2075.4499780273436,
      "text": " Und das Faszinierende ist, dass die Modelle dieses Konzept, mentales Modell nach Nauer kennen und",
      "tokens": [
        50702,
        2719,
        1482,
        479,
        19601,
        259,
        811,
        5445,
        1418,
        11,
        2658,
        978,
        6583,
        4434,
        12113,
        12718,
        32082,
        11,
        4973,
        279,
        6583,
        898,
        5168,
        426,
        18120,
        28445,
        674,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968924641609192,
      "compression_ratio": 1.4948453903198242,
      "no_speech_prob": 0.13627925515174866
    },
    {
      "id": 121,
      "seek": 74304,
      "start": 2075.4499780273436,
      "end": 2082.5299951171874,
      "text": " dann wissen, was sie zu tun haben, also was es bedeutet. Und die ganzen Tool-Hersteller haben",
      "tokens": [
        51214,
        3594,
        16331,
        11,
        390,
        2804,
        2164,
        4267,
        3084,
        11,
        611,
        390,
        785,
        27018,
        13,
        2719,
        978,
        23966,
        15934,
        12,
        39,
        16398,
        14983,
        3084,
        51568
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968924641609192,
      "compression_ratio": 1.4948453903198242,
      "no_speech_prob": 0.13627925515174866
    },
    {
      "id": 122,
      "seek": 76712,
      "start": 2082.5299951171874,
      "end": 2092.690029296875,
      "text": " schon angefangen. Man kennt das ja, diese Cloud.md oder Agent.md-Files, die immer im Root liegen,",
      "tokens": [
        50364,
        4981,
        43907,
        10784,
        13,
        2458,
        37682,
        1482,
        2784,
        11,
        6705,
        8061,
        13,
        76,
        67,
        4513,
        27174,
        13,
        76,
        67,
        12,
        37,
        4680,
        11,
        978,
        5578,
        566,
        3101,
        310,
        35100,
        11,
        50872
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31616535782814026,
      "compression_ratio": 1.375,
      "no_speech_prob": 0.05496881902217865
    },
    {
      "id": 123,
      "seek": 76712,
      "start": 2092.690029296875,
      "end": 2100.2099877929686,
      "text": " wo die KI mal über das Repository rübergegangen ist und zumindest sich wichtige Sachen wie",
      "tokens": [
        50872,
        6020,
        978,
        47261,
        2806,
        4502,
        1482,
        3696,
        9598,
        827,
        367,
        12670,
        432,
        47152,
        1418,
        674,
        38082,
        3041,
        46276,
        26074,
        3355,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31616535782814026,
      "compression_ratio": 1.375,
      "no_speech_prob": 0.05496881902217865
    },
    {
      "id": 124,
      "seek": 76712,
      "start": 2100.2099877929686,
      "end": 2106.610012207031,
      "text": " Technology-Stack und die File-Struktur und sowas rausgeschrieben hat. Gehört auch zum mentalen",
      "tokens": [
        51248,
        15037,
        12,
        4520,
        501,
        674,
        978,
        26196,
        12,
        4520,
        31543,
        674,
        19766,
        296,
        17202,
        23378,
        24027,
        2385,
        13,
        2876,
        71,
        11454,
        2168,
        5919,
        4973,
        268,
        51568
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31616535782814026,
      "compression_ratio": 1.375,
      "no_speech_prob": 0.05496881902217865
    },
    {
      "id": 125,
      "seek": 79120,
      "start": 2106.610012207031,
      "end": 2114.2900048828124,
      "text": " Modell. Ich behaupte, dass wenn man eben auch so ein bisschen an dieses Warum geht, wenn man jetzt",
      "tokens": [
        50364,
        6583,
        898,
        13,
        3141,
        1540,
        13343,
        68,
        11,
        2658,
        4797,
        587,
        11375,
        2168,
        370,
        1343,
        10763,
        364,
        12113,
        25541,
        7095,
        11,
        4797,
        587,
        4354,
        50748
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2568069398403168,
      "compression_ratio": 1.5720164775848389,
      "no_speech_prob": 0.0685117319226265
    },
    {
      "id": 126,
      "seek": 79120,
      "start": 2114.2900048828124,
      "end": 2120.850002441406,
      "text": " eben zum Beispiel da das hinterlassen würde, wir benutzen hier Personal Access-Token, weil erster",
      "tokens": [
        50748,
        11375,
        5919,
        13772,
        1120,
        1482,
        23219,
        44898,
        11942,
        11,
        1987,
        38424,
        2904,
        3296,
        25317,
        17166,
        12,
        51,
        8406,
        11,
        7689,
        1189,
        3120,
        51076
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2568069398403168,
      "compression_ratio": 1.5720164775848389,
      "no_speech_prob": 0.0685117319226265
    },
    {
      "id": 127,
      "seek": 79120,
      "start": 2120.850002441406,
      "end": 2127.7300073242186,
      "text": " Weg ohne Personal Access-Token hat nicht funktioniert. Deswegen, wenn man das hinterlässt,",
      "tokens": [
        51076,
        18919,
        15716,
        25317,
        17166,
        12,
        51,
        8406,
        2385,
        1979,
        26160,
        13,
        24864,
        11,
        4797,
        587,
        1482,
        23219,
        75,
        13555,
        372,
        11,
        51420
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2568069398403168,
      "compression_ratio": 1.5720164775848389,
      "no_speech_prob": 0.0685117319226265
    },
    {
      "id": 128,
      "seek": 79120,
      "start": 2127.7300073242186,
      "end": 2135.089992675781,
      "text": " dann könnte die KI da besser werden. Und das zeigt eben auch, dass, ja, du sagst ja selbst,",
      "tokens": [
        51420,
        3594,
        17646,
        978,
        47261,
        1120,
        18021,
        4604,
        13,
        2719,
        1482,
        29250,
        11375,
        2168,
        11,
        2658,
        11,
        2784,
        11,
        1581,
        15274,
        372,
        2784,
        13053,
        11,
        51788
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2568069398403168,
      "compression_ratio": 1.5720164775848389,
      "no_speech_prob": 0.0685117319226265
    },
    {
      "id": 129,
      "seek": 81968,
      "start": 2135.690029296875,
      "end": 2142.170009765625,
      "text": " wenn man jetzt das Programmieren noch nicht gewöhnt ist, nicht jahrelang trainiert hat,",
      "tokens": [
        50394,
        4797,
        587,
        4354,
        1482,
        48244,
        5695,
        3514,
        1979,
        6906,
        23817,
        580,
        1418,
        11,
        1979,
        361,
        545,
        4419,
        656,
        3847,
        4859,
        2385,
        11,
        50718
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30347222089767456,
      "compression_ratio": 1.4978355169296265,
      "no_speech_prob": 0.0006771604530513287
    },
    {
      "id": 130,
      "seek": 81968,
      "start": 2142.170009765625,
      "end": 2147.8100244140624,
      "text": " dann fallen einem diese Sachen nicht auf. Und dann ist die Frage, ob man es schafft,",
      "tokens": [
        50718,
        3594,
        11547,
        6827,
        6705,
        26074,
        1979,
        2501,
        13,
        2719,
        3594,
        1418,
        978,
        13685,
        11,
        1111,
        587,
        785,
        956,
        29445,
        11,
        51000
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30347222089767456,
      "compression_ratio": 1.4978355169296265,
      "no_speech_prob": 0.0006771604530513287
    },
    {
      "id": 131,
      "seek": 81968,
      "start": 2147.8100244140624,
      "end": 2156.569973144531,
      "text": " das Modell richtig zu besprechen, also die Prompts richtig zu wählen. Und das macht",
      "tokens": [
        51000,
        1482,
        6583,
        898,
        13129,
        2164,
        4097,
        38951,
        11,
        611,
        978,
        15833,
        39280,
        13129,
        2164,
        24787,
        6698,
        13,
        2719,
        1482,
        10857,
        51438
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30347222089767456,
      "compression_ratio": 1.4978355169296265,
      "no_speech_prob": 0.0006771604530513287
    },
    {
      "id": 132,
      "seek": 81968,
      "start": 2156.569973144531,
      "end": 2162.649990234375,
      "text": " einen großen Unterschied aus. Genau, also vielleicht noch zwei Worte dazu. Es ist halt",
      "tokens": [
        51438,
        4891,
        23076,
        41414,
        3437,
        13,
        22340,
        11,
        611,
        12547,
        3514,
        12002,
        343,
        12752,
        13034,
        13,
        2313,
        1418,
        12479,
        51742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30347222089767456,
      "compression_ratio": 1.4978355169296265,
      "no_speech_prob": 0.0006771604530513287
    },
    {
      "id": 133,
      "seek": 84724,
      "start": 2162.649990234375,
      "end": 2169.089992675781,
      "text": " ganz spannend, weil der Sebastian Hans hat mich vor fünf Tagen, so sagt Mastodon,",
      "tokens": [
        50364,
        6312,
        49027,
        11,
        7689,
        1163,
        31102,
        17926,
        2385,
        6031,
        4245,
        28723,
        41721,
        11,
        370,
        15764,
        376,
        525,
        378,
        266,
        11,
        50686
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30003511905670166,
      "compression_ratio": 1.5127118825912476,
      "no_speech_prob": 0.06731162965297699
    },
    {
      "id": 134,
      "seek": 84724,
      "start": 2169.089992675781,
      "end": 2176.4499780273436,
      "text": " auf dieses Paper hingewiesen, Programming as Theory Building. Und ich hatte mir irgendwie",
      "tokens": [
        50686,
        2501,
        12113,
        24990,
        24895,
        1023,
        30383,
        11,
        8338,
        2810,
        382,
        29009,
        18974,
        13,
        2719,
        1893,
        13299,
        3149,
        20759,
        51054
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30003511905670166,
      "compression_ratio": 1.5127118825912476,
      "no_speech_prob": 0.06731162965297699
    },
    {
      "id": 135,
      "seek": 84724,
      "start": 2176.4499780273436,
      "end": 2182.0500146484374,
      "text": " sozusagen vorgenommen, das nochmal genauer durchzulesen, weil ich halt vermutet hatte,",
      "tokens": [
        51054,
        33762,
        4245,
        29270,
        11,
        1482,
        26509,
        12535,
        260,
        7131,
        89,
        3473,
        268,
        11,
        7689,
        1893,
        12479,
        26319,
        20364,
        13299,
        11,
        51334
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30003511905670166,
      "compression_ratio": 1.5127118825912476,
      "no_speech_prob": 0.06731162965297699
    },
    {
      "id": 136,
      "seek": 84724,
      "start": 2182.0500146484374,
      "end": 2189.7300073242186,
      "text": " dass das halt im Prinzip bedeutet, dass man eben als Menschenteam ein gemeinsames mentales Modell",
      "tokens": [
        51334,
        2658,
        1482,
        12479,
        566,
        47572,
        27018,
        11,
        2658,
        587,
        11375,
        3907,
        27773,
        1576,
        335,
        1343,
        22971,
        1632,
        4973,
        279,
        6583,
        898,
        51718
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30003511905670166,
      "compression_ratio": 1.5127118825912476,
      "no_speech_prob": 0.06731162965297699
    },
    {
      "id": 137,
      "seek": 87432,
      "start": 2189.7300073242186,
      "end": 2196.8100244140624,
      "text": " entwickelt und dass sich im Code das eben nur ausdrückt. Ich hatte die ersten Seiten überflogen",
      "tokens": [
        50364,
        43208,
        674,
        2658,
        3041,
        566,
        15549,
        1482,
        11375,
        4343,
        3437,
        16753,
        37532,
        13,
        3141,
        13299,
        978,
        17324,
        45200,
        4502,
        3423,
        8799,
        50718
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32160434126853943,
      "compression_ratio": 1.738515853881836,
      "no_speech_prob": 0.013833180069923401
    },
    {
      "id": 138,
      "seek": 87432,
      "start": 2196.8100244140624,
      "end": 2200.9699975585936,
      "text": " und da war so eine Geschichte, von wegen irgendein Team hat einen Compiler gebaut oder irgendwas.",
      "tokens": [
        50718,
        674,
        1120,
        1516,
        370,
        3018,
        28896,
        11,
        2957,
        32855,
        3418,
        27429,
        259,
        7606,
        2385,
        4891,
        6620,
        5441,
        49203,
        4513,
        47090,
        13,
        50926
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32160434126853943,
      "compression_ratio": 1.738515853881836,
      "no_speech_prob": 0.013833180069923401
    },
    {
      "id": 139,
      "seek": 87432,
      "start": 2200.9699975585936,
      "end": 2207.129970703125,
      "text": " Dann hat ein anderes Team versucht, den zu erweitern. Und daraufhin hat man das dem ursprünglichen Team",
      "tokens": [
        50926,
        7455,
        2385,
        1343,
        31426,
        7606,
        36064,
        11,
        1441,
        2164,
        1189,
        28019,
        1248,
        13,
        2719,
        18654,
        10876,
        2385,
        587,
        1482,
        1371,
        4038,
        18193,
        36216,
        10193,
        7606,
        51234
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32160434126853943,
      "compression_ratio": 1.738515853881836,
      "no_speech_prob": 0.013833180069923401
    },
    {
      "id": 140,
      "seek": 87432,
      "start": 2207.129970703125,
      "end": 2211.569973144531,
      "text": " gegeben und hat gesagt, das ursprüngliche Team, was den Compiler ursprünglich gebaut hat,",
      "tokens": [
        51234,
        32572,
        674,
        2385,
        12260,
        11,
        1482,
        4038,
        18193,
        36216,
        10185,
        7606,
        11,
        390,
        1441,
        6620,
        5441,
        4038,
        18193,
        36216,
        1739,
        49203,
        2385,
        11,
        51456
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32160434126853943,
      "compression_ratio": 1.738515853881836,
      "no_speech_prob": 0.013833180069923401
    },
    {
      "id": 141,
      "seek": 87432,
      "start": 2211.569973144531,
      "end": 2218.41,
      "text": " hat gesagt, das ist ein netter Versuch, aber das zerstört die ganze Struktur des Systems. Und hier",
      "tokens": [
        51456,
        2385,
        12260,
        11,
        1482,
        1418,
        1343,
        2533,
        391,
        12226,
        625,
        11,
        4340,
        1482,
        710,
        16398,
        11454,
        978,
        18898,
        745,
        31543,
        730,
        27059,
        13,
        2719,
        3296,
        51798
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32160434126853943,
      "compression_ratio": 1.738515853881836,
      "no_speech_prob": 0.013833180069923401
    },
    {
      "id": 142,
      "seek": 90300,
      "start": 2218.41,
      "end": 2222.41,
      "text": " ist ein viel einfacherer Weg. Und das hängt eben damit zusammen, dass das ursprüngliche Team diese",
      "tokens": [
        50364,
        1418,
        1343,
        5891,
        38627,
        4062,
        260,
        18919,
        13,
        2719,
        1482,
        276,
        29670,
        11375,
        9479,
        14311,
        11,
        2658,
        1482,
        4038,
        18193,
        36216,
        10185,
        7606,
        6705,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26905813813209534,
      "compression_ratio": 1.580246925354004,
      "no_speech_prob": 0.013216602616012096
    },
    {
      "id": 143,
      "seek": 90300,
      "start": 2222.41,
      "end": 2231.0099755859374,
      "text": " Theorie verstanden hat und das neue Team nicht. So jedenfalls meine Wahrnehmung. Das, was du",
      "tokens": [
        50564,
        440,
        17473,
        1306,
        33946,
        2385,
        674,
        1482,
        16842,
        7606,
        1979,
        13,
        407,
        12906,
        18542,
        10946,
        36357,
        716,
        8587,
        1063,
        13,
        2846,
        11,
        390,
        1581,
        50994
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26905813813209534,
      "compression_ratio": 1.580246925354004,
      "no_speech_prob": 0.013216602616012096
    },
    {
      "id": 144,
      "seek": 90300,
      "start": 2231.0099755859374,
      "end": 2235.88998046875,
      "text": " beschreibst, bedeutet ja nur, dass man in den Texten etwas hinschreibt. Das ist kein mentales",
      "tokens": [
        50994,
        17498,
        38606,
        372,
        11,
        27018,
        2784,
        4343,
        11,
        2658,
        587,
        294,
        1441,
        18643,
        268,
        9569,
        276,
        1292,
        339,
        31174,
        13,
        2846,
        1418,
        13424,
        4973,
        279,
        51238
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26905813813209534,
      "compression_ratio": 1.580246925354004,
      "no_speech_prob": 0.013216602616012096
    },
    {
      "id": 145,
      "seek": 90300,
      "start": 2235.88998046875,
      "end": 2244.4900170898436,
      "text": " Modell. Und das ist was anderes. Darüber muss man offensichtlich, weil das wäre dann sozusagen",
      "tokens": [
        51238,
        6583,
        898,
        13,
        2719,
        1482,
        1418,
        390,
        31426,
        13,
        7803,
        12670,
        6425,
        587,
        766,
        694,
        41971,
        11,
        7689,
        1482,
        14558,
        3594,
        33762,
        51668
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26905813813209534,
      "compression_ratio": 1.580246925354004,
      "no_speech_prob": 0.013216602616012096
    },
    {
      "id": 146,
      "seek": 92908,
      "start": 2244.649990234375,
      "end": 2247.129970703125,
      "text": " die nächste Episode, die man nochmal planen könnte und machen könnte.",
      "tokens": [
        50372,
        978,
        30661,
        19882,
        11,
        978,
        587,
        26509,
        1393,
        268,
        17646,
        674,
        7069,
        17646,
        13,
        50496
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26757267117500305,
      "compression_ratio": 1.4686192274093628,
      "no_speech_prob": 0.20652645826339722
    },
    {
      "id": 147,
      "seek": 92908,
      "start": 2247.129970703125,
      "end": 2256.2500268554686,
      "text": " Absolut. Der Begriff mentales Modell ist da vielleicht auch ein bisschen schwierig. Aber",
      "tokens": [
        50496,
        5813,
        2308,
        13,
        5618,
        879,
        32783,
        3074,
        4229,
        6583,
        898,
        1418,
        1120,
        12547,
        2168,
        1343,
        10763,
        37845,
        13,
        5992,
        50952
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26757267117500305,
      "compression_ratio": 1.4686192274093628,
      "no_speech_prob": 0.20652645826339722
    },
    {
      "id": 148,
      "seek": 92908,
      "start": 2256.2500268554686,
      "end": 2262.7699853515624,
      "text": " ich habe halt gemerkt, da ist was dran an diesem mentalen Modell. Und wie du ja gesagt hast,",
      "tokens": [
        50952,
        1893,
        6015,
        12479,
        7173,
        49015,
        11,
        1120,
        1418,
        390,
        32801,
        364,
        10975,
        4973,
        268,
        6583,
        898,
        13,
        2719,
        3355,
        1581,
        2784,
        12260,
        6581,
        11,
        51278
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26757267117500305,
      "compression_ratio": 1.4686192274093628,
      "no_speech_prob": 0.20652645826339722
    },
    {
      "id": 149,
      "seek": 92908,
      "start": 2262.7699853515624,
      "end": 2269.569973144531,
      "text": " die KI nimmt Text und produziert Text. Das heißt, man muss irgendwo, wenn man es schaffen will,",
      "tokens": [
        51278,
        978,
        47261,
        38891,
        18643,
        674,
        28093,
        4859,
        18643,
        13,
        2846,
        13139,
        11,
        587,
        6425,
        40865,
        11,
        4797,
        587,
        785,
        30888,
        486,
        11,
        51618
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26757267117500305,
      "compression_ratio": 1.4686192274093628,
      "no_speech_prob": 0.20652645826339722
    },
    {
      "id": 150,
      "seek": 95416,
      "start": 2269.569973144531,
      "end": 2277.2500268554686,
      "text": " im Text abbilden. Aber das Faszinierende daran ist auch, wenn man dieses Konzept betrachtet",
      "tokens": [
        50364,
        566,
        18643,
        410,
        16248,
        268,
        13,
        5992,
        1482,
        479,
        19601,
        259,
        811,
        5445,
        24520,
        1418,
        2168,
        11,
        4797,
        587,
        12113,
        12718,
        32082,
        778,
        81,
        48833,
        50748
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23836436867713928,
      "compression_ratio": 1.588477373123169,
      "no_speech_prob": 0.08498796075582504
    },
    {
      "id": 151,
      "seek": 95416,
      "start": 2277.2500268554686,
      "end": 2284.370021972656,
      "text": " und jetzt zum Beispiel Legacy Modernisation machen möchte mit der KI und sagt, warum ist",
      "tokens": [
        50748,
        674,
        4354,
        5919,
        13772,
        42838,
        19814,
        7623,
        7069,
        14570,
        2194,
        1163,
        47261,
        674,
        15764,
        11,
        24331,
        1418,
        51104
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23836436867713928,
      "compression_ratio": 1.588477373123169,
      "no_speech_prob": 0.08498796075582504
    },
    {
      "id": 152,
      "seek": 95416,
      "start": 2284.370021972656,
      "end": 2290.370021972656,
      "text": " dieser Code eigentlich Legacy? Weil die Entwickler sagen alle, muss neu geschrieben werden. Warum muss",
      "tokens": [
        51104,
        9053,
        15549,
        10926,
        42838,
        30,
        18665,
        978,
        29397,
        1918,
        8360,
        5430,
        11,
        6425,
        22510,
        47397,
        4604,
        13,
        25541,
        6425,
        51404
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23836436867713928,
      "compression_ratio": 1.588477373123169,
      "no_speech_prob": 0.08498796075582504
    },
    {
      "id": 153,
      "seek": 95416,
      "start": 2290.370021972656,
      "end": 2296.41,
      "text": " er neu geschrieben werden? Weil das mentale Modell fehlt, weil das Warum fehlt. Und wenn ich dann mit",
      "tokens": [
        51404,
        1189,
        22510,
        47397,
        4604,
        30,
        18665,
        1482,
        3074,
        1220,
        6583,
        898,
        47994,
        11,
        7689,
        1482,
        25541,
        47994,
        13,
        2719,
        4797,
        1893,
        3594,
        2194,
        51706
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23836436867713928,
      "compression_ratio": 1.588477373123169,
      "no_speech_prob": 0.08498796075582504
    },
    {
      "id": 154,
      "seek": 98100,
      "start": 2296.41,
      "end": 2302.88998046875,
      "text": " der KI es umschreiben lasse, in eine moderne Sprache, habe ich dann das mentale Modell. Kann",
      "tokens": [
        50364,
        1163,
        47261,
        785,
        1105,
        6145,
        25946,
        2439,
        405,
        11,
        294,
        3018,
        10494,
        716,
        7702,
        6000,
        11,
        6015,
        1893,
        3594,
        1482,
        3074,
        1220,
        6583,
        898,
        13,
        29074,
        50688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.290771484375,
      "compression_ratio": 1.6546763181686401,
      "no_speech_prob": 0.05659312382340431
    },
    {
      "id": 155,
      "seek": 98100,
      "start": 2302.88998046875,
      "end": 2309.4900170898436,
      "text": " dann der Entwickler weiterarbeiten? Das finde ich so faszinierend. Eine interessante Erkenntnis.",
      "tokens": [
        50688,
        3594,
        1163,
        29397,
        1918,
        8988,
        43918,
        30,
        2846,
        17841,
        1893,
        370,
        283,
        19601,
        259,
        811,
        521,
        13,
        17664,
        24372,
        3300,
        41838,
        10661,
        13,
        51018
      ],
      "temperature": 0.0,
      "avg_logprob": -0.290771484375,
      "compression_ratio": 1.6546763181686401,
      "no_speech_prob": 0.05659312382340431
    },
    {
      "id": 156,
      "seek": 98100,
      "start": 2309.4900170898436,
      "end": 2316.0500146484374,
      "text": " Meiner Ansicht nach ist das Grundproblem dabei, dass man an der Stelle nicht wahrhaben will,",
      "tokens": [
        51018,
        1923,
        4564,
        14590,
        1405,
        5168,
        1418,
        1482,
        13941,
        47419,
        14967,
        11,
        2658,
        587,
        364,
        1163,
        26629,
        1979,
        21628,
        7821,
        268,
        486,
        11,
        51346
      ],
      "temperature": 0.0,
      "avg_logprob": -0.290771484375,
      "compression_ratio": 1.6546763181686401,
      "no_speech_prob": 0.05659312382340431
    },
    {
      "id": 157,
      "seek": 98100,
      "start": 2316.0500146484374,
      "end": 2321.690029296875,
      "text": " dass Schriftverentwicklung ein sozialer Prozess ist. Und das ist das, was ich glaubte,",
      "tokens": [
        51346,
        2658,
        2065,
        35742,
        331,
        317,
        16038,
        17850,
        1343,
        31541,
        260,
        1705,
        37575,
        1418,
        13,
        2719,
        1482,
        1418,
        1482,
        11,
        390,
        1893,
        23210,
        975,
        11,
        51628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.290771484375,
      "compression_ratio": 1.6546763181686401,
      "no_speech_prob": 0.05659312382340431
    },
    {
      "id": 158,
      "seek": 98100,
      "start": 2321.690029296875,
      "end": 2325.8100244140624,
      "text": " was man aus diesem Paper, was ich eben nicht gelesen habe, vielleicht rauslesen kann. Dass",
      "tokens": [
        51628,
        390,
        587,
        3437,
        10975,
        24990,
        11,
        390,
        1893,
        11375,
        1979,
        4087,
        17403,
        6015,
        11,
        12547,
        17202,
        904,
        268,
        4028,
        13,
        22306,
        51834
      ],
      "temperature": 0.0,
      "avg_logprob": -0.290771484375,
      "compression_ratio": 1.6546763181686401,
      "no_speech_prob": 0.05659312382340431
    },
    {
      "id": 159,
      "seek": 101040,
      "start": 2325.9699975585936,
      "end": 2331.329982910156,
      "text": " dieses soziale Modell im Code Ausdruck findet. Und das ist dann halt trivial, wenn ich eine AI",
      "tokens": [
        50372,
        12113,
        31541,
        68,
        6583,
        898,
        566,
        15549,
        9039,
        67,
        8161,
        27752,
        13,
        2719,
        1482,
        1418,
        3594,
        12479,
        26703,
        11,
        4797,
        1893,
        3018,
        7318,
        50640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2978630065917969,
      "compression_ratio": 1.7651515007019043,
      "no_speech_prob": 0.009856331162154675
    },
    {
      "id": 160,
      "seek": 101040,
      "start": 2331.329982910156,
      "end": 2335.9699975585936,
      "text": " ansetze, dass das dieses mentale Modell und den sozialen Prozess nicht abbildet. Und dann ist",
      "tokens": [
        50640,
        1567,
        302,
        1381,
        11,
        2658,
        1482,
        12113,
        3074,
        1220,
        6583,
        898,
        674,
        1441,
        31541,
        268,
        1705,
        37575,
        1979,
        410,
        16248,
        302,
        13,
        2719,
        3594,
        1418,
        50872
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2978630065917969,
      "compression_ratio": 1.7651515007019043,
      "no_speech_prob": 0.009856331162154675
    },
    {
      "id": 161,
      "seek": 101040,
      "start": 2335.9699975585936,
      "end": 2343.2900048828124,
      "text": " halt Schluss. Das ist eine Vielkonzeption. Und das ist eine von den Sachen, die mich so ärgert. In",
      "tokens": [
        50872,
        12479,
        36573,
        13,
        2846,
        1418,
        3018,
        35931,
        18295,
        32082,
        313,
        13,
        2719,
        1482,
        1418,
        3018,
        2957,
        1441,
        26074,
        11,
        978,
        6031,
        370,
        3775,
        70,
        911,
        13,
        682,
        51238
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2978630065917969,
      "compression_ratio": 1.7651515007019043,
      "no_speech_prob": 0.009856331162154675
    },
    {
      "id": 162,
      "seek": 101040,
      "start": 2343.2900048828124,
      "end": 2347.8100244140624,
      "text": " diesem AI-Bereich ist eine Vielkonzeption über Softwareentwicklung. Softwareentwicklung ist",
      "tokens": [
        51238,
        10975,
        7318,
        12,
        33,
        323,
        480,
        1418,
        3018,
        35931,
        18295,
        32082,
        313,
        4502,
        27428,
        317,
        16038,
        17850,
        13,
        27428,
        317,
        16038,
        17850,
        1418,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2978630065917969,
      "compression_ratio": 1.7651515007019043,
      "no_speech_prob": 0.009856331162154675
    },
    {
      "id": 163,
      "seek": 101040,
      "start": 2347.8100244140624,
      "end": 2353.170009765625,
      "text": " ein sozialer Prozess. Und die fundamentalen Schwierigkeiten sind meiner Ansicht nach",
      "tokens": [
        51464,
        1343,
        31541,
        260,
        1705,
        37575,
        13,
        2719,
        978,
        8088,
        268,
        17576,
        811,
        37545,
        3290,
        20529,
        14590,
        1405,
        5168,
        51732
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2978630065917969,
      "compression_ratio": 1.7651515007019043,
      "no_speech_prob": 0.009856331162154675
    },
    {
      "id": 164,
      "seek": 103776,
      "start": 2353.2499658203124,
      "end": 2356.649990234375,
      "text": " sozial. Und das wird nicht dadurch besser, dass ich eine Maschine da reinsetze. Aber",
      "tokens": [
        50368,
        31541,
        13,
        2719,
        1482,
        4578,
        1979,
        35472,
        18021,
        11,
        2658,
        1893,
        3018,
        5224,
        36675,
        1120,
        47200,
        302,
        1381,
        13,
        5992,
        50538
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3022310733795166,
      "compression_ratio": 1.4895397424697876,
      "no_speech_prob": 0.10804307460784912
    },
    {
      "id": 165,
      "seek": 103776,
      "start": 2356.649990234375,
      "end": 2363.41,
      "text": " anderes Thema. Wir sollten offensichtlich noch eine Episode mindestens planen. Wollen wir",
      "tokens": [
        50538,
        31426,
        16306,
        13,
        4347,
        29096,
        766,
        694,
        41971,
        3514,
        3018,
        19882,
        1575,
        42624,
        1393,
        268,
        13,
        343,
        26669,
        1987,
        50876
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3022310733795166,
      "compression_ratio": 1.4895397424697876,
      "no_speech_prob": 0.10804307460784912
    },
    {
      "id": 166,
      "seek": 103776,
      "start": 2363.41,
      "end": 2371.0900537109374,
      "text": " sprechen, wie du es auf die Webseite bekommen hast? Ich würde ganz gern jetzt schon mal auf",
      "tokens": [
        50876,
        27853,
        11,
        3355,
        1581,
        785,
        2501,
        978,
        9573,
        405,
        642,
        19256,
        6581,
        30,
        3141,
        11942,
        6312,
        38531,
        4354,
        4981,
        2806,
        2501,
        51260
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3022310733795166,
      "compression_ratio": 1.4895397424697876,
      "no_speech_prob": 0.10804307460784912
    },
    {
      "id": 167,
      "seek": 103776,
      "start": 2371.0900537109374,
      "end": 2377.8100244140624,
      "text": " die andere Idee eingehen. Wegen dem sozialen Prozess. Weil ich das so faszinierend fand.",
      "tokens": [
        51260,
        978,
        10490,
        32651,
        30061,
        2932,
        13,
        492,
        1766,
        1371,
        31541,
        268,
        1705,
        37575,
        13,
        18665,
        1893,
        1482,
        370,
        283,
        19601,
        259,
        811,
        521,
        38138,
        13,
        51596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3022310733795166,
      "compression_ratio": 1.4895397424697876,
      "no_speech_prob": 0.10804307460784912
    },
    {
      "id": 168,
      "seek": 106240,
      "start": 2378.5299951171874,
      "end": 2387.170009765625,
      "text": " Ich kam dann irgendwie auf die Idee, dass man ja auch mal die Folgen nach Gast sortieren könnte",
      "tokens": [
        50400,
        3141,
        9727,
        3594,
        20759,
        2501,
        978,
        32651,
        11,
        2658,
        587,
        2784,
        2168,
        2806,
        978,
        15255,
        1766,
        5168,
        31988,
        1333,
        5695,
        17646,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28194984793663025,
      "compression_ratio": 1.591489315032959,
      "no_speech_prob": 0.11415708810091019
    },
    {
      "id": 169,
      "seek": 106240,
      "start": 2387.170009765625,
      "end": 2392.5700341796874,
      "text": " und auf die Seite bringen könnte. War irgendwie so eine Idee. Könnte man doch mal machen. Und",
      "tokens": [
        50832,
        674,
        2501,
        978,
        19748,
        27519,
        17646,
        13,
        3630,
        20759,
        370,
        3018,
        32651,
        13,
        29077,
        9358,
        587,
        9243,
        2806,
        7069,
        13,
        2719,
        51102
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28194984793663025,
      "compression_ratio": 1.591489315032959,
      "no_speech_prob": 0.11415708810091019
    },
    {
      "id": 170,
      "seek": 106240,
      "start": 2392.5700341796874,
      "end": 2397.5700341796874,
      "text": " die Idee dabei war halt, also ich finde diese verschiedenen Ebenen, wie man die KI verwendet.",
      "tokens": [
        51102,
        978,
        32651,
        14967,
        1516,
        12479,
        11,
        611,
        1893,
        17841,
        6705,
        41043,
        462,
        1799,
        268,
        11,
        3355,
        587,
        978,
        47261,
        1306,
        20128,
        302,
        13,
        51352
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28194984793663025,
      "compression_ratio": 1.591489315032959,
      "no_speech_prob": 0.11415708810091019
    },
    {
      "id": 171,
      "seek": 106240,
      "start": 2397.5700341796874,
      "end": 2403.2900048828124,
      "text": " Und wir haben jetzt zum Beispiel bei dem Transkriptionsprozess habe ich mit KI gekodet.",
      "tokens": [
        51352,
        2719,
        1987,
        3084,
        4354,
        5919,
        13772,
        4643,
        1371,
        6531,
        74,
        470,
        9799,
        4318,
        37575,
        6015,
        1893,
        2194,
        47261,
        14037,
        378,
        302,
        13,
        51638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28194984793663025,
      "compression_ratio": 1.591489315032959,
      "no_speech_prob": 0.11415708810091019
    },
    {
      "id": 172,
      "seek": 108788,
      "start": 2403.88998046875,
      "end": 2411.0900537109374,
      "text": " Wir haben mit KI machen wir ein Review. Und in dem Prozess, die Zusammenfassung,",
      "tokens": [
        50394,
        4347,
        3084,
        2194,
        47261,
        7069,
        1987,
        1343,
        19954,
        13,
        2719,
        294,
        1371,
        1705,
        37575,
        11,
        978,
        29442,
        69,
        40828,
        11,
        50754
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2957589328289032,
      "compression_ratio": 1.503999948501587,
      "no_speech_prob": 0.015892548486590385
    },
    {
      "id": 173,
      "seek": 108788,
      "start": 2411.0900537109374,
      "end": 2415.690029296875,
      "text": " läuft ja selbst auch mit KI. Müssen wir gleich auch dran denken. Da gibt es ja jetzt ein aktuelles",
      "tokens": [
        50754,
        31807,
        2784,
        13053,
        2168,
        2194,
        47261,
        13,
        21295,
        8718,
        1987,
        11699,
        2168,
        32801,
        28780,
        13,
        3933,
        6089,
        785,
        2784,
        4354,
        1343,
        36267,
        279,
        50984
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2957589328289032,
      "compression_ratio": 1.503999948501587,
      "no_speech_prob": 0.015892548486590385
    },
    {
      "id": 174,
      "seek": 108788,
      "start": 2415.690029296875,
      "end": 2423.2900048828124,
      "text": " Problem, was wir haben. Und so habe ich dann eben auch gedacht, Mensch, wir haben jetzt diese ganzen",
      "tokens": [
        50984,
        11676,
        11,
        390,
        1987,
        3084,
        13,
        2719,
        370,
        6015,
        1893,
        3594,
        11375,
        2168,
        33296,
        11,
        27773,
        11,
        1987,
        3084,
        4354,
        6705,
        23966,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2957589328289032,
      "compression_ratio": 1.503999948501587,
      "no_speech_prob": 0.015892548486590385
    },
    {
      "id": 175,
      "seek": 108788,
      "start": 2423.2900048828124,
      "end": 2430.5299951171874,
      "text": " 180 Folgen. Wow, da hat sich was angesammelt. Und da ist überall irgendwo unstrukturiert der",
      "tokens": [
        51364,
        11971,
        15255,
        1766,
        13,
        3153,
        11,
        1120,
        2385,
        3041,
        390,
        31138,
        5136,
        2018,
        13,
        2719,
        1120,
        1418,
        38035,
        40865,
        18799,
        31543,
        4859,
        1163,
        51726
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2957589328289032,
      "compression_ratio": 1.503999948501587,
      "no_speech_prob": 0.015892548486590385
    },
    {
      "id": 176,
      "seek": 111512,
      "start": 2430.5299951171874,
      "end": 2438.8100244140624,
      "text": " Gast mit genannt. Und das könnte man ja jetzt mit der KI rausziehen. Und ich habe einfach mal die",
      "tokens": [
        50364,
        31988,
        2194,
        1049,
        39878,
        13,
        2719,
        1482,
        17646,
        587,
        2784,
        4354,
        2194,
        1163,
        47261,
        17202,
        28768,
        13,
        2719,
        1893,
        6015,
        7281,
        2806,
        978,
        50778
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2622023820877075,
      "compression_ratio": 1.6294642686843872,
      "no_speech_prob": 0.09524235874414444
    },
    {
      "id": 177,
      "seek": 111512,
      "start": 2438.8100244140624,
      "end": 2444.2900048828124,
      "text": " KI drauf angesetzt. Habe gesagt, guck mal, hier ist das Repository und mach dir mal Gedanken,",
      "tokens": [
        50778,
        47261,
        22763,
        31138,
        3524,
        13,
        389,
        4488,
        12260,
        11,
        695,
        547,
        2806,
        11,
        3296,
        1418,
        1482,
        3696,
        9598,
        827,
        674,
        2246,
        4746,
        2806,
        44612,
        11,
        51052
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2622023820877075,
      "compression_ratio": 1.6294642686843872,
      "no_speech_prob": 0.09524235874414444
    },
    {
      "id": 178,
      "seek": 111512,
      "start": 2444.2900048828124,
      "end": 2450.93001953125,
      "text": " wie könnte man das rausziehen. Iterier mal drüber. Da war es schon mal faszinierend,",
      "tokens": [
        51052,
        3355,
        17646,
        587,
        1482,
        17202,
        28768,
        13,
        286,
        391,
        811,
        2806,
        1224,
        12670,
        13,
        3933,
        1516,
        785,
        4981,
        2806,
        283,
        19601,
        259,
        811,
        521,
        11,
        51384
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2622023820877075,
      "compression_ratio": 1.6294642686843872,
      "no_speech_prob": 0.09524235874414444
    },
    {
      "id": 179,
      "seek": 111512,
      "start": 2450.93001953125,
      "end": 2455.4899560546874,
      "text": " weil ich gesagt habe, du KI kannst du bitte drüber iterieren. Und die KI hat gesagt,",
      "tokens": [
        51384,
        7689,
        1893,
        12260,
        6015,
        11,
        1581,
        47261,
        20853,
        1581,
        23231,
        1224,
        12670,
        17138,
        5695,
        13,
        2719,
        978,
        47261,
        2385,
        12260,
        11,
        51612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2622023820877075,
      "compression_ratio": 1.6294642686843872,
      "no_speech_prob": 0.09524235874414444
    },
    {
      "id": 180,
      "seek": 114008,
      "start": 2455.609951171875,
      "end": 2462.4899560546874,
      "text": " ich mache mir mal einen Plan. Und der Plan sind eigentlich maximal fünf Schritte bei Claude.",
      "tokens": [
        50370,
        1893,
        28289,
        3149,
        2806,
        4891,
        8112,
        13,
        2719,
        1163,
        8112,
        3290,
        10926,
        49336,
        28723,
        46191,
        9786,
        4643,
        12947,
        2303,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2818862497806549,
      "compression_ratio": 1.8480392694473267,
      "no_speech_prob": 0.03787699341773987
    },
    {
      "id": 181,
      "seek": 114008,
      "start": 2462.4899560546874,
      "end": 2469.5299951171874,
      "text": " Das heißt, erster Schritt, erste Episode, Gast rausziehen. Zweite Episode, Gast rausziehen.",
      "tokens": [
        50714,
        2846,
        13139,
        11,
        1189,
        3120,
        33062,
        11,
        20951,
        19882,
        11,
        31988,
        17202,
        28768,
        13,
        32475,
        642,
        19882,
        11,
        31988,
        17202,
        28768,
        13,
        51066
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2818862497806549,
      "compression_ratio": 1.8480392694473267,
      "no_speech_prob": 0.03787699341773987
    },
    {
      "id": 182,
      "seek": 114008,
      "start": 2469.5299951171874,
      "end": 2474.690029296875,
      "text": " Dritte Episode, Gast rausziehen. Vierte Episode, Gast rausziehen. Aus den anderen Episoden,",
      "tokens": [
        51066,
        2491,
        9786,
        19882,
        11,
        31988,
        17202,
        28768,
        13,
        691,
        23123,
        19882,
        11,
        31988,
        17202,
        28768,
        13,
        9039,
        1441,
        11122,
        9970,
        271,
        33482,
        11,
        51324
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2818862497806549,
      "compression_ratio": 1.8480392694473267,
      "no_speech_prob": 0.03787699341773987
    },
    {
      "id": 183,
      "seek": 114008,
      "start": 2474.690029296875,
      "end": 2482.0500146484374,
      "text": " Gast rausziehen. Und genau so hat das Modell dann gearbeitet. Mit den ersten vier Episoden ist gut",
      "tokens": [
        51324,
        31988,
        17202,
        28768,
        13,
        2719,
        12535,
        370,
        2385,
        1482,
        6583,
        898,
        3594,
        7394,
        32401,
        13,
        10821,
        1441,
        17324,
        17634,
        9970,
        271,
        33482,
        1418,
        5228,
        51692
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2818862497806549,
      "compression_ratio": 1.8480392694473267,
      "no_speech_prob": 0.03787699341773987
    },
    {
      "id": 184,
      "seek": 116664,
      "start": 2482.0500146484374,
      "end": 2489.2499658203124,
      "text": " klargekommen. Und dann ist es abgedriftet und konnte nicht mehr iterieren. Da habe ich dann",
      "tokens": [
        50364,
        14743,
        432,
        13675,
        13,
        2719,
        3594,
        1418,
        785,
        410,
        3004,
        35742,
        302,
        674,
        24058,
        1979,
        5417,
        17138,
        5695,
        13,
        3933,
        6015,
        1893,
        3594,
        50724
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2263011932373047,
      "compression_ratio": 1.5964125394821167,
      "no_speech_prob": 0.0646056979894638
    },
    {
      "id": 185,
      "seek": 116664,
      "start": 2489.2499658203124,
      "end": 2495.0500146484374,
      "text": " gemerkt, es könnte ja ein Programm schreiben, ein Skript, was iteriert. Und dann in dem Skript",
      "tokens": [
        50724,
        7173,
        49015,
        11,
        785,
        17646,
        2784,
        1343,
        48244,
        48546,
        11,
        1343,
        7324,
        470,
        662,
        11,
        390,
        17138,
        4859,
        13,
        2719,
        3594,
        294,
        1371,
        7324,
        470,
        662,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2263011932373047,
      "compression_ratio": 1.5964125394821167,
      "no_speech_prob": 0.0646056979894638
    },
    {
      "id": 186,
      "seek": 116664,
      "start": 2495.0500146484374,
      "end": 2499.0500146484374,
      "text": " habe ich aber die KI wieder nicht mehr zur Verfügung, um das zu extrahieren. Das war",
      "tokens": [
        51014,
        6015,
        1893,
        4340,
        978,
        47261,
        6216,
        1979,
        5417,
        7147,
        43026,
        11,
        1105,
        1482,
        2164,
        2857,
        71,
        5695,
        13,
        2846,
        1516,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2263011932373047,
      "compression_ratio": 1.5964125394821167,
      "no_speech_prob": 0.0646056979894638
    },
    {
      "id": 187,
      "seek": 116664,
      "start": 2499.0500146484374,
      "end": 2504.2499658203124,
      "text": " so ein Ding, was ich dann da angegangen bin. Aber viel faszinierender fand ich es,",
      "tokens": [
        51214,
        370,
        1343,
        20558,
        11,
        390,
        1893,
        3594,
        1120,
        15495,
        47152,
        5171,
        13,
        5992,
        5891,
        283,
        19601,
        259,
        811,
        3216,
        38138,
        1893,
        785,
        11,
        51474
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2263011932373047,
      "compression_ratio": 1.5964125394821167,
      "no_speech_prob": 0.0646056979894638
    },
    {
      "id": 188,
      "seek": 118884,
      "start": 2504.2499658203124,
      "end": 2514.93001953125,
      "text": " als das Ganze irgendwo stand, mehr oder weniger fertig war. Ja, da waren viele Fehler drin. Aber",
      "tokens": [
        50364,
        3907,
        1482,
        35206,
        40865,
        1463,
        11,
        5417,
        4513,
        23224,
        31362,
        1516,
        13,
        3530,
        11,
        1120,
        11931,
        9693,
        48101,
        24534,
        13,
        5992,
        50898
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29461511969566345,
      "compression_ratio": 1.4517766237258911,
      "no_speech_prob": 0.06849519163370132
    },
    {
      "id": 189,
      "seek": 118884,
      "start": 2514.93001953125,
      "end": 2521.93001953125,
      "text": " du dann drüber geguckt hast und ich irgendwie gemerkt habe, dass du mir freundlich sagen",
      "tokens": [
        50898,
        1581,
        3594,
        1224,
        12670,
        23982,
        47800,
        6581,
        674,
        1893,
        20759,
        7173,
        49015,
        6015,
        11,
        2658,
        1581,
        3149,
        2130,
        997,
        1739,
        8360,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29461511969566345,
      "compression_ratio": 1.4517766237258911,
      "no_speech_prob": 0.06849519163370132
    },
    {
      "id": 190,
      "seek": 118884,
      "start": 2521.93001953125,
      "end": 2528.88998046875,
      "text": " wolltest, Ralf, das was da rausgekommen ist, das ist totaler Käse, weil das passt überhaupt nicht",
      "tokens": [
        51248,
        8181,
        31636,
        11,
        497,
        1678,
        11,
        1482,
        390,
        1120,
        17202,
        432,
        13675,
        1418,
        11,
        1482,
        1418,
        3217,
        260,
        40502,
        405,
        11,
        7689,
        1482,
        37154,
        20023,
        1979,
        51596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29461511969566345,
      "compression_ratio": 1.4517766237258911,
      "no_speech_prob": 0.06849519163370132
    },
    {
      "id": 191,
      "seek": 121348,
      "start": 2528.97005859375,
      "end": 2539.4500390625,
      "text": " zur bestehenden Architektur. War so, richtig? Also genau, soll ich kurz sozusagen ausholen.",
      "tokens": [
        50368,
        7147,
        1151,
        13301,
        8896,
        10984,
        642,
        2320,
        374,
        13,
        3630,
        370,
        11,
        13129,
        30,
        2743,
        12535,
        11,
        7114,
        1893,
        20465,
        33762,
        257,
        1498,
        11940,
        13,
        50892
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40771040320396423,
      "compression_ratio": 1.4897959232330322,
      "no_speech_prob": 0.25025591254234314
    },
    {
      "id": 192,
      "seek": 121348,
      "start": 2539.4500390625,
      "end": 2545.5700341796874,
      "text": " Also die eine Sache war halt, wir haben ja eine Jacke-basierte Webseite. Das heißt,",
      "tokens": [
        50892,
        2743,
        978,
        3018,
        31452,
        1516,
        12479,
        11,
        1987,
        3084,
        2784,
        3018,
        9538,
        330,
        12,
        16342,
        23123,
        9573,
        405,
        642,
        13,
        2846,
        13139,
        11,
        51198
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40771040320396423,
      "compression_ratio": 1.4897959232330322,
      "no_speech_prob": 0.25025591254234314
    },
    {
      "id": 193,
      "seek": 121348,
      "start": 2545.5700341796874,
      "end": 2549.84994140625,
      "text": " wir haben im Prinzip Markdown-Files, die hat irgendwie gerendert werden mit Ruby-Skripten.",
      "tokens": [
        51198,
        1987,
        3084,
        566,
        47572,
        3934,
        5093,
        12,
        37,
        4680,
        11,
        978,
        2385,
        20759,
        5713,
        521,
        911,
        4604,
        2194,
        19907,
        12,
        50,
        74,
        470,
        662,
        268,
        13,
        51412
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40771040320396423,
      "compression_ratio": 1.4897959232330322,
      "no_speech_prob": 0.25025591254234314
    },
    {
      "id": 194,
      "seek": 121348,
      "start": 2549.84994140625,
      "end": 2556.129970703125,
      "text": " Und das macht eben, in Produktion macht das halt GitHub-Pages. Und im Prinzip ist das ja ein CMS.",
      "tokens": [
        51412,
        2719,
        1482,
        10857,
        11375,
        11,
        294,
        11793,
        9780,
        10857,
        1482,
        12479,
        23331,
        12,
        47,
        1660,
        13,
        2719,
        566,
        47572,
        1418,
        1482,
        2784,
        1343,
        33124,
        13,
        51726
      ],
      "temperature": 0.0,
      "avg_logprob": -0.40771040320396423,
      "compression_ratio": 1.4897959232330322,
      "no_speech_prob": 0.25025591254234314
    },
    {
      "id": 195,
      "seek": 124072,
      "start": 2556.2900048828124,
      "end": 2563.0900537109374,
      "text": " Das heißt, ich habe Content, der ist als MD-Files da und dann wird er irgendwie gerendert. Und was",
      "tokens": [
        50372,
        2846,
        13139,
        11,
        1893,
        6015,
        30078,
        11,
        1163,
        1418,
        3907,
        22521,
        12,
        37,
        4680,
        1120,
        674,
        3594,
        4578,
        1189,
        20759,
        5713,
        521,
        911,
        13,
        2719,
        390,
        50712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34946078062057495,
      "compression_ratio": 1.4904942512512207,
      "no_speech_prob": 0.012809102423489094
    },
    {
      "id": 196,
      "seek": 124072,
      "start": 2563.0900537109374,
      "end": 2568.84994140625,
      "text": " jetzt irgendwie rausgekommen ist, in deinem Fall war eine MD-Seite, die aber in Wirklichkeit HTML",
      "tokens": [
        50712,
        4354,
        20759,
        17202,
        432,
        13675,
        1418,
        11,
        294,
        25641,
        443,
        7465,
        1516,
        3018,
        22521,
        12,
        10637,
        642,
        11,
        978,
        4340,
        294,
        4347,
        9056,
        9238,
        17995,
        51000
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34946078062057495,
      "compression_ratio": 1.4904942512512207,
      "no_speech_prob": 0.012809102423489094
    },
    {
      "id": 197,
      "seek": 124072,
      "start": 2568.84994140625,
      "end": 2575.5299951171874,
      "text": " hatte. Also in Markdown kann ich HTML einbetten und JavaScript-Code. Und wo dann im Prinzip alle",
      "tokens": [
        51000,
        13299,
        13,
        2743,
        294,
        3934,
        5093,
        4028,
        1893,
        17995,
        1343,
        10671,
        1147,
        674,
        15778,
        12,
        34,
        1429,
        13,
        2719,
        6020,
        3594,
        566,
        47572,
        5430,
        51334
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34946078062057495,
      "compression_ratio": 1.4904942512512207,
      "no_speech_prob": 0.012809102423489094
    },
    {
      "id": 198,
      "seek": 124072,
      "start": 2575.5299951171874,
      "end": 2581.3699609375,
      "text": " Gäste rausgesammelt wurden aus, ich weiß nicht, irgendeiner Datenquelle, YAML-File oder so. Und",
      "tokens": [
        51334,
        460,
        737,
        2941,
        17202,
        2880,
        5136,
        2018,
        21105,
        3437,
        11,
        1893,
        13385,
        1979,
        11,
        3418,
        27429,
        4564,
        31126,
        32743,
        11,
        398,
        2865,
        43,
        12,
        37,
        794,
        4513,
        370,
        13,
        2719,
        51626
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34946078062057495,
      "compression_ratio": 1.4904942512512207,
      "no_speech_prob": 0.012809102423489094
    },
    {
      "id": 199,
      "seek": 126596,
      "start": 2581.3699609375,
      "end": 2589.41,
      "text": " wenn ich dann etwas gesucht habe, hat das halt der JavaScript-Code auf dem Client gemacht. Das",
      "tokens": [
        50364,
        4797,
        1893,
        3594,
        9569,
        5019,
        10084,
        6015,
        11,
        2385,
        1482,
        12479,
        1163,
        15778,
        12,
        34,
        1429,
        2501,
        1371,
        2033,
        1196,
        12293,
        13,
        2846,
        50766
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27065616846084595,
      "compression_ratio": 1.4979591369628906,
      "no_speech_prob": 0.026305872946977615
    },
    {
      "id": 200,
      "seek": 126596,
      "start": 2589.41,
      "end": 2592.7299462890624,
      "text": " ist wahrscheinlich die einzige Möglichkeit, wie man das hinbekommen kann. Also meine,",
      "tokens": [
        50766,
        1418,
        30957,
        978,
        47743,
        30662,
        11,
        3355,
        587,
        1482,
        14102,
        650,
        13675,
        4028,
        13,
        2743,
        10946,
        11,
        50932
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27065616846084595,
      "compression_ratio": 1.4979591369628906,
      "no_speech_prob": 0.026305872946977615
    },
    {
      "id": 201,
      "seek": 126596,
      "start": 2592.7299462890624,
      "end": 2597.690029296875,
      "text": " wie soll ich sagen, meine Intuition wäre halt, ich will eigentlich irgendwo einen Server haben,",
      "tokens": [
        50932,
        3355,
        7114,
        1893,
        8360,
        11,
        10946,
        5681,
        19080,
        14558,
        12479,
        11,
        1893,
        486,
        10926,
        40865,
        4891,
        25684,
        3084,
        11,
        51180
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27065616846084595,
      "compression_ratio": 1.4979591369628906,
      "no_speech_prob": 0.026305872946977615
    },
    {
      "id": 202,
      "seek": 126596,
      "start": 2597.690029296875,
      "end": 2601.93001953125,
      "text": " der halt sucht. Das können wir nicht, weil wir GitHub-Pages nur haben. Und deswegen ist",
      "tokens": [
        51180,
        1163,
        12479,
        1270,
        83,
        13,
        2846,
        6310,
        1987,
        1979,
        11,
        7689,
        1987,
        23331,
        12,
        47,
        1660,
        4343,
        3084,
        13,
        2719,
        26482,
        1418,
        51392
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27065616846084595,
      "compression_ratio": 1.4979591369628906,
      "no_speech_prob": 0.026305872946977615
    },
    {
      "id": 203,
      "seek": 128652,
      "start": 2601.93001953125,
      "end": 2610.690029296875,
      "text": " das wahrscheinlich der einzige Weg. Was sich da in meiner Erinnerung aber so gezeigt hat,",
      "tokens": [
        50364,
        1482,
        30957,
        1163,
        47743,
        18919,
        13,
        3027,
        3041,
        1120,
        294,
        20529,
        3300,
        19166,
        1063,
        4340,
        370,
        48661,
        2385,
        11,
        50802
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3228440582752228,
      "compression_ratio": 1.6523298025131226,
      "no_speech_prob": 0.5415931344032288
    },
    {
      "id": 204,
      "seek": 128652,
      "start": 2610.690029296875,
      "end": 2616.93001953125,
      "text": " ist, dass das so aufgepfropft wird. Also wir haben CSS-Files, wie sich das gehört. Und ich würde",
      "tokens": [
        50802,
        1418,
        11,
        2658,
        1482,
        370,
        35031,
        25302,
        1513,
        844,
        4578,
        13,
        2743,
        1987,
        3084,
        24387,
        12,
        37,
        4680,
        11,
        3355,
        3041,
        1482,
        21544,
        13,
        2719,
        1893,
        11942,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3228440582752228,
      "compression_ratio": 1.6523298025131226,
      "no_speech_prob": 0.5415931344032288
    },
    {
      "id": 205,
      "seek": 128652,
      "start": 2616.93001953125,
      "end": 2620.609951171875,
      "text": " jetzt erwarten, dass diese CSS-Sachen wiederverwendet werden. Werden halt nicht wiederverwendet,",
      "tokens": [
        51114,
        4354,
        21715,
        11719,
        11,
        2658,
        6705,
        24387,
        12,
        50,
        11646,
        6216,
        331,
        20128,
        302,
        4604,
        13,
        343,
        7783,
        268,
        12479,
        1979,
        6216,
        331,
        20128,
        302,
        11,
        51298
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3228440582752228,
      "compression_ratio": 1.6523298025131226,
      "no_speech_prob": 0.5415931344032288
    },
    {
      "id": 206,
      "seek": 128652,
      "start": 2620.609951171875,
      "end": 2624.129970703125,
      "text": " da ist halt irgendwie eigener Kram drin. Solche Sachen halt nicht. Es ist halt was Eigenes,",
      "tokens": [
        51298,
        1120,
        1418,
        12479,
        3418,
        1766,
        67,
        8699,
        10446,
        260,
        591,
        2356,
        24534,
        13,
        7026,
        1876,
        26074,
        12479,
        1979,
        13,
        2313,
        1418,
        12479,
        390,
        30586,
        279,
        11,
        51474
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3228440582752228,
      "compression_ratio": 1.6523298025131226,
      "no_speech_prob": 0.5415931344032288
    },
    {
      "id": 207,
      "seek": 128652,
      "start": 2624.129970703125,
      "end": 2628.690029296875,
      "text": " was halt getrennt ist von dem Rest, zum Beispiel in Bezug auf CSS oder nicht eigene",
      "tokens": [
        51474,
        390,
        12479,
        483,
        1095,
        580,
        1418,
        2957,
        1371,
        13094,
        11,
        5919,
        13772,
        294,
        879,
        29742,
        2501,
        24387,
        4513,
        1979,
        38549,
        51702
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3228440582752228,
      "compression_ratio": 1.6523298025131226,
      "no_speech_prob": 0.5415931344032288
    },
    {
      "id": 208,
      "seek": 131328,
      "start": 2628.690029296875,
      "end": 2635.2900048828124,
      "text": " JavaScript-Dateien oder so. Und das andere ist halt, also das hat mich dazu gerade sozusagen",
      "tokens": [
        50364,
        15778,
        12,
        35,
        473,
        1053,
        4513,
        370,
        13,
        2719,
        1482,
        10490,
        1418,
        12479,
        11,
        611,
        1482,
        2385,
        6031,
        13034,
        12117,
        33762,
        50694
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4006060063838959,
      "compression_ratio": 1.2660549879074097,
      "no_speech_prob": 0.12386978417634964
    },
    {
      "id": 209,
      "seek": 131328,
      "start": 2635.2900048828124,
      "end": 2639.2900048828124,
      "text": " angestiftet, das sozusagen deutlich zu sagen.",
      "tokens": [
        50694,
        2562,
        377,
        2008,
        302,
        11,
        1482,
        33762,
        24344,
        2164,
        8360,
        13,
        50894
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4006060063838959,
      "compression_ratio": 1.2660549879074097,
      "no_speech_prob": 0.12386978417634964
    }
  ],
  "language": "german"
}