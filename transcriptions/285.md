# Folge 285 - Experiencing Generative AI mit Oliver Zeigermann Hallo, ich bin Eberhard Wolff.

Freitags mache ich oder Lisa Moritz einen Livestream zum Thema Software-Architektur, oft zusammen mit Gästen.

Dieser Podcast ist das Audio des Streams.

Weitere Folgen, Sketchnotes und vieles mehr findet ihr unter software-architektur.tv.

Software-Architektur im Stream.

Hallo und herzlich willkommen zu einer neuen Episode von Software-Architektur im Stream.

Heute ist das Thema Experiencing Generative AI und zu Gast ist Olli Zeigermann.

Der stellt sich auch gleich noch vor.

Für euch nur noch kurz der Hinweis, wenn ihr Fragen habt, stellt sie gerne im Twitch oder YouTube Chat, je nachdem, wo ihr gerade dabei seid.

Genau, Olli, wer bist du überhaupt?

Ich bin Olli.

Olli ist toll, steht hier hinten auf meiner Tafel.

Ich dachte gerade über dich, dass du ein bisschen Eigenwerbung rein musst.

Ist offensichtlich so.

Deswegen hätte ich es mir aufsparen können.

Ich bin Olli.

Ich bin schon ganz lange Software-Entwickler und habe in den letzten mehr als einem Jahrzehnt KI-Zeug gemacht.

Das will eigentlich keiner mehr haben.

Klassisches KI-Zeug.

Alle wollen generative KI haben, weil das irgendwie cooler ist.

Das ist am Ende ein Skill, wo ich denke, da muss man jetzt nicht so ganz krass Maschinen dafür gemacht haben.

Deswegen finde ich es auch cool, wenn wir das einem größeren Publikum nahelegen, wie das alles funktioniert.

Auch weil ich denke, da kriegt man irgendwie schon hin.

Das ist auch zum großen Teil hier mein Ziel für heute, dass wir teilen, wie funktioniert das eigentlich und was muss man so wissen und dass das eigentlich alles gar nicht so schlimm ist.

Das hört sich nach einem guten Ziel an.

Ich bin gespannt, ob wir es schaffen und freue mich schon.

Bevor wir jetzt richtig einsteigen, ich wüsste gerne, wie bist du überhaupt zu dem Thema KI gekommen?

Wenn ich mich richtig erinnere, bin ich mal durch eine Buchhandlung gelaufen und habe ein JavaScript-Buch von dir gesehen.

Ich habe zu Lisa gesagt, du kannst mich fragen, was du willst.

Das ist natürlich jetzt wie so eine peinliche Vergangenheit.

Hast du auch Angst?

Nein, ich mache auch nur ein bisschen Spaß.

Das ist völlig richtig.

Im Studium fand ich KI voll cool, aber man sieht das ja vielleicht auch in den grauen Haaren.

Ich habe in den 90er-Jahren studiert und da war KI irgendwie cool, hat man gerne gemacht.

Nichts hat funktioniert, war alles irgendwie peinlich.

Deswegen habe ich danach nicht mehr KI gemacht, weil es auch keine Jobs gab.

Ich war nicht erfolgreich, alles blöd.

Deswegen bin ich einfach ganz normal Entwickler geworden und habe ganz normal entwickelt.

Was zu dem passte, was ich schon als Teenager gemacht habe, habe ich Spiele auf dem C64 entwickelt, weil ich dachte, wie cool ist das denn?

Handwerklich bin ich so mäßig geschickt und plötzlich konnte ich aber coole Dinge tun.

Deswegen musste ich eigentlich nur zurückkehren und gar nicht was machen, was jetzt für mich neu war.

Deswegen habe ich unter anderem auch mein JavaScript entwickelt.

Irgendwie hatte ich meine Zeit lang diese Tendenz, das ein bisschen früher zu machen, was alle anderen sowieso auch machen.

Das ist jetzt eventuell ein bisschen durch, aber da bin ich auch ganz froh drüber.

Deswegen habe ich für unterschiedliche Themen, die ich mir in dem Moment angeschaut habe, immer mal wieder ein Buch geschrieben.

Unter anderem auch, damit ich mir das richtig ...

Ich meine, wenn man ein Buch schreiben muss, dann will man ja nicht nur dummes Zeug schreiben.

Deswegen nachvollziehbar. Über Buchspringen sprechen wir auch gleich noch mal.

Ich bin ja nicht der einzige Buchautor.

Lisa ja auch.

Deswegen werden wir gleich das feststellen.

Dann habe ich unterschiedliche Themen durchgeackert und habe allein dadurch mein Wissen mit Qualität gesichert.

Es ist natürlich immer so, man schreibt ein Buch.

Da sieht man auch nicht das Praktische.

Aber zumindest habe ich das gemacht.

Ich habe ein Buch geschrieben über JavaScript.

Mehrere sogar. Über React zwischendrin.

Das ist eigentlich auch eine dunkle ...

Nee, React ist schon cool.

Aber eigentlich soll das keiner wissen.

Man soll ja nicht Fähigkeiten haben, die man nicht mehr ausführen möchte.

Sonst sagen Leute, du hast doch JavaScript und React gemacht.

Mach das doch noch mal.

Da sage ich, nee, ach nee.

Das ist eigentlich ein Prinzip.

Aber das kann ich ja nicht mehr aufrechterhalten.

Dann habe ich eben irgendwann auch ein Buch über Maschinen geschrieben.

Wo es so früh war, habe ich das Gefühl, dass das noch nicht so mainstreaming war.

Das war vielleicht 2018, 2019.

Deswegen ist das Buch für mich ganz gut weggegangen.

Und außerdem war das Buch, ich habe es nicht hier, sonst würde ich es zeigen, richtig schön klein.

Ich bin ein Freund des sehr, sehr schmalen.

Deswegen bin ich auch Freund deines Buches.

Wieso?

Weil da eben die Sachen drinstehen.

Ich habe eigentlich gar keinen Bock auf Lesen.

Ich habe keinen Bock auf Schreiben.

Deswegen habe ich mit einem Freund zusammen ein sehr kurzes Buch über Maschinen geschrieben.

Das jetzt tatsächlich in der dritten Auflage immer noch verfügbar ist und immer wieder aktualisiert wird.

Aber ich glaube, so langsam haben wir das Ende der Fahnenstange erreicht.

So gut.

So ist alles passiert.

Und jetzt mache ich schon seit vielen Jahren nur noch eigentlich, naja.

Also wenn ich es mir aussagen könnte, würde ich nur noch KI-Zeug machen.

Manchmal mache ich noch etwas anderes.

Ich mache das endgültig.

Und heute machen wir KI.

Und du erklärst uns allen, was Generative AI ist und wie es funktioniert.

Und das aber so, dass es auch Dreijährige verstehen können.

Dreijährige weiß ich nicht.

Aber wir versuchen es so hinzukriegen, dass es möglichst wenig spackig und zu technisch ist.

Nur weil man es kann.

Sehr gut.

Das müssen wir eben so gut hinkriegen, wie wir es hinkriegen.

Und deine Rolle ist, dass du immer, wenn du das Gefühl hast, das kann man so nicht oder was soll das denn?

Immer sofort fragen.

Und dann versuchen wir, das ein bisschen noch gerader zu ziehen.

Mach mal.

So, bevor wir damit loslegen, muss ich einmal die Kamera wechseln.

Wir haben hier unterschiedliche Medien für euch vorbereitet.

Ich hoffe, dass das hier bei uns funktioniert.

Aber wir haben es geprobt.

Also wenn es nicht funktioniert.

Wirklich irgendwie günster Pech.

Wir sehen jetzt mein Buch.

Also ich habe, auch wenn es so wirkt, ich habe Olli nicht bezahlt, Werbung für dieses Buch zu machen.

Er macht das ganz freiwillig.

Das mache ich ganz freiwillig.

Und dieses Buch, ihr seht, ich habe eine Desktop-Kamera.

Und die habe ich deswegen, weil ich dafür berüchtigt bin, dass ich sehr, sehr schlecht zeichne.

Aber mithilfe dieses Buchs auf quasi die nächste Ebene gehoben wurde.

Und jetzt zumindest so zeichne, dass die Leute, die dabei sind, wenn ich es zeichne, verstehen, was ich mache.

Alle anderen verstehen es immer noch nicht, weil das irgendwie so ein wirrer Wahnsinn ist.

Wenn ihr jetzt sagt, hey, wieso, das ist ja gar nicht, das sieht ja immer noch mistig aus, was Olli macht, trotz des Buchs.

Ihr habt ja nicht gesehen, wie es vorher aussieht.

Vorher konntest du es eigentlich immer nur noch sofort wegschmeißen.

Jetzt kannst du es irgendwie, wenn du dabei warst, eventuell benutzen.

Dieses Buch ist voll cool.

Hoffentlich hießst du da noch anders.

Ja, ich habe geheiratet im Oktober letztes Jahr.

Also ich hatte vor kurzem einen einjährigen Hochzeitstag.

Ja, danke.

Herzlichen Glückwunsch nachträglich.

Gibt es denn eine neue Auflage mit neuem Namen?

Nein, es gibt keine neue Auflage.

Dafür müsste die erste erstmal ausverkauft werden.

Also es ist gut, dass du Werbung machst.

Also ich kann es nur empfehlen.

Ich mache es einmal kurz auf.

Das geht natürlich nicht um dieses Buch.

Aber machen wir mal sowas hier.

Versteht schon.

Man hat irgendwie die Sachen, die man typischerweise immer malt.

Bei mir hört es ja schon bei Kästen und Pfeilen auf, die ich immer noch nicht richtig hin habe.

Aber wenn man ein bisschen krasser ist als ich, kriegt man sowas hier hin.

Eine Uhr fand ich gerade gut.

Aber jetzt gucken wir erstmal, was du zu KI malen kannst.

Das ist ja gut.

Ich wollte dir jetzt genug Credits geben.

Hast du gemacht.

Das habe ich jetzt gemacht.

Und ist auch, glaube ich, absolut angemessen.

Das heißt, alles, was ich euch zeige, sketche ich hier.

Also ich habe hier meine Stifte.

Ist der gute?

Ist der gute Stift.

Und ich mache das, während ich sketche, damit die Leute, die das später nur hören und nicht sehen, ungefähr auch in derselben Reihenfolge, wie ich deinem Laber folgen kann.

Wenn man so einen Zettel einfach zeigt und dazu erklärt, hilft das den Leuten, die in dem Moment nichts sehen, hilft das wenig.

Ich schalte jetzt mal kurz zurück.

Auf den OlliOlli.

Auf den OlliOlli.

Und zeige euch mal eine Anwendung, eine sehr, sehr typische Anwendung mit GenAI, damit wir überhaupt sehen, worüber sprechen wir eigentlich.

Und nachdem wir das gemacht haben, genau, da ist sie.

Perfekt.

Sehr gut.

Nachdem wir das gemacht haben, bröseln wir das ein bisschen auf und gucken, was passiert da eigentlich?

Und wieso passiert das so?

Und ist das eigentlich eine gute Idee?

Und was passiert da zum Beispiel auch nicht?

Weil es gibt noch eine ganze Reihe von Missverständnissen.

So, das ist hier auf Englisch das Ding.

Es ist ein GPT.

Das hieß so früher.

Das war irgendwie mal ein bisschen in Mode im Bereich von OpenAI.

Das ist ein bisschen aus der Mode gekommen.

Ich glaube, es gibt eher strategische Gründe dafür, dass man die eigentlich alle überprüfen müsste und so.

Technisch ist das aber voll cool.

Das heißt, du kannst dir einen eigenen, eigentlich Assistenten, eigenen Chatbot, wie immer man das nennen möchte, einen kustomisierten Prompt bauen.

Also ein eigenes Roboter-KI-System.

Und mit diesem System kannst du dann sprechen.

Und bei ChatGPT ist das so, dass du hier ein paar Fragen vorformulieren kannst, sodass Leute, die gar nicht wissen, was sollen die jetzt machen, auf einen dieser Bubbles hier klicken können.

Und dann passiert das so.

Ich klicke jetzt gerade auf den Schertenscreen.

Wenn man sich wundert, was macht Oli da?

Er klickt auf der Screenshot.

Ich habe also gefragt, how does GNI work?

Habt ihr vielleicht für einen Moment, da hätte ich vorher mal vielleicht auch Hinweise sollen gesehen, dass er in einer Knowledge Base sucht.

Weil ich es ja auch nicht hingewiesen habe, machen wir das nochmal.

Ach nee, ich klicke schon wieder auf das falsche Fenster.

Es wird jetzt aber so.

Also jetzt sind wir wieder am Anfang.

Und jetzt achtet mal darauf, wenn ich da draufklicke, passiert gleich, was ganz oben steht.

Ganz kurz stand da Searching und irgendwie sonst was.

Aber wirklich sehr, sehr, sehr kurz.

Wir hätten vielleicht einen richtigen Moment einen Screenshot machen können.

Habe ich nicht geschafft.

Ist so.

So, das Ding Antworten jetzt.

Ich habe was anderes gefragt.

What are typical types?

Genau, eben hattest du das, how does GNI work?

Und jetzt hast du die dritte Puppe angeklickt.

Okay, alles klar.

Deswegen habe ich auch was anderes bekommen.

Das ist egal, weil im Moment interessieren wir uns gar nicht für diese Inhalte, sondern was ist da eigentlich passiert, sodass ich diese Antwort bekomme.

Die Antwort ist rausgekommen.

Da sagt er uns sowas wie Instruction Prompting, Strukturierungsaufgaben und so.

Und in der einfachsten Variante davon hätte ich mir gar nicht einen Assistenten bauen müssen oder irgendeinen Experten, sondern ich hätte das ja einfach in die Chatbox von JTBT oder in einem anderen Promptsystem einbauen können.

Ja, genau.

Irgendeine Idee, warum ich das nicht gemacht habe?

Bestimmt hast du, du möchtest wahrscheinlich auf sowas raus, wie du hast irgendwelche tollen Dokumente oder Dinge, die du durchsuchen können möchtest, weil du wahrscheinlich die Idee hattest, das für ein internes Projekt zu nutzen, vielleicht im Kontext einer Versicherung.

Also jetzt nicht dieses Ding konkret, aber vielleicht könnte man dann irgendwelche Dinge, die man nicht im Internet findet, hinterlegen und dann weiß man, dass die KI auch dort suchen wird und eben nicht nur durchs Internet suchen wird.

Super cool.

Ich habe das auch mal so als eine Frage an die Leute draußen gestellt, dass sie jetzt einmal darüber nachdenken.

Nein, nein, nein.

Ich dachte, das ist hier ein Test.

Ich wollte bestehen.

Ja, okay.

Da gibt es einige Punkte.

Hast du dich mit einem Prüfungsprotokoll vorbereitet?

Das wirkt mir schon so, als hättest du die Frage geahnt.

Ach so.

Alles nur Spaß.

Also das ist alles völlig super, was du gesagt hast.

Es gibt unterschiedliche Gründe dafür.

Aber bevor wir uns die Gründe angucken, machen wir einmal die Motorhaube auf, sodass wir überhaupt das genauer verstehen, worüber wir gerade hier reden.

Ich habe noch mal eine ganz blöde Frage, bevor wir die Motorhaube öffnen.

Wofür steht GPT überhaupt?

Ich weiß es nicht.

Okay.

Aber wir können ja das Ding mal fragen.

Das habe ich irgendwann mal gegoogelt oder so.

Oder wahrscheinlich irgendeine Chat-GPT-Titel.

What does GPT stand for?

So, da haben wir es.

Generative pre-trained transformer.

Hört sich einfach viel geiler an als GPT.

Schade, dass man das einfach nicht wusste.

Ja, das stimmt eigentlich.

Generative pre-trained transformer.

Generativ, okay, das macht Sinn.

Das erzeugt Dinge.

Vor-trainiert, ja.

Nicht vor, sondern erst fertig trainiert.

Und dann transformen wir einfach die Architektur.

Schon cool, ne?

Ja.

Gefällt mir auch gut.

Jetzt darfst du auch die Motorhaube öffnen.

Achso, Motorhaube.

Wollten wir aufmachen?

Machen wir auf?

Genau.

Du klickst, ja.

Sehr gut.

Habe ich auf den richtigen Dings hier geklickt?

Hallo?

Das weiß ich noch nicht.

Doch, doch, das ist nur verzögert.

Ich dachte, ich hätte schon wieder Fehler gemacht.

Also, wie sieht so ein System aus?

Also, das ist in allen...

Das gibt es nicht nur bei Chat-GPT, sondern auch ganz vielen anderen Quellen.

Das sieht immer ziemlich ähnlich aus.

Und die Idee ist auch immer ziemlich ähnlich.

Also, du hast so ein Ding angelegt und dann sagst du dem, wie er sich verhalten soll.

Das würde man typischerweise wie so einen System-Prompt ansehen.

Das ist der System-Prompt, der unabhängig von der Frage, die dir konkret gestellt wird, das generelle Verhalten von dem System betreibt.

Das gibst du so rein.

Das ist ein relativ langer System-Prompt.

Zumindest für meinen Geschmack.

Ich bin der Verfechter von sehr kurzen System-Prompts.

Aber das hängt wirklich davon ab, was man so vorhat.

Außerdem ist es alles Meinung und Geschmack häufig.

So, da sage ich, antworte mal für Leute, die vielleicht nicht so technisch sind.

Und dann stehen da Dinge drin.

Jetzt steht hier unter anderem...

Achso, ich habe auf der falschen Seite gescrollt.

Ich sage dem Ding, benutze bitte die Information, die drangehängt ist.

Und zwar soll es Informationen sein, die gesucht wird.

Also, was heißt das jetzt?

Ich habe in dem Fall einfach aus Faulheit zwei Texte drangehängt, die ich mir vorher tatsächlich auch durch dieses Ding generieren lassen und die ich persönlich kuratiert habe, also soll heißen, durchgelesen habe und gesagt habe, ist spitze.

Das soll unser Wissensblatt sein.

Das heißt, ich habe Kontrolle gewonnen in dem Fall, weil ich weiß, was zur Beantwortung benutzt wird, ist dieses Wissen.

Das funktioniert nicht immer.

Es gibt immer Fälle, also jedes System macht manchmal Dinge, die es nicht tun soll, wenn man entweder unwirklich fragt oder sehr geschickt.

Aber die Wahrscheinlichkeit, dass es das tut, was ich will und auf Basis dieser Daten antwortet, ist eben höher, als hätte ich diese Daten nicht drangehängt.

In dem Fall könnte man natürlich sagen, diese Daten hast du ja vorher mit ChatGBT generiert.

Das heißt, dieses Wissen war ja in ChatGBT schon drin.

Aber ich weiß eben, es ist jetzt nur dieses Wissen und ich weiß, dieses Wissen wird benutzt.

Du hast noch ein anderes Beispiel gesagt, das eigentlich cooler ist, nämlich wo ChatGBT oder ein anderer Chatbot oder Assistent dieses Wissen nicht hat.

Und dann kann man dem Wissen dranhängen, das vorher unbekannt war.

Also zum Beispiel Wissen, das gar nicht im Internet ist und nicht zum Training benutzt, hoffentlich nicht zum Training benutzt wurde, sonst hat man vielleicht ein anderes Problem.

Aber es gibt auch viele Sachen, die sollen gar nicht ins Internet.

Die meisten Sachen, vertrauliche Sachen, sehr vertrauliche Sachen, personenbezogene Sachen, die sollen auf gar keinen Fall ins Internet.

Personenbezogene Sachen würde man jetzt auch nicht in ChatGBT hauen, hoffe ich zumindest, sondern in andere Systeme.

Aber das Prinzip bleibt gleich.

Das heißt, du kannst zum einen dafür sorgen, dass sogenannte Halluzination, die verschwindet nicht, aber dass du sie kleiner machst, indem du sagst, nur dieses angehängte Zeug bitte benutzen.

Das heißt, das Ding fängt nicht einfach an, über etwas zu labern, von dem es keine Ahnung hat, zumindest nicht trivial keine Ahnung hat, sondern gleich etwas im Graubereich.

Und zum anderen könntest du Dinge tun, die es sonst gar nicht wüsste.

Ich hänge das jetzt an und sage ihm, greife nur darauf zurück.

Heißt das jetzt, wenn du den jetzt fragst, gib mir das beste Milchreisrezept, dann wird er keine Antwort ausspucken, weil das Milchreisrezept wahrscheinlich nicht in deinen Testdateien ist.

So wäre unsere Hoffnung.

Das probieren wir mal kurz aus.

Aber das ist dann quasi diese Fehlergeschichte.

Es kann sein, dass er jetzt trotzdem etwas ausspuckt, aber in den meisten Fällen wird er nichts ausspucken.

Ich mache mal auf Deutsch.

Das ist ja eigentlich auf Englisch.

Das Ding gibt mir das beste Milchreisrezept.

Richtigerweise ist da sogar so ein Prompt drin, dass er sagen soll, tut mir leid, weiß ich nicht.

Das ist aber etwas, das ist extrem schwer bis unmöglich, das so einem System irgendwie beizubringen, außer wenn du Wissen dranhängst.

Also Chat2BT und auch fast alle anderen Sprachmodelle sind ja so People Pleaser oder so Beratercharaktere.

Ich darf das sagen, weil ich auch immer Berater war.

Man sagt ja nie, ich jetzt schon, ich bin ja nicht mehr Berater.

Man sagt ja als Berater ungern, das weiß ich leider, ich habe nicht den blassesten Schimmer, sondern plappert man ja irgendein Zeug, das im Zweifelsfall gut klingt und die Leute verwirrt.

Das ist nicht der beste Stil und im Bereich eines Sprachmodells schon gar nicht.

Und deswegen haben wir es jetzt ja hingekriegt, dass er es nicht tut.

Man könnte jetzt argumentieren, der weiß doch das beste Milchreisrezept.

Das könnte er ja antworten.

Aber das ist ja genau die Kontrolle, die wir hier ausüben.

Wir wissen nicht so genau, was die Leute sagen.

Ich habe ja die Frage vorhin formuliert, wie heißt meine Oma?

Und das ungetunte, das ist ja GPT5, das ungetunte GPT5 hätte nicht gesagt, sorry, weiß ich nicht, sondern hätte gesagt, ja, das kommt so darauf an.

Es könnte ja sein, dass deine Oma typischerweise so heißen könnte und fängt an, irgendeinen Quatsch zu farseln.

Ist auch schon besser als früher.

Früher hätten die einfach so etwas gesagt wie Maria oder so.

Aber das ist ja genau die Antwort, die wir haben wollen.

Man hätte ja auch noch antworten können, wir sind ein System für Gen AI, frag mal was über Gen AI, sonst kriegst du ja nichts aus mir rausgekitzelt.

Ich muss dann natürlich jetzt der GPT ohne Custom GPT fragen, wie deine Oma heißt.

Aber dazu liegen leider keine öffentlich bekannten Informationen vor.

Du hast das leider nicht auf deinem öffentlichen Profil geteilt.

Aber das ist doch schon mal nicht schlecht.

Das ist besser als Maria Zeigermann, genau.

Genau, und wenn man es ein bisschen weniger offensichtlich macht, wenn man nicht so ganz konkret super genau nach Fakten fragt, kriegt man das Ding natürlich immer reingedichtet.

Und dann plappert das einem irgendwas vor oder behauptet, dass es Dinge kann, die es nicht kann.

Und das wollen wir eben nicht.

Ich würde denken, die größte Herausforderung bei Benutzung von generativer KI ist Kontrolle zu haben.

Das klingt jetzt ein bisschen nach Zwangsreaktor, aber finde ich eigentlich nicht.

Sondern als klassischer Softwareentwickler, wenn du da die Kontrolle verloren hast, wenn du nicht mehr weißt, wie dein System funktioniert, oder wie du es debuggen kannst oder im Heile machen, dann bist du ja eigentlich verloren, hast du unwartbaren Wahnsinn.

Und hier haben wir unwartbaren Wahnsinn von der Stange.

Das heißt also, sobald du ein System einfach so benutzt, und dann fümmelst du am Pomp drum und weißt eigentlich gar nicht mehr, wo da unten ist, dann bist du verloren.

Das heißt, hier müssen wir anders als bei Softwareentwicklungen nicht dafür sorgen, dass es nicht unwartbar wird, sondern wir müssen erstmal dafür sorgen, dass wir überhaupt irgendeine Form von Kontrolle hier einziehen können.

Irgendeine Form von, wir haben irgendwas im Griff und können irgendwas beschränken.

Deswegen klingt es erstmal vielleicht auch komisch, gerade bei meiner Persönlichkeitsstruktur, aber ich würde sagen, an dieser Stelle muss man kontrollieren, so doll man kann.

Und das haben wir an dieser Stelle getan.

Das heißt, das ist unsere Anwendung.

Ich muss mal kurz drüber nachdenken, ob ich irgendwas nicht gesagt habe, was ich hätte sagen sollen.

Ich glaube nicht.

Mein Plan für jetzt gleich ist, dass wir uns zusammen eine kleine Skizze machen, was hier eigentlich alles passiert gerade.

Wieso ist das eigentlich so?

Welche Teile passieren hier?

Und welche Teile passieren eigentlich gar nicht?

Oder was glaubt man manchmal, was passiert?

Was dann tatsächlich gar nicht passiert.

Dafür wechsel ich jetzt mal.

Oder gibt es noch irgendeine Unklarheit?

Ich möchte jetzt endlich, dass du es mir mit Bild erklärst.

Ich bin fast bereit, umzuschalten.

Warte mal, jetzt muss ich mal richtige Knöpfe.

Kamera.

Das ist unsere Tafel.

Das hier festgeklebte Gaffer-Tape ist natürlich klar, deswegen muss ich ein bisschen vorsichtig sein, dass ich es nicht verschiebe.

Also, was ist jetzt passiert?

Wie gesagt, ich muss versuchen, das so zu erklären, dass auch Leute, die nur zuhören, das verstehen.

Also, wir haben ja Dokumente in unser Chatbot, in unser DPD und unseren Assistenten da reingetan.

Das heißt, irgendwo habe ich Dokumente gesammelt.

Die nenne ich jetzt mal Docs, ein bisschen kürzer.

Diese Docs, die sind von mir kuratiert und ich gehe davon aus, dass das, was da drin steht, das ist korrekt.

Wenn das schon stimmt, dann habe ich verloren.

So, hier Dokumente.

Wie gesagt, das sah früher alles viel schlimmer aus.

Die Anzahl der Dokumente ist erstmal egal.

Es kann auch ein Dokument sein oder ganz viele.

Ich habe jetzt zwei gehabt.

Ist im Prinzip egal.

Gibt es da ein Limit?

Also, blöde Frage, gibt es da ein Limit?

Das kann ich dir nicht sagen.

Das ist je nach System unterschiedlich.

Ich glaube, bei diesen GPDs gibt es eins.

Es wäre jetzt nicht technisch beschränkt, sondern eher, dass man sagt, was macht Sinn oder auf wie viel haben wir Böcke.

Aber im Prinzip können das so viele sein, wie man da reingestopft kriegt.

So, wenn ich das jetzt mache.

Ich habe euch das nicht gezeigt, wie ich die hochlade, weil ich da keine Lust so hatte.

Ich habe die aber irgendwann hochgeladen.

Da sind Dinge passiert.

Und was dann passiert ist, ist, dass diese Dokumente gewandelt wurden in Schnipsel oder sogenannte Chunks.

Ich schreibe jetzt mal Chunks hin, weil man das wahrscheinlich eher finden würde, wenn man jetzt im Internet sucht und nicht schnipselt.

Und das, was diese Chunks erzeugt, ist Chunking.

Das heißt, irgendein Algorithmus guckt sich an, was habe ich denn da eigentlich rumliegen.

Schlaue Algorithmen würden auch sehen, oh, das ist HTML, das chunke ich mal so, dass ich vielleicht eine Sektion habe oder so.

Oder man würde sehen, das ist Text und man würzelt irgendwie vielleicht nach Paragrafen oder Satzenden Chunks.

Und man zerschnipselt also diese Dokumente in kleinere Schnipsel.

Das heißt, auf einem Dokument wird jetzt ein Paragraf, zwei Paragrafen oder drei Paragrafen oder wie für auch immer.

Das hängt von der Chunking-Strategie ab.

Also das ist manchmal voreingestellt.

Bei Chattopedi kann ich mir das gar nicht aussuchen.

Manchmal kann man sich das aussuchen, wenn man sowas wie SpringII benutzt oder Langchain oder so.

Da kann man sich aussuchen, wie macht man das.

Oder man kann es auch selbst implementieren.

Und manchmal ist es auch sinnvoll, das selbst zu implementieren, weil man vielleicht sehr spezielle Eigenschaften von den Dokumenten hat.

Aber im normalen Fall würde es Sinn machen zu sagen, wir machen das mein Ding nach Paragrafen.

Oder man sagt, die Dokumente sind sowieso klein, die machen wir komplett als Chunk.

Hängt eben davon ab.

Ist aber auch wichtig, wie man chunkt.

Also wenn man das vergeigt, dann kriegt man auch kein gutes Ergebnis.

Häufig sind aber so Standardmechanismen, wie sehen sowas wie Chattopedi drin sind, sind gut.

Also die funktionieren für den typischen Fall.

Diese Chunks, die sind jetzt Grundlage dafür, dass ich später diese Schnipsel wiederfinde.

Weil ich möchte ja, wenn ich später eine Anfrage stelle, möchte ich die richtigen Schnipsel für meine Anfrage bekommen, um die dann zusammen mit meiner Anfrage ins Sprachmodell zu schieben und das Sprachmodell ergibt mir die Antwort.

Dann habe ich dem Sprachmodell gesagt, bitte nur diese Information, die ich dir jetzt gegeben habe, auch zur Verantwortung der Frage benutzen.

Das ist der Trick.

Also eigentlich ein ziemlich simpler Taschenspielertrick.

Und damit das geht, passiert etwas, was häufig den Leuten überhaupt nicht bewusst ist, dass da ein zweites und völlig anderes Sprachmodell eine Rolle spielt.

Auch ein Transformer, aber der ist nicht generativ, sondern ein Transformer, der in der Lage ist, wenn er gut ist, im Idealfall ist er in der Lage, die Semantik von einem Schnipsel in eine Koordinate oder anders ausgedrückt in einen Vektor zu transformieren.

Das meine ich mal kurz hin.

Das heißt, hier kommt jetzt, ich habe hier so eine leichte Vorlage, die ist aber auch zum Mittel gut.

Das heißt, das wird jetzt hier in sogenannte Vektoren übersetzt oder eine Koordinate im Koordinatensystem, also 1, 2, 2 usw.

Typische Dimensionen von solchen Vektoren sind jetzt nicht wie bei mir 3, sondern so etwas wie 1536 oder noch mehr.

Das heißt, wir können uns das in 2-Dimensionalen vorstellen oder maximal 3-Dimensionalen, weil unser Hören einfach nicht mehr hergibt.

Das ist aber eben in über 1.000-Dimensionalen Raum.

Ich glaube, irgendwann ist das auch reicht, dass man sagt, reicht jetzt.

Aber man kann sich das so vorstellen, dass jede Dimension so etwas wie eine semantische Ebene ist.

Keine semantische, also so etwas wie, meinetwegen, das könnte eine Ebene sein, so etwas wie Alter.

Und die andere Ebene könnte so etwas sein wie Ernsthaftigkeit vom Text.

Das sind aber nicht Ebenen, die wir als Menschen uns vorstellen können, sondern eben Ebenen, die für so ein Sprachmodell Sinn machen.

Das ist fremdartig für uns.

Wir können uns diese Ebenen auch nicht sinnvoll angucken.

Wir wissen einfach nur, es gibt diese Ebenen und es ist eben so.

Jetzt passiert Folgendes.

Wir haben diese Vektoren und Vektoren kann man speichern.

Und Vektoren haben auch die Eigenschaft, dass man da auf Ähnlichkeitsmaße definieren kann.

Ich speichere die mal kurz in der Vektordatenbank.

Hier geht es jetzt so weiter, die gehen hier so rein.

Jetzt male ich das nur in 2D, aufgrund von der Dimension kann ich nicht hinmalen.

Ich würde jetzt meinen, dieser Schnipsel landet 1,2,2 hier.

Dieser zweite Schnipsel 2,4,6 landet da oben.

Und jeder dieser Schnipsel bekommt eine Position innerhalb dieses Koordinatensystems.

Das wäre hier eine Vektordatenbank.

Ein Vertreter davon wäre zum Beispiel Quadrant.

Das ist ein deutscher Hersteller einer Vektordatenbank aus Berlin, meine ich.

Benutze ich gut, also kann ich empfehlen.

Schleichwerbung eigentlich, aber warum soll man nicht sagen, dass Sachen gut sind.

Es gibt aber auch zum Beispiel Postgres, hat auch ein Plugin oder wie es heißt, ist auch gut.

Ist auch tatsächlich irgendwie, ist inzwischen ganz gut abgegangen.

Und dann würde ich jetzt unterschiedliche Chunks anhand ihrer Samantik irgendwo im Raum platzieren.

Und man würde immer denken, was immer unser Ähnlichkeitsmaß ist.

Also man könnte ja sagen, Ähnlichkeitsmaß könnte einfach sein, was ist jetzt euclidisch in der Nähe.

Also sowas hier, die sind alle ähnlich.

Es gibt andere Ähnlichkeitsmaße, die typischer sind, dass man sich eher den Winkel anguckt.

Also wenn jetzt zum Beispiel, muss ich mal kurz konstruieren.

Der Winkel jetzt hier zum Koordinatenursprung, der würde ja so aussehen.

Der Winkel von dem hier würde so aussehen.

Die sind ziemlich unterschiedlich.

Und wenn man da auf ein Kosinus macht, kriegt man eben ein Maß zwischen 0 und 1.

Also je ähnlicher der Winkel ist, desto, desto, desto nulliger oder desto einziger?

Desto einziger, oder?

Wir sind wir beide nicht, Kosinus müsste 1 sein, deswegen ist es, glaube ich, Kosinus.

Also je ähnlicher, desto einziger.

Wenn ich mir den Kosinus vorstelle, kriege ich gerade nicht hin, aber es müsste eigentlich 1 sein.

Sonst wäre es quatschig, sonst hätte es ja auch Sinus sein können.

Die sind also ziemlich unähnlich, aber wenn jetzt hier hinten einer liegt, wenn ich da jetzt irgendwie den Winkel mache, dann sind die ja total ähnlich.

Das heißt, der Winkel hier so ist total klein und die sind ähnlich.

Es hat sich irgendwie herausgestellt, dass dieses hier unten, obwohl es total intuitiv ist und man denken würde, so machst du das, häufig nicht so gute Ergebnisse liefert wie das hier.

Aber das ist alles was, das kriegen wir alles nicht mit.

Also ab dem Moment des Chunkings kriegen wir es nicht mit, die Vektordb kriegen wir nicht mit und diese Ähnlichkeitsberechnung, ob jetzt über Kosinus oder Euklid, kriegen wir auch gar nicht mit. Überhaupt nicht.

Das Ähnlichkeitsding ist dann wahrscheinlich das, was das LLM übernimmt, oder?

Nein, also das ist aber eine sehr gute Frage.

Also das LLM macht das nicht, sondern das macht die Vektordatenbank.

Also entweder sagst du das gar nicht und die Vektordatenbank überlegt sich, was sie für klug hält im Default oder du gibst es bei der Anfrage an die Vektordatenbank.

Also kannst du zum Beispiel sagen, Ähnlichkeitsmaß hier so einfach Euklidisch oder hier Kosinusmetrik und du kannst auch sagen, wie viel Treffer soll es eigentlich geben.

Dann kannst du sagen, wir wollen immer zwei Treffer.

Das heißt, auch wenn du jetzt nichts in der Nähe hättest wie bei dem hier, würdest du sagen, der hier ist nah, der ist nah, obwohl die eigentlich zu weit weg sind.

Oder du kannst sagen, da gibt es eben keinen Treffer.

Oder du kannst eben auch sagen, so ein Schwellwert.

Also wenn es zu weit weg ist, gibt es eben keinen Treffer.

Das hängt davon ab, was du so für Andamen triffst über deine Inhalte und ob du immer einen Inhalt haben möchtest oder ob du die bei deinem gar nicht findest, anstatt irgendwas Quatschiges.

Ist das was, was ich jetzt der GPT auch einfach fragen kann?

Also kann ich sagen, ich weiß, dass hinter dir eine Vektordatenbank liegt, ich suche nach dem und dem und um jetzt zu wissen, ob du Quatsch redest, möchte ich gerne die drei besten Treffer von dir in deiner Vektordatenbank dazu haben, zwar nach Winkelmaß.

Also ob man das machen kann, habe ich noch nie ausprobiert.

Typischerweise, also der normale Chat hat das ja gar nicht.

Das heißt, du müsstest dir so einen Assistenten mal bauen oder wie immer man das nennt und dann mal ausprobieren.

Ich weiß es nicht, vielleicht könnte er das, vielleicht würde er es ausspucken.

Mich würde es auf jeden Fall interessieren, weil das konntest du da nirgendwo einstellen.

Vielleicht ist das irgendwo abgespeichert oder es weiß ein anderer Chatbot.

Wäre auch interessant.

Manchmal fragt man ja auch per Pexity, wenn man was über Chat-GPT wissen will und dann soll die da vielleicht manchmal ein bisschen, weiß ich nicht, freier von der Leber erzählen.

Kann aber auch irgendwie ein falscher Eindruck sein.

Okay, also wir haben die Vektordatenbank und jetzt suchen wir Spellenwerte beziehungsweise nahe beieinander liegende Dinge.

Genau und bisher ist noch überhaupt gar kein generatives Modell hier irgendwie in Kraft getreten.

Also das ist die Vektordatenbank hier.

Kein generatives Modell.

Das Ding, das aus diesen Chunks das gemacht hat, nennt man Embedding-Modell.

Und das ist nicht so wichtig, dass man das wirklich weiß.

Da wird manchmal so mit klug geschissen.

Das ist ein sogenanntes Encoder-Modell.

Und Encoder-Modell erzeugt im Gegensatz zu einem Decoder-Modell keinen Text.

Kannst du nicht machen.

Ein Modell wäre ein Decoder-Modell, ein Embedding-Modell wäre ein Encoder-Modell.

Und Encoder heißt sowas, wie du steckst da zwangslang Text rein, aber da kommt kein langer Text raus, sondern ein Vektor.

Und das ist der Vektor.

Und das Ding ist eben so trainiert durch unterschiedliche Geschichten.

Mein Gen-AI-Expert könnte das alles im Detail erklären, weil das alles in Journalismus-Basis steht.

Durch unterschiedliches Trainingsverfahren wird eben mehr oder weniger gut dafür gesorgt, dass du es wirklich schaffst, die Bedeutung von so einem Text als Koordinate zu codieren.

Ja, der Trick.

Wenn das nicht klappt, wenn du das irgendwie kodierst, dann kommt das ja gar nicht hin.

Dann ist das ja alles Unfug.

So, ich wechsel mal kurz die Farbe, weil es jetzt natürlich spannend weitergeht.

Mal rot.

Hier, guck mal.

Flickern, die kann man ja gar nicht sehen.

Die habe ich von meiner Tafel gepflückt.

Wir machen blau.

Warum hat man das jetzt gemacht?

Das hat man ja gemacht, bevor man überhaupt irgendeine Anfrage gemacht hat.

Jetzt stelle ich meine Anfrage an das System.

Hat man leider nur so ganz kurz gesehen.

Dann tritt tatsächlich das Sprachmodell in Kraft, guckt sich die Anfrage an, guckt sich den eigenen Prompt an, hat zudem eine Reihe von Tools, also von Werkzeugen, die es aufrufen kann, definiert und findet also raus, okay, im Prompt steht, ich soll bitte mein Suchwerkzeug benutzen und die Suchanfrage auch wieder in so einen Vektor kodieren und dann nach Ähnlichkeit suchen.

Sprachmodell weiß das also hier.

Wir können hier langsam LLM hinschreiben.

Vielleicht machen wir das ein bisschen anders.

Wir haben das ein bisschen anders hingemalt.

Machen wir das ein bisschen anders.

Das heißt, der Mensch, dieser bitte nicht sterben, wenn ihr meine Männchen hier siehst.

Das ist so süß.

Das ist doch super.

Der Mensch fragt jetzt was.

Das LLM sagt, sagt mir mal was über GenAI.

Dann hat das Modell Wissen darüber.

Oh, ich habe Zugriff auf eine Vektordatenbank und es hat im Prompt auch noch Informationen darüber, dass es das bitte benutzen soll und dann formuliert das LLM die Frage dieses Benutzers hier häufig noch um und macht es ein bisschen klarer.

Also die Frage irgendwie zu wirr ist oder zu lang, dann formuliert es die Frage häufig um in etwas, das sehr gut für diese Vektordatenbank funktioniert.

Das heißt, die Frage wird jetzt hier in die Vektordatenbank geschickt, umformuliert und die ruft wieder genau das gleiche auf.

Also hier oben.

Es schankt nicht, sondern es ruft das Embedding Modell auf.

Da kommt wieder ein Vektor raus und der Vektor ist jetzt meinetwegen da.

Kann man sich diese umformulierte Anfrage irgendwie auch anzeigen lassen?

Also es wäre ja schon interessant zu wissen.

Bei anderen Systemen geht das.

Ich gucke mal hier in Mainz.

Nee, das zeigt er nicht an.

Also das passiert aber fast immer.

Also ich habe jetzt gerade mal geguckt.

Wir können es ja noch mal irgendwie vernünftig machen.

Genau, ich habe jetzt, dass man dein Screen wieder sieht, gemacht.

Habe ich gesehen.

Also, how does the AI work?

Mal gucken.

Ganz kurz stand da sowas wie Query.

Das ist aber leider, ich habe das Gefühl, das hier haben die irgendwann mal angefangen und haben dann immer gedacht, oh, haben wir doch keinen Bock drauf, weil das ein bisschen stiefmütterlich behandelt wird.

Aber in anderen Systemen kannst du das definitiv sehen und auch debuggen.

In anderen Systemen kannst du auch gegen so einen Agenten oder Assistenten kannst du auch von außen ein Request machen und dann siehst du das erst recht.

Dann siehst du ganz genau, was da passiert.

Können wir hier leider nicht machen, aber das passiert.

Also es gibt ja andere Systeme, die das ein bisschen besser können.

Ich habe das jetzt hier einfach genommen, weil das eben frei und einfach verfügbar ist.

Jeder, der irgendwie was machen will.

Genau, aber das wird also umformuliert und dann, genau, richtig.

Dann landet das da und jetzt haben wir genau, was wir eben schon besprochen haben.

Jetzt ist also diese Vektordatenbank angehalten.

Entweder, weiß ich nicht, sowas hier zu machen, also einen Umkreis da umzubilden.

Mal sehen, ob ich das hinkriege.

Sollte eigentlich ein Kreis sein.

Ja, so gut wie es eben möglich ist.

Oder eben zu gucken, was viel typischer ist, aber weniger intuitiv.

Nach dem Winkel und dann im Kosinusmaß guckt, was ist, was hat denn hier.

Also machen wir lieber Kosinusmaß.

Den hier, dann mit dem und dann dem.

Und dann würde man sagen, guck mal, diese beiden liegen da noch in der Nähe.

Diese beiden hier werden jetzt zurück übersetzt. Übersetzt ist ein bisschen hochgestochen, weil es wird eigentlich nichts übersetzt, sondern die Vektordatenbank hat nicht nur das Embedding gespeichert, also nicht nur das hier.

Nicht nur das, sondern auch die Chunks dazu.

Man weiß also auch noch, welcher Text liegt da ursprünglich hinter.

Liefert dann diese Chunks, also meinetwegen das WC dieser hier und der hier, liefert das jetzt hier als Kontext an das LLM.

Sagt so, hier die beiden, hier x und x, bitteschön.

Und zusammen mit der ursprünglichen Anfrage, der Query-Anfrage und diesen beiden Antworten, sagt jetzt das Modell höchstwahrscheinlich, das ist schon so ein bisschen agentisch, dass das Ding entscheiden kann, was mache ich jetzt.

Wird das Modell höchstwahrscheinlich sagen, jetzt gebe ich dir meine Antwort.

Nimmt also dieses Zeug und macht mit ganz normalen LLM-Mechanismen, erzeugt es uns jetzt am Ende die Antwort, die wir uns wünschen.

Also hier würde jetzt rauskommen, was auch immer.

Also wenn er was findet, schon mal gut, dann würde er sagen, okay, versuchen wir das mal auf Basis des Gefundenen zu beantworten und sagen dann, das haben wir jetzt hier mal gefragt, wie funktioniert JNI und dann kriegst du die Antwort.

Oder da ist nichts rausgekommen.

Es könnte sein, dass das System hier gar nichts gefunden hat, da ist gar nichts rausgekommen.

Dann sagt das anhand unseres Prompts eben, das musst du dann eben dementsprechend schlau prompten, sagt das Ding eben, ich habe leider dazu kein Wissen gefunden, deswegen sage ich dir nichts.

Das ist ja genau unser Trick.

Wenn da nichts so drinsteht, wenn da nichts über Milchreis drinsteht, dann hoffen wir eben, dass das System uns sagt, dazu habe ich leider keine Information.

Oder aber es kann sogar sein, dass das LLM sagt, ich mache gar nicht erst diesen Request dagegen, weil das, was du mir gegeben hast, so quatschig ist, müssen wir gar nicht erst anfangen.

Oder das ist gar keine Frage oder es ist irgendwie so, dass das Ding den Eindruck bekommt, das machen wir einfach nicht.

Zum Beispiel ein Prompt würde jetzt drinstehen, wenn das keine Frage ist, bitte den Menschen erstmal eine vernünftige Frage zu stellen.

Das ist also auch eine Möglichkeit.

Und ganz viel davon ist schon, weil es eben auch eigentlich ganz praktisch ist, schon außerhalb unserer Kontrolle, weil wir jetzt gar nicht wissen, was macht das LLM als nächstes.

Das ist wirklich in der Kontrolle des LLMs.

Wir sagen dem, folgende Tools hast du, oder entscheidest du das selbst.

Und dann sind Dinge.

Ich habe nochmal eine Frage.

Jetzt hast du uns beigebracht, wie das mit so Agenten, also wie dieses Custom GPT aufgebaut ist.

Meine Frage ist auch jetzt vom Wording egal.

Also JetGPT selber, arbeitet das denn auch so?

Also quasi die Lerndaten, die irgendwann da waren, können wir uns als Docs erklären?

Auch diese Lerndaten wurden als Chunks, dann Embedding gemacht und in die VectorDB gepackt.

Also ist das quasi auch das Grundgerüst von JetGPT oder wäre das nochmal anders?

Das ist tatsächlich ganz anders.

Und das ist aber auch wichtig zu verstehen, dass es ganz anders ist.

Und es ist wahrscheinlich auch einer der typischen Missverständnisse, dass man denkt, JetGPT würde auf eine Datenbank zugreifen.

Man könnte diese Datenbank sehen und da würde das Wissen drin liegen.

Das ist nicht so.

Ich muss auf der Rückseite achten.

Ich kann es nicht umdrehen, weil ich das ja festgetapet habe.

Ich dachte, ich könnte die Rückseite benutzen.

Stattdessen müssen wir jetzt mal unser Meisterwerk fotografieren.

Es ist wirklich gut geworden, Oli.

Dein Lob bedeutet mir sehr viel, weil ich auch sehe, wie du das malen kannst.

Dass es jetzt formell nicht großartig ist, da müssen wir uns nicht drüber unterhalten.

Aber zumindest viel besser als früher.

Jetzt müssen wir das also löschen.

Es war auch so schön bunt.

Ich mag das Bunte inzwischen ganz gern.

Ich hoffe, dass wir uns nicht nachher nochmal darauf beziehen müssen.

Jetzt hast du ja ein Foto gemacht.

Dann kannst du es in die Kamera halten.

Ich kann es auf meinem Handy?

Ja, blöde Idee.

Wir beziehen uns nicht nochmal drauf.

Jetzt erklärst du mir, wie das eigentlich funktioniert.

Das ist das, was total wahnsinnig erscheint.

Insbesondere, wenn man denkt, man fragt jetzt nach Fakten, ist, wie das eigentlich jetzt funktioniert.

Wir wissen eigentlich nicht, wie groß Chattopaddy ist.

Wir wissen nicht die Architektur, wir wissen nicht die Parameteranzahl.

Wenn ich Chattopaddy sage, meine ich eigentlich GPT-5.

Wir wissen nicht mal, ob GPT-5 ein Modell ist oder ganz viele.

Wahrscheinlich sind es ganz viele unterschiedliche Modelle, die je nachdem, was man gerade braucht, instrumentiert werden.

Wir tun mal so, damit wir nicht völlig durchdrehen, als wäre das genau ein Modell.

Wir wissen auch, wie das funktioniert.

Oder wir gucken uns anderes Modell an, so etwas wie Lama.

Lama ist ein Modell von Meta, ist open source.

Da kennen wir die Architektur, da kennen wir die Anzahl der Parameter.

Das würde ich mal kurz machen.

Ist das für dich okay?

Ja, bitte.

Also tun wir mal so, als würden wir jetzt nicht mit GPT-4 oder 5 reden, sondern mit Lama.

Es gibt Lama-4, das dürfen wir in der EU nicht benutzen, weil das auf Bilder kann.

Da gibt es Urheberrechtsprobleme, also machen wir nicht.

Das heißt, was man benutzen könnte, wäre Lama 3.3.

Das hat zum Beispiel in einer in der größten sinnvoll verwendbaren Form 70 Milliarden Parameter.

Das ist eine Architektur, die wir nicht verstehen.

Die hat sich einfach mal etabliert und hat 70 Milliarden Parameter.

Diese Parameter sind mehr oder weniger sinnvoll besetzt durch Training.

Ich versuche es mal hier.

Lama wird jetzt hier ein Kasten, weil ich nichts anderes kann als Kasten.

Das ist unser Lama-Modell.

Was es kann, ist, wenn es fertig trainiert ist, so etwas machen wie, du gibst dem eine Reihe von eigentlich Tokens, aber wir tun mal so, als wäre das Wörter.

Du gibst dem eine Reihe von Wörtern.

Das erste Wort ist How, da geht es das.

Ich tippe jetzt gerade das ein, was zuerst auf unserem Bildschirm stand, damit es realistisch ist.

How does Gen AI work?

Das ist also ein Sprung von Wörtern.

Was Lama jetzt machen kann, ist das zu nehmen, nach und nach zu konsumieren und zu sagen, okay, alles klar, nächstes Wort, nächstes Wort, nächstes Wort, nächstes Wort und dann aber zu sagen, was ist das nächste sinnvolle Wort?

Also das ist trainiert worden und ich erkläre gleich, wie das passiert, dass es weiß, anhand seiner 70 Milliarden Parameter hier, was ist das wahrscheinlichste nächste Wort?

Also in unserem Fall, ich gucke mal in die Antwort.

Warte mal kurz.

Entschuldige, ich muss einmal kurz zurückgehen, weil hier steht tatsächlich, wonach es gesucht hat.

How Gen AI works ist Keyword Summary.

Ich glaube, das ist, also ich hoffe, dass das die Frage ist.

Ja, okay, jetzt bin ich auch wieder bei dir.

Du bist wieder beim alten Bild und du hattest erzählt, dass der User was eintippt und die AI oder das LLM verbessert, dass da noch so damit die die Anfrage in der Vektor DB Sinn ergibt.

Ob das wirklich das ist, bin ich nicht hundertprozentig, aber so würde sowas aussehen.

Also es ist fast die Anfrage, wo irgendwie so, dass es besser in die Vektor Daten passt.

Also es könnte sein, ich kann es nur nicht garantieren, dass doch mal genau die Bildschirm geteilt, weil wir sehen jetzt ja, das, was ich da links hingeschrieben habe, ist diese Frage und das, was jetzt generiert wird, ist das hier.

Was das Sprachmodell jetzt machen würde, es ist generativ, das heißt, es ist lustig.

Es kommt auch generative genau raus.

Generative wäre das erste neu erzeugte Wort.

Generative habe ich mich genauso hier nicht vertilgt, aber verschrieben.

Das würde jetzt quasi an das, was schon erzeugt wurde, so rangehängt werden.

Gehört da irgendwie dazu und der Prozess startet von neuem.

Das heißt, das Modell erzeugt jetzt Wort für Wort.

Man sieht das ja auch teilweise, dass er nach und nach diese Wörter schreibt.

Und der Kontext, in dem dieses System die Wörter schreibt, der wird eben immer länger, weil dieses Wort wird erzeugt, an den Kontext rangehängt und immer so weiter.

Um jetzt dabei zu bleiben, teile ich mal wieder komplett mein kleines Kindertäfelchen hier.

Bei Umschalten ist irgendwas Schlimmes passiert.

Ich kann dir aber nicht sagen, was das Schlimme war.

Ich war mir sehr unsicher, ob mein Internet weg ist oder deins.

Deswegen habe ich blöd in die Kamera geguckt und geschwiegen.

Aber schön, dass du wieder da bist.

Ich kann es dir beim Besten wenig sagen, aber es geht jetzt ja wieder.

Vielleicht ist irgendwas abgesammelt wegen Umschalten.

Das ist einfach Pech.

Das nützt ja nichts.

Jetzt habe ich natürlich völlig vergessen, was ich zuletzt erzählt habe.

Du hattest dein Bildschirm geteilt und du hattest gerade noch erzählt, dass das ja wortweise wieder in den Kontext geht und dass dieser ganze Kontext dann genutzt wird.

Und dann ist es sehr, sehr spontan passiert, dass du weg warst.

Zum Glück weiß ich noch, was ich eben gesagt habe.

Es ist jetzt aber so, dass zum Beispiel die Schnipsel, die wir vorher rausgesucht haben, hier auch drinstehen.

Das heißt, Teil des Kontextes ist nicht nur die Frage, sondern auch die Schnipsel, die vielleicht davor oder nach meiner Frage stehen.

Und es wird da alles reingekippt.

Und so kommen die Schnipsel da rein, aber nicht weil Valdama oder GPT-4 oder 5 oder sonst was irgendeinen Zugriff auf eine Datenbank hätte, sondern die Schnipsel sind vorher, bevor das Ding überhaupt irgendwie jemals anfängt zu leben, oder zumindest bevor es anfängt, meine letztliche Antwort zu generieren.

Die werden da reingetan.

Okay, also ist die Funktionsweise eine andere.

Das Basismodell funktioniert anders, als wenn ich jetzt so ein Custom-Ding trainiere.

Genau so.

Und jetzt, ich muss mal kurz gucken, dass wir das noch schaffen, aber wir schaffen das in der Zeit.

Jetzt habe ich ja eigentlich was ganz anderes versprochen.

Ich habe nicht versprochen, zu erzählen, wie funktioniert jetzt dieses LLM oder wie ist die Funktionsweise, sondern woher weiß ich das eigentlich.

Die Trainingsverfahren dazu sind teilweise halbgeheim, teilweise nicht so richtig geheim, teilweise werden die ein bisschen oberflächlich dargestellt, aber ich sage mal, so das Typische.

Deswegen muss ich das einmal kurz hier löschen oder zumindest Teile davon.

Ich lösche mal das da oben.

Also was für die Leute, die das Bild nicht sehen können.

Da oben heißt es, ich habe jetzt den Kontext gelöscht und hier steht jetzt auf meinem Bild noch, How does JNI work?

Und nun beginnt man also, ist man nicht mehr im Vorhersagemodus, sondern im Trainingsmodus.

Dieses L hier auf unserer Skizze, das LLM, ist quasi random initialisiert.

Das ist dumm.

Das hat noch nichts gelernt.

Und was man jetzt macht, ist, dass man aus einer möglichst sinnvollen Menge von gesammeltem Wissen, das kann im Zweifelsfall das ganze Internet sein, inklusive allem Quatsch, allem Bias, allem Umfug, allem, wenn du fragst, meinetwegen, sag mir mal den Namen einer Putzkraft, dann kommen da Frauen raus.

Wenn du sagst, sag mir mal den Namen eines CTOs, dann kommen da Männernamen raus.

Also alles, was im Moment in der Welt so zu finden ist und was man zum Training benutzt, weiß das Ding auch danach.

Also Bias im Sinne von ungewünschten Bias hat es eben gleich mitgelehrt, merkt man auch.

Aber das ist vielleicht jetzt noch ein ganz anderes Thema, aber erstmal, dass man es weiß.

Das heißt, alles, was du da reinsteckst, lernt das Ding jetzt.

Und das kann im Zweifelsfall, wie gesagt, das ganze Internet sein.

Wie man das wirklich so trainiert, dass es am Ende geht, wissen nur eine Handvoll Leute.

Eine Hardware dafür, das zu trainieren, haben noch weniger.

Das sind nur eine Handvoll Firmen, die sich das überhaupt leisten können.

Das kann ein paar Tausend, vielleicht sogar noch mehr GPUs sein, die man dafür benötigt.

Wahnsinnig viel Zeit und wahnsinnig viel Trainingstime.

Deswegen ist das für uns, selbst ein sinnvolles Nachtrainieren, typischerweise nicht möglich.

Das heißt, man nimmt dieses Zeug von der Stange.

Und jetzt habe ich immer noch nicht gesagt, wie es funktioniert.

Es funktioniert jetzt so.

Und das ist eigentlich sehr vereinfacht, weil es noch weitere Schritte gibt, die das noch ein bisschen weiter ausweiten.

Aber das müssen wir sehen, wann wir aufhören mit Details.

Es funktioniert jetzt so, dass dieses Modell jetzt das nächste Wort vorhersagt.

Das nächste Wort, das würde jetzt vielleicht als nächsten Satz sagen, als nächstes Wort nicht Generative, sondern Katze.

Deutsches Wort Katze.

Ist im Vokabular des Modells drin, sagt Katze.

Jetzt, weil man ungefähr ahnt, noch wie Maschinenlernen funktioniert, jetzt guckt man typischerweise sich an, was hätte rauskommen sollen.

Man weiß, was rauskommen soll, weil man ja mit einem Text trainiert hat, der hier jetzt ein richtiges nächstes Wort gehalten hätte.

Nehmen wir mal an, das wäre wirklich Generative gewesen.

Dann sieht man, oh, da kommt Katze raus.

Da hätte aber Generative rauskommen sollen.

Dann baut man daraus eine Fehlerfunktion.

Und mit dieser Fehlerfunktion verändert man jetzt das Modell.

Das heißt, hier ist irgendeine Art von Fehler.

Der sagt, guck mal, Token XY ist rausgekommen, hätte aber Token YX sein können.

Das heißt, man propagiert jetzt den Fehler.

Mir fällt jetzt kein gutes Symbol ein.

Propagiert man hier rein und verändert die Parameter in dem Sinne, sodass die Wahrscheinlichkeit für das richtige Wort viel höher wird.

Wie man das macht, ist ganz, ganz klassisches Maschinenlernen.

Das ist ganz, ganz klassisches Maschinenlernen von neuronalen Netzen.

Das würde jetzt den Rahmen total sprengen.

Das zu erklären, würde auch gar keinen Sinn machen, weil es zu technisch ist.

Aber das können wir.

Das können wir schon ganz lange.

Das heißt, dass wir rausfinden durch Backpropagation, was müsste man verändern, damit eher das Ergebnis rauskommt als das.

Das können wir.

Das können wir ganz lange.

Dass wir das parallel mit ganz vielen GPUs trainieren können, können wir noch nicht so lange.

Das geht erst seit dem Attention Mechanismus 2018, weiß ich nicht.

Das wissen vielleicht andere noch besser, aber das geht seitlerweile.

Und seitdem kann man sowas auch hochparallel mit riesigen Modellen trainieren.

Und das ist das grundsätzliche Konzept.

Noch ein vielleicht dazu, was man auch noch macht, ist, dass man jetzt nicht nur sowas macht, welches fortkommt jetzt als nächstes, weil du möchtest ja eigentlich, dass diese Modelle nicht das nächste Wort sagen, sondern dass die deine Anweisungen befolgen.

Deswegen macht man aus solchen Basismodellen, die so trainiert wurden und noch ein bisschen anders, häufig Instruction-Modelle.

Und diese Instruction-Modelle sind eben gut da drin, zu gucken, wie würde man auf so eine Frage, das ist jetzt so ein Fragezeichen hier, wie würde man gut auf so eine Frage antworten.

Weil wenn du ein Modell benutzen würdest, das nicht Instruction-gekündigt ist, würde das vielleicht die Frage weiter ausformulieren.

Du willst ja aber eine Antwort.

Das heißt, die Modelle werden damit trainiert, dass man im nächsten Schritt nachtrainiert, dass man sagt, wie würde eine gute Unterhaltung aussehen.

Früher gab es häufig beide Versionen, also diese Basismodelle und die Instruction-Modelle.

Vielleicht gibt es inzwischen auch immer noch beide, aber das Typische, was man will, ist so ein Instruction-Modell.

Die heißen dann auch Instruct.

Der Name A3370B hat ja dann Instruct, so fix.

Und wenn man sich fragt, was ist denn das eine, was ist das andere?

Instruct ist eben darauf getuned, dass Sie Fragen beantworten, und das andere ist ein Basismodell, dass nicht das tut, was man will.

So weiß man schon.

Soll ich mehr Details erklären, oder wollen wir noch über andere Dinge reden?

Es können beides tun.

Ja, gute Frage.

Ich gebe die mal weiter an das Publikum.

Was hättet ihr lieber?

Und während die Leute antworten.

Was kann ich denn machen als User, damit weniger Quatsch rauskommt?

Du hattest ja eben schon Systemprompts erwähnt.

Theoretisch könnte ich doch jede meine Anfrage an normales LLM mit einem Systemprompt starten.

Ich muss ja nicht jetzt nur fragen, wie heißt die Oma von Olli Zeigermann.

Das ist das blödeste Beispiel ever.

Aber ich könnte ihm ja schon sagen, wenn du wirklich keine Ahnung hast, dann antworte mir gar nicht.

Oder bitte antworte mal mit dem statistisch wahrscheinlichsten Ergebnis.

Also was kann ich als User tun, um bessere Antworten zu erhalten?

Also dem System zu sagen, antworte mal bitte nur, wenn du es weißt, hilft nicht.

Weil das Ding ja gar nicht weiß, was es nicht weiß.

Das wäre so, als würdest du zu mir sagen, Olli, sag mal bitte nicht so viel dummes Zeug.

Ich weiß ja gar nicht, dass ich dummes Zeug rede.

Das funktioniert leider nicht, außer wenn du sagst, sag mal bitte nur Sachen, von denen du Ahnung hast und davon hast du jetzt Ahnung.

Bitteschön, hier ist das Wissen.

Das heißt, ohne dieses RAC-System hätten wir das nicht machen können.

Also es geht wirklich nur, wenn du Dokumente reinlädst, dass du dir sicher sein kannst, dass du keinen Quatsch sagst, weil es nicht weiß, was von den Trainingsdaten auch schon Quatsch gewesen sein könnte.

Definitiv oder eventuell nicht nur Trainingsdaten Quatsch gewesen, sondern das ist einfach gar nicht in den Trainingsdaten drin gewesen.

Wenn man so ein System fragt, was ist das aktuellste Sprachmodell von OpenAI und du fragst GPT-4, dann sagt das natürlich GPT-4, weil das nie was, also naja, nicht natürlich, aber wenn du vielleicht eine ältere Version von GPT-4 hast, die vielleicht mit den Daten trainiert worden ist, weißt du vielleicht gar nicht, dass es GPT-5 gibt.

Oder eben Informationen, die niemals im Internet standen, irgendwas über deine Privatgeschichten, das weiß ich ja auch nicht.

Und noch eine Einschränkung, du hast ja gesagt, nur dann kann ich mir sicher sein, wenn ich RAC mache, dass es keinen Quatsch redet, ich kann mir sicher sein, dass es keinen Quatsch redet und deine Aufgabe, wenn man das jetzt noch einen Tick ernster meint als wir, wäre jetzt zu gucken, in welchen Fällen redet das den Quatsch, dass du siehst, wo ist die Grenze von dem System und das ist nie so.

Also diese Hoffnung von, wenn man nur lange genug daran dreht, dann macht das Ding keinen Quatsch mehr, ist mit verfügbarer oder absehbarer Technik nicht möglich.

Das heißt, das Ding wird immer Quatsch reden, du willst es aber natürlich so gering machen wie möglich, so gering wie du es eben schaffst.

Das ist natürlich auch eine schlechte Nachricht, weil du willst ja eigentlich nicht, dass es Quatsch redet, du hast keine volle Kontrolle und das ist der Preis, den wir dafür bezahlen, dass wir eben so ein mächtiges Stück Software benutzen, dass wir die Kontrolle verlieren und uns Schritt für Schritt irgendwie so ein bisschen wieder davon zurückerobern, aber sicher nicht alles.

Also keine gute Nachricht.

Das andere, jetzt habe ich noch eine gute Nachricht, weil ich das jetzt irgendwie nicht einfach so lassen möchte.

Du hast ja gesagt, kann ich ihm nicht sagen, nimm immer nur das wahrscheinlichste Wort, das geht.

Dafür muss ich das leider nochmal wieder abwischen hier, um das zu illustrieren, wie das eigentlich jetzt wirklich funktioniert.

Ich habe jetzt eine Vereinfachung gemacht, sehr viele.

Die eine davon kann ich jetzt mal zurücknehmen, nämlich wenn das Sprachmodell vorher sagt, welches Wort als nächstes kommen soll, dann sagt es nicht einfach das nächste Wort, sondern was da wirklich rauskommt, ist eine Wahrscheinlichkeitsverteilung über die wahrscheinlichsten nächsten Wörter.

Ach und sagst du gleich das Wort Temperatur?

Das sag ich gleich.

Ja super, okay, dann fühle ich mich besser verstanden bei dem Temperatur.

Also als hätte ich das Wort besser verstanden, das wollte ich sagen.

Ach so, das jetzt schon?

Das ist ja cool.

Ich habe da noch nicht mal irgendwie meine wirren Zeichnungen abgeleicht hier.

Aber du hast Wahrscheinlichkeitsverteilung gesagt.

Ah, Wahrscheinlichkeitsverteilung gesagt.

Ich dachte, nur durch Präsenz des großen Mais, das hätte ich jetzt eben schon großartig gefingert.

Nehmen wir mal an, da würde eine Wahrscheinlichkeitsverteilung, ohne irgendwie da Hitze anzulegen oder so, würde sowas rauskommen.

Das hier, hier ist Wahrscheinlichkeit.

Was ist das Zeichen für Wahrscheinlichkeit?

Keine Ahnung.

Irgendwie so ein großes, durchgestrichenes O oder so?

Ich weiß es gerade auch nicht genau.

Kann man da nicht relativ viel fragen?

Naja, das ist mir jetzt zu aktiv.

So, das hier ist jetzt zum Beispiel Wort 1, das wäre jetzt hier meinetwegen Katze.

Das zweite Wort wäre Hund.

Das dritte Pferd oder so, ist schon klar, in welche Richtung das geht.

Das vierte ist jetzt Maus.

Und die Höhe dieser Balken, die jetzt sortiert ist, geht an, wo ist die höchste Wahrscheinlichkeit und was ist die Wahrscheinlichkeit für die anderen Wörter.

Macht Sinn?

Ja.

Und in dem Fall wäre es eben so, Katze ist die höchste Wahrscheinlichkeit.

Wenn du das jetzt normierst, würde das in Summe auch 1 ergeben.

Also alle Wahrscheinlichkeiten, wenn man die vernünftig normiert hat, würden am Ende 1 ergeben.

Und jetzt ist das typische Verfahren, dass du darüber sampelst, dass du so guckst, mit so und so viel, sagen wir mal Katze hatte jetzt 50%, Hund war 30%, Pferd noch 20% und Maus waren 10% meinetwegen.

Dann würde jetzt mit 50% die Wahrscheinlichkeit Katze gesagt werden.

Und mit 30% die Wahrscheinlichkeit Hund, dann 20% Pferd und 10% Maus.

Das würde aber auch bedeuten, wenn du es so lassen würdest, dann würde das bei der gleichen Anfrage exakt dem gleichen Modell, exakt dem gleichen Setting beim nächsten Mal vielleicht eine ganz andere Antwort geben und auch eine ganz andere Antwort generieren, weil das Ding eben nicht deterministisch sagt Katze, sondern nur mit 50% Wahrscheinlichkeit Katze.

Jetzt kommen wir zum Thema Temperatur.

Also man kann sich das vielleicht so vorstellen wie, du kannst jetzt ein bisschen Hitze drauf machen und dann würden, wenn du jetzt mal die Temperatur viel höher machst, dann würdest du jetzt zum Beispiel die Wahrscheinlichkeit für Hund höher werden, die für Pferd auch, das würde immer noch irgendwie sortiert bleiben, aber die würden sich alle ganz noch annähern.

Und wenn du das richtig kochen würdest und richtig irgendwie Dampf drauf machst, dann sind irgendwann alle Wahrscheinlichkeiten fast gleich.

Das wäre eine hohe Temperatur.

Temperatur 0, das Gegenteil davon, würde bedeuten, das ist jetzt irgendwie, kriege ich jetzt glaube ich nicht mehr in die Zeichnung reingefetzt, dass die Wahrscheinlichkeit für Hund, also das zweitwahrscheinlichste Pferd, drittwahrscheinlichste Maus, viertwahrscheinlichste auf praktisch 0 runtergehen und das hier auf 100%, was effektiv bedeutet, dass du einfach das Wahrscheinlichste nimmst.

Wenn man einfach das Wahrscheinlichste sowieso nehmen kann, das machst du auch bei vielen Systemen, dass du einstellen, dass du einfach sagst, nimm das Wahrscheinlichste.

Dann hättest du einen Determinismus.

Wenn alles gleich bleibt, das Modell ist gleich, alles drumherum ist gleich, dann gibt dir das Ding bei gleicher Frage immer die selbe Antwort.

Das ist für uns Kontrollmenschen, insbesondere wenn du was nachvollziehen willst, natürlich toll, da stellen wir die Temperatur auf 0, aber dann ist es eben auch nicht kreativ, wenn du jetzt irgendwie sowas gerne hättest wie, schreib mir mal ein tolles Gedicht, willst ja nicht jedes Mal dasselbe Gedicht kriegen, sondern runterführen.

Ja, das stimmt.

Ich weiß, unsere Zeit ist vorbei, das sehe ich gerade.

Wir haben aber noch eine Frage im Chat, die würde ich wohl noch beantworten.

Also ich nicht, aber du möchtest sie beantworten, so hoffe ich doch.

Blende die mal ein, dann kannst du auch lustig mitlesen.

Und zwar hat Dewtooth geschrieben, dass er leider zu spät eingeschaltet hat.

Er fragt, genau, und Ziel ist es so weit wie möglich, Determinismus in der Arbeit mit LLMs reinzubringen.

Was ist eure Meinung zu M-Babel Framework von Rod Johnson?

Von dem M-Babel Framework habe ich noch nie irgendwas gehört.

Deswegen habe ich dazu keine Meinung.

Weiß ich nicht, finde ich ja spannend.

Jetzt kriegen wir ja natürlich keine Diskussion.

Das google ich dann später.

Ich vermute, das kann irgendwas, was Determinismus da reinbringt.

Das wäre cool.

Das muss ich aber schuldig bleiben, weil ich das einfach gar nicht kenne.

Noch nie was davon gehört.

Rod Johnson ist der Springtrip, glaube ich.

Der Springtrip, das weiß ich auch nicht.

Das musst du alles googlen.

Okay, also das finde ich raus.

Ja, das ist der Springtrip.

Da war ich jetzt schnell genug zu googlen.

Wir vermuten jetzt einfach, dass es im M-Babel Framework irgendwas mit Determinismus macht.

Mir wird es wahrscheinlich gefallen, weil ich das gut finde, wenn was Deterministisch wird.

Muss ich aber schuldig bleiben.

Okay.

Das wurde nochmal bespätetigt.

Das ist wirklich der Springtrip.

Du googlest das im Nachgang.

Hast du irgendwelche sozialen Medien, wo du noch lustig deine Erfahrungen teilst? Überschaubar.

Kann man mich einfach in die E-Mail schicken?

Okay.

Wo bin ich denn überhaupt noch?

Weiß ich nicht.

Ich auch nicht.

Das weißt du natürlich nicht.

Ich habe mich eher selbst gefreut.

Ja, okay.

Willst du nochmal machen, dass man dich sieht, statt dem Bild?

So zum Ende.

Ich muss das machen.

Aber eben haben wir ja sehr schlechte Erfahrungen.

Oder habe ich traumatisierende Erfahrungen gehabt?

Ich trau mich nicht jetzt.

Eventuell bin ich einfach gleich weg.

Das kann sein.

Dann mache ich die Verabschiedung alleine.

Da bist du und man hört dich.

Du bist noch ein bisschen stockend, aber jetzt bist du auch flüssig.

Ja, so bin ich.

Sehr gut.

Ja, Olli, war eine coole Folge.

Ich merke, du hättest auch noch weitere zwei Stunden über das Thema schnacken können ohne Probleme.

Wahrscheinlich.

Ja, aber ich finde, wir haben es tatsächlich geschafft.

Die Sachen, die mir richtig am Herzen liegen und die mir wichtig sind, haben wir jetzt ja hingemalt, inklusive dieser Temperaturgeschichte.

Ich bin ganz zufrieden.

Alles andere wären noch Details gewesen.

Ich bin ja ein Freund des Wesentlichen.

Wir haben alles gesagt.

Mehr muss nicht gesagt werden.

Sehr gut.

Ich muss dir noch einen Kommentar geben.

Am Anfang kam eine Begrüßung.

Ich konnte aber den Namen nicht gut aussprechen.

Du hast auch mal 80er-Jahre-Videospiele in Hackertons programmiert.

Das ist völlig richtig.

Ich habe 80er-Jahre-Videospiele in Hackertons programmiert.

Aber diese Sachen würde ich tatsächlich gerne wieder tun.

Das zählt nicht zu den Dingen, wo ich dachte, das würde ich alles nicht mehr machen.

Spiele programmieren ist ja das Coolste, was man überhaupt tun kann, finde ich.

Kreativ sein, Sinn und Idee haben, in der Lage sein, die Idee umzusetzen.

Und 80er-Jahre-Videospiele.

Computerspiele hatten ja den Vorteil, dass man die wirklich alleine und teilweise in wenigen Tagen umsetzen konnte.

Das kann man jetzt natürlich nicht mehr.

Das ist peinlich.

Das ist sehr schade.

Das würde ich gerne wieder machen.

Dann haben wir das auch noch so zum Ende hingeklärt.

Dann würde ich sagen, danke, Olli, dass du da warst.

Vielleicht hast du ja irgendwann in Zukunft noch mal Lust, dabei zu sein.

Ich hatte viel Spaß mit dir.

Und habe was gelernt.

Ich auch.

Dann wünsche ich euch da draußen allen noch ganz viel Spaß.

Ich kann noch Werbung machen für nächste Woche.

Am 31.10. sprechen nämlich Eberhard und Ralf über KI.

So passend zum Thema Halloween.

Aber es geht um praktische Erfahrungen damit.

Also bestimmt auch ein spannender Vortrag.

Und euch wünsche ich ein schönes Wochenende und noch einen schönen Freitag.

Bis dahin.

Tschüss.

Tschüss.