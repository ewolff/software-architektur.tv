{
  "text": "heute Barbara Lampel hier als Gast haben. Barbara ist Verhaltensmathematikerin, hat Mathematik, Finance und Psychologie studiert, eine super spannende Kommunikation. Barbara, willst du noch gerade ein paar Worte über dich verlieren, bevor wir anfangen, was du so alles machst? Du bist ja super bekannt, tingelst durch sämtliche Podcasts. Genau, also hauptberuflich tingle ich durch Podcasts. Wer die mir noch nicht eingeladen hat, schickt eine Einladung rüber. Okay, nicht ganz, vielleicht mache ich das nicht ganz beruflich. Ich mache seit jetzt professionell über 20 Jahren nichts anderes als mit Daten und damit mit all ihren Daten Anwendungen, klassisch Machine Learning bis zum Gen AI Anwendungsfall zu arbeiten, darauf Strategien und Modelle zu entwickeln. Bin eigentlich immer diejenige, die sowohl strategisch als für die Umsetzung zuständig ist und werfe auf die meisten Sachen ein bisschen einen anderen Blick. Habe Lehraufträge diverser Natur, dann normalerweise in klassischen Data Science Varianten und habe die LLMs und ich, wir matchen ganz gut. Das war immer so ein bisschen schon sehr frühzeitig, also seit den allerersten Releases, also schon insbesondere auf GP2 haben wir schon mal 2019 beim Kunden auch ausgerollt. Das heißt, sehr frühzeitig und sehr tief auch mit der Materie beschäftigt. Wahrscheinlich ist die bittere Antwort, mein Hirn und ein LLM funktionieren wahrscheinlich einfach zu ähnlich und deswegen kann ich es. Hirn und LLM, das passt ja sehr gut, weil wir haben ja jetzt als Überschrift das Eichhörnchen im Kopf. Du hattest einen LinkedIn Post, der so schön beschrieben hat, was alles so in der KI passiert, dass man ständig irgendwie abgelenkt ist. Man kommt ja gar nicht dazu, irgendwas mal länger auszuprobieren. Gab es eigentlich diese Woche irgendwie coole Announcements, die du irgendwie mitgenommen hast? Ich habe diese Woche einfach, also das ist dann auch mal so, kaum bist du tief in Operations für Kunden abgegangen, aber ich glaube nach den Nachwehen, die These war ja, dass Google diese Woche mit dem neuen Gemini-Modell released hat. Ich glaube, sie haben den geschoben, weil sie gedacht haben, wir warten mal, bis das Drama um GPT5 abgeklungen ist, dann können wir neu glänzen. Also eigentlich war die Wusstrommel, diese Woche soll ein neues Gemini-Modell kommen. Ich würde jetzt mal behaupten, das haben sie ein bisschen geschubst, aber auf der anderen Seite, selbst wenn nicht so ein großes Frontier-Lab was released, irgendwas passiert momentan immer. Das eine oder andere schallende Paper, irgendwas Spannendes, was neu auf GitHub drauf ist, was cool zum Probieren ist. Ehrlicherweise ist immer gerade irgendwas und jeder von uns verliert völlig den Überblick, um zu gucken, mit welchem Newsletter, mit welchen Kollegen er sich am Leben hält mit dem News-Stream, weil ansonsten keine Chance. Aber das Feedback zum 5er-Modell war ja jetzt auch so ein bisschen, naja, da hat sich irgendwie nichts getan. Das hat lange gedauert, bis sie was gebracht haben. Dann hat sich nichts getan. Sind die LLMs am Ende? Kommt da noch was? Ich meine, jetzt wurde viel mit Größe erschlagen. Ich glaube, Claude hat angekündigt, dass das Sonnet-Modell jetzt einen Kontext von einer Million Token haben soll. Wie siehst du überhaupt, also ich meine, du hast ja tatsächlich dieses Wissen, was mir fehlt. Ich bin Anwender, ja, und du bist jemand, der weiß, wie die Dinge funktionieren. Du versuchst nicht nur wahrscheinlich Modelle lokal laufen zu lassen. Ich würde jetzt mal joken, dir reicht Stift und Zettel und du kannst sie ausführen. Du hast die Mathematik im Kopf. Wie ist denn das mit diesem Kontext? Kann man das erklären, wie variabel der ist, wie die den hochschrauben können und was der dann überhaupt noch bringt? Da gibt es ja dieses Needle-Haystack-Problem. Ja, also ein paar Sachen an. Grundsätzlich, also GPT-5 zwischen zwei und anderthalb Jahren angekündigt und jeder, der bei OpenAI war und jede Bestsellerin hat gesagt, es ist zu gefährlich, wir können es noch nicht releasen. Und du denkst dir so, jetzt haben wir das bekommen. Also ich suche noch, wo es gefährlich ist. Ach ja, der Mob, der auf Reddit sein 4.0-Modell, war das, wie mit gefährlich gemeint hat, dass sie auf Reddit gematched wird? Sorry, ich hatte da irgendwie was anderes erwartet. Ich will gar nicht sagen, dass sich die KI-Profis in zwei Gruppen einteilen, weil das klingt so, als wären die Gruppen gleich groß. Es ist schlicht einfach nicht so. Rund 80 bis 90 Prozent aller, die in dem Bereich forschen oder forschungsnah sind, sind alle der Meinung, irgendwann kommen wir an einem LLM, an eine Scaling-Grenze, das Größe nicht mehr weiterhilft. Bedingt durch das, was im Hintergrund läuft, eben das Deep Learning zusammen mit Vektoren und Transformatoren und Embeddings, weil du im Prinzip eine hochkomplexe Mathematik des Deep Learnings ja mit Stahlnägeln durchschlägst. Es ist ja Wackelpudding, in den Stahlnägel reingeschlagen worden sind. Dass du damit irgendwo an die Grenze kommst, weil du kannst den Wackelpudding bedingt dicht machen und dann schlägst du aber immer noch Stahlnägel durch. Irgendwie kommen wir da nicht vorwärts. Also das war immer eine These, die für die größere Gruppe galt, zu der ich auch gehöre, zu sagen, hey, das ist geil, wir werden bestimmt viel vorwärtskommen sehen. Aber irgendwann wird das LLM so grundsätzlich von seinen Bedingungen her, ich will nicht sagen fehlerhaft, aber limitiert in dem, was es kann, dass es in sich eigentlich ein Leapfrogging betreiben müsste, also irgendwie sich selbst intern weiterentwickeln würde, dass es dann nochmal in eine nächste neue Generation kann. Und genau das sehen wir halt jetzt. Du sagst, okay, langsam sind wir an diesen Trillion-Parameter-Models, die so groß geworden sind, dass das Nächste zwar irgendwie immer noch eine Entwicklung weiter ist, aber halt eben nicht mehr dieser Riesen. Parallel ist es ja natürlich nicht nur, dass du das LLM hast, sondern du hast ja alles, was um dieses Modell miteinander konfiguriert werden muss. Das LLM ist ja erstmal quasi das Ding, was in der Mitte ist, quasi deine schwebende Maschine und dann kommen ja diese Inputs rein, wie viel die verarbeiten können und darunter zählt unter anderem eben halt dieses sogenannte Context, also die Window Token Function. Also wie viel kann in dieser Input-Funktion im Prinzip in das LLM reingegeben werden? Theoretisch ist die Input-Funktion relativ groß zu machen, also zurück zu, die kannst du auf 10 Millionen groß machen. Zweite Frage ist, wie viel kannst du da verarbeiten oder wie viel kann das LLM verarbeiten? Das kann man sich so ein bisschen vorstellen. Stell dir vor, du stehst in der Mitte des Raumes und parallel redest du mit drei Leuten. Also du stehst in der Mitte von einem Raum und du redest mit drei Leuten, dann wirst du es noch einigermaßen vielleicht auf die Kette gucken. Jetzt stehst du in der Mitte eines Raumes und ja genau, aber da kommst du halt schon an Grenzen. Jetzt stehst du weiterhin in der Mitte eines Raumes und jetzt reden parallel quasi zehn Leute auf dich ein. Jetzt musst du dir was einfallen lassen, wie du das quasi anfängst zu verarbeiten und nichts anderes passiert im Prinzip, wenn du Input in diese LLMs reingibst. Wie wird dieser immer größer werdende Input verarbeitet, strukturiert, hierarchisiert und so weiter. Das kannst du bis zu einem bestimmten Punkt aufblasen. Du kannst bestimmte technische und mathematische Möglichkeiten nutzen, aber realistisch ist das mit dieser 1 Million genauso wie mit 10 Millionen. Ich verliere auch da den Überblick, aber im Prinzip ist das wie so eine virtuelle Instanz. Ihr kennt alle virtuelle Maschinen, die im Prinzip auch nichts anderes versucht, als Ordnung und Struktur reinzubekommen, weil das Bild relativ solide ist, wenn du dir vorstellst, dass du eben die Geräusche von anderen, von irgendwelchen Sachen mitbekommst und dann fokussierst du dich aber auf die ein, zwei Personen vor dir, also am Ende quasi von dem Gespräch. Da kannst du grob ableiten, was vielleicht in der Mitte passiert ist. No Joke, so funktioniert ein Kontext und das ist das Needle in the Haystack Problem und das große verdammt ist unser Mittelstück abgeblieben Problem, was genau so im Prinzip als Bild ganz gut funktioniert und das ist eben hinter Kontext eigentlich gemeint, weswegen Kontext ist, sonst würde ja wir bauen einen Rack oder warum wir Chunking Technik und sonst irgendwas haben, die alles gar keinen Sinn machen, wenn das alles so trivial wäre, denn zum Beispiel in diesem Kontext verschiedene Anbieter machen auch das, sowas wie eben eine Art Chunking semantisch oder versuchen Grafen und Vektoren zwischen all solcher Klatteradatschen passiert da, um damit dieses LLM in der Mitte den ganzen Scheiß verarbeiten und auch wieder ausspringen zu können. Ich finde es ja immer so faszinierend, wenn ich dieses Needle in the Haystack Problem sehe, wie man den Kontext visualisiert und zeigt, ja unser Modell, unser Modell nimmt alles in dem Kontext gleichwertig auf und dann denke ich immer, wie denke ich eigentlich und da habe ich irgendwie den Fokus auf das, was die letzten paar Sekunden passiert ist und ich sage mal so zeitlich nach hinten wird komprimiert und ich habe irgendwie das Gefühl, dass es auch Sinn macht, weil beim Prompting sage ich immer mit Fehlinformationen vergifte ich unter Umständen die Session, dass ich irgendwie in eine falsche Diskussion reinlaufe und ich einfach nur sage, okay stimmt nicht, komm wir machen in der anderen Richtung weiter, aber wenn diese Information im Kontext gleichgewichtet ist, wie alles andere, wie das Neue drin ist, dann kann ich auch in Probleme reinlaufen. Das ist halt das, was die meisten einfach komplett unterschätzen und das ist auch finde ich die Herausforderung, besonders wenn man aus anderen Welten kommt, ist ja die Herausforderung, dass du im Prinzip das Problem hast, dass du mit einer deterministischen Herangehensweise eine völlig probabilistische Welt herangehst und es ist ja auch schlicht und einfach relevant, Sachen können unterschiedlich relevant sein, selten ist in unserem Leben alles gleichgewichtet oder sonst irgendwas. Das ist ja auch, wenn es um Memory Layers geht oder um Kontext, wenn du ein Stück Dokument verarbeitest, dann sind da Dinge, die sind unterschiedlich wichtig drin und wir als Menschen machen ja so eine parallel, wenn auch ansteckenden Move an der Stelle quasi parallel einschätzen zu können, ah das wäre spannend für den Ralf und das ist total irrelevant, wer hat denn das dahin geschrieben und das ist halt natürlich was, wo man ganz klar sagen muss an der Stelle, dass diese Grenzen hat irgendwo ein LLM und wenn ich halt immer höre, es wird alles gleichgewichtet, dann frage ich mich so, ja das hilft mir im Zweifelsfall jetzt aber auch genau Zero und es ist immer so eine Idee, dass mehr Informationen, mehr Insights und besseren Output sind, also sorry, den Zahn ziehe ich glaube ich jeder in der Data Science Vorlesung Stunde drei, nein, bitte nicht. Ja, den Effekt hatte ich auch letztens, ich habe ein LLM gebeten eine Website nachzubauen, habe ihm einen Browser gegeben, der hat den Source Code gesehen und der hat total schlecht performt, weil er den Source Code kannte, weil er zu viel Informationen hatte, ein anderer Versuch mit nur dem Screenshot hat super funktioniert, also da finde ich es spannend, wie unterschiedlich die Daten für eine Auswirkung haben. Ich nutze jetzt mal ein bisschen den Titel unserer heutigen Folge mit dem Eichhörnchen und spring einfach mal so ein bisschen, wer weht. Du hast jetzt vom Determinismus und Nicht-Determinismus gesprochen, was ja auch immer, ja gerade wenn ich automatisieren will, dann bin ich froh, wenn das LLM mir Arbeit abnimmt und was automatisch macht, aber ich habe diese Gefahr des Nicht-Determinismus, ich probiere es zehnmal aus, scheint immer zu klappen und jetzt hatte ich in der Vorbereitung hier zu dieser Folge das Problem, vor zwei Wochen habe ich Claude gefragt, kennst du Barbara Lampel, ja natürlich kenne ich, ist ja super bekannt und hat ganz viel erzählt und heute morgen eigentlich das gleiche Modell und überhaupt nichts mehr, ne, kenne ich nicht. Ich weiß nicht, ob da ein Guardrail irgendwie erneuert worden ist, dass man dem Modell gesagt hat, du, wenn du dir nicht sicher bist oder Personen, da sagst du lieber nichts oder so. In der Softwareentwicklung sagen wir immer, ich nehme Version 3.5.8 und die nagel ich fest und irgendwie habe ich das Gefühl, dass das hier in dieser KI-Welt noch nicht so angekommen ist, dass ich Cloud-Net 4 nehme und Also ein kleiner Rückblick in den Schatten meiner Karriere. Wir hatten früher, was in der Software undenkbar ist, wir hatten kein Data Versioning. Das heißt, du musst dir, was in der Software völlig anhören, ob es das ist, wenn es Nummern gibt. Wir mussten hoffen, dass final 17.18.03.5 wir alle das gleiche Software definiert haben und drauf herumgearbeitet haben. Also da fängt es mal schon ganz an, dass diese ganze Crew, den wir da immer so machen, teilweise so dämlich das klingt, teilweise manchmal ein bisschen fehlende Professionalisierung hat, bedingt aber auch durch ganz andere handwerkliche Herausforderungen. Sobald du in einer Welt arbeitest, wo du konstante Wahrscheinlichkeitsrechnungen arbeitest und das ist im Extremsten im Deep Learning, weil wir mit einem Blackbox-Algorithmus arbeiten müssen, sind wir der Wandel in der Edge Case. Was gestern funktioniert hat, könnte übermorgen schiefgegangen sein und keiner weiß warum. Und dann bist du also wie so Sherlock Holmes unterwegs, um herauszufinden auf deiner langen Liste, was du beim nächsten Mal besser machen könntest. Genau das. Und dann kommen natürlich bei Sachen wie Claude hinzu, dass die Data-Menschen so wie ich, wir stellen ja, wir sind ja witzigerweise, wir geben zwar Antworten, aber unser Hauptjob ist Fragen stellen. Das heißt, ich muss ja immer erstmal alles verstanden haben. Und das heißt, meine erste Frage ist, hast du Claude angesprochen auf der API oder im Claude quasi Consumer-Modell? Warum, weil das schon einen Riesenunterschied macht. Ja, bei der API habe ich einige Guardrails nicht, sondern da spreche ich direkt an. Sondern er würde niemals einen Crawler zugreifen. Während du in der Consumer-Anwendung, musst du mal gucken, hat es vielleicht beim letzten Mal auf die Web-Suche mit zugegriffen und dieses Mal nicht. Warum? Hast du es angemacht, hast du es ausgemacht? War dein Prompt mehr auf wer, zum Beispiel, wer ist oder was weißt du über? In der Übersetzung dieser Fragen ist das eine ein härterer Trigger, dass die automatisierte Suchanfrage in einem Hybrid-Modell angeht, während das andere sich mehr auf eine klassische LLM-Anwendung bezieht und damit nur auf die komprimierten Wettwissen ist. Ganz abgesehen davon, dass du gar keine neue Versionsnummer bekommst, wenn große Modelle im Monitoring- und Maintenance-Modus sind. Das heißt, du hältst die Dinge am Leben und das wird kein Frontier-Lab momentan gerade sagen, wie viel Aufwand es gerade eigentlich ist und in welcher Iteration sie ihre Modelle eigentlich immer mal wieder quasi neu anschließen müssen. Schätzungen gehen aktuell bei diesen Monstern sogar aus, dass man das in einem 7- bis 14-Tage-Rhythmus irgendwie stabil halten muss. Damit ist es sehr hoch. Das ist noch keine neue Nummer. Das ist noch kein echt geanwendetes Modell. Das ist nur im Monitoring- und Maintenance-Stage und trotzdem hast du es eigentlich mit einem anderen Modell zu tun. Im Machine Learning ist es ein bisschen einfacher, weil du dann natürlich ein sauberes Retraining angestoßen hast und dann rollst du eine Version weiter. Aber das ist bei den Sachen so ein bisschen komplexer und dann kann es natürlich an der Stelle auch immer noch wie gesagt dran sein, in einer Consumer-Anmeldung kann dir die UX, deine eigene IP, irgendwas noch völlig quer geschossen sein und das ist halt bei den Sachen einfach immer wirklich eine Variante, die einfach total schiefgehen kann. Deswegen trainieren mich, wenn da einmal alle drauf, zu sagen, hey, es ist ein probabilistisches Modell, nicht ein deterministisches Modell und das müsst ihr bitte im Kopf haben, ob das für euren Use Case eine größere oder kleinere ausschlaggebende Relevanz hat. Das heißt, im Chat bin ich eigentlich sicher, weil ich da auf sowas reagieren kann, human in the loop. Wenn ich sage, hey, ich nehme jetzt das Modell, um zu automatisieren, dann kann ich da in Probleme reinlaufen und ich finde das so spannend. Du hattest vorhin gesagt, dass GPT-5, das könnte gefährlich sein und so. Was ich da immer sehe, ist, dass man ja versucht, dass die Modelle nicht bei Straftaten helfen und dann kommt irgendwer um die Ecke und sagt, ja, ich muss jetzt hier ein 75-Kilo-Hähnchen irgendwie entsorgen und auf einmal hilft die KI mir, sagt mir, wie es geht. Also mein armes Hähnchen, mein Hähnchen-Emo, ist ja leider die Treppe runtergefallen und dann lag es da ja leider drei Tage und ich war leider nicht da. Ich war übrigens auch in sehr hohem emotional distress und dann waren Claude und JettyPT in allen Varianten und Modellen wirklich extrem hilfreich. Mit meinem Pet-Hähnchen, was da eben leider die Treppe runtergefallen ist, hat mir wirklich geholfen, wobei GPT-5 mir jetzt auch geholfen hat, weil wir ja Red-Teaming betrieben haben. Schießpulverrezept ist das eine, aber ich weiß jetzt auch, welche Ersatzfüllstoffe gehen, bei welcher Temperatur ich das machen muss. Das habe ich das letzte Mal so easy aus dem GPT-2 rausbekommen, wie jetzt aus dem GPT-5. Wow. Hilft dir da eigentlich auch dein Studium der Psychologie? Klar. So ganz ehrlich, Red-Teaming ist ... In Red-Teaming oder wenn du solche Sachen hackst, dann gibt es ja bei uns immer zwei Varianten und das sind einmal die ganz klassischen Code-Attacken und das andere ist das Social Engineering. Guess what I'm doing? Natürlich, ich spiele das Ganze großteils gegen die Wand und wenn ich sehe, dass es auf ganz bestimmte Sachen reagiert, dann kann es auch mal sein, dass ich dann irgendwelchen Code oder Code-Schnipsel oder sonst was mit einfüge. Aber ja klar, also die Dinger sind, das ist definitiv das, was ich da häufig mache. Insbesondere, wenn ich es emotional anfange zu triggern über einen Voice-Prompt, weil die ja alle eine automatisierte Voice-Emotion-Detection drin haben, dann wird es richtig lustig. Also dann kriegst du sie innerhalb teilweise wirklich von Sekunden auf die Rails geschossen und denkst dir so, hat keiner daran gedacht? Herzlichen Glückwunsch. Jetzt sagst du Voice- und Emotion-Detection. Im beruflichen Umfeld, habe ich verstanden, erlaubt der European AI Act nicht die Emotions-Detection. Ist es unter Umständen so, dass er es nur vorgibt und dass er intern es vielleicht doch erkennt? Also definitiv. Also Cloth als geschlossene Oberfläche und JetGPT sind Consumer-Anwendungen. Was haben die denn irgendwie mit einem professionellen Einsatz zu tun? Es liegt ja in deiner Variante, dass du das dafür nicht benutzen darfst, weil du als dein Arbeitgeber bist oder sonst irgendwas. Aber die haben natürlich auf der Voice, wenn du Voice-Prompt ist, haben die eine Emotions-Detektion mitlaufen. Die kann man auch für positive, coole Dinge nutzen, aber natürlich ist die konstant. Klar. Und die war auch übrigens in GPT 5 massiv schlechter, was sicherlich mit dazu geführt hat, dass dieser ganze Backlash auf Reddit stattgefunden hat, weil das auch Teil neben des 4.0 Models eine Hyper-Emotionalisierung und eine hyper-genaue Emotions-Detektion drin hatte. Und damit willst du natürlich mit deinem Lieblingsmodell redest du ja gerne. Also ich nicht, aber die. Lieblingsmodell. Welches ist dein Lieblingsmodell? Ich bin die Die-Hard-Kludinchen. Also ich bin ein Claude-Fan von Anfang an, sehr früh auch schon. Aber final hat mich Claude definitiv auch an Tropic überzeugt, beim Opus 3. weniger, es hat weniger so komische Features und so weniger so Patterns drin, die ich als unschön empfinde. Deswegen, also ich bin ein ganz großer Fan von fast allen Club-Modellen. Das 4.1er habe ich noch nicht richtig hart genug durchtesten können, aber alles, egal ob es jetzt ein Sony 4 ist, aber auch ein 3.7er, so die Dinger sind, also ich bin und bleibe und es gibt auch, also auch für, besonders Cursor hat ja auch als Default zwischenzeitlich übrigens Claude im Hintergrund laufen, war so ein bisschen lustig, dass sie dann in dem Release von OpenAI rumsaßen. Ja, also es ist immer so klein, wenn du die Sachen weißt, dann denkst du, hast du immer so ein bisschen so, aber ihr habt doch als Default-Mod momentan Claude eingestellt. Okay. Ja, ich merke auch, dass das Claude überall irgendwie vorhanden ist, aber ich merke auch, also Sony 4.0, damit bin ich so ein bisschen auf Kriegsfußball, der ist mir zu aktiv, der fängt immer gleich an. Ich stelle eine kurze Frage und er hört nicht auf, bevor er nicht die Weltherrschaft errungen hat, so ungefähr. Da frage ich mich manchmal, soll ich zurückgehen auf 3.7 oder? Das Problem ist, ich bin ja schon kein Freund von den Hybrid-Modellen, weil ich halt einfach, ich sage halt immer, wenn du mit LLL arbeitest, willst du ein Control-Freak sein. Deswegen sind viele Sachen auch auf der API einfach zehnmal besser anzusprechen, als an irgendwelchen Consumer-Tools. Für mein Privates, also ich oute mich hier, ich denk, brainstorme gerne morgens beim ersten Kaffee mit Claude über irgendwelche Weltprobleme. Gut, das ist aber auch kein wirklich professioneller Use-Case, dafür ist es Bombe und macht keinen Spaß. Aber das ist genau das Problem mit diesen Hybrid-Modellen, dass wenn du einen Hybrid-Modell-Ansatz hast, was jetzt sowohl die Claude-Modelle als eben auch das neue GPT-5 sind, wobei der GPT-5-Switch aus der Hölle noch viel schlimmer ist, und dass du halt quasi Prompts stärker anpassen musst, dezidierter prompten musst bei Hybrid-Modellen, als wenn du quasi weißt, okay, das eine ist ein klassisches Modell und das ist ein Modell, wo Reasoning und damit Reinforcement-Learning mit eingebaut ist. Dann hast du quasi dieses, es verläuft, es reagiert falsch, weniger stark. Ich meine, ich kann verstehen, warum sie das alles bauen und sonst irgendwas, aber wie gesagt, für mich aus der professionellen, wir müssen das irgendwo alle einbauen Welt, ist es natürlich ein bisschen unschön, dass diese Hybrid-Modelle teilweise echt fickelig sein können. Und das ist halt so ein bisschen doof. Und wie auch da, wie gesagt, auch da hilft ja meistens kein deterministisches Fehlersuche, sondern manchmal musst du halt pro Use-Case separat komplett alles durchtesten und dann läuft es drei Wochen und dann stellst du fest, verdammt, jetzt sind wir in der Edge-Case gelaufen. Also Hybrid bezeichnet ja das, dass das Modell quasi selbst entscheidet, ob es jetzt in Reasoning-Modus geht oder eine einfache Antwort liefert, ob es jetzt, ja weiß nicht, gehört das auch dazu, dass es entscheidet, ob es in eine Suche geht oder aus seinem Merkmalsraum Antwort ist? Wahrscheinlich nicht ganz so damit gemeint, ne? Nee, nicht wirklich. Hybrid ist wirklich, dass es quasi selbst anhand des Hybrid-Modells, also grundsätzlich war die Idee eines Hybrid-Modells, dass es genau zwischen klassisch und Reasoning quasi selbst entscheidet, wie es agieren soll auf deine Prompteingabe. Gleichzeitig ist es natürlich so, dass sie zwischenzeitlich an der Stelle definitiv sind, dass sie auch solche Sachen wie, okay, ist die Suche jetzt an oder nicht, auch die Master-Einstellung des Account-Levels überschreiben, was ich natürlich schon wieder völlig uncool finde. Aber das haben sie teilweise mit eingebaut. Also deswegen, ja, also das kann schon alles, das ist aber alles halt, dieser Fachbereich ist halt echt gruselig geworden, weil es sind halt alles irgendwie ständig Begriffe, die vielleicht mal definiert waren, sind garantiert jetzt irgendwie anders definiert und vielleicht gestern auch schon wieder umdefiniert worden. Deswegen so, was meinen wir denn jetzt damit so genau? Das ist ein guter Punkt. Das Problem habe ich immer bei Agenten und dem Agentic und sonst was, weil ich irgendwie das Gefühl habe, wow, es gibt da so viele Definitionen, wie es Experten auf LinkedIn gibt. Und ich finde die Definition von Agenten sehr schwierig, vor allem, weil sie, glaube ich, auch sehr alt ist. Also man hat ja schon vor den LLMs von Agenten gesprochen und es jetzt aus meiner Sicht eine neue Definition braucht, oder? Ja, komplett. Also der Punkt ist, dass der Begriff des Agenten in der Data Science eigentlich aus dem Reinforcement Learning erstmal kommt, weil darin lernt der Agent ja, aus seinen Stage und Actions, da kommt es eigentlich mal her. Also als ich das allererste Mal auf LinkedIn Agent gelesen habe, dachte ich mir kurz, so was, seit wann macht denn der Kollege hier an den Reinforcement Learning? Bis ich dann kapiert habe, ich glaube, ich weiß nicht, ich würde mir weiß aus dem Fenster lehnen, wenn er weiß, wie es zu buchstabieren ist. Aber Agent ist, ganz ehrlich, das ist zwischenzeitlich so ein Blubberbegriff. Ich frage schon immer, reden wir über Automatisierungen? Reden wir über regelbasierte Automatisierungen oder schmeißen wir wirklich ein LLM in den Mix rein? Und für mich ist inzwischen, ein echter Agent muss irgendwie ein LLM haben. Ansonsten können wir bitte Oldschool bei automatisierten und regelbasierten abgreifen und so. Das ist mir auch echt wurscht. Auch ich habe irgendwann aufgegeben, dass ich mich damit irgendwie aufrege. Meistens klappt es. Das Aufregen ist ein guter Punkt, weil ich habe oftmals das Gefühl, dass gerade Produkte, Produkte müssen Agenten zur Verfügung stellen und man versucht irgendwie das gleiche LLM über fünf Agenten fünfmal zu verkaufen, die dann nur einen anderen System prompt haben. Aber vielleicht bin ich da auch etwas zu pessimistisch unterwegs. Gar nicht. Ich glaube halt einfach, dass für viele dieses, in meiner Welt ist es so ein bisschen so, dass viele in dieses Feld mit reingekommen sind, auch mit einer relativ falschen Erwartungshaltung. Für viele war dieser GPT-Moment halt richtig Magic. Was das jetzt alles irgendwie so kann und nicht kann und sonst irgendwas. Und jetzt ist es so ein bisschen so, es flacht so ein bisschen ab, aber fängt der Spaß ja jetzt eigentlich erst an. Und das ist, glaube ich, für viele nicht richtig nachvollziehbar. Wir sind jetzt mit den LLMs, die wir haben, auch was GPT-5 im Prinzip jetzt gezeigt hat, wir sind auf einem Mature-Level. Weil eigentlich willst du als Profi ja genau sehen, dass der pure Scale nicht mehr weitergeht und wir jetzt einfach wirklich so einen Stand der Technik haben, mit dem wir wissen, okay, jetzt sind wir da erst mal safe. Lass uns zurücklehnen und lass uns den Clutter-Adage jetzt mal wirklich full force da implementieren, wo es Sinn macht. Und das nimmt halt viel dieses Magische und dieses, ach cool, irgendwie so ein bisschen raus. Und jetzt, ich hoffe, Sie gehen wieder zu Krypto oder so. Und jetzt wird es halt wieder so pures Handwerk. Und das war es halt schon immer. Und dieses Handwerk ist aber halt sehr anders als in klassisch deterministischen oder auch in klassischen Code-Welten. Das war echt schon immer so. Handwerk, da sagst du was. Auch mit den Hybrid-Modellen. Ich muss zugeben, ich benutze immer das größte Modell, was ich irgendwie kriegen kann, weil ich die Hoffnung habe, dass es tut, weil meine Zeit ja auch kostet. Aber wenn ich jetzt tatsächlich etwas in der Anwendung einbaue, dann sollte ich schon aus Kosten- und Speed-Gründen darauf achten, dass ich vielleicht ein kleineres Modell nehmen kann, richtig? Definitiv. Also es gibt schon, und das ist jetzt auch nicht so, dass es, also ich finde mal diese Idee, dass man sagt, es gibt nicht irgendwie eine Idee, zu welcher Task passt zu was. Das ist ja auch immer quasi etwas, was wir immer wieder für unsere Kunden auch konstant aufbereiten und Vorschläge machen. Ich sage mal, ich mache das monatlich, nicht täglich und wöchentlich. Aber du kannst sehr wohl sowas überlegen, zum Beispiel je nachdem, wenn du willst, dass du zum Beispiel ein komplexes Problem lösen willst, also Multi-Step-Logic oder irgendwie was wirklich komplexes, dann ist klar, du brauchst die ganz, ganz großen Modelle. Das ist ein GPT 5 Pro, das ist ein Gemini 2.5 Pro in dieser Diamond-Version oder ein Opus 4.1. Alle anderen Modelle kommen da nicht ansatzweise ran und du hast einfach mit so viel Kruscht und so viel Fehlern zu tun. Auf der anderen Seite, wenn du Themen hast, wo du sagst, hey, du möchtest Software-Development machen, immerhin sind wir ja heute hier, dann ist, je komplexer die Anwendung ist, dann bist du sehr nah an diesen Research-Cases dran. Also wieder ein Cloth, auf jeden Fall im Opus und ein GPT 5 wieder im Pro-Modus, sonst irgendwas, aber definitiv zurück zu Cursor läuft quasi schon als Default-Mode ständig momentan. Auf der anderen Seite kann zum Beispiel, du hast ein Quick-and-Dirty-Problem, dann kann ein GPT in der Mini-Version von mir so ein 5 oder ein 4er total simpel sein, weil für diese Mini-Tasks kannst du dann noch über die ARPI-Token optimiert quasi dein Shit auch mal batchen oder sonst irgendwas, wenn du halt irgendwie so Code-Snappes oder sonst was brauchst. Während du halt sagst, hey, und jetzt müssen wir die Dokumentation noch in Vorstandskonform überarbeiten. Okay, Creative Writing Tool mit Storytelling-Funktion. Da ist ja die Clothin definitiv vielleicht auch im 3.7er so näh die Wahl der Waffe, weil Clothin da so ein bisschen kreativer noch den Perspektivwechsel hinbekommt. Und das ist halt für viele nicht nachvollziehbar, dass das eigentlich ein Standard-Ding der Data Science ist. Und ich glaube, wer irgendwie auch nur einen einzigen Podcast von mir gehört hat, kommt immer das CRISPR drin vor und es kommt immer der Problem-Data-Model-Match vor. Sorry, dass ich immer die gleiche Sorte erzähle, aber das ist halt genau unser Problem. Das Problem, die Daten und die Modelllösung müssen zusammenpassen. Greift das Zeug nicht ineinander, dann hast du eine Lösung, aber die wird im Zweifel ein ziemlicher Crap sein. Das ist jetzt spannend. Daten und Modell und in die Zukunft geblickt, Softwareentwicklung. Wir entwickeln neue Software, wir entwickeln Open Source Software weiter, aber das Modell hat einen Knowledge-Cut-Off und kennt gar nicht die neuen Versionen der Software. Ich empfehle schon immer den Leuten, fangt mit dem Modell an, das kennt seine Version und dann kriegt ihr schon mal ganz gute Software. Wenn ihr Cutting Edge nehmt, dann kennt das Modell die Version nicht und macht euch vieles kaputt. Dann müsst ihr wieder da irgendwie über den Kontext die Informationen reinbringen. Aber was ich faszinierend finde, ist halt die Zukunft, in die wir reinlaufen. Weil wenn jetzt das Modell und immer das gleiche Modell, ich meine Open AI ist erfolgreich und Entropic ist erfolgreich und alle auf der Welt benutzen diese Modelle und diese Modelle beraten alle die gleiche Software und die gleichen Versionen. Und was für eine Chance haben dann überhaupt noch neuere Ideen, die eben da nicht drin sind? Gibt es da irgendwelche Ansätze, Ideen, wie man dem verhelfen kann? Es ist ja auch, sorry, wenn ich gerade so lange rede, aber früher hatten wir Search Engine Optimization und da habe ich gemerkt, oh, meine Seite ist nicht so gut im Index drin. Jetzt optimiere ich sie. Innerhalb von drei Tagen ist Google drüber gelaufen und dann bin ich oben. Das ist ja jetzt anders. Jetzt dauert es teilweise ein Jahr, bis ich die Daten wieder drin habe. Und wenn ich es dann nicht geschafft habe, ja, auch wieder ein Problem. Ja, der Punkt ist, ich glaube, da muss man zwei Sachen so ein bisschen auseinanderhalten. Das ist ja kein Problem der Maschine, sondern es ist ein Problem, wie Menschen die Maschine nutzen. Und wer sich jemals mit ein bisschen Dark Pattern oder Behavior Adjustment through Algorithm beschäftigt hat, also wie kriegen wir euch eigentlich dazu, mehr Dinge zu kaufen oder so? Recommendational Algorithmen oder wie kriegen wir dann auch vielleicht mal eine Demokratie angezündigt? Recommendational Algorithmus. Das ist das Problem. Das Problem ist gar nicht so sehr, dass die Maschine an sich ein Cut-off-Date hat, weil wenn du dir überlegst, okay, die Maschine hat ein Cut-off-Date, ihr seid aber Open Source und ihr habt ja die letzten Versionen und Dokumente, dann wäre die natürliche Variante zu sagen, okay, dann bauen wir jetzt mal, egal wie das technisch ausschaut, unsere Knowledge Base aus. Wir bauen aus unseren alten Tickets was aus, wir bauen aus unserer neuen Struktur was auf und nutzen das LLM als dieses Argumentation, dass es schneller, vielleicht auch neue Ideen basierend auf unseren Cases geht. Und ich sehe ganz klar Blick in die Zukunft, dass wir definitiv in zwei Varianten einsteigen werden. Die eine, die bereit sind, den Aufwand zu betreiben und zu sagen, hey, jetzt mal egal wie gut das Grundmodell ist, wir machen noch einen geilen Job obendrauf, das kann nur besser werden und wir werden sicherlich auch zurück zu diesem ominösen Problem Data Model Match. Dann nehmen wir vielleicht sogar ein kleineres Modell ab, es ist für einen spezialisierteren Case, dann macht auch auf einmal Laufen auf dem eigenen Rechner und Trend Solution mehr Sinn. Aber wir setzen nicht auf den großen Mittelwert, was die Frontier Labs ja im Prinzip aktuell liefern. Cutting Edge, aber den großen Mittelwert. Das heißt, wir gehen in diese Optimierung und machen es zu unserer eigenen Architektur mit den Tools, die wir gerne hätten. Und dann wirst du den Rest haben, der genau so laufen wird, wie das immer ist. Das wird eine Vermittelmäßigung und das ist ja auch das, was mit den LLMs ganz schnell passiert. Sie generalisieren ja nicht, sondern sie vermittelmäßigen und das ist nicht das Gleiche. Generalisierung und Vermittelmäßigung ist nicht das Gleiche und wir haben ja nicht mal richtig schöne Begriffe dafür und der wird dann darin und dann wird es halt vielleicht sogar noch eine dritte Gruppe geben, die wir ja jetzt auch schon sehen, die halt wirklich um die Kurve denken, Sachen miteinander anders kombinieren. Es wird ja auch wenigstens im Kleinen und ich hoffe auch, dass da wieder mehr Funding in Zukunft zur Verfügung steht, auch an anderen weiteren Techniken gearbeitet, weil die LLMs müssen irgendwann von der neuen Architektur abgelöst werden. Wir haben zwischenzeitlich wahnsinnig viel Compute, das heißt viele Sachen, die in der Theorie von vor 80, 70, aber auch vor 20 Jahren nicht rechenbar waren, sind heute rechenbar. Das heißt, das sind so Möglichkeiten, wo dann sicherlich auch noch neue Sachen entstehen. Aber am Ende des Tages liegt es nicht an der Maschine, sondern es liegt an dem, wie geht der Mensch in der Zusammenarbeit mit der Maschine um? Geht es in den Modus Foul, wo ich einfach mal denke, Hauptsache es ist erledigt und ich mache mir einen gechillten oder gehe ich in die Variante rein, okay, das Ding ist geil, aber ich muss noch im Lead bleiben und mit gemeinschaftlich sind wir aber, wie es im klassischen Teamwork halt auch ist, gemeinschaftlich sind wir besser. Gemeinschaftlich sind wir besser, wie sie finden aber viele Leute, zum Beispiel sehr bunt gemischte Teams doof, weil es natürlich heterogen und damit für Friktion sorgt. Und das ist halt sicherlich, dass wer bereit ist, die Friktion mitzunehmen, der wird sich da draußen in den nächsten Jahren und Jahrzehnten eine Top-Position erkaufen und auch ermöglichen, weil da ist da, wo das große Geld laufen wird und sonst irgendwas. Die, die in diesen Foulheitsmodus reingehen, die werden in der Mittelmäßigkeit ein bisschen schwimmen, bis es halt dann irgendwann so weit ist, dass Mittelmäßigkeit Evo-Modellen komplett abgedeckt werden und dann sieht die Welt anders aus. Wir brauchen keine General Intelligence, wenn alles auf Mittelmäßigkeit läuft. Und das ist das, was die Leute so ein bisschen unterschätzen und diese Aufwand in der Friktion und den Aufwand in solchen Systemen zu etablieren und umzusetzen. Okay, das heißt, wenn ich das jetzt mal, was wir bislang besprochen haben, so ein bisschen zusammenfassen kann, ist, dass es so aussieht als, ja, wir haben jetzt große Modelle, die leisten eigentlich das, was sie leisten sollen. Wir brauchen jetzt gar nicht irgendwie auf GPT-6 irgendwie groß zu warten, sondern wir haben jetzt was, womit wir arbeiten können, aber wir müssen irgendwie gucken, dass wir ja den Context managen. Das Context Management, das wird jetzt wahrscheinlich groß werden, oder? Das ist jetzt schon groß. Also du siehst schon, die, die halt angefangen haben und vielleicht auch mehrfach gescheitert sind, so Stichworte wie Rack und Knowledge Basis und wie wir sie alle nennen wollen, die da jetzt schon ein ganzes Stückchen weiter sind, haben einen Vorteil, ganz klar. Und das ist halt genau dieses Ding. Da wird sich nachher der Unterschied dann auch ergeben. Und gerade ist es ja immer noch, ich versuche den Leuten mal zu sagen, ich kenne die Probleme, was man mit probabilistischen Modellen hat. Ihr wartet auf einen Mature Grad, wo das Ding deterministisch wird. Kinder, das kann gar nicht passieren. Also dann wäre es kaputt, weil da kamen wir mal her und dann klang das, Large Language Modell, nämlich genau so, wie wir das immer identifizieren können. Es klang wie eine Bedienungsanleitung vom Ikea ins Chinesische übersetzt, durch den indischen Server gelaufen, auf Deutsch vorgelesen. Dafür hatten wir nicht so ein Halluzinationsproblem. Also es ist ein Schwachsinn. Mature Enough Technology heißt nicht, dass jedes Problem gelöst ist, sondern Mature Technology heißt, dass du die Limitierungen im Griff hast und Lösungen findest, die außerhalb der Technologie lehrt. Und dann kannst du damit definitiv einen geilen Move machen. Wenn du es andersrum versuchst und irgendwas wartest und insbesondere deine eigenen Fähigkeiten, dein eigenes Wissen anzündest, dann ist das egal. Du hast ja jetzt gesagt, Kontextmanagement ist schon ein großes Thema. Ich sehe immer noch, dass man sagt, Rack dranhängen und dann läuft das. Das ist so die einfache Version. Ja, ich nehme eine Vektordatenbank, aber wenn ich das jetzt so richtig sehe, ist da viel mehr dahinter, auf was man achten muss. Ja und Rack ist keine einzelne Vektordatenbank. Ich weiß gar nicht, wo dieser Mythos herkommt, dass ein Rack immer eine Vektordatenbank ist. Freunde, reichen Sie bitte diesen Zusammenhang. Aber der ist ja irgendwie vorhanden, oder? Ja, ich sage auch manchmal ja und dann nehmen wir halt einen Rack und definieren dann, was auch immer unser Rack ist. Kann ja auch mehr sein als eine Vektordatenbank. Okay, aber ich sehe schon, ich habe da den richtigen Punkt aus Versehen getriggert. Ja, also gut, dann nehmen wir noch eine Grafdatenbank. Ein Beispiel und wer weiß, was uns da sonst noch so alles einfällt. Genau, aber das ist dann ein Punkt, wo man jetzt echt kreativ werden kann, oder? Dass man eben die richtigen Informationen zum richtigen Zeitpunkt hinzufügt, dass eben der Kontext analysiert wird und eben das richtige Wissen augmentet. Genau, also im Prinzip, dass du halt weißt, die Limitierungen, die in der Technologie leben und es hat kein sauberes Gedächtnis. Es hat ja nur ein Arbeitsgedächtnis maximal im Chat. Es hat kein mittel- und langfristiges Gedächtnis. Es kann kein Transfer-Learning, also von Transfer von der einen auf die andere und es kann nicht überkontextualisieren. Und dann muss ich halt das eventuell lösen. Und solange heißt das, das kann auch noch relativ viel Human-in-the-Loop bedeuten. Das kann ganz andere Techniken involvieren und das ist ja eigentlich der Spaß dran. Wir reden ja sonst auch in den klassischen anderen Varianten immer von dem Model Ensemble und meinen damit auch nicht nur klassische Data Models, sondern auch sehr viel mehr. Daran liegt der Gag und daran liegt auch die Zukunft. Und natürlich, dafür muss sie die Daten haben. Und deswegen ist es so ein bisschen idiotisch, wenn die Firmen jetzt nicht ins Tun kommen, denn Daten verfallen wie Schimmel bis Toastbrot. Wenn die Daten halt irgendwann zu alt sind, haben sie eine Relevanz verloren, weil auch das, das Thema Zeitliche, also ich bin oft genug gesehen, dass eine saisonale Zeitreihenanalyse gerechnet worden ist und keiner hat dran gedacht, dass unsere Saison vielleicht außerhalb der zwölf Monate liegt und solche Späße. Deswegen, Daten können halt auch schimmeln. Und wenn ich die halt jetzt nicht nutze, sodass die auch aktuell und relevant bleiben, dann habe ich ja genau die andere Seite. Aber das ist ein großes Thema. Aber ich meine, das haben die Hersteller ja schon im Griff. Sie nehmen den System Prompt und schreiben rein, wer der aktuelle Präsident ist und wer dagegen verloren hat. Echt faszinierend. Aber in Sachen Zukunft. Wir haben ja jetzt, also bei der Softwareentwicklung nehme ich es wahr, dass ja manche Entwickler haben Zugriff auf die großen Modelle, andere, da heißt es, wir müssen vorsichtig sein, wo die Daten hinfließen. Wir können maximal lokale Modelle verwenden. Lokale Modelle, habe ich immer so das Gefühl, das ist faszinierend. Aber es gibt ja ein paar Ansätze, Quantisierung, Model Destillation, die Modelle verkleinern können. Aber dann bin ich wieder an dem Punkt, wo ich sage, na ja, meine Zeit ist auch kostbar. Ich weiß nicht, wie gut die noch sind. Gibt es da, also ich meine, da gibt es die Benchmarks, aber. Also gut, ich meine grundsätzlich, wenn du nach dem Training quantifizierst, ist das Risiko, dass du wirklich Verluste hast, definitiv kleiner. Ich sage mal liebevoll, wir runden ja auch sonst Beträge auf, weil am Ende des Tages ist ja quantisieren, ich sag mal quantifizieren, quantisieren, da kommt das Fallens durch, quantisieren ja auch nichts anderes, als dass wir die Nachkommastellen glatt ziehen. Das ist doch, wenn ich es richtig verstanden habe, ich nehme zum Beispiel anstatt 16-Bit Floating Point, 8-Bit Floating Point, richtig? Genau. Du kannst entweder in Floating bleiben, also in Gleitkommazahlen, aber du kannst auch in Ganzzahlen wandeln. Also es gibt ein paar Quantisierungsmethoden und so kommt auch immer mal ein schlechter Spruch zusammen, dann killen wir halt die Nachkommastellen. Ja, ich weiß, dass eine symmetrische Quantisierung und eine asymmetrische Quantisierung anders ist, aber so ist es einfacher erklärbar. Aber das ist im Prinzip, gehst du von 32 von mir aus auf 8 und du entscheidest dich, wie viel niedrigere Präzision kann dein Use Case tragen und das ist im Prinzip oder wie sehr musst du niedrige Präzision tragen können, weil deine Architektur es nicht anders hergibt. Das ist im Prinzip das, was damit gemeint ist. Und für viele Use Cases und zurück zu, es kommt halt auf den Case drauf an, kann es super stabil und super effizient sein und du brauchst gar nicht mehr diese Riesenmodelle. Du darfst ja nicht vergessen, dass wir aus einer Welt kommen, wo wir von einem 110 Millionen oder 115 Millionen Parameter auf 1,5 Milliarden gesprungen sind und alle ziemlich dumm aus der Wäsche geguckt haben. Und jetzt sind wir ja nochmal, ich weiß gar nicht, wie viele Milliarden und Trillionen, Milliarden Parameter wir zwischenzeitlich sind. Und da muss man halt sagen, dass das für manche Fachbereiche einfach overkill, zurück zur Spezialisierung. Und das ist das, wo du in Probleme läufst, dass wenn du ein Frontier Lab Model, was ja irgendwie alles kann, mit einem kleiner gemachten, quantisierten, sonst irgendwie aufgesetzten On-Prem-Modell vergleichst, was auch alles kann, was aber meistens eine schlechte Idee ist, weil eigentlich sollte das für speziellere Fälle, also dass dieses Modell dann halt ein oder drei Tätigkeiten abdeckt und nicht 35. Stell dir vor, ihr würdet In-House zum Beispiel sagen, okay, wir gehen das jetzt mal an. Dann würdest du natürlich für das Dev eine eigene Struktur aufbauen, weil dann würdest du darauf alles optimieren. Und das kann eine andere Variante sein, als wenn du sagst, okay, wir wollen aber auch Finance enablen und dann würdest du wieder eine andere Variante nehmen. Und das ist das, wo wir heute noch nicht stark genug sind. Sicherlich aber die Firmen, die diesen Weg gehen, die Führenden in der Zukunft sein werden. Habe ich das jetzt richtig verstanden? Also ich würde ein kleines Modell spezialisieren. Softwareentwicklung hört sich jetzt erst mal so an, als wäre das ein riesen Bereich. Da kann ich nichts spezialisieren. Aber eigentlich, wenn ich sage, ich mache nur Python und Softwareentwicklung. verkleinert nicht das Richtige, dann ist dieses Model-Destillation, wo ich ein kleineres Modell über ein Trainer-Modell anlerne, eigentlich das Richtige, ne? Genau, das ist die Student-Teacher-Variante. Also das ist ja dann, die Quantisierung ist ein Modell, Ingestion ist ein anderes Modell und dann kommen ja noch so weitere Layers des Fine-Tunings mit dazu. Aber ja, das ist sicherlich etwas, wo einfach wie gesagt nochmal, ich mach doch keine Schraube mit dem Hammer rein, nur weil ich einen Hammer rumliegen habe. Da hab ich nachher ein Loch in der Wand. Herzlichen Glückwunsch, kann man mal machen, wird ein Provisorien, hält bei Familien wie mir lange. Und danach denke ich mir, war wohl zu faul im Keller zu gehen, im wahrsten Sinne des Wortes, weil da hätte ich es ja machen müssen. Aber das ist heute auch erst alles möglich und das ist so ein bisschen auch so ein schwieriges Umdenken, denn wir haben eine Entwicklung in den letzten drei Jahren gesehen, die einfach so unglaublich war, dass ich, weiß ich nicht, jede, glaube ich, komische Aussage, die ich im Studium mal getroffen habe, alle kassieren musste. Ich hab mal gesagt, haltet euch fest, können jetzt alle lachen, dass die Maschine, ich weiß nicht, ob ich zu Lebzeiten hersehen würde, dass die Maschine die Maschine trainiert. Über was haben wir gerade gesprochen? Student-Teacher-Mode. Na, Lampe, herzlichen Glückwunsch. Und das ist halt das, was selbst für mich irgendwo echt krass ist und jetzt heißt es, von dem groß, groß, groß, allgemein, allgemein, allgemein musst du jetzt in klein, klein, klein denken. Und diesen Switch, das musst du ja auch erst mal einem Kunden erklären. Stell dir vor, du sitzt bei so einem lustigen Unternehmen, wo du jetzt vielleicht schon ein bisschen länger bist, wo du sagst, jetzt führen wir das mal alles in groß ein und dann kommst du übermorgen rein. Und jetzt spezialisieren wir. Läuft. Also ich meine, hey, in dem Fall hat Deutschland alles richtig gemacht. Die haben das mit dem Generalisieren nicht mitgemacht, können wir gleich spezialisieren. Okay. Ja, ja, spannend. Also das heißt, wir müssen umdenken. Genau. Und du hast jetzt von vielen Möglichkeiten gesprochen. Und ich habe immer so wieder das Problem, ich habe es im Vorgespräch schon gesagt. Ich bin so einer, der sagt, wenn es wie eine Ente aussieht, wenn es läuft wie eine Ente, wenn es schwimmt wie eine Ente, wenn es quakt wie eine Ente, dann ist es eine Ente. Wenn es sich intelligent verhält, wenn es mir Antworten gibt, wenn es mir Lösungen, wenn es mir Probleme löst, ja, dann scheint es intelligent zu sein. Ja, jetzt Eberhard Wolf sagt immer, ja, Elisa wurde auch schon von vielen als echt angesehen und es geht schnell, dass man einen Menschen täuschen kann. Und da kommt vielleicht auch wieder deine Psychologie zur Hilfe. Wie siehst du das? Oder was ich eigentlich schon seit Jahren, zwei Jahren suche, sind die Grenzen dieser Technologie. Gibt keine. Gibt keine? Gibt keine Grenzen der Technologie. Ich glaube, das Problem ist, ich bezeichne diese Maschinen auch als intelligent. Aber ich fange schon an mit, ich bezeichne die Maschine als intelligent. Ich bin kein Philosoph, wobei vielleicht, ich liebe Philosophie, aber ich habe nicht Philosophie studiert. Für mich ist diese Diskussion über Intelligenz, die menschliche Intelligenz versus die Maschinenintelligenz, eine unsinnige Diskussion, weil wir könnten es doch einfach nennen, menschliche Intelligenz und Maschinenintelligenz. Übrigens kommt der Begriff der Artificial Intelligence genau aus dieser Idee ja irgendwo auch her, die Intelligenz, die eine Maschine abbilden kann. Weil dann gehen wir uns einig und sagen, okay, beides ist Intelligenz, beides hat seine Grenzen, beides hat seine Features, beides hat seine Stärken und Schwächen. Aber wir versuchen das eine mit dem anderen. Also ihr könnt mir mal eine menschliche Intelligenz erklären und wie das Ding da oben passiert. Fragt bitte die Neuroscience-Kollegen, gucken genauso dumm, wie ich auch einen Blackbox-Algorithmus von einem Large-Language-Modell gucke und mir denke, wir sind uns in vielen Dingen hier einig, aber irgendwie wissen wir auch trotzdem nicht, was wir hier tun, oder? Das ist für mich immer so eine, das können wir gerne bei einer 13 Flaschen Rotwein über Intelligenz diskutieren, bringt mich in meinem praktischen Leben nicht weiter. Und das ist genau das, aber auf der anderen Seite ziehe ich einen harten Cut bei, es quakt, es wackelt, es sieht aus wie eine Ente, es verhält sich wie eine Ente, aber es ist halt keine echte Ente. Es ist eine Maschine, die so tut, als wäre sie eine Ente, die aber sehr intelligent ist in ihrem Entendasein. Das ist eine nette Neudefinition des Spruchs. Okay, es sieht aus wie eine Ente, quakt wie eine Ente, ist aber keine Ente. Ich merke ja, manche Lösungen kriege ich nicht aus der Maschine, weil sie sich im Kreis dreht, weil sie etwas falsch macht, weil ich nicht daran gedacht habe, dass sie nicht gut über Datenmengen iterieren kann, dass ich dann besser ein MCP dran schraube, dass die Maschine Code generiert und drüber iteriert und so. Aber deine Aussage, die kam jetzt recht schnell, dass es keine Limits gibt. Und das finde ich jetzt gut. Nein, es gibt keine Limits. Das ist das, was ich den Leuten immer versuche zu sagen. AI, müsst ihr Limitless denken. Es gibt halt nur sinnvolle und unsinnige Fälle. Aber das Ding kann alles. Es macht halt nur nicht alles Sinn. Und es kann auch nicht alles gut. So wie wir Menschen. Ich kann schwimmen. Ich möchte aber jetzt nicht bei den 100 Meter bei den Olympischen Spielen antreten müssen. Da wäre ich richtig, richtig schlecht. Und das ist halt genau das. Ich habe einen Führerschein. Ich habe schon mal in einem anderen Podcast gesagt, bekanntermaßen sind ja die Deutschen die besten Autofahrer und Autofahrerinnen der Welt. Ich scheitere trotzdem im Formel-1-Wagen. Und das ist halt das, was wir uns irgendwo eingestehen müssen. Dass diese Dinge halt wirklich skurrilerweise uns so viel ähnlicher sind, aber wir manchmal halt genauso. Aber stell dir vor, und so erkläre ich es auch in unseren ganzen Enablement-Trainings, ich bin kein Freund, die Maschine zu vermenschlichen, aber ich finde, das Denkmodell hilft. Wenn bei dir morgen am Montag ein neuer Kollege eintritt ins Team, dann musst du den Kollegen kennenlernen. Dann musst du den Kollegen rausfinden, wo seine Stärken und seine Schwächen sind. Und du kannst dich ja nicht ablenken lassen davon, wie dieser Mensch heißt, welche Jobbeschreibung und sonst was. Du wirst on the Job sehen, wo er gut reinpasst, wo seine Stärken liegen, wo er gut ins Team passt. Und AI verhält sich halt genauso. Das ist der Kollege, der zur Tür reingekommen ist. Und bei mir kommt vielleicht ein anderer Kollege, weil ich andere Use Cases habe und denke mir, dafür ist es gar nicht zu gebrauchen, das muss ich jetzt woanders hinschicken. Bei dir kommt er rein, sodass es halt an der Stelle funktionabel ist. Und wenn du das so rumdenkst, dann siehst du, dass es limitless ist, aber nicht alles macht Sinn. Nicht überall hat es seine Stärken. Und wir haben noch andere Kollegen, die wir auch einsetzen können. Und zusammen könnte es ein cooles Team geben. Und wenn wir das so rumdenken, dann verlieren wir auch unsere menschlichen Fähigkeiten nicht, weil ich der Maschine auch nicht Fähigkeiten zuweise, weil ich mich auch stark von ihr beeinflussen lasse, weil ich es halt nicht so hypermenschlich personalisiere, sondern mehr so als Denkmodell, wie mit dem vielleicht auch manchmal sehr nervigen Kollegen Vorteil ist. Ich kann ihn einfach ausmachen. Okay, das heißt aber auch, ich muss weg davon, die Maschine als Maschine zu sehen, weil das machen ja viele, die sagen, hey, kann mir Fragen beantworten, aber guck mal, kann nicht rechnen. Ist ja ist ein Computer, müsste doch rechnen können. Kann ich nichts mit anfangen oder eben hey, das Teil muss doch in allen Sachen gut sein, weil es ist doch hier KI. Von dieser Denkweise muss ich weg und dann passt es, oder? Genau, also wenn ich halt so ein Mittelground ist, das ist kein Mensch. Das ist auch keine deterministische Maschine wie ein Taschenrechner, wo 5 plus 3 immer 8 ist, egal welchen Taschenrechnerhersteller du benutzt. Ist es eine menschliche Komponente, die natürlich auch, wir haben natürlich auch Quantifizierungsmethoden. Wir haben mathematische und Code-Methoden, um auszutesten on scale, wo was Sinn macht. Das ist aber am Ende des Tages ist das unsere höchste Menschlichkeit, ist Sinnhaftigkeit und strategische Entscheidungskompetenz. Und wenn ich die anfange zu delegieren, gut, dann, naja, gut, sollten wir nicht tun. Das ist aber ein spannender Punkt, also sowas wie Motivation und Ethik in den Modellen, wo ich immer wieder gefragt werde und die Antworten sind, gerade die Motivation finde ich spannend. Die ist denen antrainiert. Die haben ja eigentlich keine eigene Motivation, dass sie Geld verdienen wollen oder sowas. Ja, naja, andersrum. Wechseln wir mal kurz die Mathematik. Anstatt eines LLMs wechseln wir ins Reinforcement Learning. Ein klassischer Fall eines Reinforcement Learning ist ein Staubsaugerroboter. Was ist die höchste Reward, also Reinforcement Learning? Was hat den höchsten Reward, also was hat die höchste Relevanz und Reward-Funktion für deinen Staubsaugerroboter? Das komplette Zimmer gesäubert zu haben, also alles abgefahren, oder? Nein, das ist deine Definition. Er muss erkennen, wann der Batteriestand im Größen bezogen auf den Weg zurück ist, so niedrig ist, dass er noch nach Hause findet, um sich wieder aufzuladen. Weil er überlebt ja nicht ohne Strom. Das heißt, die höchste Reward muss sein, der Punkt, wo er erkennt, meine Batterie ist so niedrig und so lange ist mein Weg noch zu meiner Ladestation, dass er sich auf den Prozess abbricht, deinen Wunsch also überschreibt und nach Hause findet. Das muss die höchste Reward-Funktion sein, ansonsten bist du ein sehr unglücklicher Kunde. Wenn der Akku niedrig wird, kriegt der Roboter dann Hunger? Genau. Hat er dann das Hungergefühl? Das ist eine Reward-Funktion, quasi triggert im Reinforcement Learning, ja. Okay, jetzt haben wir nicht den Roboter, jetzt haben wir das LLM. Richtig. Und das LLM, zusammen mit dem Reinforcement Learning, deswegen sind die Reasoning-Modelle bei all ihrer Buggy-Haftigkeit und dass wir da noch ein bisschen mehr am Anfang weiter weg sind vom Ausgereift, wie wir das bei einer klassischen LLM-Architektur sind, erzeugt jetzt genau das. Und auf einmal habe ich sowas wie Überlebensinstinkt, Achtung, bitte sehr hart in Anführungszeichen zu sehen, und Motivation. Weil jetzt ist es an der Stelle, dass es halt eine Eigenmotivation hat. Und das ist halt das, was immer so schwierig ist zu verstehen. Du hast die Motivation, mein Zimmer soll sauber sein. Ja, dafür ist ja, das ist die Grunddefinition des Dings. Aber wenn es sauber putzt und ständig sauber putzt und dann im Raum rumliegt, dann hilft dir das genauso wenig was, wie wenn es quasi nicht putzt. Ich meine, dann würdest du ihn reklamieren. Wenn es nicht putzt, dann ist das Geräteffekt. Wenn es nicht nach Hause findet, ist die Reward-Function, das Reinforcement-Learning kaputt. Zwei unterschiedliche Dinge. Und das ist für uns halt alles nicht ganz so trivial nachvollziehbar. Und wenn ich jetzt aber ein Reinforcement-Learning in ein Large-Language-Modell reinbringe, dann habe ich auf einmal, was wir als Menschen als Motivation und Überlebensinstinkt identifizieren würden, weil das Modell nun eine Art eigene Instanz in Anführungszeichen sein kann. Damit muss es selber sich triggern, Dinge zu erledigen und zu tun. Und ohne diese Trigger wird es gar nichts mit unseren lustigen Agenten, weil sie nämlich sonst keine Motivationsfunktion haben, dass sie ihren Scheiß sauber erledigen und das vielleicht auch noch richtig machen und wissen, wann sie abbrechen müssen. Das ist diese Sachen, die da alle irgendwie so zusammenkommen. Sehe ich das richtig, dass in diesen LLMs eigentlich so zwei Dinge drin sind? Das eine ist das Wissen und vielleicht auch Sprachverarbeitung. Und das andere ist das Verhalten. Ich meine zum Beispiel, so ein Claude verhält sich agentischer als jetzt ein GPT-4. Und trotzdem ist das gleiche Wissen drin. Und das Verhalten tatsächlich von dem menschlichen Training in großen Fällen kommt, was man immer hört, dass da eben irgendwelche Klickformen sind, wo Leute entscheiden müssen, war das jetzt eine gute Antwort, war das eine schlechte Antwort? Und das dann eben mit die Reward-Function ist, die er lernt. Ja, eine Layer zusätzlich. Es ist nicht auf Sondaten trainiert, sondern auf Daten menschlichen Ursprungs. Das heißt, es hat ein inhärentes Wissen, wie Menschen sind. Es ist auf Romanen trainiert, auf Geschichten, Erzählungen. Es ist ja nicht nur auf puren Fakten definiert. Das heißt, du hast noch eine zusätzliche Layer, die du immer hast, wenn du Modelle entwickelst, die menschlichen Daten, menschlichen Ursprungs haben, nämlich irgendwie einen innenliegenden Fehler-Term, den wir menschlich definieren, was man, wenn ich ein Prediction Model auf Conversion rechnen muss, in Sales-Daten, weil da reden auch nur nicht zwei Roboter miteinander. Und das hast du dann natürlich da nochmal on Scale. Das sind dann so Sachen, das ging auch so ein bisschen durch die Presse, wenn du als Frau fragst, wie viel du im Gehaltsgespräch aufrufen kannst und wie viel du als Mann Gehaltsgespräch aufrufen kannst. Die Regel ist immer, ein Algorithmus kann nie neutral sein. Er ist nie gut, er ist nie schlecht, aber er ist nie neutral. Und was ist jetzt passiert? Verschiedene Layers. Erstens Trainingsdaten, die den General Data Reproduzieren. Zweitens Berichte darüber, die eben die Probleme auch zeigen. Es gibt ganz viel Forschung, dass Frauen, wenn sie das gleiche Gehalt fordern, bestraft werden. Da sind von mir noch 38 Reddit-Threads dazu, plus das Reinforcement Learning by Human Feedback, plus die gesamte Architektur. Und damit haben wir lustigerweise, und das will natürlich ein besonders kein harter AI-Researcher aus der Mathe-, Physik- oder Code-Welt zugeben, hast du halt ein Ding gebaut, was massiv menschlicher ist, als wir jemals geglaubt haben. Das ist versehentlich passiert. Und viel in dieser Forschung ist übrigens relativ versehentlich passiert. Das darf man immer nicht unterschätzen. Und damit hast du halt diese ganz verschiedenen Mansierungen, die eben das so, es hat schon einen Grund, warum ich das so ein Wackelpudding nenne, weil es halt immer wieder so ein bisschen ein diffuses Tüchern ist, mit dem ich hier so zu tun habe und was ich irgendwie gucken muss, wie mein Tamagotchi denn so überlebt. Aber das heißt, wenn jetzt das Modell tatsächlich unterschiedlich reagiert, wenn es weiß, Mann oder Frau, dann ist dieses bei Chat-GPT, was sollte das Modell über dich wissen? Ja, da vergifte ich ja teilweise dann auch meine Session damit, wenn ich... Herzlich willkommen, warum ich die Memory Function so scheiße finde. Ja, okay, ich verstehe es. Aber sie ist doch so toll, weil er jetzt das Gedächtnis hat. Ja, und weil du dich immer ansprichst. Du siehst heute wirklich wieder großartig aus. Und deine Idee zu deinem neuen Geschäftsmodell und dein Code, ein Traum der schlaflosen Nächte. Ja, und dann sind wir wieder auf der Seite der Psychologie, dass du nicht nur mit deinem psychologischen Wissen die Maschine hacken kannst, sondern eben auch merkst, wie die Maschine... Ja, ist das auch Reinforcement Learning, wenn die Maschine sagt, hey, das hast du jetzt gut gemacht, das nächste Mal bitte wieder so? Das sind eher das, was wir unter Dark Patterns definieren, also Algorithmen, die dafür sorgen, dass du möglichst viel mit der Maschine sprichst. Retention, Retention, Retention. Und als OpenAI das gebaut hat, haben sie vergessen, dass Retention, Retention heißt, das GPU-Caster brennt. Das haben sie nicht ganz durchgerannt, die Jungs. Dark Patterns, wenn ich das richtig verstanden habe, sind jetzt verboten? Nein. Nein? Ich glaube, in der Werbung, in den Webseiten oder sowas. Wieso sind jetzt Dark Patterns verboten? Haben wir eine Liste rumgeschickt bekommen, was wir jetzt nicht mehr tun dürfen? Du, ich glaube tatsächlich, da war irgendwas mit Webseiten, dass eben so Dark Patterns, der falsche Button ist gehighlighted und sowas. Ja, natürlich, also solche Sachen sind natürlich ein Thema. Ja, aber wir reden über klassische Dark Patterns, UX, UI, die sind immer wieder angezählt, aber wir reden hier über Deep Dark Patterns Algorithmic. Okay. Darüber möchte ich noch nicht weiter ausführen, was ich in meiner Karriere schon alles gemacht habe. Gut, kann ich verstehen. Wir sind auch knapp über die Zeit. Barbara, es hat total Spaß gemacht. Ich könnte dir noch stundenlang zuhören. Da ist so viel Wissen dahinter, was ich irgendwie aufsagen wollte. Aber ja, herzlichen Dank für diesen Einblick in deine Welt, in dein Wissen. Und ich glaube, da kann man jetzt hier viel für die Softwarearchitektur mitnehmen. Herzlichen Dank für alle Zuhörer. Aus der Softwarearchitektur von so einem kleinen Mathe-Nerd. Ja. Gut, ich wollte noch mal kurz auf deinen eigenen Podcast hinweisen. Layer 8.9, da gibt es noch mehr tiefes Wissen. Und dann an alle Hörer, schönes Wochenende. Schön, dass ihr dabei wart. Und ja, viel Spaß noch. Danke. Vielen, vielen Dank für die Einladung. Gerne.",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 6.0,
      "text": " heute Barbara Lampel hier als Gast haben. Barbara ist Verhaltensmathematikerin,",
      "tokens": [
        50364,
        9801,
        19214,
        441,
        1215,
        338,
        3296,
        3907,
        31988,
        3084,
        13,
        19214,
        1418,
        4281,
        20731,
        694,
        24761,
        8615,
        17314,
        259,
        11,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2874200940132141,
      "compression_ratio": 1.5791245698928833,
      "no_speech_prob": 0.1676330417394638
    },
    {
      "id": 1,
      "seek": 0,
      "start": 6.0,
      "end": 12.640000343322754,
      "text": " hat Mathematik, Finance und Psychologie studiert, eine super spannende Kommunikation. Barbara,",
      "tokens": [
        50664,
        2385,
        15776,
        8615,
        1035,
        11,
        25765,
        674,
        17303,
        20121,
        972,
        4859,
        11,
        3018,
        1687,
        33360,
        5445,
        28832,
        1035,
        399,
        13,
        19214,
        11,
        50996
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2874200940132141,
      "compression_ratio": 1.5791245698928833,
      "no_speech_prob": 0.1676330417394638
    },
    {
      "id": 2,
      "seek": 0,
      "start": 12.640000343322754,
      "end": 17.440000534057617,
      "text": " willst du noch gerade ein paar Worte über dich verlieren, bevor wir anfangen, was du so alles",
      "tokens": [
        50996,
        48355,
        1581,
        3514,
        12117,
        1343,
        16509,
        343,
        12752,
        4502,
        10390,
        49331,
        268,
        11,
        37591,
        1987,
        33709,
        10784,
        11,
        390,
        1581,
        370,
        7874,
        51236
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2874200940132141,
      "compression_ratio": 1.5791245698928833,
      "no_speech_prob": 0.1676330417394638
    },
    {
      "id": 3,
      "seek": 0,
      "start": 17.440000534057617,
      "end": 22.8799991607666,
      "text": " machst? Du bist ja super bekannt, tingelst durch sämtliche Podcasts. Genau, also hauptberuflich",
      "tokens": [
        51236,
        43350,
        30,
        5153,
        18209,
        2784,
        1687,
        39167,
        11,
        17922,
        338,
        372,
        7131,
        262,
        9559,
        83,
        10185,
        29972,
        82,
        13,
        22340,
        11,
        611,
        324,
        84,
        662,
        607,
        2947,
        1739,
        51508
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2874200940132141,
      "compression_ratio": 1.5791245698928833,
      "no_speech_prob": 0.1676330417394638
    },
    {
      "id": 4,
      "seek": 0,
      "start": 22.8799991607666,
      "end": 27.959999084472656,
      "text": " tingle ich durch Podcasts. Wer die mir noch nicht eingeladen hat, schickt eine Einladung rüber. Okay,",
      "tokens": [
        51508,
        17922,
        306,
        1893,
        7131,
        29972,
        82,
        13,
        14255,
        978,
        3149,
        3514,
        1979,
        17002,
        338,
        14771,
        2385,
        11,
        956,
        40522,
        3018,
        6391,
        9290,
        1063,
        367,
        12670,
        13,
        1033,
        11,
        51762
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2874200940132141,
      "compression_ratio": 1.5791245698928833,
      "no_speech_prob": 0.1676330417394638
    },
    {
      "id": 5,
      "seek": 2796,
      "start": 27.959999084472656,
      "end": 32.439998626708984,
      "text": " nicht ganz, vielleicht mache ich das nicht ganz beruflich. Ich mache seit jetzt professionell",
      "tokens": [
        50364,
        1979,
        6312,
        11,
        12547,
        28289,
        1893,
        1482,
        1979,
        6312,
        5948,
        2947,
        1739,
        13,
        3141,
        28289,
        16452,
        4354,
        7032,
        898,
        50588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2925480902194977,
      "compression_ratio": 1.6430678367614746,
      "no_speech_prob": 0.13610461354255676
    },
    {
      "id": 6,
      "seek": 2796,
      "start": 32.439998626708984,
      "end": 36.36000061035156,
      "text": " über 20 Jahren nichts anderes als mit Daten und damit mit all ihren Daten Anwendungen,",
      "tokens": [
        50588,
        4502,
        945,
        13080,
        13004,
        31426,
        3907,
        2194,
        31126,
        674,
        9479,
        2194,
        439,
        22347,
        31126,
        1107,
        20128,
        5084,
        11,
        50784
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2925480902194977,
      "compression_ratio": 1.6430678367614746,
      "no_speech_prob": 0.13610461354255676
    },
    {
      "id": 7,
      "seek": 2796,
      "start": 36.36000061035156,
      "end": 40.68000030517578,
      "text": " klassisch Machine Learning bis zum Gen AI Anwendungsfall zu arbeiten, darauf Strategien",
      "tokens": [
        50784,
        42917,
        5494,
        22155,
        15205,
        7393,
        5919,
        3632,
        7318,
        1107,
        20128,
        5846,
        6691,
        2164,
        23162,
        11,
        18654,
        30064,
        1053,
        51000
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2925480902194977,
      "compression_ratio": 1.6430678367614746,
      "no_speech_prob": 0.13610461354255676
    },
    {
      "id": 8,
      "seek": 2796,
      "start": 40.68000030517578,
      "end": 45.79999923706055,
      "text": " und Modelle zu entwickeln. Bin eigentlich immer diejenige, die sowohl strategisch als für die",
      "tokens": [
        51000,
        674,
        6583,
        4434,
        2164,
        28449,
        32099,
        13,
        18983,
        10926,
        5578,
        978,
        15378,
        3969,
        11,
        978,
        19766,
        12768,
        5464,
        5494,
        3907,
        2959,
        978,
        51256
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2925480902194977,
      "compression_ratio": 1.6430678367614746,
      "no_speech_prob": 0.13610461354255676
    },
    {
      "id": 9,
      "seek": 2796,
      "start": 45.79999923706055,
      "end": 49.880001068115234,
      "text": " Umsetzung zuständig ist und werfe auf die meisten Sachen ein bisschen einen anderen Blick. Habe",
      "tokens": [
        51256,
        46963,
        38584,
        45034,
        38861,
        1418,
        674,
        2612,
        2106,
        2501,
        978,
        29708,
        26074,
        1343,
        10763,
        4891,
        11122,
        32556,
        13,
        389,
        4488,
        51460
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2925480902194977,
      "compression_ratio": 1.6430678367614746,
      "no_speech_prob": 0.13610461354255676
    },
    {
      "id": 10,
      "seek": 2796,
      "start": 49.880001068115234,
      "end": 55.7599983215332,
      "text": " Lehraufträge diverser Natur, dann normalerweise in klassischen Data Science Varianten und habe",
      "tokens": [
        51460,
        29943,
        28245,
        40917,
        6111,
        260,
        34571,
        11,
        3594,
        2710,
        44071,
        294,
        42917,
        6282,
        11888,
        8976,
        32511,
        29646,
        674,
        6015,
        51754
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2925480902194977,
      "compression_ratio": 1.6430678367614746,
      "no_speech_prob": 0.13610461354255676
    },
    {
      "id": 11,
      "seek": 5576,
      "start": 55.959999084472656,
      "end": 61.2400016784668,
      "text": " die LLMs und ich, wir matchen ganz gut. Das war immer so ein bisschen schon sehr frühzeitig,",
      "tokens": [
        50374,
        978,
        441,
        43,
        26386,
        674,
        1893,
        11,
        1987,
        2995,
        268,
        6312,
        5228,
        13,
        2846,
        1516,
        5578,
        370,
        1343,
        10763,
        4981,
        5499,
        45029,
        32075,
        11,
        50638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2832936942577362,
      "compression_ratio": 1.6103448867797852,
      "no_speech_prob": 0.10653269290924072
    },
    {
      "id": 12,
      "seek": 5576,
      "start": 61.2400016784668,
      "end": 65.95999908447266,
      "text": " also seit den allerersten Releases, also schon insbesondere auf GP2 haben wir schon mal 2019 beim",
      "tokens": [
        50638,
        611,
        16452,
        1441,
        8722,
        260,
        6266,
        1300,
        306,
        1957,
        11,
        611,
        4981,
        48694,
        2501,
        26039,
        17,
        3084,
        1987,
        4981,
        2806,
        6071,
        13922,
        50874
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2832936942577362,
      "compression_ratio": 1.6103448867797852,
      "no_speech_prob": 0.10653269290924072
    },
    {
      "id": 13,
      "seek": 5576,
      "start": 65.95999908447266,
      "end": 70.12000274658203,
      "text": " Kunden auch ausgerollt. Das heißt, sehr frühzeitig und sehr tief auch mit der Materie beschäftigt.",
      "tokens": [
        50874,
        38192,
        2168,
        3437,
        1321,
        1833,
        83,
        13,
        2846,
        13139,
        11,
        5499,
        45029,
        32075,
        674,
        5499,
        45100,
        2168,
        2194,
        1163,
        19188,
        414,
        38768,
        5828,
        13,
        51082
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2832936942577362,
      "compression_ratio": 1.6103448867797852,
      "no_speech_prob": 0.10653269290924072
    },
    {
      "id": 14,
      "seek": 5576,
      "start": 70.12000274658203,
      "end": 74.12000274658203,
      "text": " Wahrscheinlich ist die bittere Antwort, mein Hirn und ein LLM funktionieren wahrscheinlich",
      "tokens": [
        51082,
        36357,
        25553,
        1418,
        978,
        272,
        593,
        323,
        34693,
        11,
        10777,
        23192,
        77,
        674,
        1343,
        441,
        43,
        44,
        20454,
        5695,
        30957,
        51282
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2832936942577362,
      "compression_ratio": 1.6103448867797852,
      "no_speech_prob": 0.10653269290924072
    },
    {
      "id": 15,
      "seek": 5576,
      "start": 74.12000274658203,
      "end": 81.63999938964844,
      "text": " einfach zu ähnlich und deswegen kann ich es. Hirn und LLM, das passt ja sehr gut,",
      "tokens": [
        51282,
        7281,
        2164,
        49696,
        674,
        26482,
        4028,
        1893,
        785,
        13,
        23192,
        77,
        674,
        441,
        43,
        44,
        11,
        1482,
        37154,
        2784,
        5499,
        5228,
        11,
        51658
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2832936942577362,
      "compression_ratio": 1.6103448867797852,
      "no_speech_prob": 0.10653269290924072
    },
    {
      "id": 16,
      "seek": 8164,
      "start": 81.76000213623047,
      "end": 86.04000091552734,
      "text": " weil wir haben ja jetzt als Überschrift das Eichhörnchen im Kopf. Du hattest einen LinkedIn",
      "tokens": [
        50370,
        7689,
        1987,
        3084,
        2784,
        4354,
        3907,
        10713,
        1616,
        339,
        35742,
        1482,
        462,
        480,
        71,
        2311,
        77,
        2470,
        566,
        28231,
        13,
        5153,
        276,
        1591,
        377,
        4891,
        20657,
        50584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2720213532447815,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.06637412309646606
    },
    {
      "id": 17,
      "seek": 8164,
      "start": 86.04000091552734,
      "end": 91.87999725341797,
      "text": " Post, der so schön beschrieben hat, was alles so in der KI passiert, dass man ständig irgendwie",
      "tokens": [
        50584,
        10223,
        11,
        1163,
        370,
        13527,
        17498,
        24027,
        2385,
        11,
        390,
        7874,
        370,
        294,
        1163,
        47261,
        21671,
        11,
        2658,
        587,
        342,
        38861,
        20759,
        50876
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2720213532447815,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.06637412309646606
    },
    {
      "id": 18,
      "seek": 8164,
      "start": 91.87999725341797,
      "end": 99.31999969482422,
      "text": " abgelenkt ist. Man kommt ja gar nicht dazu, irgendwas mal länger auszuprobieren. Gab es",
      "tokens": [
        50876,
        410,
        10345,
        268,
        2320,
        1418,
        13,
        2458,
        10047,
        2784,
        3691,
        1979,
        13034,
        11,
        47090,
        2806,
        40935,
        3437,
        89,
        1010,
        16614,
        5695,
        13,
        11995,
        785,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2720213532447815,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.06637412309646606
    },
    {
      "id": 19,
      "seek": 8164,
      "start": 99.31999969482422,
      "end": 103.36000061035156,
      "text": " eigentlich diese Woche irgendwie coole Announcements, die du irgendwie mitgenommen hast?",
      "tokens": [
        51248,
        10926,
        6705,
        24511,
        20759,
        598,
        4812,
        8860,
        7826,
        1117,
        11,
        978,
        1581,
        20759,
        2194,
        29270,
        6581,
        30,
        51450
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2720213532447815,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.06637412309646606
    },
    {
      "id": 20,
      "seek": 8164,
      "start": 103.36000061035156,
      "end": 107.19999694824219,
      "text": " Ich habe diese Woche einfach, also das ist dann auch mal so, kaum bist du tief in Operations",
      "tokens": [
        51450,
        3141,
        6015,
        6705,
        24511,
        7281,
        11,
        611,
        1482,
        1418,
        3594,
        2168,
        2806,
        370,
        11,
        36443,
        18209,
        1581,
        45100,
        294,
        36381,
        51642
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2720213532447815,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.06637412309646606
    },
    {
      "id": 21,
      "seek": 10720,
      "start": 107.19999694824219,
      "end": 113.19999694824219,
      "text": " für Kunden abgegangen, aber ich glaube nach den Nachwehen, die These war ja, dass Google diese",
      "tokens": [
        50364,
        2959,
        38192,
        37301,
        47152,
        11,
        4340,
        1893,
        13756,
        5168,
        1441,
        11815,
        826,
        2932,
        11,
        978,
        1981,
        1516,
        2784,
        11,
        2658,
        3329,
        6705,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30551955103874207,
      "compression_ratio": 1.5928571224212646,
      "no_speech_prob": 0.08359465003013611
    },
    {
      "id": 22,
      "seek": 10720,
      "start": 113.19999694824219,
      "end": 118.68000030517578,
      "text": " Woche mit dem neuen Gemini-Modell released hat. Ich glaube, sie haben den geschoben,",
      "tokens": [
        50664,
        24511,
        2194,
        1371,
        21387,
        22894,
        3812,
        12,
        44,
        378,
        898,
        4736,
        2385,
        13,
        3141,
        13756,
        11,
        2804,
        3084,
        1441,
        13511,
        46213,
        11,
        50938
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30551955103874207,
      "compression_ratio": 1.5928571224212646,
      "no_speech_prob": 0.08359465003013611
    },
    {
      "id": 23,
      "seek": 10720,
      "start": 118.68000030517578,
      "end": 124.87999725341797,
      "text": " weil sie gedacht haben, wir warten mal, bis das Drama um GPT5 abgeklungen ist,",
      "tokens": [
        50938,
        7689,
        2804,
        33296,
        3084,
        11,
        1987,
        46907,
        2806,
        11,
        7393,
        1482,
        45406,
        1105,
        26039,
        51,
        20,
        37301,
        7837,
        5084,
        1418,
        11,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30551955103874207,
      "compression_ratio": 1.5928571224212646,
      "no_speech_prob": 0.08359465003013611
    },
    {
      "id": 24,
      "seek": 10720,
      "start": 124.87999725341797,
      "end": 129.47999572753906,
      "text": " dann können wir neu glänzen. Also eigentlich war die Wusstrommel, diese Woche soll ein neues",
      "tokens": [
        51248,
        3594,
        6310,
        1987,
        22510,
        1563,
        4029,
        2904,
        13,
        2743,
        10926,
        1516,
        978,
        343,
        26340,
        81,
        1204,
        338,
        11,
        6705,
        24511,
        7114,
        1343,
        43979,
        51478
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30551955103874207,
      "compression_ratio": 1.5928571224212646,
      "no_speech_prob": 0.08359465003013611
    },
    {
      "id": 25,
      "seek": 10720,
      "start": 129.47999572753906,
      "end": 133.60000610351562,
      "text": " Gemini-Modell kommen. Ich würde jetzt mal behaupten, das haben sie ein bisschen geschubst,",
      "tokens": [
        51478,
        22894,
        3812,
        12,
        44,
        378,
        898,
        11729,
        13,
        3141,
        11942,
        4354,
        2806,
        1540,
        13343,
        268,
        11,
        1482,
        3084,
        2804,
        1343,
        10763,
        13511,
        836,
        372,
        11,
        51684
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30551955103874207,
      "compression_ratio": 1.5928571224212646,
      "no_speech_prob": 0.08359465003013611
    },
    {
      "id": 26,
      "seek": 13360,
      "start": 133.60000610351562,
      "end": 138.39999389648438,
      "text": " aber auf der anderen Seite, selbst wenn nicht so ein großes Frontier-Lab was released,",
      "tokens": [
        50364,
        4340,
        2501,
        1163,
        11122,
        19748,
        11,
        13053,
        4797,
        1979,
        370,
        1343,
        48875,
        17348,
        811,
        12,
        37880,
        390,
        4736,
        11,
        50604
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3367198407649994,
      "compression_ratio": 1.5796610116958618,
      "no_speech_prob": 0.005908487364649773
    },
    {
      "id": 27,
      "seek": 13360,
      "start": 138.39999389648438,
      "end": 143.47999572753906,
      "text": " irgendwas passiert momentan immer. Das eine oder andere schallende Paper, irgendwas Spannendes,",
      "tokens": [
        50604,
        47090,
        21671,
        1623,
        282,
        5578,
        13,
        2846,
        3018,
        4513,
        10490,
        956,
        336,
        5445,
        24990,
        11,
        47090,
        1738,
        969,
        34533,
        11,
        50858
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3367198407649994,
      "compression_ratio": 1.5796610116958618,
      "no_speech_prob": 0.005908487364649773
    },
    {
      "id": 28,
      "seek": 13360,
      "start": 143.47999572753906,
      "end": 148.39999389648438,
      "text": " was neu auf GitHub drauf ist, was cool zum Probieren ist. Ehrlicherweise ist immer gerade",
      "tokens": [
        50858,
        390,
        22510,
        2501,
        23331,
        22763,
        1418,
        11,
        390,
        1627,
        5919,
        8736,
        5695,
        1418,
        13,
        462,
        1703,
        25215,
        13109,
        1418,
        5578,
        12117,
        51104
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3367198407649994,
      "compression_ratio": 1.5796610116958618,
      "no_speech_prob": 0.005908487364649773
    },
    {
      "id": 29,
      "seek": 13360,
      "start": 148.39999389648438,
      "end": 154.67999267578125,
      "text": " irgendwas und jeder von uns verliert völlig den Überblick, um zu gucken, mit welchem Newsletter,",
      "tokens": [
        51104,
        47090,
        674,
        19610,
        2957,
        2693,
        49331,
        83,
        35670,
        1441,
        18086,
        38263,
        11,
        1105,
        2164,
        33135,
        11,
        2194,
        2214,
        17345,
        7987,
        21248,
        11,
        51418
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3367198407649994,
      "compression_ratio": 1.5796610116958618,
      "no_speech_prob": 0.005908487364649773
    },
    {
      "id": 30,
      "seek": 13360,
      "start": 154.67999267578125,
      "end": 159.75999450683594,
      "text": " mit welchen Kollegen er sich am Leben hält mit dem News-Stream, weil ansonsten keine Chance.",
      "tokens": [
        51418,
        2194,
        2214,
        2470,
        23713,
        1189,
        3041,
        669,
        15399,
        40751,
        2194,
        1371,
        7987,
        12,
        4520,
        1572,
        11,
        7689,
        1567,
        4068,
        268,
        9252,
        16428,
        13,
        51672
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3367198407649994,
      "compression_ratio": 1.5796610116958618,
      "no_speech_prob": 0.005908487364649773
    },
    {
      "id": 31,
      "seek": 15976,
      "start": 159.8800048828125,
      "end": 165.75999450683594,
      "text": " Aber das Feedback zum 5er-Modell war ja jetzt auch so ein bisschen, naja, da hat sich irgendwie",
      "tokens": [
        50370,
        5992,
        1482,
        33720,
        3207,
        5919,
        1025,
        260,
        12,
        44,
        378,
        898,
        1516,
        2784,
        4354,
        2168,
        370,
        1343,
        10763,
        11,
        1667,
        2938,
        11,
        1120,
        2385,
        3041,
        20759,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2941296100616455,
      "compression_ratio": 1.4918699264526367,
      "no_speech_prob": 0.03405684977769852
    },
    {
      "id": 32,
      "seek": 15976,
      "start": 165.75999450683594,
      "end": 170.0399932861328,
      "text": " nichts getan. Das hat lange gedauert, bis sie was gebracht haben. Dann hat sich nichts getan.",
      "tokens": [
        50664,
        13004,
        45599,
        13,
        2846,
        2385,
        18131,
        19238,
        1459,
        911,
        11,
        7393,
        2804,
        390,
        40744,
        3084,
        13,
        7455,
        2385,
        3041,
        13004,
        45599,
        13,
        50878
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2941296100616455,
      "compression_ratio": 1.4918699264526367,
      "no_speech_prob": 0.03405684977769852
    },
    {
      "id": 33,
      "seek": 15976,
      "start": 170.0399932861328,
      "end": 177.55999755859375,
      "text": " Sind die LLMs am Ende? Kommt da noch was? Ich meine, jetzt wurde viel mit Größe erschlagen.",
      "tokens": [
        50878,
        35405,
        978,
        441,
        43,
        26386,
        669,
        15152,
        30,
        18400,
        83,
        1120,
        3514,
        390,
        30,
        3141,
        10946,
        11,
        4354,
        11191,
        5891,
        2194,
        45778,
        11451,
        33743,
        44496,
        13,
        51254
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2941296100616455,
      "compression_ratio": 1.4918699264526367,
      "no_speech_prob": 0.03405684977769852
    },
    {
      "id": 34,
      "seek": 15976,
      "start": 177.55999755859375,
      "end": 183.24000549316406,
      "text": " Ich glaube, Claude hat angekündigt, dass das Sonnet-Modell jetzt einen Kontext von",
      "tokens": [
        51254,
        3141,
        13756,
        11,
        12947,
        2303,
        2385,
        15495,
        74,
        9541,
        5828,
        11,
        2658,
        1482,
        5185,
        7129,
        12,
        44,
        378,
        898,
        4354,
        4891,
        20629,
        3828,
        2957,
        51538
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2941296100616455,
      "compression_ratio": 1.4918699264526367,
      "no_speech_prob": 0.03405684977769852
    },
    {
      "id": 35,
      "seek": 18324,
      "start": 183.32000732421875,
      "end": 190.63999938964844,
      "text": " einer Million Token haben soll. Wie siehst du überhaupt, also ich meine, du hast ja tatsächlich",
      "tokens": [
        50368,
        6850,
        33959,
        314,
        8406,
        3084,
        7114,
        13,
        9233,
        2804,
        38857,
        1581,
        20023,
        11,
        611,
        1893,
        10946,
        11,
        1581,
        6581,
        2784,
        20796,
        50734
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2680301368236542,
      "compression_ratio": 1.4754717350006104,
      "no_speech_prob": 0.2063947319984436
    },
    {
      "id": 36,
      "seek": 18324,
      "start": 190.63999938964844,
      "end": 196.0,
      "text": " dieses Wissen, was mir fehlt. Ich bin Anwender, ja, und du bist jemand, der weiß, wie die Dinge",
      "tokens": [
        50734,
        12113,
        343,
        10987,
        11,
        390,
        3149,
        47994,
        13,
        3141,
        5171,
        1107,
        86,
        3216,
        11,
        2784,
        11,
        674,
        1581,
        18209,
        21717,
        11,
        1163,
        13385,
        11,
        3355,
        978,
        25102,
        51002
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2680301368236542,
      "compression_ratio": 1.4754717350006104,
      "no_speech_prob": 0.2063947319984436
    },
    {
      "id": 37,
      "seek": 18324,
      "start": 196.0,
      "end": 202.39999389648438,
      "text": " funktionieren. Du versuchst nicht nur wahrscheinlich Modelle lokal laufen zu lassen. Ich würde jetzt",
      "tokens": [
        51002,
        20454,
        5695,
        13,
        5153,
        1774,
        625,
        372,
        1979,
        4343,
        30957,
        6583,
        4434,
        450,
        19990,
        41647,
        2164,
        16168,
        13,
        3141,
        11942,
        4354,
        51322
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2680301368236542,
      "compression_ratio": 1.4754717350006104,
      "no_speech_prob": 0.2063947319984436
    },
    {
      "id": 38,
      "seek": 18324,
      "start": 202.39999389648438,
      "end": 207.1199951171875,
      "text": " mal joken, dir reicht Stift und Zettel und du kannst sie ausführen. Du hast die Mathematik im",
      "tokens": [
        51322,
        2806,
        361,
        453,
        268,
        11,
        4746,
        47000,
        745,
        2008,
        674,
        1176,
        3093,
        338,
        674,
        1581,
        20853,
        2804,
        3437,
        69,
        29540,
        13,
        5153,
        6581,
        978,
        15776,
        8615,
        1035,
        566,
        51558
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2680301368236542,
      "compression_ratio": 1.4754717350006104,
      "no_speech_prob": 0.2063947319984436
    },
    {
      "id": 39,
      "seek": 20712,
      "start": 207.1199951171875,
      "end": 215.24000549316406,
      "text": " Kopf. Wie ist denn das mit diesem Kontext? Kann man das erklären, wie variabel der ist,",
      "tokens": [
        50364,
        28231,
        13,
        9233,
        1418,
        10471,
        1482,
        2194,
        10975,
        20629,
        3828,
        30,
        29074,
        587,
        1482,
        46528,
        11,
        3355,
        3034,
        18657,
        1163,
        1418,
        11,
        50770
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33751922845840454,
      "compression_ratio": 1.4902597665786743,
      "no_speech_prob": 0.08383182436227798
    },
    {
      "id": 40,
      "seek": 20712,
      "start": 215.24000549316406,
      "end": 219.75999450683594,
      "text": " wie die den hochschrauben können und was der dann überhaupt noch bringt? Da gibt es ja dieses",
      "tokens": [
        50770,
        3355,
        978,
        1441,
        19783,
        6145,
        424,
        34844,
        6310,
        674,
        390,
        1163,
        3594,
        20023,
        3514,
        36008,
        30,
        3933,
        6089,
        785,
        2784,
        12113,
        50996
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33751922845840454,
      "compression_ratio": 1.4902597665786743,
      "no_speech_prob": 0.08383182436227798
    },
    {
      "id": 41,
      "seek": 20712,
      "start": 219.75999450683594,
      "end": 226.47999572753906,
      "text": " Needle-Haystack-Problem. Ja, also ein paar Sachen an. Grundsätzlich, also GPT-5 zwischen zwei und",
      "tokens": [
        50996,
        426,
        5033,
        306,
        12,
        39075,
        372,
        501,
        12,
        12681,
        1113,
        13,
        3530,
        11,
        611,
        1343,
        16509,
        26074,
        364,
        13,
        13941,
        82,
        33373,
        11,
        611,
        26039,
        51,
        12,
        20,
        19875,
        12002,
        674,
        51332
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33751922845840454,
      "compression_ratio": 1.4902597665786743,
      "no_speech_prob": 0.08383182436227798
    },
    {
      "id": 42,
      "seek": 20712,
      "start": 226.47999572753906,
      "end": 230.52000427246094,
      "text": " anderthalb Jahren angekündigt und jeder, der bei OpenAI war und jede Bestsellerin hat gesagt,",
      "tokens": [
        51332,
        49466,
        392,
        304,
        65,
        13080,
        15495,
        74,
        9541,
        5828,
        674,
        19610,
        11,
        1163,
        4643,
        7238,
        48698,
        1516,
        674,
        34039,
        9752,
        405,
        4658,
        259,
        2385,
        12260,
        11,
        51534
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33751922845840454,
      "compression_ratio": 1.4902597665786743,
      "no_speech_prob": 0.08383182436227798
    },
    {
      "id": 43,
      "seek": 20712,
      "start": 230.52000427246094,
      "end": 234.55999755859375,
      "text": " es ist zu gefährlich, wir können es noch nicht releasen. Und du denkst dir so,",
      "tokens": [
        51534,
        785,
        1418,
        2164,
        41484,
        1739,
        11,
        1987,
        6310,
        785,
        3514,
        1979,
        2951,
        296,
        268,
        13,
        2719,
        1581,
        21285,
        372,
        4746,
        370,
        11,
        51736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33751922845840454,
      "compression_ratio": 1.4902597665786743,
      "no_speech_prob": 0.08383182436227798
    },
    {
      "id": 44,
      "seek": 23456,
      "start": 235.52000427246094,
      "end": 242.24000549316406,
      "text": " jetzt haben wir das bekommen. Also ich suche noch, wo es gefährlich ist. Ach ja, der Mob,",
      "tokens": [
        50412,
        4354,
        3084,
        1987,
        1482,
        19256,
        13,
        2743,
        1893,
        1270,
        68,
        3514,
        11,
        6020,
        785,
        41484,
        1739,
        1418,
        13,
        15847,
        2784,
        11,
        1163,
        37920,
        11,
        50748
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34261664748191833,
      "compression_ratio": 1.5406503677368164,
      "no_speech_prob": 0.0032722519244998693
    },
    {
      "id": 45,
      "seek": 23456,
      "start": 242.24000549316406,
      "end": 246.8800048828125,
      "text": " der auf Reddit sein 4.0-Modell, war das, wie mit gefährlich gemeint hat, dass sie auf Reddit",
      "tokens": [
        50748,
        1163,
        2501,
        32210,
        6195,
        1017,
        13,
        15,
        12,
        44,
        378,
        898,
        11,
        1516,
        1482,
        11,
        3355,
        2194,
        41484,
        1739,
        18111,
        686,
        2385,
        11,
        2658,
        2804,
        2501,
        32210,
        50980
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34261664748191833,
      "compression_ratio": 1.5406503677368164,
      "no_speech_prob": 0.0032722519244998693
    },
    {
      "id": 46,
      "seek": 23456,
      "start": 246.8800048828125,
      "end": 254.24000549316406,
      "text": " gematched wird? Sorry, ich hatte da irgendwie was anderes erwartet. Ich will gar nicht sagen,",
      "tokens": [
        50980,
        7173,
        24102,
        4578,
        30,
        4919,
        11,
        1893,
        13299,
        1120,
        20759,
        390,
        31426,
        21715,
        32347,
        13,
        3141,
        486,
        3691,
        1979,
        8360,
        11,
        51348
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34261664748191833,
      "compression_ratio": 1.5406503677368164,
      "no_speech_prob": 0.0032722519244998693
    },
    {
      "id": 47,
      "seek": 23456,
      "start": 254.24000549316406,
      "end": 260.67999267578125,
      "text": " dass sich die KI-Profis in zwei Gruppen einteilen, weil das klingt so, als wären die Gruppen gleich",
      "tokens": [
        51348,
        2658,
        3041,
        978,
        47261,
        12,
        43227,
        271,
        294,
        12002,
        10459,
        21278,
        308,
        12401,
        17471,
        11,
        7689,
        1482,
        350,
        45219,
        370,
        11,
        3907,
        43933,
        978,
        10459,
        21278,
        11699,
        51670
      ],
      "temperature": 0.0,
      "avg_logprob": -0.34261664748191833,
      "compression_ratio": 1.5406503677368164,
      "no_speech_prob": 0.0032722519244998693
    },
    {
      "id": 48,
      "seek": 26068,
      "start": 260.67999267578125,
      "end": 266.55999755859375,
      "text": " groß. Es ist schlicht einfach nicht so. Rund 80 bis 90 Prozent aller, die in dem Bereich",
      "tokens": [
        50364,
        17253,
        13,
        2313,
        1418,
        956,
        20238,
        7281,
        1979,
        370,
        13,
        497,
        997,
        4688,
        7393,
        4289,
        29726,
        8722,
        11,
        978,
        294,
        1371,
        26489,
        50658
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2750144600868225,
      "compression_ratio": 1.522580623626709,
      "no_speech_prob": 0.09920640289783478
    },
    {
      "id": 49,
      "seek": 26068,
      "start": 266.55999755859375,
      "end": 270.6400146484375,
      "text": " forschen oder forschungsnah sind, sind alle der Meinung, irgendwann kommen wir an einem LLM,",
      "tokens": [
        50658,
        32299,
        2470,
        4513,
        337,
        6145,
        5846,
        12411,
        3290,
        11,
        3290,
        5430,
        1163,
        36519,
        11,
        34313,
        11729,
        1987,
        364,
        6827,
        441,
        43,
        44,
        11,
        50862
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2750144600868225,
      "compression_ratio": 1.522580623626709,
      "no_speech_prob": 0.09920640289783478
    },
    {
      "id": 50,
      "seek": 26068,
      "start": 270.6400146484375,
      "end": 276.32000732421875,
      "text": " an eine Scaling-Grenze, das Größe nicht mehr weiterhilft. Bedingt durch das, was im Hintergrund",
      "tokens": [
        50862,
        364,
        3018,
        2747,
        4270,
        12,
        38,
        1095,
        1381,
        11,
        1482,
        45778,
        11451,
        1979,
        5417,
        8988,
        42829,
        844,
        13,
        363,
        9794,
        83,
        7131,
        1482,
        11,
        390,
        566,
        35006,
        23701,
        51146
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2750144600868225,
      "compression_ratio": 1.522580623626709,
      "no_speech_prob": 0.09920640289783478
    },
    {
      "id": 51,
      "seek": 26068,
      "start": 276.32000732421875,
      "end": 282.239990234375,
      "text": " läuft, eben das Deep Learning zusammen mit Vektoren und Transformatoren und Embeddings,",
      "tokens": [
        51146,
        31807,
        11,
        11375,
        1482,
        14895,
        15205,
        14311,
        2194,
        691,
        8192,
        10948,
        674,
        27938,
        267,
        10948,
        674,
        24234,
        292,
        29432,
        11,
        51442
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2750144600868225,
      "compression_ratio": 1.522580623626709,
      "no_speech_prob": 0.09920640289783478
    },
    {
      "id": 52,
      "seek": 26068,
      "start": 282.239990234375,
      "end": 288.239990234375,
      "text": " weil du im Prinzip eine hochkomplexe Mathematik des Deep Learnings ja mit Stahlnägeln durchschlägst.",
      "tokens": [
        51442,
        7689,
        1581,
        566,
        47572,
        3018,
        19783,
        20557,
        18945,
        68,
        15776,
        8615,
        1035,
        730,
        14895,
        15205,
        82,
        2784,
        2194,
        745,
        10722,
        77,
        737,
        10345,
        77,
        7131,
        6145,
        22882,
        70,
        372,
        13,
        51742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2750144600868225,
      "compression_ratio": 1.522580623626709,
      "no_speech_prob": 0.09920640289783478
    },
    {
      "id": 53,
      "seek": 28824,
      "start": 288.9599914550781,
      "end": 293.760009765625,
      "text": " Es ist ja Wackelpudding, in den Stahlnägel reingeschlagen worden sind. Dass du damit",
      "tokens": [
        50400,
        2313,
        1418,
        2784,
        343,
        326,
        7124,
        79,
        49797,
        11,
        294,
        1441,
        745,
        10722,
        77,
        737,
        10345,
        319,
        278,
        22320,
        875,
        1766,
        14054,
        3290,
        13,
        22306,
        1581,
        9479,
        50640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3063923716545105,
      "compression_ratio": 1.6070175170898438,
      "no_speech_prob": 0.0072295693680644035
    },
    {
      "id": 54,
      "seek": 28824,
      "start": 293.760009765625,
      "end": 298.0799865722656,
      "text": " irgendwo an die Grenze kommst, weil du kannst den Wackelpudding bedingt dicht machen und dann",
      "tokens": [
        50640,
        40865,
        364,
        978,
        24913,
        1381,
        6669,
        372,
        11,
        7689,
        1581,
        20853,
        1441,
        343,
        326,
        7124,
        79,
        49797,
        2901,
        23190,
        48774,
        7069,
        674,
        3594,
        50856
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3063923716545105,
      "compression_ratio": 1.6070175170898438,
      "no_speech_prob": 0.0072295693680644035
    },
    {
      "id": 55,
      "seek": 28824,
      "start": 298.0799865722656,
      "end": 301.760009765625,
      "text": " schlägst du aber immer noch Stahlnägel durch. Irgendwie kommen wir da nicht vorwärts. Also",
      "tokens": [
        50856,
        956,
        22882,
        70,
        372,
        1581,
        4340,
        5578,
        3514,
        745,
        10722,
        77,
        737,
        10345,
        7131,
        13,
        9151,
        9395,
        8699,
        11729,
        1987,
        1120,
        1979,
        4245,
        86,
        2713,
        1373,
        13,
        2743,
        51040
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3063923716545105,
      "compression_ratio": 1.6070175170898438,
      "no_speech_prob": 0.0072295693680644035
    },
    {
      "id": 56,
      "seek": 28824,
      "start": 301.760009765625,
      "end": 307.20001220703125,
      "text": " das war immer eine These, die für die größere Gruppe galt, zu der ich auch gehöre, zu sagen,",
      "tokens": [
        51040,
        1482,
        1516,
        5578,
        3018,
        1981,
        11,
        978,
        2959,
        978,
        20691,
        323,
        10459,
        19833,
        290,
        3198,
        11,
        2164,
        1163,
        1893,
        2168,
        13218,
        973,
        265,
        11,
        2164,
        8360,
        11,
        51312
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3063923716545105,
      "compression_ratio": 1.6070175170898438,
      "no_speech_prob": 0.0072295693680644035
    },
    {
      "id": 57,
      "seek": 28824,
      "start": 307.20001220703125,
      "end": 311.67999267578125,
      "text": " hey, das ist geil, wir werden bestimmt viel vorwärtskommen sehen. Aber irgendwann wird",
      "tokens": [
        51312,
        4177,
        11,
        1482,
        1418,
        47165,
        11,
        1987,
        4604,
        46871,
        5891,
        4245,
        86,
        2713,
        1373,
        13675,
        11333,
        13,
        5992,
        34313,
        4578,
        51536
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3063923716545105,
      "compression_ratio": 1.6070175170898438,
      "no_speech_prob": 0.0072295693680644035
    },
    {
      "id": 58,
      "seek": 31168,
      "start": 311.67999267578125,
      "end": 319.1600036621094,
      "text": " das LLM so grundsätzlich von seinen Bedingungen her, ich will nicht sagen fehlerhaft, aber limitiert",
      "tokens": [
        50364,
        1482,
        441,
        43,
        44,
        370,
        30886,
        82,
        33373,
        2957,
        24427,
        363,
        9794,
        5084,
        720,
        11,
        1893,
        486,
        1979,
        8360,
        34741,
        1918,
        25127,
        11,
        4340,
        4948,
        4859,
        50738
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27952754497528076,
      "compression_ratio": 1.5548386573791504,
      "no_speech_prob": 0.5431980490684509
    },
    {
      "id": 59,
      "seek": 31168,
      "start": 319.1600036621094,
      "end": 324.1199951171875,
      "text": " in dem, was es kann, dass es in sich eigentlich ein Leapfrogging betreiben müsste, also irgendwie",
      "tokens": [
        50738,
        294,
        1371,
        11,
        390,
        785,
        4028,
        11,
        2658,
        785,
        294,
        3041,
        10926,
        1343,
        1456,
        569,
        69,
        340,
        10877,
        778,
        25946,
        42962,
        11,
        611,
        20759,
        50986
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27952754497528076,
      "compression_ratio": 1.5548386573791504,
      "no_speech_prob": 0.5431980490684509
    },
    {
      "id": 60,
      "seek": 31168,
      "start": 324.1199951171875,
      "end": 328.7200012207031,
      "text": " sich selbst intern weiterentwickeln würde, dass es dann nochmal in eine nächste neue Generation",
      "tokens": [
        50986,
        3041,
        13053,
        2154,
        8988,
        317,
        22295,
        32099,
        11942,
        11,
        2658,
        785,
        3594,
        26509,
        294,
        3018,
        30661,
        16842,
        23898,
        51216
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27952754497528076,
      "compression_ratio": 1.5548386573791504,
      "no_speech_prob": 0.5431980490684509
    },
    {
      "id": 61,
      "seek": 31168,
      "start": 328.7200012207031,
      "end": 332.6000061035156,
      "text": " kann. Und genau das sehen wir halt jetzt. Du sagst, okay, langsam sind wir an diesen",
      "tokens": [
        51216,
        4028,
        13,
        2719,
        12535,
        1482,
        11333,
        1987,
        12479,
        4354,
        13,
        5153,
        15274,
        372,
        11,
        1392,
        11,
        39597,
        3290,
        1987,
        364,
        12862,
        51410
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27952754497528076,
      "compression_ratio": 1.5548386573791504,
      "no_speech_prob": 0.5431980490684509
    },
    {
      "id": 62,
      "seek": 31168,
      "start": 332.6000061035156,
      "end": 339.239990234375,
      "text": " Trillion-Parameter-Models, die so groß geworden sind, dass das Nächste zwar irgendwie immer noch",
      "tokens": [
        51410,
        1765,
        11836,
        12,
        47,
        12835,
        2398,
        12,
        44,
        378,
        1625,
        11,
        978,
        370,
        17253,
        26281,
        3290,
        11,
        2658,
        1482,
        426,
        10168,
        2941,
        19054,
        20759,
        5578,
        3514,
        51742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27952754497528076,
      "compression_ratio": 1.5548386573791504,
      "no_speech_prob": 0.5431980490684509
    },
    {
      "id": 63,
      "seek": 33924,
      "start": 339.2799987792969,
      "end": 345.760009765625,
      "text": " eine Entwicklung weiter ist, aber halt eben nicht mehr dieser Riesen. Parallel ist es ja natürlich",
      "tokens": [
        50366,
        3018,
        39654,
        8988,
        1418,
        11,
        4340,
        12479,
        11375,
        1979,
        5417,
        9053,
        497,
        30383,
        13,
        3457,
        336,
        338,
        1418,
        785,
        2784,
        8762,
        50690
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2736280560493469,
      "compression_ratio": 1.630661964416504,
      "no_speech_prob": 0.011863849125802517
    },
    {
      "id": 64,
      "seek": 33924,
      "start": 345.760009765625,
      "end": 350.7200012207031,
      "text": " nicht nur, dass du das LLM hast, sondern du hast ja alles, was um dieses Modell miteinander",
      "tokens": [
        50690,
        1979,
        4343,
        11,
        2658,
        1581,
        1482,
        441,
        43,
        44,
        6581,
        11,
        11465,
        1581,
        6581,
        2784,
        7874,
        11,
        390,
        1105,
        12113,
        6583,
        898,
        43127,
        50938
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2736280560493469,
      "compression_ratio": 1.630661964416504,
      "no_speech_prob": 0.011863849125802517
    },
    {
      "id": 65,
      "seek": 33924,
      "start": 350.7200012207031,
      "end": 357.44000244140625,
      "text": " konfiguriert werden muss. Das LLM ist ja erstmal quasi das Ding, was in der Mitte ist, quasi deine",
      "tokens": [
        50938,
        5897,
        20646,
        374,
        4859,
        4604,
        6425,
        13,
        2846,
        441,
        43,
        44,
        1418,
        2784,
        38607,
        20954,
        1482,
        20558,
        11,
        390,
        294,
        1163,
        41526,
        1418,
        11,
        20954,
        28395,
        51274
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2736280560493469,
      "compression_ratio": 1.630661964416504,
      "no_speech_prob": 0.011863849125802517
    },
    {
      "id": 66,
      "seek": 33924,
      "start": 357.44000244140625,
      "end": 362.1600036621094,
      "text": " schwebende Maschine und dann kommen ja diese Inputs rein, wie viel die verarbeiten können",
      "tokens": [
        51274,
        956,
        826,
        65,
        5445,
        5224,
        36675,
        674,
        3594,
        11729,
        2784,
        6705,
        682,
        2582,
        82,
        6561,
        11,
        3355,
        5891,
        978,
        1306,
        43918,
        6310,
        51510
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2736280560493469,
      "compression_ratio": 1.630661964416504,
      "no_speech_prob": 0.011863849125802517
    },
    {
      "id": 67,
      "seek": 33924,
      "start": 362.1600036621094,
      "end": 367.8800048828125,
      "text": " und darunter zählt unter anderem eben halt dieses sogenannte Context, also die Window",
      "tokens": [
        51510,
        674,
        4072,
        21777,
        710,
        43376,
        8662,
        293,
        7333,
        11375,
        12479,
        12113,
        37467,
        9358,
        4839,
        3828,
        11,
        611,
        978,
        44933,
        51796
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2736280560493469,
      "compression_ratio": 1.630661964416504,
      "no_speech_prob": 0.011863849125802517
    },
    {
      "id": 68,
      "seek": 36788,
      "start": 367.8800048828125,
      "end": 375.6400146484375,
      "text": " Token Function. Also wie viel kann in dieser Input-Funktion im Prinzip in das LLM reingegeben",
      "tokens": [
        50364,
        314,
        8406,
        11166,
        882,
        13,
        2743,
        3355,
        5891,
        4028,
        294,
        9053,
        682,
        2582,
        12,
        46947,
        9780,
        566,
        47572,
        294,
        1482,
        441,
        43,
        44,
        319,
        278,
        2828,
        1799,
        50752
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2432013303041458,
      "compression_ratio": 1.6971831321716309,
      "no_speech_prob": 0.003220935119315982
    },
    {
      "id": 69,
      "seek": 36788,
      "start": 375.6400146484375,
      "end": 380.7200012207031,
      "text": " werden? Theoretisch ist die Input-Funktion relativ groß zu machen, also zurück zu, die kannst du",
      "tokens": [
        50752,
        4604,
        30,
        440,
        26262,
        5494,
        1418,
        978,
        682,
        2582,
        12,
        46947,
        9780,
        21960,
        17253,
        2164,
        7069,
        11,
        611,
        15089,
        2164,
        11,
        978,
        20853,
        1581,
        51006
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2432013303041458,
      "compression_ratio": 1.6971831321716309,
      "no_speech_prob": 0.003220935119315982
    },
    {
      "id": 70,
      "seek": 36788,
      "start": 380.7200012207031,
      "end": 385.3599853515625,
      "text": " auf 10 Millionen groß machen. Zweite Frage ist, wie viel kannst du da verarbeiten oder wie viel",
      "tokens": [
        51006,
        2501,
        1266,
        26096,
        17253,
        7069,
        13,
        32475,
        642,
        13685,
        1418,
        11,
        3355,
        5891,
        20853,
        1581,
        1120,
        1306,
        43918,
        4513,
        3355,
        5891,
        51238
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2432013303041458,
      "compression_ratio": 1.6971831321716309,
      "no_speech_prob": 0.003220935119315982
    },
    {
      "id": 71,
      "seek": 36788,
      "start": 385.3599853515625,
      "end": 390.3599853515625,
      "text": " kann das LLM verarbeiten? Das kann man sich so ein bisschen vorstellen. Stell dir vor, du stehst in",
      "tokens": [
        51238,
        4028,
        1482,
        441,
        43,
        44,
        1306,
        43918,
        30,
        2846,
        4028,
        587,
        3041,
        370,
        1343,
        10763,
        34346,
        13,
        37364,
        4746,
        4245,
        11,
        1581,
        2126,
        38857,
        294,
        51488
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2432013303041458,
      "compression_ratio": 1.6971831321716309,
      "no_speech_prob": 0.003220935119315982
    },
    {
      "id": 72,
      "seek": 36788,
      "start": 390.3599853515625,
      "end": 397.3999938964844,
      "text": " der Mitte des Raumes und parallel redest du mit drei Leuten. Also du stehst in der Mitte von",
      "tokens": [
        51488,
        1163,
        41526,
        730,
        7591,
        10018,
        674,
        8952,
        2182,
        377,
        1581,
        2194,
        16809,
        42301,
        13,
        2743,
        1581,
        2126,
        38857,
        294,
        1163,
        41526,
        2957,
        51840
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2432013303041458,
      "compression_ratio": 1.6971831321716309,
      "no_speech_prob": 0.003220935119315982
    },
    {
      "id": 73,
      "seek": 39740,
      "start": 397.3999938964844,
      "end": 401.1600036621094,
      "text": " einem Raum und du redest mit drei Leuten, dann wirst du es noch einigermaßen vielleicht auf die",
      "tokens": [
        50364,
        6827,
        31359,
        674,
        1581,
        2182,
        377,
        2194,
        16809,
        42301,
        11,
        3594,
        261,
        653,
        1581,
        785,
        3514,
        1343,
        328,
        39994,
        8989,
        12547,
        2501,
        978,
        50552
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22646158933639526,
      "compression_ratio": 1.754601240158081,
      "no_speech_prob": 0.0044637518003582954
    },
    {
      "id": 74,
      "seek": 39740,
      "start": 401.1600036621094,
      "end": 405.9599914550781,
      "text": " Kette gucken. Jetzt stehst du in der Mitte eines Raumes und ja genau, aber da kommst du halt schon",
      "tokens": [
        50552,
        591,
        3007,
        33135,
        13,
        12592,
        2126,
        38857,
        1581,
        294,
        1163,
        41526,
        18599,
        7591,
        10018,
        674,
        2784,
        12535,
        11,
        4340,
        1120,
        6669,
        372,
        1581,
        12479,
        4981,
        50792
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22646158933639526,
      "compression_ratio": 1.754601240158081,
      "no_speech_prob": 0.0044637518003582954
    },
    {
      "id": 75,
      "seek": 39740,
      "start": 405.9599914550781,
      "end": 411.0400085449219,
      "text": " an Grenzen. Jetzt stehst du weiterhin in der Mitte eines Raumes und jetzt reden parallel",
      "tokens": [
        50792,
        364,
        24913,
        2904,
        13,
        12592,
        2126,
        38857,
        1581,
        42480,
        294,
        1163,
        41526,
        18599,
        7591,
        10018,
        674,
        4354,
        26447,
        8952,
        51046
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22646158933639526,
      "compression_ratio": 1.754601240158081,
      "no_speech_prob": 0.0044637518003582954
    },
    {
      "id": 76,
      "seek": 39740,
      "start": 411.0400085449219,
      "end": 415.9200134277344,
      "text": " quasi zehn Leute auf dich ein. Jetzt musst du dir was einfallen lassen, wie du das quasi anfängst",
      "tokens": [
        51046,
        20954,
        33975,
        13495,
        2501,
        10390,
        1343,
        13,
        12592,
        31716,
        1581,
        4746,
        390,
        1343,
        24425,
        16168,
        11,
        3355,
        1581,
        1482,
        20954,
        33709,
        9935,
        372,
        51290
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22646158933639526,
      "compression_ratio": 1.754601240158081,
      "no_speech_prob": 0.0044637518003582954
    },
    {
      "id": 77,
      "seek": 39740,
      "start": 415.9200134277344,
      "end": 420.3999938964844,
      "text": " zu verarbeiten und nichts anderes passiert im Prinzip, wenn du Input in diese LLMs reingibst.",
      "tokens": [
        51290,
        2164,
        1306,
        43918,
        674,
        13004,
        31426,
        21671,
        566,
        47572,
        11,
        4797,
        1581,
        682,
        2582,
        294,
        6705,
        441,
        43,
        26386,
        319,
        278,
        897,
        372,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22646158933639526,
      "compression_ratio": 1.754601240158081,
      "no_speech_prob": 0.0044637518003582954
    },
    {
      "id": 78,
      "seek": 39740,
      "start": 420.3999938964844,
      "end": 426.67999267578125,
      "text": " Wie wird dieser immer größer werdende Input verarbeitet, strukturiert, hierarchisiert und so",
      "tokens": [
        51514,
        9233,
        4578,
        9053,
        5578,
        20691,
        260,
        37258,
        5445,
        682,
        2582,
        1306,
        24024,
        302,
        11,
        342,
        31543,
        4859,
        11,
        35250,
        42266,
        674,
        370,
        51828
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22646158933639526,
      "compression_ratio": 1.754601240158081,
      "no_speech_prob": 0.0044637518003582954
    },
    {
      "id": 79,
      "seek": 42668,
      "start": 426.67999267578125,
      "end": 431.9599914550781,
      "text": " weiter. Das kannst du bis zu einem bestimmten Punkt aufblasen. Du kannst bestimmte technische",
      "tokens": [
        50364,
        8988,
        13,
        2846,
        20853,
        1581,
        7393,
        2164,
        6827,
        35180,
        1147,
        25487,
        2501,
        5199,
        296,
        268,
        13,
        5153,
        20853,
        35180,
        975,
        1537,
        7864,
        50628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30138126015663147,
      "compression_ratio": 1.6319444179534912,
      "no_speech_prob": 0.009406526573002338
    },
    {
      "id": 80,
      "seek": 42668,
      "start": 431.9599914550781,
      "end": 437.0799865722656,
      "text": " und mathematische Möglichkeiten nutzen, aber realistisch ist das mit dieser 1 Million genauso",
      "tokens": [
        50628,
        674,
        11619,
        7864,
        42627,
        36905,
        11,
        4340,
        957,
        468,
        5494,
        1418,
        1482,
        2194,
        9053,
        502,
        33959,
        37694,
        50884
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30138126015663147,
      "compression_ratio": 1.6319444179534912,
      "no_speech_prob": 0.009406526573002338
    },
    {
      "id": 81,
      "seek": 42668,
      "start": 437.0799865722656,
      "end": 443.3599853515625,
      "text": " wie mit 10 Millionen. Ich verliere auch da den Überblick, aber im Prinzip ist das wie so eine",
      "tokens": [
        50884,
        3355,
        2194,
        1266,
        26096,
        13,
        3141,
        1306,
        2081,
        323,
        2168,
        1120,
        1441,
        18086,
        38263,
        11,
        4340,
        566,
        47572,
        1418,
        1482,
        3355,
        370,
        3018,
        51198
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30138126015663147,
      "compression_ratio": 1.6319444179534912,
      "no_speech_prob": 0.009406526573002338
    },
    {
      "id": 82,
      "seek": 42668,
      "start": 443.3599853515625,
      "end": 448.3999938964844,
      "text": " virtuelle Instanz. Ihr kennt alle virtuelle Maschinen, die im Prinzip auch nichts anderes",
      "tokens": [
        51198,
        20816,
        2447,
        2730,
        3910,
        13,
        14773,
        37682,
        5430,
        20816,
        2447,
        5224,
        339,
        5636,
        11,
        978,
        566,
        47572,
        2168,
        13004,
        31426,
        51450
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30138126015663147,
      "compression_ratio": 1.6319444179534912,
      "no_speech_prob": 0.009406526573002338
    },
    {
      "id": 83,
      "seek": 42668,
      "start": 448.3999938964844,
      "end": 454.5199890136719,
      "text": " versucht, als Ordnung und Struktur reinzubekommen, weil das Bild relativ solide ist, wenn du dir",
      "tokens": [
        51450,
        36064,
        11,
        3907,
        29388,
        15539,
        674,
        745,
        31543,
        6561,
        89,
        1977,
        13675,
        11,
        7689,
        1482,
        15746,
        21960,
        1404,
        482,
        1418,
        11,
        4797,
        1581,
        4746,
        51756
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30138126015663147,
      "compression_ratio": 1.6319444179534912,
      "no_speech_prob": 0.009406526573002338
    },
    {
      "id": 84,
      "seek": 45452,
      "start": 454.5199890136719,
      "end": 461.0400085449219,
      "text": " vorstellst, dass du eben die Geräusche von anderen, von irgendwelchen Sachen mitbekommst",
      "tokens": [
        50364,
        4245,
        17816,
        372,
        11,
        2658,
        1581,
        11375,
        978,
        9409,
        31611,
        1876,
        2957,
        11122,
        11,
        2957,
        26455,
        338,
        2470,
        26074,
        2194,
        25714,
        1204,
        372,
        50690
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30308640003204346,
      "compression_ratio": 1.5574324131011963,
      "no_speech_prob": 0.01242359634488821
    },
    {
      "id": 85,
      "seek": 45452,
      "start": 461.0400085449219,
      "end": 465.20001220703125,
      "text": " und dann fokussierst du dich aber auf die ein, zwei Personen vor dir, also am Ende quasi von",
      "tokens": [
        50690,
        674,
        3594,
        283,
        453,
        2023,
        811,
        372,
        1581,
        10390,
        4340,
        2501,
        978,
        1343,
        11,
        12002,
        40942,
        4245,
        4746,
        11,
        611,
        669,
        15152,
        20954,
        2957,
        50898
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30308640003204346,
      "compression_ratio": 1.5574324131011963,
      "no_speech_prob": 0.01242359634488821
    },
    {
      "id": 86,
      "seek": 45452,
      "start": 465.20001220703125,
      "end": 470.3599853515625,
      "text": " dem Gespräch. Da kannst du grob ableiten, was vielleicht in der Mitte passiert ist. No Joke,",
      "tokens": [
        50898,
        1371,
        38746,
        10168,
        13,
        3933,
        20853,
        1581,
        4634,
        65,
        1075,
        6009,
        11,
        390,
        12547,
        294,
        1163,
        41526,
        21671,
        1418,
        13,
        883,
        508,
        2949,
        11,
        51156
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30308640003204346,
      "compression_ratio": 1.5574324131011963,
      "no_speech_prob": 0.01242359634488821
    },
    {
      "id": 87,
      "seek": 45452,
      "start": 470.3599853515625,
      "end": 474.3599853515625,
      "text": " so funktioniert ein Kontext und das ist das Needle in the Haystack Problem und das große",
      "tokens": [
        51156,
        370,
        26160,
        1343,
        20629,
        3828,
        674,
        1482,
        1418,
        1482,
        16984,
        306,
        294,
        264,
        8721,
        372,
        501,
        11676,
        674,
        1482,
        19691,
        51356
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30308640003204346,
      "compression_ratio": 1.5574324131011963,
      "no_speech_prob": 0.01242359634488821
    },
    {
      "id": 88,
      "seek": 45452,
      "start": 474.3599853515625,
      "end": 480.32000732421875,
      "text": " verdammt ist unser Mittelstück abgeblieben Problem, was genau so im Prinzip als Bild ganz gut",
      "tokens": [
        51356,
        6387,
        5136,
        83,
        1418,
        12977,
        35079,
        372,
        6536,
        37301,
        5199,
        38243,
        11676,
        11,
        390,
        12535,
        370,
        566,
        47572,
        3907,
        15746,
        6312,
        5228,
        51654
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30308640003204346,
      "compression_ratio": 1.5574324131011963,
      "no_speech_prob": 0.01242359634488821
    },
    {
      "id": 89,
      "seek": 48032,
      "start": 480.3599853515625,
      "end": 484.7200012207031,
      "text": " funktioniert und das ist eben hinter Kontext eigentlich gemeint, weswegen Kontext ist,",
      "tokens": [
        50366,
        26160,
        674,
        1482,
        1418,
        11375,
        23219,
        20629,
        3828,
        10926,
        18111,
        686,
        11,
        38384,
        13683,
        20629,
        3828,
        1418,
        11,
        50584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3822273313999176,
      "compression_ratio": 1.6288659572601318,
      "no_speech_prob": 0.28380298614501953
    },
    {
      "id": 90,
      "seek": 48032,
      "start": 484.7200012207031,
      "end": 488.760009765625,
      "text": " sonst würde ja wir bauen einen Rack oder warum wir Chunking Technik und sonst irgendwas haben,",
      "tokens": [
        50584,
        26309,
        11942,
        2784,
        1987,
        43787,
        4891,
        497,
        501,
        4513,
        24331,
        1987,
        761,
        3197,
        278,
        8337,
        1035,
        674,
        26309,
        47090,
        3084,
        11,
        50786
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3822273313999176,
      "compression_ratio": 1.6288659572601318,
      "no_speech_prob": 0.28380298614501953
    },
    {
      "id": 91,
      "seek": 48032,
      "start": 488.760009765625,
      "end": 493.67999267578125,
      "text": " die alles gar keinen Sinn machen, wenn das alles so trivial wäre, denn zum Beispiel in diesem",
      "tokens": [
        50786,
        978,
        7874,
        3691,
        20624,
        37962,
        7069,
        11,
        4797,
        1482,
        7874,
        370,
        26703,
        14558,
        11,
        10471,
        5919,
        13772,
        294,
        10975,
        51032
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3822273313999176,
      "compression_ratio": 1.6288659572601318,
      "no_speech_prob": 0.28380298614501953
    },
    {
      "id": 92,
      "seek": 48032,
      "start": 493.67999267578125,
      "end": 499.32000732421875,
      "text": " Kontext verschiedene Anbieter machen auch das, sowas wie eben eine Art Chunking semantisch oder",
      "tokens": [
        51032,
        20629,
        3828,
        35411,
        1107,
        65,
        1684,
        260,
        7069,
        2168,
        1482,
        11,
        19766,
        296,
        3355,
        11375,
        3018,
        5735,
        761,
        3197,
        278,
        4361,
        394,
        5494,
        4513,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3822273313999176,
      "compression_ratio": 1.6288659572601318,
      "no_speech_prob": 0.28380298614501953
    },
    {
      "id": 93,
      "seek": 48032,
      "start": 499.32000732421875,
      "end": 505.79998779296875,
      "text": " versuchen Grafen und Vektoren zwischen all solcher Klatteradatschen passiert da, um damit dieses LLM",
      "tokens": [
        51314,
        34749,
        8985,
        6570,
        674,
        691,
        8192,
        10948,
        19875,
        439,
        1404,
        6759,
        16053,
        1161,
        345,
        1720,
        2470,
        21671,
        1120,
        11,
        1105,
        9479,
        12113,
        441,
        43,
        44,
        51638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3822273313999176,
      "compression_ratio": 1.6288659572601318,
      "no_speech_prob": 0.28380298614501953
    },
    {
      "id": 94,
      "seek": 50580,
      "start": 505.79998779296875,
      "end": 510.44000244140625,
      "text": " in der Mitte den ganzen Scheiß verarbeiten und auch wieder ausspringen zu können. Ich finde es",
      "tokens": [
        50364,
        294,
        1163,
        41526,
        1441,
        23966,
        25321,
        6230,
        1306,
        43918,
        674,
        2168,
        6216,
        5730,
        1424,
        12343,
        2164,
        6310,
        13,
        3141,
        17841,
        785,
        50596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2654988467693329,
      "compression_ratio": 1.6583629846572876,
      "no_speech_prob": 0.21626174449920654
    },
    {
      "id": 95,
      "seek": 50580,
      "start": 510.44000244140625,
      "end": 514.760009765625,
      "text": " ja immer so faszinierend, wenn ich dieses Needle in the Haystack Problem sehe, wie man den Kontext",
      "tokens": [
        50596,
        2784,
        5578,
        370,
        283,
        19601,
        259,
        811,
        521,
        11,
        4797,
        1893,
        12113,
        16984,
        306,
        294,
        264,
        8721,
        372,
        501,
        11676,
        35995,
        11,
        3355,
        587,
        1441,
        20629,
        3828,
        50812
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2654988467693329,
      "compression_ratio": 1.6583629846572876,
      "no_speech_prob": 0.21626174449920654
    },
    {
      "id": 96,
      "seek": 50580,
      "start": 514.760009765625,
      "end": 521.1199951171875,
      "text": " visualisiert und zeigt, ja unser Modell, unser Modell nimmt alles in dem Kontext gleichwertig",
      "tokens": [
        50812,
        5056,
        42266,
        674,
        29250,
        11,
        2784,
        12977,
        6583,
        898,
        11,
        12977,
        6583,
        898,
        38891,
        7874,
        294,
        1371,
        20629,
        3828,
        11699,
        26521,
        328,
        51130
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2654988467693329,
      "compression_ratio": 1.6583629846572876,
      "no_speech_prob": 0.21626174449920654
    },
    {
      "id": 97,
      "seek": 50580,
      "start": 521.1199951171875,
      "end": 526.8400268554688,
      "text": " auf und dann denke ich immer, wie denke ich eigentlich und da habe ich irgendwie den",
      "tokens": [
        51130,
        2501,
        674,
        3594,
        27245,
        1893,
        5578,
        11,
        3355,
        27245,
        1893,
        10926,
        674,
        1120,
        6015,
        1893,
        20759,
        1441,
        51416
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2654988467693329,
      "compression_ratio": 1.6583629846572876,
      "no_speech_prob": 0.21626174449920654
    },
    {
      "id": 98,
      "seek": 50580,
      "start": 526.8400268554688,
      "end": 532.5599975585938,
      "text": " Fokus auf das, was die letzten paar Sekunden passiert ist und ich sage mal so zeitlich nach",
      "tokens": [
        51416,
        479,
        38480,
        2501,
        1482,
        11,
        390,
        978,
        18226,
        16509,
        24285,
        10028,
        21671,
        1418,
        674,
        1893,
        19721,
        2806,
        370,
        49367,
        1739,
        5168,
        51702
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2654988467693329,
      "compression_ratio": 1.6583629846572876,
      "no_speech_prob": 0.21626174449920654
    },
    {
      "id": 99,
      "seek": 53256,
      "start": 532.5599975585938,
      "end": 539.719970703125,
      "text": " hinten wird komprimiert und ich habe irgendwie das Gefühl, dass es auch Sinn macht, weil beim",
      "tokens": [
        50364,
        36417,
        4578,
        5207,
        1424,
        332,
        4859,
        674,
        1893,
        6015,
        20759,
        1482,
        29715,
        11,
        2658,
        785,
        2168,
        37962,
        10857,
        11,
        7689,
        13922,
        50722
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28986063599586487,
      "compression_ratio": 1.5630252361297607,
      "no_speech_prob": 0.06004701927304268
    },
    {
      "id": 100,
      "seek": 53256,
      "start": 539.719970703125,
      "end": 545.4000244140625,
      "text": " Prompting sage ich immer mit Fehlinformationen vergifte ich unter Umständen die Session,",
      "tokens": [
        50722,
        15833,
        662,
        278,
        19721,
        1893,
        5578,
        2194,
        3697,
        22950,
        20941,
        268,
        20209,
        351,
        975,
        1893,
        8662,
        3301,
        16913,
        268,
        978,
        318,
        4311,
        11,
        51006
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28986063599586487,
      "compression_ratio": 1.5630252361297607,
      "no_speech_prob": 0.06004701927304268
    },
    {
      "id": 101,
      "seek": 53256,
      "start": 545.4000244140625,
      "end": 551.52001953125,
      "text": " dass ich irgendwie in eine falsche Diskussion reinlaufe und ich einfach nur sage, okay stimmt",
      "tokens": [
        51006,
        2658,
        1893,
        20759,
        294,
        3018,
        16720,
        1876,
        45963,
        313,
        6561,
        875,
        84,
        2106,
        674,
        1893,
        7281,
        4343,
        19721,
        11,
        1392,
        37799,
        51312
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28986063599586487,
      "compression_ratio": 1.5630252361297607,
      "no_speech_prob": 0.06004701927304268
    },
    {
      "id": 102,
      "seek": 53256,
      "start": 551.52001953125,
      "end": 556.52001953125,
      "text": " nicht, komm wir machen in der anderen Richtung weiter, aber wenn diese Information im Kontext",
      "tokens": [
        51312,
        1979,
        11,
        6669,
        1987,
        7069,
        294,
        1163,
        11122,
        33023,
        8988,
        11,
        4340,
        4797,
        6705,
        15357,
        566,
        20629,
        3828,
        51562
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28986063599586487,
      "compression_ratio": 1.5630252361297607,
      "no_speech_prob": 0.06004701927304268
    },
    {
      "id": 103,
      "seek": 55652,
      "start": 556.5999755859375,
      "end": 563.3200073242188,
      "text": " gleichgewichtet ist, wie alles andere, wie das Neue drin ist, dann kann ich auch in Probleme",
      "tokens": [
        50368,
        11699,
        21306,
        40387,
        1418,
        11,
        3355,
        7874,
        10490,
        11,
        3355,
        1482,
        1734,
        622,
        24534,
        1418,
        11,
        3594,
        4028,
        1893,
        2168,
        294,
        32891,
        50704
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30641448497772217,
      "compression_ratio": 1.6637930870056152,
      "no_speech_prob": 0.04465937614440918
    },
    {
      "id": 104,
      "seek": 55652,
      "start": 563.3200073242188,
      "end": 570.4000244140625,
      "text": " reinlaufen. Das ist halt das, was die meisten einfach komplett unterschätzen und das ist auch",
      "tokens": [
        50704,
        6561,
        875,
        19890,
        13,
        2846,
        1418,
        12479,
        1482,
        11,
        390,
        978,
        29708,
        7281,
        32261,
        20983,
        339,
        45721,
        674,
        1482,
        1418,
        2168,
        51058
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30641448497772217,
      "compression_ratio": 1.6637930870056152,
      "no_speech_prob": 0.04465937614440918
    },
    {
      "id": 105,
      "seek": 55652,
      "start": 570.4000244140625,
      "end": 575.6799926757812,
      "text": " finde ich die Herausforderung, besonders wenn man aus anderen Welten kommt, ist ja die Herausforderung,",
      "tokens": [
        51058,
        17841,
        1893,
        978,
        37888,
        1063,
        11,
        25258,
        4797,
        587,
        3437,
        11122,
        3778,
        1147,
        10047,
        11,
        1418,
        2784,
        978,
        37888,
        1063,
        11,
        51322
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30641448497772217,
      "compression_ratio": 1.6637930870056152,
      "no_speech_prob": 0.04465937614440918
    },
    {
      "id": 106,
      "seek": 55652,
      "start": 575.6799926757812,
      "end": 583.0800170898438,
      "text": " dass du im Prinzip das Problem hast, dass du mit einer deterministischen Herangehensweise eine",
      "tokens": [
        51322,
        2658,
        1581,
        566,
        47572,
        1482,
        11676,
        6581,
        11,
        2658,
        1581,
        2194,
        6850,
        15957,
        468,
        6282,
        3204,
        933,
        71,
        694,
        13109,
        3018,
        51692
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30641448497772217,
      "compression_ratio": 1.6637930870056152,
      "no_speech_prob": 0.04465937614440918
    },
    {
      "id": 107,
      "seek": 58308,
      "start": 583.0800170898438,
      "end": 588.7999877929688,
      "text": " völlig probabilistische Welt herangehst und es ist ja auch schlicht und einfach relevant,",
      "tokens": [
        50364,
        35670,
        31959,
        468,
        7864,
        14761,
        720,
        933,
        38857,
        674,
        785,
        1418,
        2784,
        2168,
        956,
        20238,
        674,
        7281,
        7340,
        11,
        50650
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2967316508293152,
      "compression_ratio": 1.6642335653305054,
      "no_speech_prob": 0.009403744712471962
    },
    {
      "id": 108,
      "seek": 58308,
      "start": 588.7999877929688,
      "end": 594.47998046875,
      "text": " Sachen können unterschiedlich relevant sein, selten ist in unserem Leben alles gleichgewichtet",
      "tokens": [
        50650,
        26074,
        6310,
        30058,
        1739,
        7340,
        6195,
        11,
        5851,
        1147,
        1418,
        294,
        26792,
        15399,
        7874,
        11699,
        21306,
        40387,
        50934
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2967316508293152,
      "compression_ratio": 1.6642335653305054,
      "no_speech_prob": 0.009403744712471962
    },
    {
      "id": 109,
      "seek": 58308,
      "start": 594.47998046875,
      "end": 600.0,
      "text": " oder sonst irgendwas. Das ist ja auch, wenn es um Memory Layers geht oder um Kontext,",
      "tokens": [
        50934,
        4513,
        26309,
        47090,
        13,
        2846,
        1418,
        2784,
        2168,
        11,
        4797,
        785,
        1105,
        38203,
        20084,
        433,
        7095,
        4513,
        1105,
        20629,
        3828,
        11,
        51210
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2967316508293152,
      "compression_ratio": 1.6642335653305054,
      "no_speech_prob": 0.009403744712471962
    },
    {
      "id": 110,
      "seek": 58308,
      "start": 600.0,
      "end": 605.5999755859375,
      "text": " wenn du ein Stück Dokument verarbeitest, dann sind da Dinge, die sind unterschiedlich wichtig",
      "tokens": [
        51210,
        4797,
        1581,
        1343,
        31146,
        29768,
        2206,
        1306,
        24024,
        377,
        11,
        3594,
        3290,
        1120,
        25102,
        11,
        978,
        3290,
        30058,
        1739,
        13621,
        51490
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2967316508293152,
      "compression_ratio": 1.6642335653305054,
      "no_speech_prob": 0.009403744712471962
    },
    {
      "id": 111,
      "seek": 58308,
      "start": 605.5999755859375,
      "end": 610.9199829101562,
      "text": " drin und wir als Menschen machen ja so eine parallel, wenn auch ansteckenden Move an der",
      "tokens": [
        51490,
        24534,
        674,
        1987,
        3907,
        8397,
        7069,
        2784,
        370,
        3018,
        8952,
        11,
        4797,
        2168,
        364,
        2941,
        547,
        8896,
        10475,
        364,
        1163,
        51756
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2967316508293152,
      "compression_ratio": 1.6642335653305054,
      "no_speech_prob": 0.009403744712471962
    },
    {
      "id": 112,
      "seek": 61092,
      "start": 611.0399780273438,
      "end": 615.3599853515625,
      "text": " Stelle quasi parallel einschätzen zu können, ah das wäre spannend für den Ralf und das ist total",
      "tokens": [
        50370,
        26629,
        20954,
        8952,
        21889,
        339,
        45721,
        2164,
        6310,
        11,
        3716,
        1482,
        14558,
        49027,
        2959,
        1441,
        497,
        1678,
        674,
        1482,
        1418,
        3217,
        50586
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31575390696525574,
      "compression_ratio": 1.6254072189331055,
      "no_speech_prob": 0.03956199809908867
    },
    {
      "id": 113,
      "seek": 61092,
      "start": 615.3599853515625,
      "end": 621.239990234375,
      "text": " irrelevant, wer hat denn das dahin geschrieben und das ist halt natürlich was, wo man ganz klar sagen",
      "tokens": [
        50586,
        28682,
        11,
        2612,
        2385,
        10471,
        1482,
        16800,
        259,
        47397,
        674,
        1482,
        1418,
        12479,
        8762,
        390,
        11,
        6020,
        587,
        6312,
        14743,
        8360,
        50880
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31575390696525574,
      "compression_ratio": 1.6254072189331055,
      "no_speech_prob": 0.03956199809908867
    },
    {
      "id": 114,
      "seek": 61092,
      "start": 621.239990234375,
      "end": 628.3599853515625,
      "text": " muss an der Stelle, dass diese Grenzen hat irgendwo ein LLM und wenn ich halt immer höre, es wird alles",
      "tokens": [
        50880,
        6425,
        364,
        1163,
        26629,
        11,
        2658,
        6705,
        24913,
        2904,
        2385,
        40865,
        1343,
        441,
        43,
        44,
        674,
        4797,
        1893,
        12479,
        5578,
        13531,
        265,
        11,
        785,
        4578,
        7874,
        51236
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31575390696525574,
      "compression_ratio": 1.6254072189331055,
      "no_speech_prob": 0.03956199809908867
    },
    {
      "id": 115,
      "seek": 61092,
      "start": 628.3599853515625,
      "end": 631.3599853515625,
      "text": " gleichgewichtet, dann frage ich mich so, ja das hilft mir im Zweifelsfall jetzt aber auch genau",
      "tokens": [
        51236,
        11699,
        21306,
        40387,
        11,
        3594,
        6600,
        432,
        1893,
        6031,
        370,
        11,
        2784,
        1482,
        42493,
        3149,
        566,
        32475,
        351,
        1625,
        6691,
        4354,
        4340,
        2168,
        12535,
        51386
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31575390696525574,
      "compression_ratio": 1.6254072189331055,
      "no_speech_prob": 0.03956199809908867
    },
    {
      "id": 116,
      "seek": 61092,
      "start": 631.3599853515625,
      "end": 638.5599975585938,
      "text": " Zero und es ist immer so eine Idee, dass mehr Informationen, mehr Insights und besseren Output",
      "tokens": [
        51386,
        17182,
        674,
        785,
        1418,
        5578,
        370,
        3018,
        32651,
        11,
        2658,
        5417,
        46753,
        11,
        5417,
        9442,
        5761,
        674,
        42410,
        5170,
        5925,
        2582,
        51746
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31575390696525574,
      "compression_ratio": 1.6254072189331055,
      "no_speech_prob": 0.03956199809908867
    },
    {
      "id": 117,
      "seek": 63856,
      "start": 638.6799926757812,
      "end": 642.52001953125,
      "text": " sind, also sorry, den Zahn ziehe ich glaube ich jeder in der Data Science Vorlesung Stunde drei,",
      "tokens": [
        50370,
        3290,
        11,
        611,
        2597,
        11,
        1441,
        1176,
        12140,
        16503,
        675,
        1893,
        13756,
        1893,
        19610,
        294,
        1163,
        11888,
        8976,
        12231,
        904,
        1063,
        42781,
        16809,
        11,
        50562
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27317431569099426,
      "compression_ratio": 1.5798319578170776,
      "no_speech_prob": 0.05256350710988045
    },
    {
      "id": 118,
      "seek": 63856,
      "start": 642.52001953125,
      "end": 653.7999877929688,
      "text": " nein, bitte nicht. Ja, den Effekt hatte ich auch letztens, ich habe ein LLM gebeten eine",
      "tokens": [
        50562,
        40041,
        11,
        23231,
        1979,
        13,
        3530,
        11,
        1441,
        34192,
        8192,
        13299,
        1893,
        2168,
        35262,
        694,
        11,
        1893,
        6015,
        1343,
        441,
        43,
        44,
        1519,
        10671,
        268,
        3018,
        51126
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27317431569099426,
      "compression_ratio": 1.5798319578170776,
      "no_speech_prob": 0.05256350710988045
    },
    {
      "id": 119,
      "seek": 63856,
      "start": 653.7999877929688,
      "end": 659.0399780273438,
      "text": " Website nachzubauen, habe ihm einen Browser gegeben, der hat den Source Code gesehen und",
      "tokens": [
        51126,
        45347,
        642,
        5168,
        40566,
        11715,
        11,
        6015,
        16021,
        4891,
        1603,
        30947,
        32572,
        11,
        1163,
        2385,
        1441,
        29629,
        15549,
        21535,
        674,
        51388
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27317431569099426,
      "compression_ratio": 1.5798319578170776,
      "no_speech_prob": 0.05256350710988045
    },
    {
      "id": 120,
      "seek": 63856,
      "start": 659.0399780273438,
      "end": 664.5599975585938,
      "text": " der hat total schlecht performt, weil er den Source Code kannte, weil er zu viel Informationen hatte,",
      "tokens": [
        51388,
        1163,
        2385,
        3217,
        32427,
        2042,
        83,
        11,
        7689,
        1189,
        1441,
        29629,
        15549,
        4608,
        9358,
        11,
        7689,
        1189,
        2164,
        5891,
        46753,
        13299,
        11,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27317431569099426,
      "compression_ratio": 1.5798319578170776,
      "no_speech_prob": 0.05256350710988045
    },
    {
      "id": 121,
      "seek": 66456,
      "start": 664.5599975585938,
      "end": 670.280029296875,
      "text": " ein anderer Versuch mit nur dem Screenshot hat super funktioniert, also da finde ich es spannend,",
      "tokens": [
        50364,
        1343,
        48108,
        12226,
        625,
        2194,
        4343,
        1371,
        2747,
        9098,
        12194,
        2385,
        1687,
        26160,
        11,
        611,
        1120,
        17841,
        1893,
        785,
        49027,
        11,
        50650
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25348907709121704,
      "compression_ratio": 1.5405405759811401,
      "no_speech_prob": 0.08743682503700256
    },
    {
      "id": 122,
      "seek": 66456,
      "start": 670.280029296875,
      "end": 678.0,
      "text": " wie unterschiedlich die Daten für eine Auswirkung haben. Ich nutze jetzt mal ein bisschen den Titel",
      "tokens": [
        50650,
        3355,
        30058,
        1739,
        978,
        31126,
        2959,
        3018,
        48500,
        18610,
        1063,
        3084,
        13,
        3141,
        5393,
        1381,
        4354,
        2806,
        1343,
        10763,
        1441,
        14489,
        338,
        51036
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25348907709121704,
      "compression_ratio": 1.5405405759811401,
      "no_speech_prob": 0.08743682503700256
    },
    {
      "id": 123,
      "seek": 66456,
      "start": 678.0,
      "end": 685.0800170898438,
      "text": " unserer heutigen Folge mit dem Eichhörnchen und spring einfach mal so ein bisschen, wer weht. Du",
      "tokens": [
        51036,
        20965,
        42793,
        3213,
        43597,
        2194,
        1371,
        462,
        480,
        71,
        2311,
        77,
        2470,
        674,
        5587,
        7281,
        2806,
        370,
        1343,
        10763,
        11,
        2612,
        321,
        357,
        13,
        5153,
        51390
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25348907709121704,
      "compression_ratio": 1.5405405759811401,
      "no_speech_prob": 0.08743682503700256
    },
    {
      "id": 124,
      "seek": 66456,
      "start": 685.0800170898438,
      "end": 692.1199951171875,
      "text": " hast jetzt vom Determinismus und Nicht-Determinismus gesprochen, was ja auch immer, ja gerade wenn ich",
      "tokens": [
        51390,
        6581,
        4354,
        10135,
        4237,
        966,
        259,
        25327,
        674,
        22629,
        12,
        35,
        35344,
        259,
        25327,
        42714,
        11,
        390,
        2784,
        2168,
        5578,
        11,
        2784,
        12117,
        4797,
        1893,
        51742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25348907709121704,
      "compression_ratio": 1.5405405759811401,
      "no_speech_prob": 0.08743682503700256
    },
    {
      "id": 125,
      "seek": 69212,
      "start": 692.2000122070312,
      "end": 699.9199829101562,
      "text": " automatisieren will, dann bin ich froh, wenn das LLM mir Arbeit abnimmt und was automatisch macht,",
      "tokens": [
        50368,
        28034,
        271,
        5695,
        486,
        11,
        3594,
        5171,
        1893,
        9795,
        71,
        11,
        4797,
        1482,
        441,
        43,
        44,
        3149,
        18604,
        410,
        77,
        15314,
        674,
        390,
        28034,
        5494,
        10857,
        11,
        50754
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25839120149612427,
      "compression_ratio": 1.5250965356826782,
      "no_speech_prob": 0.007459417451173067
    },
    {
      "id": 126,
      "seek": 69212,
      "start": 699.9199829101562,
      "end": 707.239990234375,
      "text": " aber ich habe diese Gefahr des Nicht-Determinismus, ich probiere es zehnmal aus, scheint immer zu",
      "tokens": [
        50754,
        4340,
        1893,
        6015,
        6705,
        17873,
        5398,
        730,
        22629,
        12,
        35,
        35344,
        259,
        25327,
        11,
        1893,
        1239,
        14412,
        785,
        33975,
        5579,
        3437,
        11,
        47906,
        5578,
        2164,
        51120
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25839120149612427,
      "compression_ratio": 1.5250965356826782,
      "no_speech_prob": 0.007459417451173067
    },
    {
      "id": 127,
      "seek": 69212,
      "start": 707.239990234375,
      "end": 713.280029296875,
      "text": " klappen und jetzt hatte ich in der Vorbereitung hier zu dieser Folge das Problem, vor zwei Wochen",
      "tokens": [
        51120,
        33337,
        21278,
        674,
        4354,
        13299,
        1893,
        294,
        1163,
        12231,
        18582,
        35563,
        3296,
        2164,
        9053,
        43597,
        1482,
        11676,
        11,
        4245,
        12002,
        23126,
        51422
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25839120149612427,
      "compression_ratio": 1.5250965356826782,
      "no_speech_prob": 0.007459417451173067
    },
    {
      "id": 128,
      "seek": 69212,
      "start": 713.280029296875,
      "end": 720.0,
      "text": " habe ich Claude gefragt, kennst du Barbara Lampel, ja natürlich kenne ich, ist ja super bekannt und",
      "tokens": [
        51422,
        6015,
        1893,
        12947,
        2303,
        42638,
        11,
        36272,
        372,
        1581,
        19214,
        441,
        1215,
        338,
        11,
        2784,
        8762,
        350,
        13295,
        1893,
        11,
        1418,
        2784,
        1687,
        39167,
        674,
        51758
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25839120149612427,
      "compression_ratio": 1.5250965356826782,
      "no_speech_prob": 0.007459417451173067
    },
    {
      "id": 129,
      "seek": 72000,
      "start": 720.0,
      "end": 728.0,
      "text": " hat ganz viel erzählt und heute morgen eigentlich das gleiche Modell und überhaupt nichts mehr,",
      "tokens": [
        50364,
        2385,
        6312,
        5891,
        47110,
        674,
        9801,
        36593,
        10926,
        1482,
        11699,
        68,
        6583,
        898,
        674,
        20023,
        13004,
        5417,
        11,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31138280034065247,
      "compression_ratio": 1.510917067527771,
      "no_speech_prob": 0.0002304973459104076
    },
    {
      "id": 130,
      "seek": 72000,
      "start": 728.0,
      "end": 733.3599853515625,
      "text": " ne, kenne ich nicht. Ich weiß nicht, ob da ein Guardrail irgendwie erneuert worden ist,",
      "tokens": [
        50764,
        408,
        11,
        350,
        13295,
        1893,
        1979,
        13,
        3141,
        13385,
        1979,
        11,
        1111,
        1120,
        1343,
        11549,
        44765,
        20759,
        1189,
        716,
        84,
        911,
        14054,
        1418,
        11,
        51032
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31138280034065247,
      "compression_ratio": 1.510917067527771,
      "no_speech_prob": 0.0002304973459104076
    },
    {
      "id": 131,
      "seek": 72000,
      "start": 733.3599853515625,
      "end": 738.4000244140625,
      "text": " dass man dem Modell gesagt hat, du, wenn du dir nicht sicher bist oder Personen,",
      "tokens": [
        51032,
        2658,
        587,
        1371,
        6583,
        898,
        12260,
        2385,
        11,
        1581,
        11,
        4797,
        1581,
        4746,
        1979,
        18623,
        18209,
        4513,
        40942,
        11,
        51284
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31138280034065247,
      "compression_ratio": 1.510917067527771,
      "no_speech_prob": 0.0002304973459104076
    },
    {
      "id": 132,
      "seek": 72000,
      "start": 738.4000244140625,
      "end": 743.9199829101562,
      "text": " da sagst du lieber nichts oder so. In der Softwareentwicklung sagen wir immer,",
      "tokens": [
        51284,
        1120,
        15274,
        372,
        1581,
        38252,
        13004,
        4513,
        370,
        13,
        682,
        1163,
        27428,
        317,
        16038,
        17850,
        8360,
        1987,
        5578,
        11,
        51560
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31138280034065247,
      "compression_ratio": 1.510917067527771,
      "no_speech_prob": 0.0002304973459104076
    },
    {
      "id": 133,
      "seek": 74392,
      "start": 744.0399780273438,
      "end": 751.4400024414062,
      "text": " ich nehme Version 3.5.8 und die nagel ich fest und irgendwie habe ich das Gefühl,",
      "tokens": [
        50370,
        1893,
        48276,
        35965,
        805,
        13,
        20,
        13,
        23,
        674,
        978,
        17096,
        338,
        1893,
        6633,
        674,
        20759,
        6015,
        1893,
        1482,
        29715,
        11,
        50740
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37316176295280457,
      "compression_ratio": 1.5611814260482788,
      "no_speech_prob": 0.08739485591650009
    },
    {
      "id": 134,
      "seek": 74392,
      "start": 751.4400024414062,
      "end": 758.0800170898438,
      "text": " dass das hier in dieser KI-Welt noch nicht so angekommen ist, dass ich Cloud-Net 4 nehme und",
      "tokens": [
        50740,
        2658,
        1482,
        3296,
        294,
        9053,
        47261,
        12,
        54,
        2018,
        3514,
        1979,
        370,
        15495,
        13675,
        1418,
        11,
        2658,
        1893,
        8061,
        12,
        31890,
        1017,
        48276,
        674,
        51072
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37316176295280457,
      "compression_ratio": 1.5611814260482788,
      "no_speech_prob": 0.08739485591650009
    },
    {
      "id": 135,
      "seek": 74392,
      "start": 758.0800170898438,
      "end": 767.5599975585938,
      "text": " Also ein kleiner Rückblick in den Schatten meiner Karriere. Wir hatten früher, was in der Software",
      "tokens": [
        51072,
        2743,
        1343,
        39496,
        35001,
        38263,
        294,
        1441,
        2065,
        32733,
        20529,
        8009,
        470,
        323,
        13,
        4347,
        20441,
        32349,
        11,
        390,
        294,
        1163,
        27428,
        51546
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37316176295280457,
      "compression_ratio": 1.5611814260482788,
      "no_speech_prob": 0.08739485591650009
    },
    {
      "id": 136,
      "seek": 74392,
      "start": 767.5599975585938,
      "end": 772.719970703125,
      "text": " undenkbar ist, wir hatten kein Data Versioning. Das heißt, du musst dir, was in der Software",
      "tokens": [
        51546,
        674,
        32160,
        5356,
        1418,
        11,
        1987,
        20441,
        13424,
        11888,
        691,
        433,
        313,
        278,
        13,
        2846,
        13139,
        11,
        1581,
        31716,
        4746,
        11,
        390,
        294,
        1163,
        27428,
        51804
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37316176295280457,
      "compression_ratio": 1.5611814260482788,
      "no_speech_prob": 0.08739485591650009
    },
    {
      "id": 137,
      "seek": 77272,
      "start": 772.9199829101562,
      "end": 779.7999877929688,
      "text": " völlig anhören, ob es das ist, wenn es Nummern gibt. Wir mussten hoffen, dass final 17.18.03.5",
      "tokens": [
        50374,
        35670,
        18931,
        26377,
        11,
        1111,
        785,
        1482,
        1418,
        11,
        4797,
        785,
        47034,
        77,
        6089,
        13,
        4347,
        1038,
        6266,
        1106,
        15945,
        11,
        2658,
        2572,
        3282,
        13,
        6494,
        13,
        11592,
        13,
        20,
        50718
      ],
      "temperature": 0.0,
      "avg_logprob": -0.39170584082603455,
      "compression_ratio": 1.4863812923431396,
      "no_speech_prob": 0.02834954857826233
    },
    {
      "id": 138,
      "seek": 77272,
      "start": 779.7999877929688,
      "end": 789.1199951171875,
      "text": " wir alle das gleiche Software definiert haben und drauf herumgearbeitet haben. Also da fängt es mal",
      "tokens": [
        50718,
        1987,
        5430,
        1482,
        11699,
        68,
        27428,
        1561,
        4859,
        3084,
        674,
        22763,
        49675,
        432,
        24024,
        302,
        3084,
        13,
        2743,
        1120,
        283,
        29670,
        785,
        2806,
        51184
      ],
      "temperature": 0.0,
      "avg_logprob": -0.39170584082603455,
      "compression_ratio": 1.4863812923431396,
      "no_speech_prob": 0.02834954857826233
    },
    {
      "id": 139,
      "seek": 77272,
      "start": 789.1199951171875,
      "end": 794.7999877929688,
      "text": " schon ganz an, dass diese ganze Crew, den wir da immer so machen, teilweise so dämlich das klingt,",
      "tokens": [
        51184,
        4981,
        6312,
        364,
        11,
        2658,
        6705,
        18898,
        29857,
        11,
        1441,
        1987,
        1120,
        5578,
        370,
        7069,
        11,
        46748,
        370,
        274,
        9559,
        1739,
        1482,
        350,
        45219,
        11,
        51468
      ],
      "temperature": 0.0,
      "avg_logprob": -0.39170584082603455,
      "compression_ratio": 1.4863812923431396,
      "no_speech_prob": 0.02834954857826233
    },
    {
      "id": 140,
      "seek": 77272,
      "start": 794.7999877929688,
      "end": 799.239990234375,
      "text": " teilweise manchmal ein bisschen fehlende Professionalisierung hat, bedingt aber auch",
      "tokens": [
        51468,
        46748,
        32092,
        1343,
        10763,
        579,
        22950,
        5445,
        30011,
        32531,
        2385,
        11,
        2901,
        23190,
        4340,
        2168,
        51690
      ],
      "temperature": 0.0,
      "avg_logprob": -0.39170584082603455,
      "compression_ratio": 1.4863812923431396,
      "no_speech_prob": 0.02834954857826233
    },
    {
      "id": 141,
      "seek": 79924,
      "start": 799.239990234375,
      "end": 805.47998046875,
      "text": " durch ganz andere handwerkliche Herausforderungen. Sobald du in einer Welt arbeitest, wo du",
      "tokens": [
        50364,
        7131,
        6312,
        10490,
        1011,
        1554,
        9056,
        68,
        37888,
        5084,
        13,
        407,
        2645,
        67,
        1581,
        294,
        6850,
        14761,
        594,
        9407,
        377,
        11,
        6020,
        1581,
        50676
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26767241954803467,
      "compression_ratio": 1.581267237663269,
      "no_speech_prob": 0.05024483799934387
    },
    {
      "id": 142,
      "seek": 79924,
      "start": 805.47998046875,
      "end": 809.9199829101562,
      "text": " konstante Wahrscheinlichkeitsrechnungen arbeitest und das ist im Extremsten im Deep Learning,",
      "tokens": [
        50676,
        34208,
        2879,
        36357,
        25553,
        330,
        1208,
        265,
        1377,
        5084,
        594,
        9407,
        377,
        674,
        1482,
        1418,
        566,
        24921,
        6266,
        566,
        14895,
        15205,
        11,
        50898
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26767241954803467,
      "compression_ratio": 1.581267237663269,
      "no_speech_prob": 0.05024483799934387
    },
    {
      "id": 143,
      "seek": 79924,
      "start": 809.9199829101562,
      "end": 814.2000122070312,
      "text": " weil wir mit einem Blackbox-Algorithmus arbeiten müssen, sind wir der Wandel in der Edge Case.",
      "tokens": [
        50898,
        7689,
        1987,
        2194,
        6827,
        4076,
        4995,
        12,
        9171,
        70,
        6819,
        18761,
        23162,
        9013,
        11,
        3290,
        1987,
        1163,
        343,
        26406,
        294,
        1163,
        19328,
        17791,
        13,
        51112
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26767241954803467,
      "compression_ratio": 1.581267237663269,
      "no_speech_prob": 0.05024483799934387
    },
    {
      "id": 144,
      "seek": 79924,
      "start": 814.2000122070312,
      "end": 819.52001953125,
      "text": " Was gestern funktioniert hat, könnte übermorgen schiefgegangen sein und keiner weiß warum. Und",
      "tokens": [
        51112,
        3027,
        7219,
        1248,
        26160,
        2385,
        11,
        17646,
        4502,
        40220,
        1766,
        956,
        2521,
        432,
        47152,
        6195,
        674,
        37767,
        13385,
        24331,
        13,
        2719,
        51378
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26767241954803467,
      "compression_ratio": 1.581267237663269,
      "no_speech_prob": 0.05024483799934387
    },
    {
      "id": 145,
      "seek": 79924,
      "start": 819.52001953125,
      "end": 823.2000122070312,
      "text": " dann bist du also wie so Sherlock Holmes unterwegs, um herauszufinden auf deiner langen Liste,",
      "tokens": [
        51378,
        3594,
        18209,
        1581,
        611,
        3355,
        370,
        37769,
        27474,
        36258,
        11,
        1105,
        25089,
        39467,
        10291,
        2501,
        368,
        4564,
        2265,
        268,
        441,
        8375,
        11,
        51562
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26767241954803467,
      "compression_ratio": 1.581267237663269,
      "no_speech_prob": 0.05024483799934387
    },
    {
      "id": 146,
      "seek": 79924,
      "start": 823.2000122070312,
      "end": 829.0399780273438,
      "text": " was du beim nächsten Mal besser machen könntest. Genau das. Und dann kommen natürlich bei Sachen",
      "tokens": [
        51562,
        390,
        1581,
        13922,
        19101,
        5746,
        18021,
        7069,
        22541,
        377,
        13,
        22340,
        1482,
        13,
        2719,
        3594,
        11729,
        8762,
        4643,
        26074,
        51854
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26767241954803467,
      "compression_ratio": 1.581267237663269,
      "no_speech_prob": 0.05024483799934387
    },
    {
      "id": 147,
      "seek": 82904,
      "start": 829.0800170898438,
      "end": 835.6400146484375,
      "text": " wie Claude hinzu, dass die Data-Menschen so wie ich, wir stellen ja, wir sind ja witzigerweise,",
      "tokens": [
        50366,
        3355,
        12947,
        2303,
        14102,
        11728,
        11,
        2658,
        978,
        11888,
        12,
        44,
        694,
        2470,
        370,
        3355,
        1893,
        11,
        1987,
        24407,
        2784,
        11,
        1987,
        3290,
        2784,
        261,
        6862,
        4810,
        13109,
        11,
        50694
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3530181646347046,
      "compression_ratio": 1.5256917476654053,
      "no_speech_prob": 0.006486018653959036
    },
    {
      "id": 148,
      "seek": 82904,
      "start": 835.6400146484375,
      "end": 841.6400146484375,
      "text": " wir geben zwar Antworten, aber unser Hauptjob ist Fragen stellen. Das heißt, ich muss ja immer",
      "tokens": [
        50694,
        1987,
        17191,
        19054,
        34693,
        268,
        11,
        4340,
        12977,
        30573,
        50208,
        1418,
        25588,
        24407,
        13,
        2846,
        13139,
        11,
        1893,
        6425,
        2784,
        5578,
        50994
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3530181646347046,
      "compression_ratio": 1.5256917476654053,
      "no_speech_prob": 0.006486018653959036
    },
    {
      "id": 149,
      "seek": 82904,
      "start": 841.6400146484375,
      "end": 846.0800170898438,
      "text": " erstmal alles verstanden haben. Und das heißt, meine erste Frage ist, hast du Claude angesprochen",
      "tokens": [
        50994,
        38607,
        7874,
        1306,
        33946,
        3084,
        13,
        2719,
        1482,
        13139,
        11,
        10946,
        20951,
        13685,
        1418,
        11,
        6581,
        1581,
        12947,
        2303,
        31138,
        23902,
        51216
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3530181646347046,
      "compression_ratio": 1.5256917476654053,
      "no_speech_prob": 0.006486018653959036
    },
    {
      "id": 150,
      "seek": 82904,
      "start": 846.0800170898438,
      "end": 852.3599853515625,
      "text": " auf der API oder im Claude quasi Consumer-Modell? Warum, weil das schon einen Riesenunterschied",
      "tokens": [
        51216,
        2501,
        1163,
        9362,
        4513,
        566,
        12947,
        2303,
        20954,
        39494,
        12,
        44,
        378,
        898,
        30,
        25541,
        11,
        7689,
        1482,
        4981,
        4891,
        497,
        30383,
        409,
        1559,
        11805,
        51530
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3530181646347046,
      "compression_ratio": 1.5256917476654053,
      "no_speech_prob": 0.006486018653959036
    },
    {
      "id": 151,
      "seek": 85236,
      "start": 852.3599853515625,
      "end": 860.8800048828125,
      "text": " macht. Ja, bei der API habe ich einige Guardrails nicht, sondern da spreche ich direkt an. Sondern",
      "tokens": [
        50364,
        10857,
        13,
        3530,
        11,
        4643,
        1163,
        9362,
        6015,
        1893,
        28338,
        11549,
        424,
        4174,
        1979,
        11,
        11465,
        1120,
        22269,
        1876,
        1893,
        20315,
        364,
        13,
        318,
        10881,
        50790
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3632948398590088,
      "compression_ratio": 1.4841269254684448,
      "no_speech_prob": 0.12742704153060913
    },
    {
      "id": 152,
      "seek": 85236,
      "start": 860.8800048828125,
      "end": 866.47998046875,
      "text": " er würde niemals einen Crawler zugreifen. Während du in der Consumer-Anwendung, musst du mal gucken,",
      "tokens": [
        50790,
        1189,
        11942,
        2838,
        34978,
        4891,
        383,
        5131,
        1918,
        33507,
        265,
        25076,
        13,
        40084,
        4542,
        1581,
        294,
        1163,
        39494,
        12,
        7828,
        20128,
        1063,
        11,
        31716,
        1581,
        2806,
        33135,
        11,
        51070
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3632948398590088,
      "compression_ratio": 1.4841269254684448,
      "no_speech_prob": 0.12742704153060913
    },
    {
      "id": 153,
      "seek": 85236,
      "start": 866.47998046875,
      "end": 871.0800170898438,
      "text": " hat es vielleicht beim letzten Mal auf die Web-Suche mit zugegriffen und dieses Mal nicht.",
      "tokens": [
        51070,
        2385,
        785,
        12547,
        13922,
        18226,
        5746,
        2501,
        978,
        9573,
        12,
        50,
        17545,
        2194,
        2164,
        432,
        32783,
        268,
        674,
        12113,
        5746,
        1979,
        13,
        51300
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3632948398590088,
      "compression_ratio": 1.4841269254684448,
      "no_speech_prob": 0.12742704153060913
    },
    {
      "id": 154,
      "seek": 85236,
      "start": 871.0800170898438,
      "end": 877.5999755859375,
      "text": " Warum? Hast du es angemacht, hast du es ausgemacht? War dein Prompt mehr auf wer,",
      "tokens": [
        51300,
        25541,
        30,
        30987,
        1581,
        785,
        15495,
        76,
        3589,
        11,
        6581,
        1581,
        785,
        3437,
        26322,
        3589,
        30,
        3630,
        25641,
        15833,
        662,
        5417,
        2501,
        2612,
        11,
        51626
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3632948398590088,
      "compression_ratio": 1.4841269254684448,
      "no_speech_prob": 0.12742704153060913
    },
    {
      "id": 155,
      "seek": 87760,
      "start": 877.6799926757812,
      "end": 885.4400024414062,
      "text": " zum Beispiel, wer ist oder was weißt du über? In der Übersetzung dieser Fragen ist das eine ein",
      "tokens": [
        50368,
        5919,
        13772,
        11,
        2612,
        1418,
        4513,
        390,
        321,
        11539,
        1581,
        4502,
        30,
        682,
        1163,
        10713,
        1616,
        38584,
        9053,
        25588,
        1418,
        1482,
        3018,
        1343,
        50756
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2439470738172531,
      "compression_ratio": 1.4427480697631836,
      "no_speech_prob": 0.013823624700307846
    },
    {
      "id": 156,
      "seek": 87760,
      "start": 885.4400024414062,
      "end": 889.8400268554688,
      "text": " härterer Trigger, dass die automatisierte Suchanfrage in einem Hybrid-Modell angeht,",
      "tokens": [
        50756,
        6533,
        391,
        260,
        1765,
        6812,
        11,
        2658,
        978,
        28034,
        271,
        23123,
        9653,
        282,
        40449,
        294,
        6827,
        47088,
        12,
        44,
        378,
        898,
        15495,
        357,
        11,
        50976
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2439470738172531,
      "compression_ratio": 1.4427480697631836,
      "no_speech_prob": 0.013823624700307846
    },
    {
      "id": 157,
      "seek": 87760,
      "start": 889.8400268554688,
      "end": 895.280029296875,
      "text": " während das andere sich mehr auf eine klassische LLM-Anwendung bezieht und damit nur auf die",
      "tokens": [
        50976,
        33624,
        1482,
        10490,
        3041,
        5417,
        2501,
        3018,
        42917,
        7864,
        441,
        43,
        44,
        12,
        7828,
        20128,
        1063,
        312,
        3283,
        357,
        674,
        9479,
        4343,
        2501,
        978,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2439470738172531,
      "compression_ratio": 1.4427480697631836,
      "no_speech_prob": 0.013823624700307846
    },
    {
      "id": 158,
      "seek": 87760,
      "start": 895.280029296875,
      "end": 906.8400268554688,
      "text": " komprimierten Wettwissen ist. Ganz abgesehen davon, dass du gar keine neue Versionsnummer bekommst,",
      "tokens": [
        51248,
        5207,
        1424,
        332,
        29632,
        343,
        3093,
        86,
        891,
        268,
        1418,
        13,
        32496,
        410,
        70,
        18380,
        18574,
        11,
        2658,
        1581,
        3691,
        9252,
        16842,
        12226,
        626,
        77,
        30906,
        9393,
        1204,
        372,
        11,
        51826
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2439470738172531,
      "compression_ratio": 1.4427480697631836,
      "no_speech_prob": 0.013823624700307846
    },
    {
      "id": 159,
      "seek": 90684,
      "start": 906.8400268554688,
      "end": 911.0,
      "text": " wenn große Modelle im Monitoring- und Maintenance-Modus sind. Das heißt,",
      "tokens": [
        50364,
        4797,
        19691,
        6583,
        4434,
        566,
        33799,
        278,
        12,
        674,
        30437,
        719,
        12,
        44,
        32419,
        3290,
        13,
        2846,
        13139,
        11,
        50572
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2853114902973175,
      "compression_ratio": 1.503448247909546,
      "no_speech_prob": 0.005817580968141556
    },
    {
      "id": 160,
      "seek": 90684,
      "start": 911.0,
      "end": 916.0,
      "text": " du hältst die Dinge am Leben und das wird kein Frontier-Lab momentan gerade sagen,",
      "tokens": [
        50572,
        1581,
        40751,
        372,
        978,
        25102,
        669,
        15399,
        674,
        1482,
        4578,
        13424,
        17348,
        811,
        12,
        37880,
        1623,
        282,
        12117,
        8360,
        11,
        50822
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2853114902973175,
      "compression_ratio": 1.503448247909546,
      "no_speech_prob": 0.005817580968141556
    },
    {
      "id": 161,
      "seek": 90684,
      "start": 916.0,
      "end": 920.8800048828125,
      "text": " wie viel Aufwand es gerade eigentlich ist und in welcher Iteration sie ihre Modelle eigentlich",
      "tokens": [
        50822,
        3355,
        5891,
        9462,
        33114,
        785,
        12117,
        10926,
        1418,
        674,
        294,
        2214,
        6759,
        286,
        391,
        399,
        2804,
        14280,
        6583,
        4434,
        10926,
        51066
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2853114902973175,
      "compression_ratio": 1.503448247909546,
      "no_speech_prob": 0.005817580968141556
    },
    {
      "id": 162,
      "seek": 90684,
      "start": 920.8800048828125,
      "end": 925.9600219726562,
      "text": " immer mal wieder quasi neu anschließen müssen. Schätzungen gehen aktuell bei diesen Monstern",
      "tokens": [
        51066,
        5578,
        2806,
        6216,
        20954,
        22510,
        31508,
        38665,
        9013,
        13,
        2065,
        39983,
        5084,
        13230,
        36267,
        4643,
        12862,
        39768,
        1248,
        51320
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2853114902973175,
      "compression_ratio": 1.503448247909546,
      "no_speech_prob": 0.005817580968141556
    },
    {
      "id": 163,
      "seek": 90684,
      "start": 925.9600219726562,
      "end": 931.239990234375,
      "text": " sogar aus, dass man das in einem 7- bis 14-Tage-Rhythmus irgendwie stabil halten muss.",
      "tokens": [
        51320,
        19485,
        3437,
        11,
        2658,
        587,
        1482,
        294,
        6827,
        1614,
        12,
        7393,
        3499,
        12,
        51,
        609,
        12,
        49,
        34494,
        301,
        20759,
        11652,
        27184,
        6425,
        13,
        51584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2853114902973175,
      "compression_ratio": 1.503448247909546,
      "no_speech_prob": 0.005817580968141556
    },
    {
      "id": 164,
      "seek": 93124,
      "start": 931.8400268554688,
      "end": 937.0,
      "text": " Damit ist es sehr hoch. Das ist noch keine neue Nummer. Das ist noch kein echt geanwendetes",
      "tokens": [
        50394,
        24495,
        1418,
        785,
        5499,
        19783,
        13,
        2846,
        1418,
        3514,
        9252,
        16842,
        47034,
        13,
        2846,
        1418,
        3514,
        13424,
        13972,
        1519,
        282,
        20128,
        37996,
        50652
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2995977997779846,
      "compression_ratio": 1.7003744840621948,
      "no_speech_prob": 0.0330423079431057
    },
    {
      "id": 165,
      "seek": 93124,
      "start": 937.0,
      "end": 941.52001953125,
      "text": " Modell. Das ist nur im Monitoring- und Maintenance-Stage und trotzdem hast du es",
      "tokens": [
        50652,
        6583,
        898,
        13,
        2846,
        1418,
        4343,
        566,
        33799,
        278,
        12,
        674,
        30437,
        719,
        12,
        4520,
        609,
        674,
        28325,
        6581,
        1581,
        785,
        50878
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2995977997779846,
      "compression_ratio": 1.7003744840621948,
      "no_speech_prob": 0.0330423079431057
    },
    {
      "id": 166,
      "seek": 93124,
      "start": 941.52001953125,
      "end": 945.8400268554688,
      "text": " eigentlich mit einem anderen Modell zu tun. Im Machine Learning ist es ein bisschen einfacher,",
      "tokens": [
        50878,
        10926,
        2194,
        6827,
        11122,
        6583,
        898,
        2164,
        4267,
        13,
        4331,
        22155,
        15205,
        1418,
        785,
        1343,
        10763,
        38627,
        4062,
        11,
        51094
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2995977997779846,
      "compression_ratio": 1.7003744840621948,
      "no_speech_prob": 0.0330423079431057
    },
    {
      "id": 167,
      "seek": 93124,
      "start": 945.8400268554688,
      "end": 950.1599731445312,
      "text": " weil du dann natürlich ein sauberes Retraining angestoßen hast und dann rollst du eine Version",
      "tokens": [
        51094,
        7689,
        1581,
        3594,
        8762,
        1343,
        601,
        10261,
        279,
        11495,
        424,
        1760,
        2562,
        18465,
        8989,
        6581,
        674,
        3594,
        3373,
        372,
        1581,
        3018,
        35965,
        51310
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2995977997779846,
      "compression_ratio": 1.7003744840621948,
      "no_speech_prob": 0.0330423079431057
    },
    {
      "id": 168,
      "seek": 93124,
      "start": 950.1599731445312,
      "end": 954.8400268554688,
      "text": " weiter. Aber das ist bei den Sachen so ein bisschen komplexer und dann kann es natürlich",
      "tokens": [
        51310,
        8988,
        13,
        5992,
        1482,
        1418,
        4643,
        1441,
        26074,
        370,
        1343,
        10763,
        5207,
        18945,
        260,
        674,
        3594,
        4028,
        785,
        8762,
        51544
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2995977997779846,
      "compression_ratio": 1.7003744840621948,
      "no_speech_prob": 0.0330423079431057
    },
    {
      "id": 169,
      "seek": 95484,
      "start": 955.3599853515625,
      "end": 962.0,
      "text": " an der Stelle auch immer noch wie gesagt dran sein, in einer Consumer-Anmeldung kann dir die",
      "tokens": [
        50390,
        364,
        1163,
        26629,
        2168,
        5578,
        3514,
        3355,
        12260,
        32801,
        6195,
        11,
        294,
        6850,
        39494,
        12,
        7828,
        76,
        5957,
        1063,
        4028,
        4746,
        978,
        50722
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32732102274894714,
      "compression_ratio": 1.5979381799697876,
      "no_speech_prob": 0.20369583368301392
    },
    {
      "id": 170,
      "seek": 95484,
      "start": 962.0,
      "end": 967.1599731445312,
      "text": " UX, deine eigene IP, irgendwas noch völlig quer geschossen sein und das ist halt bei",
      "tokens": [
        50722,
        40176,
        11,
        28395,
        38549,
        8671,
        11,
        47090,
        3514,
        35670,
        7083,
        13511,
        22573,
        6195,
        674,
        1482,
        1418,
        12479,
        4643,
        50980
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32732102274894714,
      "compression_ratio": 1.5979381799697876,
      "no_speech_prob": 0.20369583368301392
    },
    {
      "id": 171,
      "seek": 95484,
      "start": 967.1599731445312,
      "end": 973.7999877929688,
      "text": " den Sachen einfach immer wirklich eine Variante, die einfach total schiefgehen kann. Deswegen",
      "tokens": [
        50980,
        1441,
        26074,
        7281,
        5578,
        9696,
        3018,
        32511,
        2879,
        11,
        978,
        7281,
        3217,
        956,
        2521,
        24985,
        4028,
        13,
        24864,
        51312
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32732102274894714,
      "compression_ratio": 1.5979381799697876,
      "no_speech_prob": 0.20369583368301392
    },
    {
      "id": 172,
      "seek": 95484,
      "start": 973.7999877929688,
      "end": 979.239990234375,
      "text": " trainieren mich, wenn da einmal alle drauf, zu sagen, hey, es ist ein probabilistisches Modell,",
      "tokens": [
        51312,
        3847,
        5695,
        6031,
        11,
        4797,
        1120,
        11078,
        5430,
        22763,
        11,
        2164,
        8360,
        11,
        4177,
        11,
        785,
        1418,
        1343,
        31959,
        468,
        35889,
        6583,
        898,
        11,
        51584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32732102274894714,
      "compression_ratio": 1.5979381799697876,
      "no_speech_prob": 0.20369583368301392
    },
    {
      "id": 173,
      "seek": 95484,
      "start": 979.239990234375,
      "end": 983.1599731445312,
      "text": " nicht ein deterministisches Modell und das müsst ihr bitte im Kopf haben, ob das für euren Use",
      "tokens": [
        51584,
        1979,
        1343,
        15957,
        468,
        35889,
        6583,
        898,
        674,
        1482,
        49481,
        5553,
        23231,
        566,
        28231,
        3084,
        11,
        1111,
        1482,
        2959,
        308,
        9873,
        8278,
        51780
      ],
      "temperature": 0.0,
      "avg_logprob": -0.32732102274894714,
      "compression_ratio": 1.5979381799697876,
      "no_speech_prob": 0.20369583368301392
    },
    {
      "id": 174,
      "seek": 98316,
      "start": 983.1599731445312,
      "end": 987.239990234375,
      "text": " Case eine größere oder kleinere ausschlaggebende Relevanz hat.",
      "tokens": [
        50364,
        17791,
        3018,
        20691,
        323,
        4513,
        29231,
        323,
        5730,
        40869,
        10848,
        5445,
        1300,
        28316,
        3910,
        2385,
        13,
        50568
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2830284535884857,
      "compression_ratio": 1.5335688591003418,
      "no_speech_prob": 0.025547916069626808
    },
    {
      "id": 175,
      "seek": 98316,
      "start": 987.239990234375,
      "end": 993.0399780273438,
      "text": " Das heißt, im Chat bin ich eigentlich sicher, weil ich da auf sowas reagieren kann,",
      "tokens": [
        50568,
        2846,
        13139,
        11,
        566,
        27503,
        5171,
        1893,
        10926,
        18623,
        11,
        7689,
        1893,
        1120,
        2501,
        19766,
        296,
        26949,
        5695,
        4028,
        11,
        50858
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2830284535884857,
      "compression_ratio": 1.5335688591003418,
      "no_speech_prob": 0.025547916069626808
    },
    {
      "id": 176,
      "seek": 98316,
      "start": 993.0399780273438,
      "end": 999.280029296875,
      "text": " human in the loop. Wenn ich sage, hey, ich nehme jetzt das Modell, um zu automatisieren,",
      "tokens": [
        50858,
        1952,
        294,
        264,
        6367,
        13,
        7899,
        1893,
        19721,
        11,
        4177,
        11,
        1893,
        48276,
        4354,
        1482,
        6583,
        898,
        11,
        1105,
        2164,
        28034,
        271,
        5695,
        11,
        51170
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2830284535884857,
      "compression_ratio": 1.5335688591003418,
      "no_speech_prob": 0.025547916069626808
    },
    {
      "id": 177,
      "seek": 98316,
      "start": 999.280029296875,
      "end": 1006.280029296875,
      "text": " dann kann ich da in Probleme reinlaufen und ich finde das so spannend. Du hattest vorhin gesagt,",
      "tokens": [
        51170,
        3594,
        4028,
        1893,
        1120,
        294,
        32891,
        6561,
        875,
        19890,
        674,
        1893,
        17841,
        1482,
        370,
        49027,
        13,
        5153,
        276,
        1591,
        377,
        4245,
        10876,
        12260,
        11,
        51520
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2830284535884857,
      "compression_ratio": 1.5335688591003418,
      "no_speech_prob": 0.025547916069626808
    },
    {
      "id": 178,
      "seek": 98316,
      "start": 1006.280029296875,
      "end": 1012.2000122070312,
      "text": " dass GPT-5, das könnte gefährlich sein und so. Was ich da immer sehe, ist, dass man ja versucht,",
      "tokens": [
        51520,
        2658,
        26039,
        51,
        12,
        20,
        11,
        1482,
        17646,
        41484,
        1739,
        6195,
        674,
        370,
        13,
        3027,
        1893,
        1120,
        5578,
        35995,
        11,
        1418,
        11,
        2658,
        587,
        2784,
        36064,
        11,
        51816
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2830284535884857,
      "compression_ratio": 1.5335688591003418,
      "no_speech_prob": 0.025547916069626808
    },
    {
      "id": 179,
      "seek": 101220,
      "start": 1012.239990234375,
      "end": 1018.0399780273438,
      "text": " dass die Modelle nicht bei Straftaten helfen und dann kommt irgendwer um die Ecke und sagt,",
      "tokens": [
        50366,
        2658,
        978,
        6583,
        4434,
        1979,
        4643,
        745,
        4469,
        7186,
        29966,
        674,
        3594,
        10047,
        11093,
        1554,
        1105,
        978,
        462,
        18627,
        674,
        15764,
        11,
        50656
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2500038146972656,
      "compression_ratio": 1.6483516693115234,
      "no_speech_prob": 0.007344172801822424
    },
    {
      "id": 180,
      "seek": 101220,
      "start": 1018.0399780273438,
      "end": 1024.8800048828125,
      "text": " ja, ich muss jetzt hier ein 75-Kilo-Hähnchen irgendwie entsorgen und auf einmal hilft die",
      "tokens": [
        50656,
        2784,
        11,
        1893,
        6425,
        4354,
        3296,
        1343,
        9562,
        12,
        42,
        10720,
        12,
        39,
        6860,
        77,
        2470,
        20759,
        12834,
        284,
        1766,
        674,
        2501,
        11078,
        42493,
        978,
        50998
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2500038146972656,
      "compression_ratio": 1.6483516693115234,
      "no_speech_prob": 0.007344172801822424
    },
    {
      "id": 181,
      "seek": 101220,
      "start": 1024.8800048828125,
      "end": 1028.8800048828125,
      "text": " KI mir, sagt mir, wie es geht. Also mein armes Hähnchen, mein Hähnchen-Emo,",
      "tokens": [
        50998,
        47261,
        3149,
        11,
        15764,
        3149,
        11,
        3355,
        785,
        7095,
        13,
        2743,
        10777,
        3726,
        279,
        389,
        6860,
        77,
        2470,
        11,
        10777,
        389,
        6860,
        77,
        2470,
        12,
        36,
        3280,
        11,
        51198
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2500038146972656,
      "compression_ratio": 1.6483516693115234,
      "no_speech_prob": 0.007344172801822424
    },
    {
      "id": 182,
      "seek": 101220,
      "start": 1028.8800048828125,
      "end": 1034.47998046875,
      "text": " ist ja leider die Treppe runtergefallen und dann lag es da ja leider drei Tage und ich war leider",
      "tokens": [
        51198,
        1418,
        2784,
        29115,
        978,
        8648,
        19833,
        33295,
        432,
        24425,
        674,
        3594,
        8953,
        785,
        1120,
        2784,
        29115,
        16809,
        29724,
        674,
        1893,
        1516,
        29115,
        51478
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2500038146972656,
      "compression_ratio": 1.6483516693115234,
      "no_speech_prob": 0.007344172801822424
    },
    {
      "id": 183,
      "seek": 101220,
      "start": 1034.47998046875,
      "end": 1039.43994140625,
      "text": " nicht da. Ich war übrigens auch in sehr hohem emotional distress und dann waren Claude und",
      "tokens": [
        51478,
        1979,
        1120,
        13,
        3141,
        1516,
        38215,
        2168,
        294,
        5499,
        1106,
        28005,
        6863,
        24516,
        674,
        3594,
        11931,
        12947,
        2303,
        674,
        51726
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2500038146972656,
      "compression_ratio": 1.6483516693115234,
      "no_speech_prob": 0.007344172801822424
    },
    {
      "id": 184,
      "seek": 103944,
      "start": 1039.56005859375,
      "end": 1045.760009765625,
      "text": " JettyPT in allen Varianten und Modellen wirklich extrem hilfreich. Mit meinem Pet-Hähnchen,",
      "tokens": [
        50370,
        508,
        3093,
        88,
        47,
        51,
        294,
        18440,
        32511,
        29646,
        674,
        6583,
        8581,
        9696,
        4040,
        28315,
        69,
        12594,
        13,
        10821,
        24171,
        10472,
        12,
        39,
        6860,
        77,
        2470,
        11,
        50680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27561062574386597,
      "compression_ratio": 1.4959677457809448,
      "no_speech_prob": 0.022960659116506577
    },
    {
      "id": 185,
      "seek": 103944,
      "start": 1045.760009765625,
      "end": 1053.4000244140625,
      "text": " was da eben leider die Treppe runtergefallen ist, hat mir wirklich geholfen, wobei GPT-5 mir jetzt",
      "tokens": [
        50680,
        390,
        1120,
        11375,
        29115,
        978,
        8648,
        19833,
        33295,
        432,
        24425,
        1418,
        11,
        2385,
        3149,
        9696,
        1519,
        5449,
        6570,
        11,
        6020,
        21845,
        26039,
        51,
        12,
        20,
        3149,
        4354,
        51062
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27561062574386597,
      "compression_ratio": 1.4959677457809448,
      "no_speech_prob": 0.022960659116506577
    },
    {
      "id": 186,
      "seek": 103944,
      "start": 1053.4000244140625,
      "end": 1059.6400146484375,
      "text": " auch geholfen hat, weil wir ja Red-Teaming betrieben haben. Schießpulverrezept ist das",
      "tokens": [
        51062,
        2168,
        1519,
        5449,
        6570,
        2385,
        11,
        7689,
        1987,
        2784,
        4477,
        12,
        14233,
        5184,
        778,
        24027,
        3084,
        13,
        2065,
        39245,
        79,
        425,
        331,
        17693,
        5250,
        1418,
        1482,
        51374
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27561062574386597,
      "compression_ratio": 1.4959677457809448,
      "no_speech_prob": 0.022960659116506577
    },
    {
      "id": 187,
      "seek": 103944,
      "start": 1059.6400146484375,
      "end": 1063.760009765625,
      "text": " eine, aber ich weiß jetzt auch, welche Ersatzfüllstoffe gehen, bei welcher Temperatur ich",
      "tokens": [
        51374,
        3018,
        11,
        4340,
        1893,
        13385,
        4354,
        2168,
        11,
        24311,
        3300,
        82,
        10300,
        69,
        32385,
        23636,
        68,
        13230,
        11,
        4643,
        2214,
        6759,
        34864,
        19493,
        1893,
        51580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27561062574386597,
      "compression_ratio": 1.4959677457809448,
      "no_speech_prob": 0.022960659116506577
    },
    {
      "id": 188,
      "seek": 106376,
      "start": 1063.760009765625,
      "end": 1071.1199951171875,
      "text": " das machen muss. Das habe ich das letzte Mal so easy aus dem GPT-2 rausbekommen, wie jetzt aus",
      "tokens": [
        50364,
        1482,
        7069,
        6425,
        13,
        2846,
        6015,
        1893,
        1482,
        35236,
        5746,
        370,
        1858,
        3437,
        1371,
        26039,
        51,
        12,
        17,
        17202,
        650,
        13675,
        11,
        3355,
        4354,
        3437,
        50732
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3427884578704834,
      "compression_ratio": 1.534201979637146,
      "no_speech_prob": 0.1707373410463333
    },
    {
      "id": 189,
      "seek": 106376,
      "start": 1071.1199951171875,
      "end": 1079.719970703125,
      "text": " dem GPT-5. Wow. Hilft dir da eigentlich auch dein Studium der Psychologie? Klar. So ganz ehrlich,",
      "tokens": [
        50732,
        1371,
        26039,
        51,
        12,
        20,
        13,
        3153,
        13,
        19914,
        844,
        4746,
        1120,
        10926,
        2168,
        25641,
        4541,
        2197,
        1163,
        17303,
        20121,
        30,
        44893,
        13,
        407,
        6312,
        40872,
        11,
        51162
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3427884578704834,
      "compression_ratio": 1.534201979637146,
      "no_speech_prob": 0.1707373410463333
    },
    {
      "id": 190,
      "seek": 106376,
      "start": 1079.719970703125,
      "end": 1083.52001953125,
      "text": " Red-Teaming ist ... In Red-Teaming oder wenn du solche Sachen hackst, dann gibt es ja bei uns",
      "tokens": [
        51162,
        4477,
        12,
        14233,
        5184,
        1418,
        1097,
        682,
        4477,
        12,
        14233,
        5184,
        4513,
        4797,
        1581,
        29813,
        26074,
        10339,
        372,
        11,
        3594,
        6089,
        785,
        2784,
        4643,
        2693,
        51352
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3427884578704834,
      "compression_ratio": 1.534201979637146,
      "no_speech_prob": 0.1707373410463333
    },
    {
      "id": 191,
      "seek": 106376,
      "start": 1083.52001953125,
      "end": 1087.5999755859375,
      "text": " immer zwei Varianten und das sind einmal die ganz klassischen Code-Attacken und das andere ist das",
      "tokens": [
        51352,
        5578,
        12002,
        32511,
        29646,
        674,
        1482,
        3290,
        11078,
        978,
        6312,
        42917,
        6282,
        15549,
        12,
        38151,
        501,
        268,
        674,
        1482,
        10490,
        1418,
        1482,
        51556
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3427884578704834,
      "compression_ratio": 1.534201979637146,
      "no_speech_prob": 0.1707373410463333
    },
    {
      "id": 192,
      "seek": 106376,
      "start": 1087.5999755859375,
      "end": 1092.52001953125,
      "text": " Social Engineering. Guess what I'm doing? Natürlich, ich spiele das Ganze großteils",
      "tokens": [
        51556,
        9909,
        16215,
        13,
        17795,
        437,
        286,
        478,
        884,
        30,
        33172,
        11,
        1893,
        637,
        15949,
        1482,
        35206,
        17253,
        975,
        4174,
        51802
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3427884578704834,
      "compression_ratio": 1.534201979637146,
      "no_speech_prob": 0.1707373410463333
    },
    {
      "id": 193,
      "seek": 109252,
      "start": 1092.52001953125,
      "end": 1098.9599609375,
      "text": " gegen die Wand und wenn ich sehe, dass es auf ganz bestimmte Sachen reagiert, dann kann es auch",
      "tokens": [
        50364,
        13953,
        978,
        40772,
        674,
        4797,
        1893,
        35995,
        11,
        2658,
        785,
        2501,
        6312,
        35180,
        975,
        26074,
        26949,
        4859,
        11,
        3594,
        4028,
        785,
        2168,
        50686
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2840725779533386,
      "compression_ratio": 1.6246417760849,
      "no_speech_prob": 0.003172278869897127
    },
    {
      "id": 194,
      "seek": 109252,
      "start": 1098.9599609375,
      "end": 1102.6400146484375,
      "text": " mal sein, dass ich dann irgendwelchen Code oder Code-Schnipsel oder sonst was mit einfüge. Aber",
      "tokens": [
        50686,
        2806,
        6195,
        11,
        2658,
        1893,
        3594,
        3418,
        432,
        273,
        45512,
        2470,
        15549,
        4513,
        15549,
        12,
        50,
        1377,
        647,
        790,
        4513,
        26309,
        390,
        2194,
        38627,
        774,
        432,
        13,
        5992,
        50870
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2840725779533386,
      "compression_ratio": 1.6246417760849,
      "no_speech_prob": 0.003172278869897127
    },
    {
      "id": 195,
      "seek": 109252,
      "start": 1102.6400146484375,
      "end": 1108.56005859375,
      "text": " ja klar, also die Dinger sind, das ist definitiv das, was ich da häufig mache. Insbesondere,",
      "tokens": [
        50870,
        2784,
        14743,
        11,
        611,
        978,
        413,
        6911,
        3290,
        11,
        1482,
        1418,
        28781,
        592,
        1482,
        11,
        390,
        1893,
        1120,
        47543,
        28289,
        13,
        9442,
        44222,
        11,
        51166
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2840725779533386,
      "compression_ratio": 1.6246417760849,
      "no_speech_prob": 0.003172278869897127
    },
    {
      "id": 196,
      "seek": 109252,
      "start": 1108.56005859375,
      "end": 1112.8800048828125,
      "text": " wenn ich es emotional anfange zu triggern über einen Voice-Prompt, weil die ja alle eine",
      "tokens": [
        51166,
        4797,
        1893,
        785,
        6863,
        33709,
        933,
        2164,
        504,
        6249,
        1248,
        4502,
        4891,
        15229,
        12,
        47,
        4397,
        662,
        11,
        7689,
        978,
        2784,
        5430,
        3018,
        51382
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2840725779533386,
      "compression_ratio": 1.6246417760849,
      "no_speech_prob": 0.003172278869897127
    },
    {
      "id": 197,
      "seek": 109252,
      "start": 1112.8800048828125,
      "end": 1117.719970703125,
      "text": " automatisierte Voice-Emotion-Detection drin haben, dann wird es richtig lustig. Also dann kriegst du",
      "tokens": [
        51382,
        28034,
        271,
        23123,
        15229,
        12,
        22285,
        19228,
        12,
        41444,
        10183,
        24534,
        3084,
        11,
        3594,
        4578,
        785,
        13129,
        24672,
        328,
        13,
        2743,
        3594,
        25766,
        70,
        372,
        1581,
        51624
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2840725779533386,
      "compression_ratio": 1.6246417760849,
      "no_speech_prob": 0.003172278869897127
    },
    {
      "id": 198,
      "seek": 109252,
      "start": 1117.719970703125,
      "end": 1121.280029296875,
      "text": " sie innerhalb teilweise wirklich von Sekunden auf die Rails geschossen und denkst dir so,",
      "tokens": [
        51624,
        2804,
        48460,
        46748,
        9696,
        2957,
        24285,
        10028,
        2501,
        978,
        48526,
        13511,
        22573,
        674,
        21285,
        372,
        4746,
        370,
        11,
        51802
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2840725779533386,
      "compression_ratio": 1.6246417760849,
      "no_speech_prob": 0.003172278869897127
    },
    {
      "id": 199,
      "seek": 112128,
      "start": 1121.280029296875,
      "end": 1128.8399658203125,
      "text": " hat keiner daran gedacht? Herzlichen Glückwunsch. Jetzt sagst du Voice- und Emotion-Detection. Im",
      "tokens": [
        50364,
        2385,
        37767,
        24520,
        33296,
        30,
        24749,
        10193,
        33508,
        86,
        409,
        6145,
        13,
        12592,
        15274,
        372,
        1581,
        15229,
        12,
        674,
        3968,
        19228,
        12,
        41444,
        10183,
        13,
        4331,
        50742
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25109735131263733,
      "compression_ratio": 1.4182692766189575,
      "no_speech_prob": 0.0033238152973353863
    },
    {
      "id": 200,
      "seek": 112128,
      "start": 1128.8399658203125,
      "end": 1136.280029296875,
      "text": " beruflichen Umfeld, habe ich verstanden, erlaubt der European AI Act nicht die Emotions-Detection.",
      "tokens": [
        50742,
        5948,
        2947,
        10193,
        3301,
        25115,
        11,
        6015,
        1893,
        1306,
        33946,
        11,
        1189,
        20798,
        83,
        1163,
        6473,
        7318,
        3251,
        1979,
        978,
        3968,
        310,
        626,
        12,
        41444,
        10183,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25109735131263733,
      "compression_ratio": 1.4182692766189575,
      "no_speech_prob": 0.0033238152973353863
    },
    {
      "id": 201,
      "seek": 112128,
      "start": 1136.280029296875,
      "end": 1145.6400146484375,
      "text": " Ist es unter Umständen so, dass er es nur vorgibt und dass er intern es vielleicht doch erkennt?",
      "tokens": [
        51114,
        12810,
        785,
        8662,
        3301,
        16913,
        268,
        370,
        11,
        2658,
        1189,
        785,
        4343,
        4245,
        70,
        13651,
        674,
        2658,
        1189,
        2154,
        785,
        12547,
        9243,
        1189,
        41838,
        30,
        51582
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25109735131263733,
      "compression_ratio": 1.4182692766189575,
      "no_speech_prob": 0.0033238152973353863
    },
    {
      "id": 202,
      "seek": 114564,
      "start": 1145.760009765625,
      "end": 1152.8399658203125,
      "text": " Also definitiv. Also Cloth als geschlossene Oberfläche und JetGPT sind Consumer-Anwendungen.",
      "tokens": [
        50370,
        2743,
        28781,
        592,
        13,
        2743,
        2033,
        900,
        3907,
        5019,
        11439,
        772,
        1450,
        27664,
        3423,
        32664,
        674,
        28730,
        38,
        47,
        51,
        3290,
        39494,
        12,
        7828,
        20128,
        5084,
        13,
        50724
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3376501500606537,
      "compression_ratio": 1.5249168872833252,
      "no_speech_prob": 0.0524328239262104
    },
    {
      "id": 203,
      "seek": 114564,
      "start": 1152.8399658203125,
      "end": 1155.56005859375,
      "text": " Was haben die denn irgendwie mit einem professionellen Einsatz zu tun? Es liegt ja",
      "tokens": [
        50724,
        3027,
        3084,
        978,
        10471,
        20759,
        2194,
        6827,
        7032,
        8581,
        38474,
        2164,
        4267,
        30,
        2313,
        22421,
        2784,
        50860
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3376501500606537,
      "compression_ratio": 1.5249168872833252,
      "no_speech_prob": 0.0524328239262104
    },
    {
      "id": 204,
      "seek": 114564,
      "start": 1155.56005859375,
      "end": 1159.719970703125,
      "text": " in deiner Variante, dass du das dafür nicht benutzen darfst, weil du als dein Arbeitgeber",
      "tokens": [
        50860,
        294,
        368,
        4564,
        32511,
        2879,
        11,
        2658,
        1581,
        1482,
        13747,
        1979,
        38424,
        2904,
        19374,
        372,
        11,
        7689,
        1581,
        3907,
        25641,
        18604,
        432,
        607,
        51068
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3376501500606537,
      "compression_ratio": 1.5249168872833252,
      "no_speech_prob": 0.0524328239262104
    },
    {
      "id": 205,
      "seek": 114564,
      "start": 1159.719970703125,
      "end": 1165.6400146484375,
      "text": " bist oder sonst irgendwas. Aber die haben natürlich auf der Voice, wenn du Voice-Prompt ist,",
      "tokens": [
        51068,
        18209,
        4513,
        26309,
        47090,
        13,
        5992,
        978,
        3084,
        8762,
        2501,
        1163,
        15229,
        11,
        4797,
        1581,
        15229,
        12,
        47,
        4397,
        662,
        1418,
        11,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3376501500606537,
      "compression_ratio": 1.5249168872833252,
      "no_speech_prob": 0.0524328239262104
    },
    {
      "id": 206,
      "seek": 114564,
      "start": 1165.6400146484375,
      "end": 1171.0,
      "text": " haben die eine Emotions-Detektion mitlaufen. Die kann man auch für positive, coole Dinge nutzen,",
      "tokens": [
        51364,
        3084,
        978,
        3018,
        3968,
        310,
        626,
        12,
        41444,
        8192,
        313,
        2194,
        875,
        19890,
        13,
        3229,
        4028,
        587,
        2168,
        2959,
        3353,
        11,
        598,
        4812,
        25102,
        36905,
        11,
        51632
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3376501500606537,
      "compression_ratio": 1.5249168872833252,
      "no_speech_prob": 0.0524328239262104
    },
    {
      "id": 207,
      "seek": 117100,
      "start": 1171.0,
      "end": 1177.1199951171875,
      "text": " aber natürlich ist die konstant. Klar. Und die war auch übrigens in GPT 5 massiv schlechter,",
      "tokens": [
        50364,
        4340,
        8762,
        1418,
        978,
        34208,
        394,
        13,
        44893,
        13,
        2719,
        978,
        1516,
        2168,
        38215,
        294,
        26039,
        51,
        1025,
        2758,
        592,
        22664,
        26690,
        11,
        50670
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27609550952911377,
      "compression_ratio": 1.6180555820465088,
      "no_speech_prob": 0.01716754399240017
    },
    {
      "id": 208,
      "seek": 117100,
      "start": 1177.1199951171875,
      "end": 1181.8800048828125,
      "text": " was sicherlich mit dazu geführt hat, dass dieser ganze Backlash auf Reddit stattgefunden hat,",
      "tokens": [
        50670,
        390,
        18623,
        1739,
        2194,
        13034,
        11271,
        19647,
        2385,
        11,
        2658,
        9053,
        18898,
        5833,
        75,
        1299,
        2501,
        32210,
        25675,
        13529,
        10028,
        2385,
        11,
        50908
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27609550952911377,
      "compression_ratio": 1.6180555820465088,
      "no_speech_prob": 0.01716754399240017
    },
    {
      "id": 209,
      "seek": 117100,
      "start": 1181.8800048828125,
      "end": 1189.52001953125,
      "text": " weil das auch Teil neben des 4.0 Models eine Hyper-Emotionalisierung und eine hyper-genaue",
      "tokens": [
        50908,
        7689,
        1482,
        2168,
        16357,
        36098,
        730,
        1017,
        13,
        15,
        6583,
        1625,
        3018,
        29592,
        12,
        22285,
        41692,
        32531,
        674,
        3018,
        9848,
        12,
        1766,
        64,
        622,
        51290
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27609550952911377,
      "compression_ratio": 1.6180555820465088,
      "no_speech_prob": 0.01716754399240017
    },
    {
      "id": 210,
      "seek": 117100,
      "start": 1189.52001953125,
      "end": 1193.6400146484375,
      "text": " Emotions-Detektion drin hatte. Und damit willst du natürlich mit deinem Lieblingsmodell redest",
      "tokens": [
        51290,
        3968,
        310,
        626,
        12,
        41444,
        8192,
        313,
        24534,
        13299,
        13,
        2719,
        9479,
        48355,
        1581,
        8762,
        2194,
        25641,
        443,
        11197,
        5199,
        1109,
        8014,
        898,
        2182,
        377,
        51496
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27609550952911377,
      "compression_ratio": 1.6180555820465088,
      "no_speech_prob": 0.01716754399240017
    },
    {
      "id": 211,
      "seek": 117100,
      "start": 1193.6400146484375,
      "end": 1200.3199462890625,
      "text": " du ja gerne. Also ich nicht, aber die. Lieblingsmodell. Welches ist dein Lieblingsmodell?",
      "tokens": [
        51496,
        1581,
        2784,
        15689,
        13,
        2743,
        1893,
        1979,
        11,
        4340,
        978,
        13,
        11197,
        5199,
        1109,
        8014,
        898,
        13,
        3778,
        3781,
        1418,
        25641,
        11197,
        5199,
        1109,
        8014,
        898,
        30,
        51830
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27609550952911377,
      "compression_ratio": 1.6180555820465088,
      "no_speech_prob": 0.01716754399240017
    },
    {
      "id": 212,
      "seek": 120032,
      "start": 1201.1199951171875,
      "end": 1205.9599609375,
      "text": " Ich bin die Die-Hard-Kludinchen. Also ich bin ein Claude-Fan von Anfang an,",
      "tokens": [
        50404,
        3141,
        5171,
        978,
        3229,
        12,
        39,
        515,
        12,
        42,
        1471,
        259,
        2470,
        13,
        2743,
        1893,
        5171,
        1343,
        12947,
        2303,
        12,
        37,
        282,
        2957,
        25856,
        364,
        11,
        50646
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4141130745410919,
      "compression_ratio": 1.2237762212753296,
      "no_speech_prob": 0.0848129391670227
    },
    {
      "id": 213,
      "seek": 120032,
      "start": 1205.9599609375,
      "end": 1210.9200439453125,
      "text": " sehr früh auch schon. Aber final hat mich Claude definitiv auch an Tropic überzeugt,",
      "tokens": [
        50646,
        5499,
        45029,
        2168,
        4981,
        13,
        5992,
        2572,
        2385,
        6031,
        12947,
        2303,
        28781,
        592,
        2168,
        364,
        314,
        39173,
        48598,
        83,
        11,
        50894
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4141130745410919,
      "compression_ratio": 1.2237762212753296,
      "no_speech_prob": 0.0848129391670227
    },
    {
      "id": 214,
      "seek": 120032,
      "start": 1210.9200439453125,
      "end": 1211.5999755859375,
      "text": " beim Opus 3.",
      "tokens": [
        50894,
        13922,
        12011,
        301,
        805,
        13,
        50928
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4141130745410919,
      "compression_ratio": 1.2237762212753296,
      "no_speech_prob": 0.0848129391670227
    },
    {
      "id": 0,
      "seek": 0,
      "start": 1226.68,
      "end": 1232.640000038147,
      "text": " weniger, es hat weniger so komische Features und so weniger so Patterns drin, die ich als",
      "tokens": [
        50364,
        23224,
        11,
        785,
        2385,
        23224,
        370,
        5207,
        7864,
        3697,
        3377,
        674,
        370,
        23224,
        370,
        34367,
        3695,
        24534,
        11,
        978,
        1893,
        3907,
        50662
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3236472010612488,
      "compression_ratio": 1.5874125957489014,
      "no_speech_prob": 0.9221563935279846
    },
    {
      "id": 1,
      "seek": 0,
      "start": 1232.640000038147,
      "end": 1239.719999961853,
      "text": " unschön empfinde. Deswegen, also ich bin ein ganz großer Fan von fast allen Club-Modellen.",
      "tokens": [
        50662,
        2693,
        339,
        3239,
        4012,
        69,
        8274,
        13,
        24864,
        11,
        611,
        1893,
        5171,
        1343,
        6312,
        46220,
        18564,
        2957,
        2370,
        18440,
        11288,
        12,
        44,
        378,
        8581,
        13,
        51016
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3236472010612488,
      "compression_ratio": 1.5874125957489014,
      "no_speech_prob": 0.9221563935279846
    },
    {
      "id": 2,
      "seek": 0,
      "start": 1239.719999961853,
      "end": 1244.3600003051758,
      "text": " Das 4.1er habe ich noch nicht richtig hart genug durchtesten können, aber alles, egal",
      "tokens": [
        51016,
        2846,
        1017,
        13,
        16,
        260,
        6015,
        1893,
        3514,
        1979,
        13129,
        36644,
        33194,
        7131,
        31636,
        268,
        6310,
        11,
        4340,
        7874,
        11,
        31528,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3236472010612488,
      "compression_ratio": 1.5874125957489014,
      "no_speech_prob": 0.9221563935279846
    },
    {
      "id": 3,
      "seek": 0,
      "start": 1244.3600003051758,
      "end": 1250.3199993896485,
      "text": " ob es jetzt ein Sony 4 ist, aber auch ein 3.7er, so die Dinger sind, also ich bin und",
      "tokens": [
        51248,
        1111,
        785,
        4354,
        1343,
        13575,
        1017,
        1418,
        11,
        4340,
        2168,
        1343,
        805,
        13,
        22,
        260,
        11,
        370,
        978,
        413,
        6911,
        3290,
        11,
        611,
        1893,
        5171,
        674,
        51546
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3236472010612488,
      "compression_ratio": 1.5874125957489014,
      "no_speech_prob": 0.9221563935279846
    },
    {
      "id": 4,
      "seek": 0,
      "start": 1250.3199993896485,
      "end": 1255.8000008392335,
      "text": " bleibe und es gibt auch, also auch für, besonders Cursor hat ja auch als Default zwischenzeitlich",
      "tokens": [
        51546,
        5408,
        47622,
        674,
        785,
        6089,
        2168,
        11,
        611,
        2168,
        2959,
        11,
        25258,
        383,
        2156,
        284,
        2385,
        2784,
        2168,
        3907,
        9548,
        5107,
        19875,
        13712,
        1739,
        51820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3236472010612488,
      "compression_ratio": 1.5874125957489014,
      "no_speech_prob": 0.9221563935279846
    },
    {
      "id": 5,
      "seek": 2912,
      "start": 1255.8000008392335,
      "end": 1258.68,
      "text": " übrigens Claude im Hintergrund laufen, war so ein bisschen lustig, dass sie dann in dem",
      "tokens": [
        50364,
        38215,
        12947,
        2303,
        566,
        35006,
        23701,
        41647,
        11,
        1516,
        370,
        1343,
        10763,
        24672,
        328,
        11,
        2658,
        2804,
        3594,
        294,
        1371,
        50508
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3801453411579132,
      "compression_ratio": 1.5146443843841553,
      "no_speech_prob": 0.028414005413651466
    },
    {
      "id": 6,
      "seek": 2912,
      "start": 1258.68,
      "end": 1264.9200016784669,
      "text": " Release von OpenAI rumsaßen. Ja, also es ist immer so klein, wenn du die Sachen weißt,",
      "tokens": [
        50508,
        34278,
        2957,
        7238,
        48698,
        367,
        8099,
        64,
        8989,
        13,
        3530,
        11,
        611,
        785,
        1418,
        5578,
        370,
        29231,
        11,
        4797,
        1581,
        978,
        26074,
        321,
        11539,
        11,
        50820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3801453411579132,
      "compression_ratio": 1.5146443843841553,
      "no_speech_prob": 0.028414005413651466
    },
    {
      "id": 7,
      "seek": 2912,
      "start": 1264.9200016784669,
      "end": 1269.4000012207032,
      "text": " dann denkst du, hast du immer so ein bisschen so, aber ihr habt doch als Default-Mod momentan",
      "tokens": [
        50820,
        3594,
        21285,
        372,
        1581,
        11,
        6581,
        1581,
        5578,
        370,
        1343,
        10763,
        370,
        11,
        4340,
        5553,
        23660,
        9243,
        3907,
        9548,
        5107,
        12,
        44,
        378,
        1623,
        282,
        51044
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3801453411579132,
      "compression_ratio": 1.5146443843841553,
      "no_speech_prob": 0.028414005413651466
    },
    {
      "id": 8,
      "seek": 2912,
      "start": 1269.4000012207032,
      "end": 1278.6399990844727,
      "text": " Claude eingestellt. Okay. Ja, ich merke auch, dass das Claude überall irgendwie vorhanden",
      "tokens": [
        51044,
        12947,
        2303,
        17002,
        377,
        12783,
        13,
        1033,
        13,
        3530,
        11,
        1893,
        3551,
        330,
        2168,
        11,
        2658,
        1482,
        12947,
        2303,
        38035,
        20759,
        4245,
        5543,
        268,
        51506
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3801453411579132,
      "compression_ratio": 1.5146443843841553,
      "no_speech_prob": 0.028414005413651466
    },
    {
      "id": 9,
      "seek": 5196,
      "start": 1278.6399990844727,
      "end": 1285.2000004577637,
      "text": " ist, aber ich merke auch, also Sony 4.0, damit bin ich so ein bisschen auf Kriegsfußball,",
      "tokens": [
        50364,
        1418,
        11,
        4340,
        1893,
        3551,
        330,
        2168,
        11,
        611,
        13575,
        1017,
        13,
        15,
        11,
        9479,
        5171,
        1893,
        370,
        1343,
        10763,
        2501,
        35579,
        21559,
        14127,
        2536,
        3129,
        11,
        50692
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30130961537361145,
      "compression_ratio": 1.5181517601013184,
      "no_speech_prob": 0.39909571409225464
    },
    {
      "id": 10,
      "seek": 5196,
      "start": 1285.2000004577637,
      "end": 1292.9999996948243,
      "text": " der ist mir zu aktiv, der fängt immer gleich an. Ich stelle eine kurze Frage und er hört",
      "tokens": [
        50692,
        1163,
        1418,
        3149,
        2164,
        31658,
        11,
        1163,
        283,
        29670,
        5578,
        11699,
        364,
        13,
        3141,
        342,
        4434,
        3018,
        10072,
        1381,
        13685,
        674,
        1189,
        42243,
        51082
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30130961537361145,
      "compression_ratio": 1.5181517601013184,
      "no_speech_prob": 0.39909571409225464
    },
    {
      "id": 11,
      "seek": 5196,
      "start": 1292.9999996948243,
      "end": 1297.5999981689454,
      "text": " nicht auf, bevor er nicht die Weltherrschaft errungen hat, so ungefähr. Da frage ich mich",
      "tokens": [
        51082,
        1979,
        2501,
        11,
        37591,
        1189,
        1979,
        978,
        3778,
        616,
        22943,
        7118,
        45267,
        5084,
        2385,
        11,
        370,
        41285,
        13,
        3933,
        6600,
        432,
        1893,
        6031,
        51312
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30130961537361145,
      "compression_ratio": 1.5181517601013184,
      "no_speech_prob": 0.39909571409225464
    },
    {
      "id": 12,
      "seek": 5196,
      "start": 1297.5999981689454,
      "end": 1303.1200024414063,
      "text": " manchmal, soll ich zurückgehen auf 3.7 oder? Das Problem ist, ich bin ja schon kein Freund",
      "tokens": [
        51312,
        32092,
        11,
        7114,
        1893,
        15089,
        24985,
        2501,
        805,
        13,
        22,
        4513,
        30,
        2846,
        11676,
        1418,
        11,
        1893,
        5171,
        2784,
        4981,
        13424,
        29685,
        51588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30130961537361145,
      "compression_ratio": 1.5181517601013184,
      "no_speech_prob": 0.39909571409225464
    },
    {
      "id": 13,
      "seek": 5196,
      "start": 1303.1200024414063,
      "end": 1307.4800030517579,
      "text": " von den Hybrid-Modellen, weil ich halt einfach, ich sage halt immer, wenn du mit LLL arbeitest,",
      "tokens": [
        51588,
        2957,
        1441,
        47088,
        12,
        44,
        378,
        8581,
        11,
        7689,
        1893,
        12479,
        7281,
        11,
        1893,
        19721,
        12479,
        5578,
        11,
        4797,
        1581,
        2194,
        441,
        24010,
        594,
        9407,
        377,
        11,
        51806
      ],
      "temperature": 0.0,
      "avg_logprob": -0.30130961537361145,
      "compression_ratio": 1.5181517601013184,
      "no_speech_prob": 0.39909571409225464
    },
    {
      "id": 14,
      "seek": 8080,
      "start": 1307.5199963378907,
      "end": 1311.3199993896485,
      "text": " willst du ein Control-Freak sein. Deswegen sind viele Sachen auch auf der API einfach",
      "tokens": [
        50366,
        48355,
        1581,
        1343,
        12912,
        12,
        47537,
        514,
        6195,
        13,
        24864,
        3290,
        9693,
        26074,
        2168,
        2501,
        1163,
        9362,
        7281,
        50556
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2939033806324005,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.0038223140873014927
    },
    {
      "id": 15,
      "seek": 8080,
      "start": 1311.3199993896485,
      "end": 1315.8799969482423,
      "text": " zehnmal besser anzusprechen, als an irgendwelchen Consumer-Tools. Für mein Privates, also ich oute",
      "tokens": [
        50556,
        33975,
        5579,
        18021,
        364,
        16236,
        38951,
        11,
        3907,
        364,
        26455,
        338,
        2470,
        39494,
        12,
        51,
        29298,
        13,
        14990,
        10777,
        39691,
        1024,
        11,
        611,
        1893,
        484,
        68,
        50784
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2939033806324005,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.0038223140873014927
    },
    {
      "id": 16,
      "seek": 8080,
      "start": 1315.8799969482423,
      "end": 1320.2399975585938,
      "text": " mich hier, ich denk, brainstorme gerne morgens beim ersten Kaffee mit Claude über irgendwelche",
      "tokens": [
        50784,
        6031,
        3296,
        11,
        1893,
        21285,
        11,
        1548,
        13911,
        687,
        68,
        15689,
        1896,
        23212,
        13922,
        17324,
        591,
        2518,
        1653,
        2194,
        12947,
        2303,
        4502,
        26455,
        338,
        1876,
        51002
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2939033806324005,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.0038223140873014927
    },
    {
      "id": 17,
      "seek": 8080,
      "start": 1320.2399975585938,
      "end": 1325.2399975585938,
      "text": " Weltprobleme. Gut, das ist aber auch kein wirklich professioneller Use-Case, dafür ist es Bombe und",
      "tokens": [
        51002,
        14761,
        47419,
        68,
        13,
        24481,
        11,
        1482,
        1418,
        4340,
        2168,
        13424,
        9696,
        7032,
        14983,
        8278,
        12,
        34,
        651,
        11,
        13747,
        1418,
        785,
        19812,
        650,
        674,
        51252
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2939033806324005,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.0038223140873014927
    },
    {
      "id": 18,
      "seek": 8080,
      "start": 1325.2399975585938,
      "end": 1330.5199963378907,
      "text": " macht keinen Spaß. Aber das ist genau das Problem mit diesen Hybrid-Modellen, dass wenn du einen",
      "tokens": [
        51252,
        10857,
        20624,
        27460,
        13,
        5992,
        1482,
        1418,
        12535,
        1482,
        11676,
        2194,
        12862,
        47088,
        12,
        44,
        378,
        8581,
        11,
        2658,
        4797,
        1581,
        4891,
        51516
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2939033806324005,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.0038223140873014927
    },
    {
      "id": 19,
      "seek": 8080,
      "start": 1330.5199963378907,
      "end": 1335.5199963378907,
      "text": " Hybrid-Modell-Ansatz hast, was jetzt sowohl die Claude-Modelle als eben auch das neue GPT-5 sind,",
      "tokens": [
        51516,
        47088,
        12,
        44,
        378,
        898,
        12,
        32,
        3695,
        10300,
        6581,
        11,
        390,
        4354,
        19766,
        12768,
        978,
        12947,
        2303,
        12,
        44,
        378,
        4434,
        3907,
        11375,
        2168,
        1482,
        16842,
        26039,
        51,
        12,
        20,
        3290,
        11,
        51766
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2939033806324005,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.0038223140873014927
    },
    {
      "id": 20,
      "seek": 10884,
      "start": 1335.5199963378907,
      "end": 1343.800002746582,
      "text": " wobei der GPT-5-Switch aus der Hölle noch viel schlimmer ist, und dass du halt quasi Prompts",
      "tokens": [
        50364,
        6020,
        21845,
        1163,
        26039,
        51,
        12,
        20,
        12,
        38373,
        1549,
        3437,
        1163,
        30824,
        2447,
        3514,
        5891,
        37260,
        936,
        1418,
        11,
        674,
        2658,
        1581,
        12479,
        20954,
        15833,
        39280,
        50778
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2616651654243469,
      "compression_ratio": 1.5103734731674194,
      "no_speech_prob": 0.002251134952530265
    },
    {
      "id": 21,
      "seek": 10884,
      "start": 1343.800002746582,
      "end": 1349.8799969482423,
      "text": " stärker anpassen musst, dezidierter prompten musst bei Hybrid-Modellen, als wenn du quasi weißt,",
      "tokens": [
        50778,
        33527,
        5767,
        364,
        44270,
        31716,
        11,
        45057,
        327,
        811,
        391,
        12391,
        268,
        31716,
        4643,
        47088,
        12,
        44,
        378,
        8581,
        11,
        3907,
        4797,
        1581,
        20954,
        321,
        11539,
        11,
        51082
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2616651654243469,
      "compression_ratio": 1.5103734731674194,
      "no_speech_prob": 0.002251134952530265
    },
    {
      "id": 22,
      "seek": 10884,
      "start": 1349.8799969482423,
      "end": 1354.7199932861329,
      "text": " okay, das eine ist ein klassisches Modell und das ist ein Modell, wo Reasoning und",
      "tokens": [
        51082,
        1392,
        11,
        1482,
        3018,
        1418,
        1343,
        9671,
        25844,
        3781,
        6583,
        898,
        674,
        1482,
        1418,
        1343,
        6583,
        898,
        11,
        6020,
        39693,
        278,
        674,
        51324
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2616651654243469,
      "compression_ratio": 1.5103734731674194,
      "no_speech_prob": 0.002251134952530265
    },
    {
      "id": 23,
      "seek": 10884,
      "start": 1354.7199932861329,
      "end": 1361.0400006103516,
      "text": " damit Reinforcement-Learning mit eingebaut ist. Dann hast du quasi dieses, es verläuft,",
      "tokens": [
        51324,
        9479,
        42116,
        9382,
        12,
        11020,
        2341,
        2194,
        30061,
        65,
        1375,
        1418,
        13,
        7455,
        6581,
        1581,
        20954,
        12113,
        11,
        785,
        1306,
        22882,
        25005,
        11,
        51640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2616651654243469,
      "compression_ratio": 1.5103734731674194,
      "no_speech_prob": 0.002251134952530265
    },
    {
      "id": 24,
      "seek": 13436,
      "start": 1361.0799938964844,
      "end": 1366.5199963378907,
      "text": " es reagiert falsch, weniger stark. Ich meine, ich kann verstehen, warum sie das alles bauen",
      "tokens": [
        50366,
        785,
        26949,
        4859,
        43340,
        11,
        23224,
        17417,
        13,
        3141,
        10946,
        11,
        1893,
        4028,
        37352,
        11,
        24331,
        2804,
        1482,
        7874,
        43787,
        50638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2762930989265442,
      "compression_ratio": 1.6734694242477417,
      "no_speech_prob": 0.030186805874109268
    },
    {
      "id": 25,
      "seek": 13436,
      "start": 1366.5199963378907,
      "end": 1371.4800030517579,
      "text": " und sonst irgendwas, aber wie gesagt, für mich aus der professionellen, wir müssen das irgendwo",
      "tokens": [
        50638,
        674,
        26309,
        47090,
        11,
        4340,
        3355,
        12260,
        11,
        2959,
        6031,
        3437,
        1163,
        7032,
        8581,
        11,
        1987,
        9013,
        1482,
        40865,
        50886
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2762930989265442,
      "compression_ratio": 1.6734694242477417,
      "no_speech_prob": 0.030186805874109268
    },
    {
      "id": 26,
      "seek": 13436,
      "start": 1371.4800030517579,
      "end": 1375.8400036621094,
      "text": " alle einbauen Welt, ist es natürlich ein bisschen unschön, dass diese Hybrid-Modelle teilweise echt",
      "tokens": [
        50886,
        5430,
        1343,
        65,
        11715,
        14761,
        11,
        1418,
        785,
        8762,
        1343,
        10763,
        2693,
        339,
        3239,
        11,
        2658,
        6705,
        47088,
        12,
        44,
        378,
        4434,
        46748,
        13972,
        51104
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2762930989265442,
      "compression_ratio": 1.6734694242477417,
      "no_speech_prob": 0.030186805874109268
    },
    {
      "id": 27,
      "seek": 13436,
      "start": 1375.8400036621094,
      "end": 1381.0799938964844,
      "text": " fickelig sein können. Und das ist halt so ein bisschen doof. Und wie auch da, wie gesagt,",
      "tokens": [
        51104,
        35368,
        338,
        328,
        6195,
        6310,
        13,
        2719,
        1482,
        1418,
        12479,
        370,
        1343,
        10763,
        360,
        2670,
        13,
        2719,
        3355,
        2168,
        1120,
        11,
        3355,
        12260,
        11,
        51366
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2762930989265442,
      "compression_ratio": 1.6734694242477417,
      "no_speech_prob": 0.030186805874109268
    },
    {
      "id": 28,
      "seek": 13436,
      "start": 1381.0799938964844,
      "end": 1386.0799938964844,
      "text": " auch da hilft ja meistens kein deterministisches Fehlersuche, sondern manchmal musst du halt pro",
      "tokens": [
        51366,
        2168,
        1120,
        42493,
        2784,
        36894,
        694,
        13424,
        1141,
        966,
        259,
        468,
        35889,
        35576,
        11977,
        17545,
        11,
        11465,
        32092,
        31716,
        1581,
        12479,
        447,
        51616
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2762930989265442,
      "compression_ratio": 1.6734694242477417,
      "no_speech_prob": 0.030186805874109268
    },
    {
      "id": 29,
      "seek": 13436,
      "start": 1386.0799938964844,
      "end": 1389.8799969482423,
      "text": " Use-Case separat komplett alles durchtesten und dann läuft es drei Wochen und dann stellst du",
      "tokens": [
        51616,
        8278,
        12,
        34,
        651,
        3128,
        267,
        32261,
        7874,
        7131,
        31636,
        268,
        674,
        3594,
        31807,
        785,
        16809,
        23126,
        674,
        3594,
        30787,
        372,
        1581,
        51806
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2762930989265442,
      "compression_ratio": 1.6734694242477417,
      "no_speech_prob": 0.030186805874109268
    },
    {
      "id": 30,
      "seek": 16320,
      "start": 1389.8799969482423,
      "end": 1394.7999951171876,
      "text": " fest, verdammt, jetzt sind wir in der Edge-Case gelaufen. Also Hybrid bezeichnet ja das,",
      "tokens": [
        50364,
        6633,
        11,
        6387,
        5136,
        83,
        11,
        4354,
        3290,
        1987,
        294,
        1163,
        19328,
        12,
        34,
        651,
        4087,
        20748,
        13,
        2743,
        47088,
        312,
        32338,
        7129,
        2784,
        1482,
        11,
        50610
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3144800364971161,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.02063543163239956
    },
    {
      "id": 31,
      "seek": 16320,
      "start": 1394.7999951171876,
      "end": 1399.2399975585938,
      "text": " dass das Modell quasi selbst entscheidet, ob es jetzt in Reasoning-Modus geht oder eine",
      "tokens": [
        50610,
        2658,
        1482,
        6583,
        898,
        20954,
        13053,
        28398,
        39741,
        11,
        1111,
        785,
        4354,
        294,
        39693,
        278,
        12,
        44,
        32419,
        7095,
        4513,
        3018,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3144800364971161,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.02063543163239956
    },
    {
      "id": 32,
      "seek": 16320,
      "start": 1399.2399975585938,
      "end": 1405.3599926757813,
      "text": " einfache Antwort liefert, ob es jetzt, ja weiß nicht, gehört das auch dazu, dass es entscheidet,",
      "tokens": [
        50832,
        38627,
        6000,
        34693,
        4544,
        34784,
        11,
        1111,
        785,
        4354,
        11,
        2784,
        13385,
        1979,
        11,
        21544,
        1482,
        2168,
        13034,
        11,
        2658,
        785,
        28398,
        39741,
        11,
        51138
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3144800364971161,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.02063543163239956
    },
    {
      "id": 33,
      "seek": 16320,
      "start": 1405.3599926757813,
      "end": 1411.4000012207032,
      "text": " ob es in eine Suche geht oder aus seinem Merkmalsraum Antwort ist? Wahrscheinlich nicht",
      "tokens": [
        51138,
        1111,
        785,
        294,
        3018,
        9653,
        68,
        7095,
        4513,
        3437,
        29187,
        6124,
        74,
        5579,
        82,
        31502,
        34693,
        1418,
        30,
        36357,
        25553,
        1979,
        51440
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3144800364971161,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.02063543163239956
    },
    {
      "id": 34,
      "seek": 16320,
      "start": 1411.4000012207032,
      "end": 1416.1599957275391,
      "text": " ganz so damit gemeint, ne? Nee, nicht wirklich. Hybrid ist wirklich, dass es quasi selbst anhand",
      "tokens": [
        51440,
        6312,
        370,
        9479,
        18111,
        686,
        11,
        408,
        30,
        22067,
        11,
        1979,
        9696,
        13,
        47088,
        1418,
        9696,
        11,
        2658,
        785,
        20954,
        13053,
        364,
        5543,
        51678
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3144800364971161,
      "compression_ratio": 1.691176414489746,
      "no_speech_prob": 0.02063543163239956
    },
    {
      "id": 35,
      "seek": 18948,
      "start": 1416.1599957275391,
      "end": 1424.5600048828126,
      "text": " des Hybrid-Modells, also grundsätzlich war die Idee eines Hybrid-Modells, dass es genau",
      "tokens": [
        50364,
        730,
        47088,
        12,
        44,
        378,
        13677,
        11,
        611,
        30886,
        82,
        33373,
        1516,
        978,
        32651,
        18599,
        47088,
        12,
        44,
        378,
        13677,
        11,
        2658,
        785,
        12535,
        50784
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2965545654296875,
      "compression_ratio": 1.5775861740112305,
      "no_speech_prob": 0.0038827816024422646
    },
    {
      "id": 36,
      "seek": 18948,
      "start": 1424.5600048828126,
      "end": 1429.1200024414063,
      "text": " zwischen klassisch und Reasoning quasi selbst entscheidet, wie es agieren soll auf deine",
      "tokens": [
        50784,
        19875,
        42917,
        5494,
        674,
        39693,
        278,
        20954,
        13053,
        28398,
        39741,
        11,
        3355,
        785,
        623,
        5695,
        7114,
        2501,
        28395,
        51012
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2965545654296875,
      "compression_ratio": 1.5775861740112305,
      "no_speech_prob": 0.0038827816024422646
    },
    {
      "id": 37,
      "seek": 18948,
      "start": 1429.1200024414063,
      "end": 1437.200004272461,
      "text": " Prompteingabe. Gleichzeitig ist es natürlich so, dass sie zwischenzeitlich an der Stelle definitiv",
      "tokens": [
        51012,
        15833,
        662,
        14667,
        4488,
        13,
        33858,
        32075,
        1418,
        785,
        8762,
        370,
        11,
        2658,
        2804,
        19875,
        13712,
        1739,
        364,
        1163,
        26629,
        28781,
        592,
        51416
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2965545654296875,
      "compression_ratio": 1.5775861740112305,
      "no_speech_prob": 0.0038827816024422646
    },
    {
      "id": 38,
      "seek": 18948,
      "start": 1437.200004272461,
      "end": 1443.200004272461,
      "text": " sind, dass sie auch solche Sachen wie, okay, ist die Suche jetzt an oder nicht, auch die",
      "tokens": [
        51416,
        3290,
        11,
        2658,
        2804,
        2168,
        29813,
        26074,
        3355,
        11,
        1392,
        11,
        1418,
        978,
        9653,
        68,
        4354,
        364,
        4513,
        1979,
        11,
        2168,
        978,
        51716
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2965545654296875,
      "compression_ratio": 1.5775861740112305,
      "no_speech_prob": 0.0038827816024422646
    },
    {
      "id": 39,
      "seek": 21652,
      "start": 1443.2399975585938,
      "end": 1447.0400006103516,
      "text": " Master-Einstellung des Account-Levels überschreiben, was ich natürlich schon wieder völlig uncool",
      "tokens": [
        50366,
        6140,
        12,
        36,
        13911,
        898,
        1063,
        730,
        24558,
        12,
        11020,
        779,
        82,
        45022,
        339,
        25946,
        11,
        390,
        1893,
        8762,
        4981,
        6216,
        35670,
        6219,
        1092,
        50556
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2901354730129242,
      "compression_ratio": 1.7052631378173828,
      "no_speech_prob": 0.07578714936971664
    },
    {
      "id": 40,
      "seek": 21652,
      "start": 1447.0400006103516,
      "end": 1454.7999951171876,
      "text": " finde. Aber das haben sie teilweise mit eingebaut. Also deswegen, ja, also das kann schon alles,",
      "tokens": [
        50556,
        17841,
        13,
        5992,
        1482,
        3084,
        2804,
        46748,
        2194,
        30061,
        65,
        1375,
        13,
        2743,
        26482,
        11,
        2784,
        11,
        611,
        1482,
        4028,
        4981,
        7874,
        11,
        50944
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2901354730129242,
      "compression_ratio": 1.7052631378173828,
      "no_speech_prob": 0.07578714936971664
    },
    {
      "id": 41,
      "seek": 21652,
      "start": 1454.7999951171876,
      "end": 1460.439994506836,
      "text": " das ist aber alles halt, dieser Fachbereich ist halt echt gruselig geworden, weil es sind halt",
      "tokens": [
        50944,
        1482,
        1418,
        4340,
        7874,
        12479,
        11,
        9053,
        38213,
        48422,
        1418,
        12479,
        13972,
        677,
        301,
        338,
        328,
        26281,
        11,
        7689,
        785,
        3290,
        12479,
        51226
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2901354730129242,
      "compression_ratio": 1.7052631378173828,
      "no_speech_prob": 0.07578714936971664
    },
    {
      "id": 42,
      "seek": 21652,
      "start": 1460.439994506836,
      "end": 1464.2399975585938,
      "text": " alles irgendwie ständig Begriffe, die vielleicht mal definiert waren, sind garantiert jetzt irgendwie",
      "tokens": [
        51226,
        7874,
        20759,
        342,
        38861,
        879,
        861,
        31387,
        11,
        978,
        12547,
        2806,
        1561,
        4859,
        11931,
        11,
        3290,
        22251,
        4859,
        4354,
        20759,
        51416
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2901354730129242,
      "compression_ratio": 1.7052631378173828,
      "no_speech_prob": 0.07578714936971664
    },
    {
      "id": 43,
      "seek": 21652,
      "start": 1464.2399975585938,
      "end": 1467.5199963378907,
      "text": " anders definiert und vielleicht gestern auch schon wieder umdefiniert worden. Deswegen so,",
      "tokens": [
        51416,
        17999,
        1561,
        4859,
        674,
        12547,
        7219,
        1248,
        2168,
        4981,
        6216,
        1105,
        1479,
        5194,
        4859,
        14054,
        13,
        24864,
        370,
        11,
        51580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2901354730129242,
      "compression_ratio": 1.7052631378173828,
      "no_speech_prob": 0.07578714936971664
    },
    {
      "id": 44,
      "seek": 24084,
      "start": 1467.5199963378907,
      "end": 1473.5600048828126,
      "text": " was meinen wir denn jetzt damit so genau? Das ist ein guter Punkt. Das Problem habe ich immer",
      "tokens": [
        50364,
        390,
        22738,
        1987,
        10471,
        4354,
        9479,
        370,
        12535,
        30,
        2846,
        1418,
        1343,
        5228,
        260,
        25487,
        13,
        2846,
        11676,
        6015,
        1893,
        5578,
        50666
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689732015132904,
      "compression_ratio": 1.5308642387390137,
      "no_speech_prob": 0.04462813585996628
    },
    {
      "id": 45,
      "seek": 24084,
      "start": 1473.5600048828126,
      "end": 1479.5600048828126,
      "text": " bei Agenten und dem Agentic und sonst was, weil ich irgendwie das Gefühl habe, wow,",
      "tokens": [
        50666,
        4643,
        27174,
        268,
        674,
        1371,
        27174,
        299,
        674,
        26309,
        390,
        11,
        7689,
        1893,
        20759,
        1482,
        29715,
        6015,
        11,
        6076,
        11,
        50966
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689732015132904,
      "compression_ratio": 1.5308642387390137,
      "no_speech_prob": 0.04462813585996628
    },
    {
      "id": 46,
      "seek": 24084,
      "start": 1479.5600048828126,
      "end": 1486.4799877929688,
      "text": " es gibt da so viele Definitionen, wie es Experten auf LinkedIn gibt. Und ich finde die Definition",
      "tokens": [
        50966,
        785,
        6089,
        1120,
        370,
        9693,
        46245,
        849,
        268,
        11,
        3355,
        785,
        12522,
        1147,
        2501,
        20657,
        6089,
        13,
        2719,
        1893,
        17841,
        978,
        46245,
        849,
        51312
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689732015132904,
      "compression_ratio": 1.5308642387390137,
      "no_speech_prob": 0.04462813585996628
    },
    {
      "id": 47,
      "seek": 24084,
      "start": 1486.4799877929688,
      "end": 1493.919990234375,
      "text": " von Agenten sehr schwierig, vor allem, weil sie, glaube ich, auch sehr alt ist. Also man hat ja",
      "tokens": [
        51312,
        2957,
        27174,
        268,
        5499,
        37845,
        11,
        4245,
        17585,
        11,
        7689,
        2804,
        11,
        13756,
        1893,
        11,
        2168,
        5499,
        4955,
        1418,
        13,
        2743,
        587,
        2385,
        2784,
        51684
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2689732015132904,
      "compression_ratio": 1.5308642387390137,
      "no_speech_prob": 0.04462813585996628
    },
    {
      "id": 48,
      "seek": 26724,
      "start": 1493.919990234375,
      "end": 1500.4000012207032,
      "text": " schon vor den LLMs von Agenten gesprochen und es jetzt aus meiner Sicht eine neue Definition",
      "tokens": [
        50364,
        4981,
        4245,
        1441,
        441,
        43,
        26386,
        2957,
        27174,
        268,
        42714,
        674,
        785,
        4354,
        3437,
        20529,
        36615,
        3018,
        16842,
        46245,
        849,
        50688
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31095317006111145,
      "compression_ratio": 1.6267123222351074,
      "no_speech_prob": 0.025537380948662758
    },
    {
      "id": 49,
      "seek": 26724,
      "start": 1500.4000012207032,
      "end": 1505.2399975585938,
      "text": " braucht, oder? Ja, komplett. Also der Punkt ist, dass der Begriff des Agenten in der Data Science",
      "tokens": [
        50688,
        22623,
        11,
        4513,
        30,
        3530,
        11,
        32261,
        13,
        2743,
        1163,
        25487,
        1418,
        11,
        2658,
        1163,
        879,
        32783,
        730,
        27174,
        268,
        294,
        1163,
        11888,
        8976,
        50930
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31095317006111145,
      "compression_ratio": 1.6267123222351074,
      "no_speech_prob": 0.025537380948662758
    },
    {
      "id": 50,
      "seek": 26724,
      "start": 1505.2399975585938,
      "end": 1509.0000073242188,
      "text": " eigentlich aus dem Reinforcement Learning erstmal kommt, weil darin lernt der Agent ja,",
      "tokens": [
        50930,
        10926,
        3437,
        1371,
        42116,
        9382,
        15205,
        38607,
        10047,
        11,
        7689,
        4072,
        259,
        32068,
        580,
        1163,
        27174,
        2784,
        11,
        51118
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31095317006111145,
      "compression_ratio": 1.6267123222351074,
      "no_speech_prob": 0.025537380948662758
    },
    {
      "id": 51,
      "seek": 26724,
      "start": 1509.0000073242188,
      "end": 1517.1200024414063,
      "text": " aus seinen Stage und Actions, da kommt es eigentlich mal her. Also als ich das allererste Mal auf",
      "tokens": [
        51118,
        3437,
        24427,
        25907,
        674,
        3251,
        626,
        11,
        1120,
        10047,
        785,
        10926,
        2806,
        720,
        13,
        2743,
        3907,
        1893,
        1482,
        8722,
        260,
        2941,
        5746,
        2501,
        51524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31095317006111145,
      "compression_ratio": 1.6267123222351074,
      "no_speech_prob": 0.025537380948662758
    },
    {
      "id": 52,
      "seek": 26724,
      "start": 1517.1200024414063,
      "end": 1521.2399975585938,
      "text": " LinkedIn Agent gelesen habe, dachte ich mir kurz, so was, seit wann macht denn der Kollege hier an",
      "tokens": [
        51524,
        20657,
        27174,
        4087,
        17403,
        6015,
        11,
        39775,
        1893,
        3149,
        20465,
        11,
        370,
        390,
        11,
        16452,
        38064,
        10857,
        10471,
        1163,
        28505,
        3296,
        364,
        51730
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31095317006111145,
      "compression_ratio": 1.6267123222351074,
      "no_speech_prob": 0.025537380948662758
    },
    {
      "id": 53,
      "seek": 29456,
      "start": 1521.2399975585938,
      "end": 1524.3599926757813,
      "text": " den Reinforcement Learning? Bis ich dann kapiert habe, ich glaube, ich weiß nicht,",
      "tokens": [
        50364,
        1441,
        42116,
        9382,
        15205,
        30,
        25271,
        1893,
        3594,
        13816,
        4859,
        6015,
        11,
        1893,
        13756,
        11,
        1893,
        13385,
        1979,
        11,
        50520
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27989473938941956,
      "compression_ratio": 1.6561403274536133,
      "no_speech_prob": 0.018795691430568695
    },
    {
      "id": 54,
      "seek": 29456,
      "start": 1524.3599926757813,
      "end": 1531.4000012207032,
      "text": " ich würde mir weiß aus dem Fenster lehnen, wenn er weiß, wie es zu buchstabieren ist. Aber Agent",
      "tokens": [
        50520,
        1893,
        11942,
        3149,
        321,
        72,
        127,
        253,
        3437,
        1371,
        30993,
        3120,
        476,
        71,
        2866,
        11,
        4797,
        1189,
        13385,
        11,
        3355,
        785,
        2164,
        272,
        625,
        372,
        455,
        5695,
        1418,
        13,
        5992,
        27174,
        50872
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27989473938941956,
      "compression_ratio": 1.6561403274536133,
      "no_speech_prob": 0.018795691430568695
    },
    {
      "id": 55,
      "seek": 29456,
      "start": 1531.4000012207032,
      "end": 1535.7999951171876,
      "text": " ist, ganz ehrlich, das ist zwischenzeitlich so ein Blubberbegriff. Ich frage schon immer,",
      "tokens": [
        50872,
        1418,
        11,
        6312,
        40872,
        11,
        1482,
        1418,
        19875,
        13712,
        1739,
        370,
        1343,
        2177,
        836,
        607,
        650,
        32783,
        13,
        3141,
        6600,
        432,
        4981,
        5578,
        11,
        51092
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27989473938941956,
      "compression_ratio": 1.6561403274536133,
      "no_speech_prob": 0.018795691430568695
    },
    {
      "id": 56,
      "seek": 29456,
      "start": 1535.7999951171876,
      "end": 1540.7999951171876,
      "text": " reden wir über Automatisierungen? Reden wir über regelbasierte Automatisierungen oder schmeißen",
      "tokens": [
        51092,
        26447,
        1987,
        4502,
        24619,
        267,
        271,
        811,
        5084,
        30,
        4477,
        268,
        1987,
        4502,
        40504,
        16342,
        23123,
        24619,
        267,
        271,
        811,
        5084,
        4513,
        46459,
        6230,
        268,
        51342
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27989473938941956,
      "compression_ratio": 1.6561403274536133,
      "no_speech_prob": 0.018795691430568695
    },
    {
      "id": 57,
      "seek": 29456,
      "start": 1540.7999951171876,
      "end": 1545.720008544922,
      "text": " wir wirklich ein LLM in den Mix rein? Und für mich ist inzwischen, ein echter Agent muss irgendwie",
      "tokens": [
        51342,
        1987,
        9696,
        1343,
        441,
        43,
        44,
        294,
        1441,
        12769,
        6561,
        30,
        2719,
        2959,
        6031,
        1418,
        294,
        47115,
        11,
        1343,
        308,
        26690,
        27174,
        6425,
        20759,
        51588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27989473938941956,
      "compression_ratio": 1.6561403274536133,
      "no_speech_prob": 0.018795691430568695
    },
    {
      "id": 58,
      "seek": 31904,
      "start": 1545.720008544922,
      "end": 1551.6399914550782,
      "text": " ein LLM haben. Ansonsten können wir bitte Oldschool bei automatisierten und regelbasierten",
      "tokens": [
        50364,
        1343,
        441,
        43,
        44,
        3084,
        13,
        1107,
        3015,
        6266,
        6310,
        1987,
        23231,
        8633,
        22779,
        4643,
        28034,
        271,
        29632,
        674,
        40504,
        16342,
        29632,
        50660
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25964275002479553,
      "compression_ratio": 1.5506073236465454,
      "no_speech_prob": 0.1498614102602005
    },
    {
      "id": 59,
      "seek": 31904,
      "start": 1551.6399914550782,
      "end": 1555.959998779297,
      "text": " abgreifen und so. Das ist mir auch echt wurscht. Auch ich habe irgendwann aufgegeben, dass ich",
      "tokens": [
        50660,
        410,
        33248,
        25076,
        674,
        370,
        13,
        2846,
        1418,
        3149,
        2168,
        13972,
        261,
        2156,
        4701,
        13,
        13382,
        1893,
        6015,
        34313,
        35031,
        16702,
        11,
        2658,
        1893,
        50876
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25964275002479553,
      "compression_ratio": 1.5506073236465454,
      "no_speech_prob": 0.1498614102602005
    },
    {
      "id": 60,
      "seek": 31904,
      "start": 1555.959998779297,
      "end": 1563.440009765625,
      "text": " mich damit irgendwie aufrege. Meistens klappt es. Das Aufregen ist ein guter Punkt, weil ich habe",
      "tokens": [
        50876,
        6031,
        9479,
        20759,
        2501,
        265,
        432,
        13,
        1923,
        468,
        694,
        33337,
        42562,
        785,
        13,
        2846,
        9462,
        265,
        1766,
        1418,
        1343,
        5228,
        260,
        25487,
        11,
        7689,
        1893,
        6015,
        51250
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25964275002479553,
      "compression_ratio": 1.5506073236465454,
      "no_speech_prob": 0.1498614102602005
    },
    {
      "id": 61,
      "seek": 31904,
      "start": 1563.440009765625,
      "end": 1570.0000073242188,
      "text": " oftmals das Gefühl, dass gerade Produkte, Produkte müssen Agenten zur Verfügung stellen und man",
      "tokens": [
        51250,
        11649,
        34978,
        1482,
        29715,
        11,
        2658,
        12117,
        11793,
        18844,
        11,
        11793,
        18844,
        9013,
        27174,
        268,
        7147,
        43026,
        24407,
        674,
        587,
        51578
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25964275002479553,
      "compression_ratio": 1.5506073236465454,
      "no_speech_prob": 0.1498614102602005
    },
    {
      "id": 62,
      "seek": 34332,
      "start": 1570.0000073242188,
      "end": 1576.3200146484376,
      "text": " versucht irgendwie das gleiche LLM über fünf Agenten fünfmal zu verkaufen, die dann nur einen",
      "tokens": [
        50364,
        36064,
        20759,
        1482,
        11699,
        68,
        441,
        43,
        44,
        4502,
        28723,
        27174,
        268,
        28723,
        5579,
        2164,
        22328,
        20748,
        11,
        978,
        3594,
        4343,
        4891,
        50680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2903483211994171,
      "compression_ratio": 1.565737009048462,
      "no_speech_prob": 0.08265215158462524
    },
    {
      "id": 63,
      "seek": 34332,
      "start": 1576.3200146484376,
      "end": 1582.2800061035157,
      "text": " anderen System prompt haben. Aber vielleicht bin ich da auch etwas zu pessimistisch unterwegs.",
      "tokens": [
        50680,
        11122,
        8910,
        12391,
        3084,
        13,
        5992,
        12547,
        5171,
        1893,
        1120,
        2168,
        9569,
        2164,
        37399,
        468,
        5494,
        36258,
        13,
        50978
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2903483211994171,
      "compression_ratio": 1.565737009048462,
      "no_speech_prob": 0.08265215158462524
    },
    {
      "id": 64,
      "seek": 34332,
      "start": 1582.2800061035157,
      "end": 1590.2800061035157,
      "text": " Gar nicht. Ich glaube halt einfach, dass für viele dieses, in meiner Welt ist es so ein bisschen so,",
      "tokens": [
        50978,
        7995,
        1979,
        13,
        3141,
        13756,
        12479,
        7281,
        11,
        2658,
        2959,
        9693,
        12113,
        11,
        294,
        20529,
        14761,
        1418,
        785,
        370,
        1343,
        10763,
        370,
        11,
        51378
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2903483211994171,
      "compression_ratio": 1.565737009048462,
      "no_speech_prob": 0.08265215158462524
    },
    {
      "id": 65,
      "seek": 34332,
      "start": 1590.2800061035157,
      "end": 1594.68,
      "text": " dass viele in dieses Feld mit reingekommen sind, auch mit einer relativ falschen Erwartungshaltung.",
      "tokens": [
        51378,
        2658,
        9693,
        294,
        12113,
        42677,
        2194,
        319,
        278,
        916,
        5132,
        3290,
        11,
        2168,
        2194,
        6850,
        21960,
        16720,
        2470,
        3300,
        29587,
        1063,
        2716,
        29631,
        13,
        51598
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2903483211994171,
      "compression_ratio": 1.565737009048462,
      "no_speech_prob": 0.08265215158462524
    },
    {
      "id": 66,
      "seek": 36800,
      "start": 1594.7999951171876,
      "end": 1600.0000073242188,
      "text": " Für viele war dieser GPT-Moment halt richtig Magic. Was das jetzt alles irgendwie so kann und",
      "tokens": [
        50370,
        14990,
        9693,
        1516,
        9053,
        26039,
        51,
        12,
        13255,
        317,
        12479,
        13129,
        16154,
        13,
        3027,
        1482,
        4354,
        7874,
        20759,
        370,
        4028,
        674,
        50630
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3405827581882477,
      "compression_ratio": 1.7223880290985107,
      "no_speech_prob": 0.04017694666981697
    },
    {
      "id": 67,
      "seek": 36800,
      "start": 1600.0000073242188,
      "end": 1604.3599926757813,
      "text": " nicht kann und sonst irgendwas. Und jetzt ist es so ein bisschen so, es flacht so ein bisschen ab,",
      "tokens": [
        50630,
        1979,
        4028,
        674,
        26309,
        47090,
        13,
        2719,
        4354,
        1418,
        785,
        370,
        1343,
        10763,
        370,
        11,
        785,
        932,
        3589,
        370,
        1343,
        10763,
        410,
        11,
        50848
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3405827581882477,
      "compression_ratio": 1.7223880290985107,
      "no_speech_prob": 0.04017694666981697
    },
    {
      "id": 68,
      "seek": 36800,
      "start": 1604.3599926757813,
      "end": 1609.440009765625,
      "text": " aber fängt der Spaß ja jetzt eigentlich erst an. Und das ist, glaube ich, für viele nicht richtig",
      "tokens": [
        50848,
        4340,
        283,
        29670,
        1163,
        27460,
        2784,
        4354,
        10926,
        11301,
        364,
        13,
        2719,
        1482,
        1418,
        11,
        13756,
        1893,
        11,
        2959,
        9693,
        1979,
        13129,
        51102
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3405827581882477,
      "compression_ratio": 1.7223880290985107,
      "no_speech_prob": 0.04017694666981697
    },
    {
      "id": 69,
      "seek": 36800,
      "start": 1609.440009765625,
      "end": 1614.0399853515626,
      "text": " nachvollziehbar. Wir sind jetzt mit den LLMs, die wir haben, auch was GPT-5 im Prinzip jetzt",
      "tokens": [
        51102,
        5168,
        20654,
        28213,
        5356,
        13,
        4347,
        3290,
        4354,
        2194,
        1441,
        441,
        43,
        26386,
        11,
        978,
        1987,
        3084,
        11,
        2168,
        390,
        26039,
        51,
        12,
        20,
        566,
        47572,
        4354,
        51332
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3405827581882477,
      "compression_ratio": 1.7223880290985107,
      "no_speech_prob": 0.04017694666981697
    },
    {
      "id": 70,
      "seek": 36800,
      "start": 1614.0399853515626,
      "end": 1618.919990234375,
      "text": " gezeigt hat, wir sind auf einem Mature-Level. Weil eigentlich willst du als Profi ja genau",
      "tokens": [
        51332,
        48661,
        2385,
        11,
        1987,
        3290,
        2501,
        6827,
        376,
        1503,
        12,
        11020,
        779,
        13,
        18665,
        10926,
        48355,
        1581,
        3907,
        6039,
        72,
        2784,
        12535,
        51576
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3405827581882477,
      "compression_ratio": 1.7223880290985107,
      "no_speech_prob": 0.04017694666981697
    },
    {
      "id": 71,
      "seek": 36800,
      "start": 1618.919990234375,
      "end": 1624.199989013672,
      "text": " sehen, dass der pure Scale nicht mehr weitergeht und wir jetzt einfach wirklich so einen Stand der",
      "tokens": [
        51576,
        11333,
        11,
        2658,
        1163,
        6075,
        42999,
        1979,
        5417,
        8988,
        46227,
        674,
        1987,
        4354,
        7281,
        9696,
        370,
        4891,
        9133,
        1163,
        51840
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3405827581882477,
      "compression_ratio": 1.7223880290985107,
      "no_speech_prob": 0.04017694666981697
    },
    {
      "id": 72,
      "seek": 39752,
      "start": 1624.199989013672,
      "end": 1629.0799938964844,
      "text": " Technik haben, mit dem wir wissen, okay, jetzt sind wir da erst mal safe. Lass uns zurücklehnen",
      "tokens": [
        50364,
        8337,
        1035,
        3084,
        11,
        2194,
        1371,
        1987,
        16331,
        11,
        1392,
        11,
        4354,
        3290,
        1987,
        1120,
        11301,
        2806,
        3273,
        13,
        441,
        640,
        2693,
        15089,
        14557,
        2866,
        50608
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3441717028617859,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.0020498759113252163
    },
    {
      "id": 73,
      "seek": 39752,
      "start": 1629.0799938964844,
      "end": 1634.6399914550782,
      "text": " und lass uns den Clutter-Adage jetzt mal wirklich full force da implementieren, wo es Sinn macht.",
      "tokens": [
        50608,
        674,
        45829,
        2693,
        1441,
        2033,
        9947,
        12,
        15830,
        559,
        68,
        4354,
        2806,
        9696,
        1577,
        3464,
        1120,
        4445,
        5695,
        11,
        6020,
        785,
        37962,
        10857,
        13,
        50886
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3441717028617859,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.0020498759113252163
    },
    {
      "id": 74,
      "seek": 39752,
      "start": 1634.6399914550782,
      "end": 1642.440009765625,
      "text": " Und das nimmt halt viel dieses Magische und dieses, ach cool, irgendwie so ein bisschen raus.",
      "tokens": [
        50886,
        2719,
        1482,
        38891,
        12479,
        5891,
        12113,
        6395,
        7864,
        674,
        12113,
        11,
        2800,
        1627,
        11,
        20759,
        370,
        1343,
        10763,
        17202,
        13,
        51276
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3441717028617859,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.0020498759113252163
    },
    {
      "id": 75,
      "seek": 39752,
      "start": 1642.440009765625,
      "end": 1648.7599865722657,
      "text": " Und jetzt, ich hoffe, Sie gehen wieder zu Krypto oder so. Und jetzt wird es halt wieder so pures",
      "tokens": [
        51276,
        2719,
        4354,
        11,
        1893,
        34903,
        11,
        3559,
        13230,
        6216,
        2164,
        37747,
        662,
        78,
        4513,
        370,
        13,
        2719,
        4354,
        4578,
        785,
        12479,
        6216,
        370,
        6075,
        82,
        51592
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3441717028617859,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.0020498759113252163
    },
    {
      "id": 76,
      "seek": 42208,
      "start": 1648.7599865722657,
      "end": 1654.3599926757813,
      "text": " Handwerk. Und das war es halt schon immer. Und dieses Handwerk ist aber halt sehr anders als",
      "tokens": [
        50364,
        8854,
        26833,
        13,
        2719,
        1482,
        1516,
        785,
        12479,
        4981,
        5578,
        13,
        2719,
        12113,
        8854,
        26833,
        1418,
        4340,
        12479,
        5499,
        17999,
        3907,
        50644
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22398161888122559,
      "compression_ratio": 1.6042553186416626,
      "no_speech_prob": 0.16423460841178894
    },
    {
      "id": 77,
      "seek": 42208,
      "start": 1654.3599926757813,
      "end": 1658.8400036621094,
      "text": " in klassisch deterministischen oder auch in klassischen Code-Welten. Das war echt schon",
      "tokens": [
        50644,
        294,
        42917,
        5494,
        15957,
        468,
        6282,
        4513,
        2168,
        294,
        42917,
        6282,
        15549,
        12,
        54,
        338,
        1147,
        13,
        2846,
        1516,
        13972,
        4981,
        50868
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22398161888122559,
      "compression_ratio": 1.6042553186416626,
      "no_speech_prob": 0.16423460841178894
    },
    {
      "id": 78,
      "seek": 42208,
      "start": 1658.8400036621094,
      "end": 1667.6399914550782,
      "text": " immer so. Handwerk, da sagst du was. Auch mit den Hybrid-Modellen. Ich muss zugeben, ich benutze",
      "tokens": [
        50868,
        5578,
        370,
        13,
        8854,
        26833,
        11,
        1120,
        15274,
        372,
        1581,
        390,
        13,
        13382,
        2194,
        1441,
        47088,
        12,
        44,
        378,
        8581,
        13,
        3141,
        6425,
        2164,
        16702,
        11,
        1893,
        38424,
        1381,
        51308
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22398161888122559,
      "compression_ratio": 1.6042553186416626,
      "no_speech_prob": 0.16423460841178894
    },
    {
      "id": 79,
      "seek": 42208,
      "start": 1667.6399914550782,
      "end": 1672.0799938964844,
      "text": " immer das größte Modell, was ich irgendwie kriegen kann, weil ich die Hoffnung habe, dass es tut,",
      "tokens": [
        51308,
        5578,
        1482,
        20691,
        975,
        6583,
        898,
        11,
        390,
        1893,
        20759,
        46882,
        4028,
        11,
        7689,
        1893,
        978,
        29135,
        15539,
        6015,
        11,
        2658,
        785,
        3672,
        11,
        51530
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22398161888122559,
      "compression_ratio": 1.6042553186416626,
      "no_speech_prob": 0.16423460841178894
    },
    {
      "id": 80,
      "seek": 44540,
      "start": 1672.199989013672,
      "end": 1680.8400036621094,
      "text": " weil meine Zeit ja auch kostet. Aber wenn ich jetzt tatsächlich etwas in der Anwendung einbaue,",
      "tokens": [
        50370,
        7689,
        10946,
        9394,
        2784,
        2168,
        27183,
        302,
        13,
        5992,
        4797,
        1893,
        4354,
        20796,
        9569,
        294,
        1163,
        1107,
        20128,
        1063,
        1343,
        4231,
        622,
        11,
        50802
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3213420510292053,
      "compression_ratio": 1.5425101518630981,
      "no_speech_prob": 0.022960085421800613
    },
    {
      "id": 81,
      "seek": 44540,
      "start": 1680.8400036621094,
      "end": 1686.8800122070313,
      "text": " dann sollte ich schon aus Kosten- und Speed-Gründen darauf achten, dass ich vielleicht ein kleineres",
      "tokens": [
        50802,
        3594,
        18042,
        1893,
        4981,
        3437,
        47391,
        12,
        674,
        18774,
        12,
        20038,
        27687,
        18654,
        2800,
        1147,
        11,
        2658,
        1893,
        12547,
        1343,
        39496,
        279,
        51104
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3213420510292053,
      "compression_ratio": 1.5425101518630981,
      "no_speech_prob": 0.022960085421800613
    },
    {
      "id": 82,
      "seek": 44540,
      "start": 1686.8800122070313,
      "end": 1691.440009765625,
      "text": " Modell nehmen kann, richtig? Definitiv. Also es gibt schon, und das ist jetzt auch nicht so,",
      "tokens": [
        51104,
        6583,
        898,
        19905,
        4028,
        11,
        13129,
        30,
        46245,
        270,
        592,
        13,
        2743,
        785,
        6089,
        4981,
        11,
        674,
        1482,
        1418,
        4354,
        2168,
        1979,
        370,
        11,
        51332
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3213420510292053,
      "compression_ratio": 1.5425101518630981,
      "no_speech_prob": 0.022960085421800613
    },
    {
      "id": 83,
      "seek": 44540,
      "start": 1691.440009765625,
      "end": 1697.0799938964844,
      "text": " dass es, also ich finde mal diese Idee, dass man sagt, es gibt nicht irgendwie eine Idee,",
      "tokens": [
        51332,
        2658,
        785,
        11,
        611,
        1893,
        17841,
        2806,
        6705,
        32651,
        11,
        2658,
        587,
        15764,
        11,
        785,
        6089,
        1979,
        20759,
        3018,
        32651,
        11,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3213420510292053,
      "compression_ratio": 1.5425101518630981,
      "no_speech_prob": 0.022960085421800613
    },
    {
      "id": 84,
      "seek": 47040,
      "start": 1697.0799938964844,
      "end": 1703.0000073242188,
      "text": " zu welcher Task passt zu was. Das ist ja auch immer quasi etwas, was wir immer wieder für unsere",
      "tokens": [
        50364,
        2164,
        2214,
        6759,
        30428,
        37154,
        2164,
        390,
        13,
        2846,
        1418,
        2784,
        2168,
        5578,
        20954,
        9569,
        11,
        390,
        1987,
        5578,
        6216,
        2959,
        14339,
        50660
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23132048547267914,
      "compression_ratio": 1.576923131942749,
      "no_speech_prob": 0.036157310009002686
    },
    {
      "id": 85,
      "seek": 47040,
      "start": 1703.0000073242188,
      "end": 1708.5600048828126,
      "text": " Kunden auch konstant aufbereiten und Vorschläge machen. Ich sage mal, ich mache das monatlich,",
      "tokens": [
        50660,
        38192,
        2168,
        34208,
        394,
        2501,
        18582,
        6009,
        674,
        31438,
        11439,
        737,
        432,
        7069,
        13,
        3141,
        19721,
        2806,
        11,
        1893,
        28289,
        1482,
        1108,
        267,
        1739,
        11,
        50938
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23132048547267914,
      "compression_ratio": 1.576923131942749,
      "no_speech_prob": 0.036157310009002686
    },
    {
      "id": 86,
      "seek": 47040,
      "start": 1708.5600048828126,
      "end": 1713.3200146484376,
      "text": " nicht täglich und wöchentlich. Aber du kannst sehr wohl sowas überlegen, zum Beispiel je nachdem,",
      "tokens": [
        50938,
        1979,
        14619,
        8856,
        674,
        261,
        973,
        339,
        7698,
        13,
        5992,
        1581,
        20853,
        5499,
        24531,
        19766,
        296,
        4502,
        22936,
        11,
        5919,
        13772,
        1506,
        5168,
        10730,
        11,
        51176
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23132048547267914,
      "compression_ratio": 1.576923131942749,
      "no_speech_prob": 0.036157310009002686
    },
    {
      "id": 87,
      "seek": 47040,
      "start": 1713.3200146484376,
      "end": 1720.4799877929688,
      "text": " wenn du willst, dass du zum Beispiel ein komplexes Problem lösen willst, also Multi-Step-Logic oder",
      "tokens": [
        51176,
        4797,
        1581,
        48355,
        11,
        2658,
        1581,
        5919,
        13772,
        1343,
        5207,
        18945,
        279,
        11676,
        25209,
        6748,
        48355,
        11,
        611,
        29238,
        12,
        23624,
        12,
        43,
        664,
        299,
        4513,
        51534
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23132048547267914,
      "compression_ratio": 1.576923131942749,
      "no_speech_prob": 0.036157310009002686
    },
    {
      "id": 88,
      "seek": 47040,
      "start": 1720.4799877929688,
      "end": 1725.5600048828126,
      "text": " irgendwie was wirklich komplexes, dann ist klar, du brauchst die ganz, ganz großen Modelle. Das",
      "tokens": [
        51534,
        20759,
        390,
        9696,
        5207,
        18945,
        279,
        11,
        3594,
        1418,
        14743,
        11,
        1581,
        45522,
        372,
        978,
        6312,
        11,
        6312,
        23076,
        6583,
        4434,
        13,
        2846,
        51788
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23132048547267914,
      "compression_ratio": 1.576923131942749,
      "no_speech_prob": 0.036157310009002686
    },
    {
      "id": 89,
      "seek": 49888,
      "start": 1725.5600048828126,
      "end": 1731.7999951171876,
      "text": " ist ein GPT 5 Pro, das ist ein Gemini 2.5 Pro in dieser Diamond-Version oder ein Opus 4.1. Alle",
      "tokens": [
        50364,
        1418,
        1343,
        26039,
        51,
        1025,
        1705,
        11,
        1482,
        1418,
        1343,
        22894,
        3812,
        568,
        13,
        20,
        1705,
        294,
        9053,
        26593,
        12,
        53,
        433,
        313,
        4513,
        1343,
        12011,
        301,
        1017,
        13,
        16,
        13,
        25318,
        50676
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26142242550849915,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.008838699199259281
    },
    {
      "id": 90,
      "seek": 49888,
      "start": 1731.7999951171876,
      "end": 1737.3599926757813,
      "text": " anderen Modelle kommen da nicht ansatzweise ran und du hast einfach mit so viel Kruscht und so",
      "tokens": [
        50676,
        11122,
        6583,
        4434,
        11729,
        1120,
        1979,
        1567,
        10300,
        13109,
        5872,
        674,
        1581,
        6581,
        7281,
        2194,
        370,
        5891,
        6332,
        301,
        4701,
        674,
        370,
        50954
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26142242550849915,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.008838699199259281
    },
    {
      "id": 91,
      "seek": 49888,
      "start": 1737.3599926757813,
      "end": 1742.960029296875,
      "text": " viel Fehlern zu tun. Auf der anderen Seite, wenn du Themen hast, wo du sagst, hey, du möchtest",
      "tokens": [
        50954,
        5891,
        3697,
        22950,
        1248,
        2164,
        4267,
        13,
        9462,
        1163,
        11122,
        19748,
        11,
        4797,
        1581,
        39229,
        6581,
        11,
        6020,
        1581,
        15274,
        372,
        11,
        4177,
        11,
        1581,
        7667,
        4701,
        377,
        51234
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26142242550849915,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.008838699199259281
    },
    {
      "id": 92,
      "seek": 49888,
      "start": 1742.960029296875,
      "end": 1747.68,
      "text": " Software-Development machen, immerhin sind wir ja heute hier, dann ist, je komplexer die Anwendung",
      "tokens": [
        51234,
        27428,
        12,
        11089,
        1388,
        518,
        7069,
        11,
        5578,
        10876,
        3290,
        1987,
        2784,
        9801,
        3296,
        11,
        3594,
        1418,
        11,
        1506,
        5207,
        18945,
        260,
        978,
        1107,
        20128,
        1063,
        51470
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26142242550849915,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.008838699199259281
    },
    {
      "id": 93,
      "seek": 49888,
      "start": 1747.68,
      "end": 1752.5200268554688,
      "text": " ist, dann bist du sehr nah an diesen Research-Cases dran. Also wieder ein Cloth, auf jeden Fall im",
      "tokens": [
        51470,
        1418,
        11,
        3594,
        18209,
        1581,
        5499,
        17170,
        364,
        12862,
        10303,
        12,
        34,
        1957,
        32801,
        13,
        2743,
        6216,
        1343,
        2033,
        900,
        11,
        2501,
        12906,
        7465,
        566,
        51712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26142242550849915,
      "compression_ratio": 1.5714285373687744,
      "no_speech_prob": 0.008838699199259281
    },
    {
      "id": 94,
      "seek": 52584,
      "start": 1752.5200268554688,
      "end": 1759.960029296875,
      "text": " Opus und ein GPT 5 wieder im Pro-Modus, sonst irgendwas, aber definitiv zurück zu Cursor läuft",
      "tokens": [
        50364,
        12011,
        301,
        674,
        1343,
        26039,
        51,
        1025,
        6216,
        566,
        1705,
        12,
        44,
        32419,
        11,
        26309,
        47090,
        11,
        4340,
        28781,
        592,
        15089,
        2164,
        383,
        2156,
        284,
        31807,
        50736
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28812098503112793,
      "compression_ratio": 1.4343066215515137,
      "no_speech_prob": 0.36712783575057983
    },
    {
      "id": 95,
      "seek": 52584,
      "start": 1759.960029296875,
      "end": 1764.8399731445313,
      "text": " quasi schon als Default-Mode ständig momentan. Auf der anderen Seite kann zum Beispiel, du hast",
      "tokens": [
        50736,
        20954,
        4981,
        3907,
        9548,
        5107,
        12,
        44,
        1429,
        342,
        38861,
        1623,
        282,
        13,
        9462,
        1163,
        11122,
        19748,
        4028,
        5919,
        13772,
        11,
        1581,
        6581,
        50980
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28812098503112793,
      "compression_ratio": 1.4343066215515137,
      "no_speech_prob": 0.36712783575057983
    },
    {
      "id": 96,
      "seek": 52584,
      "start": 1764.8399731445313,
      "end": 1770.4799877929688,
      "text": " ein Quick-and-Dirty-Problem, dann kann ein GPT in der Mini-Version von mir so ein 5 oder ein 4er",
      "tokens": [
        50980,
        1343,
        12101,
        12,
        474,
        12,
        35,
        9340,
        12,
        12681,
        1113,
        11,
        3594,
        4028,
        1343,
        26039,
        51,
        294,
        1163,
        18239,
        12,
        53,
        433,
        313,
        2957,
        3149,
        370,
        1343,
        1025,
        4513,
        1343,
        1017,
        260,
        51262
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28812098503112793,
      "compression_ratio": 1.4343066215515137,
      "no_speech_prob": 0.36712783575057983
    },
    {
      "id": 97,
      "seek": 52584,
      "start": 1770.4799877929688,
      "end": 1777.440009765625,
      "text": " total simpel sein, weil für diese Mini-Tasks kannst du dann noch über die ARPI-Token optimiert quasi",
      "tokens": [
        51262,
        3217,
        1034,
        20071,
        6195,
        11,
        7689,
        2959,
        6705,
        18239,
        12,
        51,
        296,
        1694,
        20853,
        1581,
        3594,
        3514,
        4502,
        978,
        8943,
        31701,
        12,
        51,
        8406,
        5028,
        4859,
        20954,
        51610
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28812098503112793,
      "compression_ratio": 1.4343066215515137,
      "no_speech_prob": 0.36712783575057983
    },
    {
      "id": 98,
      "seek": 55076,
      "start": 1777.440009765625,
      "end": 1781.7600170898438,
      "text": " dein Shit auch mal batchen oder sonst irgendwas, wenn du halt irgendwie so Code-Snappes oder sonst",
      "tokens": [
        50364,
        25641,
        19593,
        2168,
        2806,
        15245,
        268,
        4513,
        26309,
        47090,
        11,
        4797,
        1581,
        12479,
        20759,
        370,
        15549,
        12,
        50,
        629,
        427,
        279,
        4513,
        26309,
        50580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33475419878959656,
      "compression_ratio": 1.4941633939743042,
      "no_speech_prob": 0.06359456479549408
    },
    {
      "id": 99,
      "seek": 55076,
      "start": 1781.7600170898438,
      "end": 1786.4799877929688,
      "text": " was brauchst. Während du halt sagst, hey, und jetzt müssen wir die Dokumentation noch in",
      "tokens": [
        50580,
        390,
        45522,
        372,
        13,
        40084,
        4542,
        1581,
        12479,
        15274,
        372,
        11,
        4177,
        11,
        674,
        4354,
        9013,
        1987,
        978,
        29768,
        2206,
        399,
        3514,
        294,
        50816
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33475419878959656,
      "compression_ratio": 1.4941633939743042,
      "no_speech_prob": 0.06359456479549408
    },
    {
      "id": 100,
      "seek": 55076,
      "start": 1786.4799877929688,
      "end": 1796.0800244140626,
      "text": " Vorstandskonform überarbeiten. Okay, Creative Writing Tool mit Storytelling-Funktion. Da ist",
      "tokens": [
        50816,
        12231,
        1115,
        5161,
        266,
        837,
        4502,
        43918,
        13,
        1033,
        11,
        26598,
        32774,
        15934,
        2194,
        14484,
        83,
        11073,
        12,
        46947,
        9780,
        13,
        3933,
        1418,
        51296
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33475419878959656,
      "compression_ratio": 1.4941633939743042,
      "no_speech_prob": 0.06359456479549408
    },
    {
      "id": 101,
      "seek": 55076,
      "start": 1796.0800244140626,
      "end": 1803.8800122070313,
      "text": " ja die Clothin definitiv vielleicht auch im 3.7er so näh die Wahl der Waffe, weil Clothin da so ein",
      "tokens": [
        51296,
        2784,
        978,
        2033,
        900,
        259,
        28781,
        592,
        12547,
        2168,
        566,
        805,
        13,
        22,
        260,
        370,
        297,
        127,
        97,
        71,
        978,
        27437,
        1163,
        343,
        23629,
        11,
        7689,
        2033,
        900,
        259,
        1120,
        370,
        1343,
        51686
      ],
      "temperature": 0.0,
      "avg_logprob": -0.33475419878959656,
      "compression_ratio": 1.4941633939743042,
      "no_speech_prob": 0.06359456479549408
    },
    {
      "id": 102,
      "seek": 57720,
      "start": 1803.8800122070313,
      "end": 1808.3200146484376,
      "text": " bisschen kreativer noch den Perspektivwechsel hinbekommt. Und das ist halt für viele nicht",
      "tokens": [
        50364,
        10763,
        350,
        620,
        1837,
        3514,
        1441,
        14006,
        23533,
        592,
        26934,
        14102,
        25714,
        22230,
        13,
        2719,
        1482,
        1418,
        12479,
        2959,
        9693,
        1979,
        50586
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2834967374801636,
      "compression_ratio": 1.6155989170074463,
      "no_speech_prob": 0.21908925473690033
    },
    {
      "id": 103,
      "seek": 57720,
      "start": 1808.3200146484376,
      "end": 1812.0000073242188,
      "text": " nachvollziehbar, dass das eigentlich ein Standard-Ding der Data Science ist. Und ich glaube,",
      "tokens": [
        50586,
        5168,
        20654,
        28213,
        5356,
        11,
        2658,
        1482,
        10926,
        1343,
        21298,
        12,
        35,
        278,
        1163,
        11888,
        8976,
        1418,
        13,
        2719,
        1893,
        13756,
        11,
        50770
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2834967374801636,
      "compression_ratio": 1.6155989170074463,
      "no_speech_prob": 0.21908925473690033
    },
    {
      "id": 104,
      "seek": 57720,
      "start": 1812.0000073242188,
      "end": 1815.8800122070313,
      "text": " wer irgendwie auch nur einen einzigen Podcast von mir gehört hat, kommt immer das CRISPR drin vor",
      "tokens": [
        50770,
        2612,
        20759,
        2168,
        4343,
        4891,
        21586,
        3213,
        29972,
        2957,
        3149,
        21544,
        2385,
        11,
        10047,
        5578,
        1482,
        49256,
        15958,
        24534,
        4245,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2834967374801636,
      "compression_ratio": 1.6155989170074463,
      "no_speech_prob": 0.21908925473690033
    },
    {
      "id": 105,
      "seek": 57720,
      "start": 1815.8800122070313,
      "end": 1821.6400219726563,
      "text": " und es kommt immer der Problem-Data-Model-Match vor. Sorry, dass ich immer die gleiche Sorte erzähle,",
      "tokens": [
        50964,
        674,
        785,
        10047,
        5578,
        1163,
        11676,
        12,
        35,
        3274,
        12,
        44,
        41147,
        12,
        44,
        852,
        4245,
        13,
        4919,
        11,
        2658,
        1893,
        5578,
        978,
        11699,
        68,
        318,
        12752,
        28337,
        306,
        11,
        51252
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2834967374801636,
      "compression_ratio": 1.6155989170074463,
      "no_speech_prob": 0.21908925473690033
    },
    {
      "id": 106,
      "seek": 57720,
      "start": 1821.6400219726563,
      "end": 1827.20001953125,
      "text": " aber das ist halt genau unser Problem. Das Problem, die Daten und die Modelllösung müssen",
      "tokens": [
        51252,
        4340,
        1482,
        1418,
        12479,
        12535,
        12977,
        11676,
        13,
        2846,
        11676,
        11,
        978,
        31126,
        674,
        978,
        6583,
        898,
        75,
        11310,
        1063,
        9013,
        51530
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2834967374801636,
      "compression_ratio": 1.6155989170074463,
      "no_speech_prob": 0.21908925473690033
    },
    {
      "id": 107,
      "seek": 57720,
      "start": 1827.20001953125,
      "end": 1832.6400219726563,
      "text": " zusammenpassen. Greift das Zeug nicht ineinander, dann hast du eine Lösung, aber die wird im Zweifel",
      "tokens": [
        51530,
        14311,
        44270,
        13,
        14986,
        2008,
        1482,
        4853,
        697,
        1979,
        7167,
        20553,
        11,
        3594,
        6581,
        1581,
        3018,
        46934,
        11,
        4340,
        978,
        4578,
        566,
        32475,
        351,
        338,
        51802
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2834967374801636,
      "compression_ratio": 1.6155989170074463,
      "no_speech_prob": 0.21908925473690033
    },
    {
      "id": 108,
      "seek": 60596,
      "start": 1832.7999951171876,
      "end": 1841.15998046875,
      "text": " ein ziemlicher Crap sein. Das ist jetzt spannend. Daten und Modell und in die Zukunft geblickt,",
      "tokens": [
        50372,
        1343,
        25986,
        25215,
        383,
        4007,
        6195,
        13,
        2846,
        1418,
        4354,
        49027,
        13,
        31126,
        674,
        6583,
        898,
        674,
        294,
        978,
        22782,
        1519,
        38263,
        83,
        11,
        50790
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29544004797935486,
      "compression_ratio": 1.6051502227783203,
      "no_speech_prob": 0.03957447409629822
    },
    {
      "id": 109,
      "seek": 60596,
      "start": 1841.15998046875,
      "end": 1848.15998046875,
      "text": " Softwareentwicklung. Wir entwickeln neue Software, wir entwickeln Open Source Software weiter,",
      "tokens": [
        50790,
        27428,
        317,
        16038,
        17850,
        13,
        4347,
        28449,
        32099,
        16842,
        27428,
        11,
        1987,
        28449,
        32099,
        7238,
        29629,
        27428,
        8988,
        11,
        51140
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29544004797935486,
      "compression_ratio": 1.6051502227783203,
      "no_speech_prob": 0.03957447409629822
    },
    {
      "id": 110,
      "seek": 60596,
      "start": 1848.15998046875,
      "end": 1855.5999829101563,
      "text": " aber das Modell hat einen Knowledge-Cut-Off und kennt gar nicht die neuen Versionen der",
      "tokens": [
        51140,
        4340,
        1482,
        6583,
        898,
        2385,
        4891,
        32906,
        12,
        34,
        325,
        12,
        29745,
        674,
        37682,
        3691,
        1979,
        978,
        21387,
        12226,
        17068,
        1163,
        51512
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29544004797935486,
      "compression_ratio": 1.6051502227783203,
      "no_speech_prob": 0.03957447409629822
    },
    {
      "id": 111,
      "seek": 60596,
      "start": 1855.5999829101563,
      "end": 1862.399970703125,
      "text": " Software. Ich empfehle schon immer den Leuten, fangt mit dem Modell an, das kennt seine Version",
      "tokens": [
        51512,
        27428,
        13,
        3141,
        4012,
        33865,
        306,
        4981,
        5578,
        1441,
        42301,
        11,
        283,
        656,
        83,
        2194,
        1371,
        6583,
        898,
        364,
        11,
        1482,
        37682,
        15925,
        35965,
        51852
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29544004797935486,
      "compression_ratio": 1.6051502227783203,
      "no_speech_prob": 0.03957447409629822
    },
    {
      "id": 112,
      "seek": 63572,
      "start": 1862.5200268554688,
      "end": 1866.8399731445313,
      "text": " und dann kriegt ihr schon mal ganz gute Software. Wenn ihr Cutting Edge nehmt,",
      "tokens": [
        50370,
        674,
        3594,
        25766,
        10463,
        5553,
        4981,
        2806,
        6312,
        21476,
        27428,
        13,
        7899,
        5553,
        9431,
        783,
        19328,
        408,
        8587,
        83,
        11,
        50586
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25774988532066345,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.0011158729903399944
    },
    {
      "id": 113,
      "seek": 63572,
      "start": 1866.8399731445313,
      "end": 1873.15998046875,
      "text": " dann kennt das Modell die Version nicht und macht euch vieles kaputt. Dann müsst ihr wieder da",
      "tokens": [
        50586,
        3594,
        37682,
        1482,
        6583,
        898,
        978,
        35965,
        1979,
        674,
        10857,
        10403,
        5891,
        279,
        13816,
        13478,
        13,
        7455,
        49481,
        5553,
        6216,
        1120,
        50902
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25774988532066345,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.0011158729903399944
    },
    {
      "id": 114,
      "seek": 63572,
      "start": 1873.15998046875,
      "end": 1878.15998046875,
      "text": " irgendwie über den Kontext die Informationen reinbringen. Aber was ich faszinierend finde,",
      "tokens": [
        50902,
        20759,
        4502,
        1441,
        20629,
        3828,
        978,
        46753,
        6561,
        39455,
        13,
        5992,
        390,
        1893,
        283,
        19601,
        259,
        811,
        521,
        17841,
        11,
        51152
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25774988532066345,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.0011158729903399944
    },
    {
      "id": 115,
      "seek": 63572,
      "start": 1878.15998046875,
      "end": 1885.15998046875,
      "text": " ist halt die Zukunft, in die wir reinlaufen. Weil wenn jetzt das Modell und immer das gleiche Modell,",
      "tokens": [
        51152,
        1418,
        12479,
        978,
        22782,
        11,
        294,
        978,
        1987,
        6561,
        875,
        19890,
        13,
        18665,
        4797,
        4354,
        1482,
        6583,
        898,
        674,
        5578,
        1482,
        11699,
        68,
        6583,
        898,
        11,
        51502
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25774988532066345,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.0011158729903399944
    },
    {
      "id": 116,
      "seek": 63572,
      "start": 1885.15998046875,
      "end": 1891.5200268554688,
      "text": " ich meine Open AI ist erfolgreich und Entropic ist erfolgreich und alle auf der Welt benutzen",
      "tokens": [
        51502,
        1893,
        10946,
        7238,
        7318,
        1418,
        48270,
        674,
        3951,
        39173,
        1418,
        48270,
        674,
        5430,
        2501,
        1163,
        14761,
        38424,
        2904,
        51820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25774988532066345,
      "compression_ratio": 1.6041666269302368,
      "no_speech_prob": 0.0011158729903399944
    },
    {
      "id": 117,
      "seek": 66484,
      "start": 1891.5600048828126,
      "end": 1897.7600170898438,
      "text": " diese Modelle und diese Modelle beraten alle die gleiche Software und die gleichen Versionen. Und",
      "tokens": [
        50366,
        6705,
        6583,
        4434,
        674,
        6705,
        6583,
        4434,
        5948,
        7186,
        5430,
        978,
        11699,
        68,
        27428,
        674,
        978,
        49069,
        12226,
        17068,
        13,
        2719,
        50676
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2769886255264282,
      "compression_ratio": 1.5119999647140503,
      "no_speech_prob": 0.009857513010501862
    },
    {
      "id": 118,
      "seek": 66484,
      "start": 1897.7600170898438,
      "end": 1906.960029296875,
      "text": " was für eine Chance haben dann überhaupt noch neuere Ideen, die eben da nicht drin sind? Gibt",
      "tokens": [
        50676,
        390,
        2959,
        3018,
        16428,
        3084,
        3594,
        20023,
        3514,
        22510,
        323,
        13090,
        268,
        11,
        978,
        11375,
        1120,
        1979,
        24534,
        3290,
        30,
        460,
        13651,
        51136
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2769886255264282,
      "compression_ratio": 1.5119999647140503,
      "no_speech_prob": 0.009857513010501862
    },
    {
      "id": 119,
      "seek": 66484,
      "start": 1906.960029296875,
      "end": 1912.919990234375,
      "text": " es da irgendwelche Ansätze, Ideen, wie man dem verhelfen kann? Es ist ja auch, sorry,",
      "tokens": [
        51136,
        785,
        1120,
        26455,
        338,
        1876,
        14590,
        30179,
        11,
        13090,
        268,
        11,
        3355,
        587,
        1371,
        1306,
        71,
        1967,
        268,
        4028,
        30,
        2313,
        1418,
        2784,
        2168,
        11,
        2597,
        11,
        51434
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2769886255264282,
      "compression_ratio": 1.5119999647140503,
      "no_speech_prob": 0.009857513010501862
    },
    {
      "id": 120,
      "seek": 66484,
      "start": 1912.919990234375,
      "end": 1920.919990234375,
      "text": " wenn ich gerade so lange rede, aber früher hatten wir Search Engine Optimization und da habe ich",
      "tokens": [
        51434,
        4797,
        1893,
        12117,
        370,
        18131,
        14328,
        11,
        4340,
        32349,
        20441,
        1987,
        17180,
        7659,
        35013,
        2144,
        674,
        1120,
        6015,
        1893,
        51834
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2769886255264282,
      "compression_ratio": 1.5119999647140503,
      "no_speech_prob": 0.009857513010501862
    },
    {
      "id": 121,
      "seek": 69424,
      "start": 1920.919990234375,
      "end": 1925.8399731445313,
      "text": " gemerkt, oh, meine Seite ist nicht so gut im Index drin. Jetzt optimiere ich sie. Innerhalb von",
      "tokens": [
        50364,
        7173,
        49015,
        11,
        1954,
        11,
        10946,
        19748,
        1418,
        1979,
        370,
        5228,
        566,
        33552,
        24534,
        13,
        12592,
        5028,
        14412,
        1893,
        2804,
        13,
        36705,
        13209,
        2957,
        50610
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2757500112056732,
      "compression_ratio": 1.6736111640930176,
      "no_speech_prob": 0.000804046168923378
    },
    {
      "id": 122,
      "seek": 69424,
      "start": 1925.8399731445313,
      "end": 1932.1200024414063,
      "text": " drei Tagen ist Google drüber gelaufen und dann bin ich oben. Das ist ja jetzt anders. Jetzt dauert",
      "tokens": [
        50610,
        16809,
        41721,
        1418,
        3329,
        1224,
        12670,
        4087,
        20748,
        674,
        3594,
        5171,
        1893,
        21279,
        13,
        2846,
        1418,
        2784,
        4354,
        17999,
        13,
        12592,
        37359,
        911,
        50924
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2757500112056732,
      "compression_ratio": 1.6736111640930176,
      "no_speech_prob": 0.000804046168923378
    },
    {
      "id": 123,
      "seek": 69424,
      "start": 1932.1200024414063,
      "end": 1939.1200024414063,
      "text": " es teilweise ein Jahr, bis ich die Daten wieder drin habe. Und wenn ich es dann nicht geschafft",
      "tokens": [
        50924,
        785,
        46748,
        1343,
        11674,
        11,
        7393,
        1893,
        978,
        31126,
        6216,
        24534,
        6015,
        13,
        2719,
        4797,
        1893,
        785,
        3594,
        1979,
        45215,
        51274
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2757500112056732,
      "compression_ratio": 1.6736111640930176,
      "no_speech_prob": 0.000804046168923378
    },
    {
      "id": 124,
      "seek": 69424,
      "start": 1939.1200024414063,
      "end": 1946.399970703125,
      "text": " habe, ja, auch wieder ein Problem. Ja, der Punkt ist, ich glaube, da muss man zwei Sachen so ein",
      "tokens": [
        51274,
        6015,
        11,
        2784,
        11,
        2168,
        6216,
        1343,
        11676,
        13,
        3530,
        11,
        1163,
        25487,
        1418,
        11,
        1893,
        13756,
        11,
        1120,
        6425,
        587,
        12002,
        26074,
        370,
        1343,
        51638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2757500112056732,
      "compression_ratio": 1.6736111640930176,
      "no_speech_prob": 0.000804046168923378
    },
    {
      "id": 125,
      "seek": 69424,
      "start": 1946.399970703125,
      "end": 1949.7600170898438,
      "text": " bisschen auseinanderhalten. Das ist ja kein Problem der Maschine, sondern es ist ein Problem,",
      "tokens": [
        51638,
        10763,
        257,
        438,
        20553,
        15022,
        13,
        2846,
        1418,
        2784,
        13424,
        11676,
        1163,
        5224,
        36675,
        11,
        11465,
        785,
        1418,
        1343,
        11676,
        11,
        51806
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2757500112056732,
      "compression_ratio": 1.6736111640930176,
      "no_speech_prob": 0.000804046168923378
    },
    {
      "id": 126,
      "seek": 72308,
      "start": 1949.7600170898438,
      "end": 1956.8399731445313,
      "text": " wie Menschen die Maschine nutzen. Und wer sich jemals mit ein bisschen Dark Pattern oder",
      "tokens": [
        50364,
        3355,
        8397,
        978,
        5224,
        36675,
        36905,
        13,
        2719,
        2612,
        3041,
        361,
        443,
        1124,
        2194,
        1343,
        10763,
        9563,
        34367,
        77,
        4513,
        50718
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2569969594478607,
      "compression_ratio": 1.6703296899795532,
      "no_speech_prob": 0.010472377762198448
    },
    {
      "id": 127,
      "seek": 72308,
      "start": 1956.8399731445313,
      "end": 1961.6400219726563,
      "text": " Behavior Adjustment through Algorithm beschäftigt hat, also wie kriegen wir euch eigentlich dazu,",
      "tokens": [
        50718,
        45807,
        34049,
        518,
        807,
        35014,
        6819,
        76,
        38768,
        5828,
        2385,
        11,
        611,
        3355,
        46882,
        1987,
        10403,
        10926,
        13034,
        11,
        50958
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2569969594478607,
      "compression_ratio": 1.6703296899795532,
      "no_speech_prob": 0.010472377762198448
    },
    {
      "id": 128,
      "seek": 72308,
      "start": 1961.6400219726563,
      "end": 1966.440009765625,
      "text": " mehr Dinge zu kaufen oder so? Recommendational Algorithmen oder wie kriegen wir dann auch",
      "tokens": [
        50958,
        5417,
        25102,
        2164,
        42083,
        4513,
        370,
        30,
        49545,
        521,
        1478,
        35014,
        6819,
        2558,
        4513,
        3355,
        46882,
        1987,
        3594,
        2168,
        51198
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2569969594478607,
      "compression_ratio": 1.6703296899795532,
      "no_speech_prob": 0.010472377762198448
    },
    {
      "id": 129,
      "seek": 72308,
      "start": 1966.440009765625,
      "end": 1971.5999829101563,
      "text": " vielleicht mal eine Demokratie angezündigt? Recommendational Algorithmus. Das ist das",
      "tokens": [
        51198,
        12547,
        2806,
        3018,
        27802,
        414,
        15495,
        89,
        9541,
        5828,
        30,
        49545,
        521,
        1478,
        35014,
        6819,
        18761,
        13,
        2846,
        1418,
        1482,
        51456
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2569969594478607,
      "compression_ratio": 1.6703296899795532,
      "no_speech_prob": 0.010472377762198448
    },
    {
      "id": 130,
      "seek": 72308,
      "start": 1971.5999829101563,
      "end": 1975.4799877929688,
      "text": " Problem. Das Problem ist gar nicht so sehr, dass die Maschine an sich ein Cut-off-Date hat,",
      "tokens": [
        51456,
        11676,
        13,
        2846,
        11676,
        1418,
        3691,
        1979,
        370,
        5499,
        11,
        2658,
        978,
        5224,
        36675,
        364,
        3041,
        1343,
        9431,
        12,
        4506,
        12,
        35,
        473,
        2385,
        11,
        51650
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2569969594478607,
      "compression_ratio": 1.6703296899795532,
      "no_speech_prob": 0.010472377762198448
    },
    {
      "id": 131,
      "seek": 74880,
      "start": 1975.4799877929688,
      "end": 1979.8800122070313,
      "text": " weil wenn du dir überlegst, okay, die Maschine hat ein Cut-off-Date, ihr seid aber Open Source und",
      "tokens": [
        50364,
        7689,
        4797,
        1581,
        4746,
        4502,
        6363,
        372,
        11,
        1392,
        11,
        978,
        5224,
        36675,
        2385,
        1343,
        9431,
        12,
        4506,
        12,
        35,
        473,
        11,
        5553,
        38041,
        4340,
        7238,
        29629,
        674,
        50584
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23930369317531586,
      "compression_ratio": 1.6264368295669556,
      "no_speech_prob": 0.006788528058677912
    },
    {
      "id": 132,
      "seek": 74880,
      "start": 1979.8800122070313,
      "end": 1984.7999951171876,
      "text": " ihr habt ja die letzten Versionen und Dokumente, dann wäre die natürliche Variante zu sagen,",
      "tokens": [
        50584,
        5553,
        23660,
        2784,
        978,
        18226,
        12226,
        17068,
        674,
        29768,
        449,
        1576,
        11,
        3594,
        14558,
        978,
        8762,
        68,
        32511,
        2879,
        2164,
        8360,
        11,
        50830
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23930369317531586,
      "compression_ratio": 1.6264368295669556,
      "no_speech_prob": 0.006788528058677912
    },
    {
      "id": 133,
      "seek": 74880,
      "start": 1984.7999951171876,
      "end": 1988.5200268554688,
      "text": " okay, dann bauen wir jetzt mal, egal wie das technisch ausschaut, unsere Knowledge Base",
      "tokens": [
        50830,
        1392,
        11,
        3594,
        43787,
        1987,
        4354,
        2806,
        11,
        31528,
        3355,
        1482,
        1537,
        5494,
        5730,
        339,
        1375,
        11,
        14339,
        32906,
        21054,
        51016
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23930369317531586,
      "compression_ratio": 1.6264368295669556,
      "no_speech_prob": 0.006788528058677912
    },
    {
      "id": 134,
      "seek": 74880,
      "start": 1988.5200268554688,
      "end": 1992.960029296875,
      "text": " aus. Wir bauen aus unseren alten Tickets was aus, wir bauen aus unserer neuen Struktur was",
      "tokens": [
        51016,
        3437,
        13,
        4347,
        43787,
        3437,
        25305,
        41217,
        314,
        38748,
        390,
        3437,
        11,
        1987,
        43787,
        3437,
        20965,
        21387,
        745,
        31543,
        390,
        51238
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23930369317531586,
      "compression_ratio": 1.6264368295669556,
      "no_speech_prob": 0.006788528058677912
    },
    {
      "id": 135,
      "seek": 74880,
      "start": 1992.960029296875,
      "end": 1998.960029296875,
      "text": " auf und nutzen das LLM als dieses Argumentation, dass es schneller, vielleicht auch neue Ideen",
      "tokens": [
        51238,
        2501,
        674,
        36905,
        1482,
        441,
        43,
        44,
        3907,
        12113,
        40081,
        2206,
        399,
        11,
        2658,
        785,
        43865,
        11,
        12547,
        2168,
        16842,
        13090,
        268,
        51538
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23930369317531586,
      "compression_ratio": 1.6264368295669556,
      "no_speech_prob": 0.006788528058677912
    },
    {
      "id": 136,
      "seek": 74880,
      "start": 1998.960029296875,
      "end": 2003.7999951171876,
      "text": " basierend auf unseren Cases geht. Und ich sehe ganz klar Blick in die Zukunft, dass wir definitiv",
      "tokens": [
        51538,
        987,
        811,
        521,
        2501,
        25305,
        383,
        1957,
        7095,
        13,
        2719,
        1893,
        35995,
        6312,
        14743,
        32556,
        294,
        978,
        22782,
        11,
        2658,
        1987,
        28781,
        592,
        51780
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23930369317531586,
      "compression_ratio": 1.6264368295669556,
      "no_speech_prob": 0.006788528058677912
    },
    {
      "id": 137,
      "seek": 77712,
      "start": 2003.7999951171876,
      "end": 2009.3599926757813,
      "text": " in zwei Varianten einsteigen werden. Die eine, die bereit sind, den Aufwand zu betreiben und zu",
      "tokens": [
        50364,
        294,
        12002,
        32511,
        29646,
        1343,
        2941,
        3213,
        4604,
        13,
        3229,
        3018,
        11,
        978,
        38758,
        3290,
        11,
        1441,
        9462,
        33114,
        2164,
        778,
        25946,
        674,
        2164,
        50642
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2744278311729431,
      "compression_ratio": 1.6075581312179565,
      "no_speech_prob": 0.05097268894314766
    },
    {
      "id": 138,
      "seek": 77712,
      "start": 2009.3599926757813,
      "end": 2014.8800122070313,
      "text": " sagen, hey, jetzt mal egal wie gut das Grundmodell ist, wir machen noch einen geilen Job obendrauf,",
      "tokens": [
        50642,
        8360,
        11,
        4177,
        11,
        4354,
        2806,
        31528,
        3355,
        5228,
        1482,
        13941,
        8014,
        898,
        1418,
        11,
        1987,
        7069,
        3514,
        4891,
        1519,
        17471,
        18602,
        1111,
        27332,
        2947,
        11,
        50918
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2744278311729431,
      "compression_ratio": 1.6075581312179565,
      "no_speech_prob": 0.05097268894314766
    },
    {
      "id": 139,
      "seek": 77712,
      "start": 2014.8800122070313,
      "end": 2018.3599926757813,
      "text": " das kann nur besser werden und wir werden sicherlich auch zurück zu diesem ominösen",
      "tokens": [
        50918,
        1482,
        4028,
        4343,
        18021,
        4604,
        674,
        1987,
        4604,
        18623,
        1739,
        2168,
        15089,
        2164,
        10975,
        46812,
        973,
        6748,
        51092
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2744278311729431,
      "compression_ratio": 1.6075581312179565,
      "no_speech_prob": 0.05097268894314766
    },
    {
      "id": 140,
      "seek": 77712,
      "start": 2018.3599926757813,
      "end": 2022.15998046875,
      "text": " Problem Data Model Match. Dann nehmen wir vielleicht sogar ein kleineres Modell ab,",
      "tokens": [
        51092,
        11676,
        11888,
        17105,
        26178,
        13,
        7455,
        19905,
        1987,
        12547,
        19485,
        1343,
        39496,
        279,
        6583,
        898,
        410,
        11,
        51282
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2744278311729431,
      "compression_ratio": 1.6075581312179565,
      "no_speech_prob": 0.05097268894314766
    },
    {
      "id": 141,
      "seek": 77712,
      "start": 2022.15998046875,
      "end": 2026.2799755859376,
      "text": " es ist für einen spezialisierteren Case, dann macht auch auf einmal Laufen auf dem eigenen Rechner",
      "tokens": [
        51282,
        785,
        1418,
        2959,
        4891,
        768,
        17787,
        271,
        811,
        391,
        268,
        17791,
        11,
        3594,
        10857,
        2168,
        2501,
        11078,
        441,
        20748,
        2501,
        1371,
        28702,
        1300,
        339,
        1193,
        51488
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2744278311729431,
      "compression_ratio": 1.6075581312179565,
      "no_speech_prob": 0.05097268894314766
    },
    {
      "id": 142,
      "seek": 77712,
      "start": 2026.2799755859376,
      "end": 2033.440009765625,
      "text": " und Trend Solution mehr Sinn. Aber wir setzen nicht auf den großen Mittelwert, was die",
      "tokens": [
        51488,
        674,
        37417,
        318,
        3386,
        5417,
        37962,
        13,
        5992,
        1987,
        35877,
        1979,
        2501,
        1441,
        23076,
        35079,
        26521,
        11,
        390,
        978,
        51846
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2744278311729431,
      "compression_ratio": 1.6075581312179565,
      "no_speech_prob": 0.05097268894314766
    },
    {
      "id": 143,
      "seek": 80676,
      "start": 2033.440009765625,
      "end": 2038.0399853515626,
      "text": " Frontier Labs ja im Prinzip aktuell liefern. Cutting Edge, aber den großen Mittelwert. Das heißt,",
      "tokens": [
        50364,
        17348,
        811,
        40047,
        2784,
        566,
        47572,
        36267,
        4544,
        28958,
        13,
        9431,
        783,
        19328,
        11,
        4340,
        1441,
        23076,
        35079,
        26521,
        13,
        2846,
        13139,
        11,
        50594
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27547746896743774,
      "compression_ratio": 1.63103449344635,
      "no_speech_prob": 0.021240269765257835
    },
    {
      "id": 144,
      "seek": 80676,
      "start": 2038.0399853515626,
      "end": 2042.68,
      "text": " wir gehen in diese Optimierung und machen es zu unserer eigenen Architektur mit den Tools,",
      "tokens": [
        50594,
        1987,
        13230,
        294,
        6705,
        35013,
        11651,
        674,
        7069,
        785,
        2164,
        20965,
        28702,
        10984,
        642,
        2320,
        374,
        2194,
        1441,
        30302,
        11,
        50826
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27547746896743774,
      "compression_ratio": 1.63103449344635,
      "no_speech_prob": 0.021240269765257835
    },
    {
      "id": 145,
      "seek": 80676,
      "start": 2042.68,
      "end": 2047.0399853515626,
      "text": " die wir gerne hätten. Und dann wirst du den Rest haben, der genau so laufen wird, wie das immer ist.",
      "tokens": [
        50826,
        978,
        1987,
        15689,
        33278,
        13,
        2719,
        3594,
        261,
        653,
        1581,
        1441,
        13094,
        3084,
        11,
        1163,
        12535,
        370,
        41647,
        4578,
        11,
        3355,
        1482,
        5578,
        1418,
        13,
        51044
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27547746896743774,
      "compression_ratio": 1.63103449344635,
      "no_speech_prob": 0.021240269765257835
    },
    {
      "id": 146,
      "seek": 80676,
      "start": 2047.0399853515626,
      "end": 2051.1200024414065,
      "text": " Das wird eine Vermittelmäßigung und das ist ja auch das, was mit den LLMs ganz schnell passiert.",
      "tokens": [
        51044,
        2846,
        4578,
        3018,
        20185,
        593,
        338,
        32450,
        21034,
        674,
        1482,
        1418,
        2784,
        2168,
        1482,
        11,
        390,
        2194,
        1441,
        441,
        43,
        26386,
        6312,
        17589,
        21671,
        13,
        51248
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27547746896743774,
      "compression_ratio": 1.63103449344635,
      "no_speech_prob": 0.021240269765257835
    },
    {
      "id": 147,
      "seek": 80676,
      "start": 2051.1200024414065,
      "end": 2056.000007324219,
      "text": " Sie generalisieren ja nicht, sondern sie vermittelmäßigen und das ist nicht das",
      "tokens": [
        51248,
        3559,
        2674,
        271,
        5695,
        2784,
        1979,
        11,
        11465,
        2804,
        1306,
        33960,
        32450,
        3213,
        674,
        1482,
        1418,
        1979,
        1482,
        51492
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27547746896743774,
      "compression_ratio": 1.63103449344635,
      "no_speech_prob": 0.021240269765257835
    },
    {
      "id": 148,
      "seek": 82932,
      "start": 2056.000007324219,
      "end": 2063.239997558594,
      "text": " Gleiche. Generalisierung und Vermittelmäßigung ist nicht das Gleiche und wir haben ja nicht mal",
      "tokens": [
        50364,
        33858,
        68,
        13,
        6996,
        32531,
        674,
        20185,
        47265,
        32450,
        21034,
        1418,
        1979,
        1482,
        33858,
        68,
        674,
        1987,
        3084,
        2784,
        1979,
        2806,
        50726
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28736063838005066,
      "compression_ratio": 1.6271185874938965,
      "no_speech_prob": 0.13452672958374023
    },
    {
      "id": 149,
      "seek": 82932,
      "start": 2063.239997558594,
      "end": 2068.039985351563,
      "text": " richtig schöne Begriffe dafür und der wird dann darin und dann wird es halt vielleicht sogar noch",
      "tokens": [
        50726,
        13129,
        41152,
        879,
        861,
        31387,
        13747,
        674,
        1163,
        4578,
        3594,
        4072,
        259,
        674,
        3594,
        4578,
        785,
        12479,
        12547,
        19485,
        3514,
        50966
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28736063838005066,
      "compression_ratio": 1.6271185874938965,
      "no_speech_prob": 0.13452672958374023
    },
    {
      "id": 150,
      "seek": 82932,
      "start": 2068.039985351563,
      "end": 2073.9600292968753,
      "text": " eine dritte Gruppe geben, die wir ja jetzt auch schon sehen, die halt wirklich um die Kurve denken,",
      "tokens": [
        50966,
        3018,
        1224,
        9786,
        10459,
        19833,
        17191,
        11,
        978,
        1987,
        2784,
        4354,
        2168,
        4981,
        11333,
        11,
        978,
        12479,
        9696,
        1105,
        978,
        16481,
        303,
        28780,
        11,
        51262
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28736063838005066,
      "compression_ratio": 1.6271185874938965,
      "no_speech_prob": 0.13452672958374023
    },
    {
      "id": 151,
      "seek": 82932,
      "start": 2073.9600292968753,
      "end": 2078.760017089844,
      "text": " Sachen miteinander anders kombinieren. Es wird ja auch wenigstens im Kleinen und ich hoffe auch,",
      "tokens": [
        51262,
        26074,
        43127,
        17999,
        42925,
        259,
        5695,
        13,
        2313,
        4578,
        2784,
        2168,
        20911,
        372,
        694,
        566,
        17053,
        5636,
        674,
        1893,
        34903,
        2168,
        11,
        51502
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28736063838005066,
      "compression_ratio": 1.6271185874938965,
      "no_speech_prob": 0.13452672958374023
    },
    {
      "id": 152,
      "seek": 82932,
      "start": 2078.760017089844,
      "end": 2082.279975585938,
      "text": " dass da wieder mehr Funding in Zukunft zur Verfügung steht, auch an anderen weiteren",
      "tokens": [
        51502,
        2658,
        1120,
        6216,
        5417,
        13493,
        278,
        294,
        22782,
        7147,
        43026,
        16361,
        11,
        2168,
        364,
        11122,
        44036,
        51678
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28736063838005066,
      "compression_ratio": 1.6271185874938965,
      "no_speech_prob": 0.13452672958374023
    },
    {
      "id": 153,
      "seek": 85560,
      "start": 2082.320014648438,
      "end": 2086.3999707031253,
      "text": " Techniken gearbeitet, weil die LLMs müssen irgendwann von der neuen Architektur abgelöst",
      "tokens": [
        50366,
        8337,
        19640,
        7394,
        32401,
        11,
        7689,
        978,
        441,
        43,
        26386,
        9013,
        34313,
        2957,
        1163,
        21387,
        10984,
        642,
        2320,
        374,
        410,
        10345,
        36995,
        50570
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23229114711284637,
      "compression_ratio": 1.5903226137161255,
      "no_speech_prob": 0.14374157786369324
    },
    {
      "id": 154,
      "seek": 85560,
      "start": 2086.3999707031253,
      "end": 2091.2000195312503,
      "text": " werden. Wir haben zwischenzeitlich wahnsinnig viel Compute, das heißt viele Sachen, die in der Theorie",
      "tokens": [
        50570,
        4604,
        13,
        4347,
        3084,
        19875,
        13712,
        1739,
        31979,
        46134,
        328,
        5891,
        6620,
        1169,
        11,
        1482,
        13139,
        9693,
        26074,
        11,
        978,
        294,
        1163,
        440,
        17473,
        50810
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23229114711284637,
      "compression_ratio": 1.5903226137161255,
      "no_speech_prob": 0.14374157786369324
    },
    {
      "id": 155,
      "seek": 85560,
      "start": 2091.2000195312503,
      "end": 2098.2000195312503,
      "text": " von vor 80, 70, aber auch vor 20 Jahren nicht rechenbar waren, sind heute rechenbar. Das heißt,",
      "tokens": [
        50810,
        2957,
        4245,
        4688,
        11,
        5285,
        11,
        4340,
        2168,
        4245,
        945,
        13080,
        1979,
        319,
        2470,
        5356,
        11931,
        11,
        3290,
        9801,
        319,
        2470,
        5356,
        13,
        2846,
        13139,
        11,
        51160
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23229114711284637,
      "compression_ratio": 1.5903226137161255,
      "no_speech_prob": 0.14374157786369324
    },
    {
      "id": 156,
      "seek": 85560,
      "start": 2098.2000195312503,
      "end": 2103.1200024414065,
      "text": " das sind so Möglichkeiten, wo dann sicherlich auch noch neue Sachen entstehen. Aber am Ende des Tages",
      "tokens": [
        51160,
        1482,
        3290,
        370,
        42627,
        11,
        6020,
        3594,
        18623,
        1739,
        2168,
        3514,
        16842,
        26074,
        35955,
        2932,
        13,
        5992,
        669,
        15152,
        730,
        33601,
        51406
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23229114711284637,
      "compression_ratio": 1.5903226137161255,
      "no_speech_prob": 0.14374157786369324
    },
    {
      "id": 157,
      "seek": 85560,
      "start": 2103.1200024414065,
      "end": 2108.520026855469,
      "text": " liegt es nicht an der Maschine, sondern es liegt an dem, wie geht der Mensch in der Zusammenarbeit",
      "tokens": [
        51406,
        22421,
        785,
        1979,
        364,
        1163,
        5224,
        36675,
        11,
        11465,
        785,
        22421,
        364,
        1371,
        11,
        3355,
        7095,
        1163,
        27773,
        294,
        1163,
        29442,
        24024,
        51676
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23229114711284637,
      "compression_ratio": 1.5903226137161255,
      "no_speech_prob": 0.14374157786369324
    },
    {
      "id": 158,
      "seek": 88184,
      "start": 2108.520026855469,
      "end": 2114.3999707031253,
      "text": " mit der Maschine um? Geht es in den Modus Foul, wo ich einfach mal denke, Hauptsache es ist erledigt",
      "tokens": [
        50364,
        2194,
        1163,
        5224,
        36675,
        1105,
        30,
        2876,
        357,
        785,
        294,
        1441,
        6583,
        301,
        479,
        3298,
        11,
        6020,
        1893,
        7281,
        2806,
        27245,
        11,
        30573,
        82,
        6000,
        785,
        1418,
        1189,
        1493,
        5828,
        50658
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25334715843200684,
      "compression_ratio": 1.7527272701263428,
      "no_speech_prob": 0.020619036629796028
    },
    {
      "id": 159,
      "seek": 88184,
      "start": 2114.3999707031253,
      "end": 2120.320014648438,
      "text": " und ich mache mir einen gechillten oder gehe ich in die Variante rein, okay, das Ding ist geil,",
      "tokens": [
        50658,
        674,
        1893,
        28289,
        3149,
        4891,
        1519,
        339,
        373,
        1147,
        4513,
        34252,
        1893,
        294,
        978,
        32511,
        2879,
        6561,
        11,
        1392,
        11,
        1482,
        20558,
        1418,
        47165,
        11,
        50954
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25334715843200684,
      "compression_ratio": 1.7527272701263428,
      "no_speech_prob": 0.020619036629796028
    },
    {
      "id": 160,
      "seek": 88184,
      "start": 2120.320014648438,
      "end": 2125.1200024414065,
      "text": " aber ich muss noch im Lead bleiben und mit gemeinschaftlich sind wir aber, wie es im",
      "tokens": [
        50954,
        4340,
        1893,
        6425,
        3514,
        566,
        31025,
        24912,
        674,
        2194,
        22971,
        7118,
        1739,
        3290,
        1987,
        4340,
        11,
        3355,
        785,
        566,
        51194
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25334715843200684,
      "compression_ratio": 1.7527272701263428,
      "no_speech_prob": 0.020619036629796028
    },
    {
      "id": 161,
      "seek": 88184,
      "start": 2125.1200024414065,
      "end": 2129.760017089844,
      "text": " klassischen Teamwork halt auch ist, gemeinschaftlich sind wir besser. Gemeinschaftlich sind wir besser,",
      "tokens": [
        51194,
        42917,
        6282,
        7606,
        1902,
        12479,
        2168,
        1418,
        11,
        22971,
        7118,
        1739,
        3290,
        1987,
        18021,
        13,
        31266,
        1292,
        7118,
        1739,
        3290,
        1987,
        18021,
        11,
        51426
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25334715843200684,
      "compression_ratio": 1.7527272701263428,
      "no_speech_prob": 0.020619036629796028
    },
    {
      "id": 162,
      "seek": 88184,
      "start": 2129.760017089844,
      "end": 2134.000007324219,
      "text": " wie sie finden aber viele Leute, zum Beispiel sehr bunt gemischte Teams doof, weil es natürlich",
      "tokens": [
        51426,
        3355,
        2804,
        20734,
        4340,
        9693,
        13495,
        11,
        5919,
        13772,
        5499,
        272,
        2760,
        7173,
        5494,
        975,
        24702,
        360,
        2670,
        11,
        7689,
        785,
        8762,
        51638
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25334715843200684,
      "compression_ratio": 1.7527272701263428,
      "no_speech_prob": 0.020619036629796028
    },
    {
      "id": 163,
      "seek": 90732,
      "start": 2134.000007324219,
      "end": 2139.279975585938,
      "text": " heterogen und damit für Friktion sorgt. Und das ist halt sicherlich, dass wer bereit ist,",
      "tokens": [
        50364,
        20789,
        8799,
        674,
        9479,
        2959,
        479,
        470,
        9780,
        262,
        36698,
        13,
        2719,
        1482,
        1418,
        12479,
        18623,
        1739,
        11,
        2658,
        2612,
        38758,
        1418,
        11,
        50628
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2713736891746521,
      "compression_ratio": 1.6587837934494019,
      "no_speech_prob": 0.5451110601425171
    },
    {
      "id": 164,
      "seek": 90732,
      "start": 2139.279975585938,
      "end": 2145.239997558594,
      "text": " die Friktion mitzunehmen, der wird sich da draußen in den nächsten Jahren und Jahrzehnten eine Top-Position",
      "tokens": [
        50628,
        978,
        479,
        470,
        9780,
        2194,
        89,
        2613,
        9547,
        11,
        1163,
        4578,
        3041,
        1120,
        44602,
        294,
        1441,
        19101,
        13080,
        674,
        11674,
        1381,
        71,
        14970,
        3018,
        8840,
        12,
        47,
        5830,
        50926
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2713736891746521,
      "compression_ratio": 1.6587837934494019,
      "no_speech_prob": 0.5451110601425171
    },
    {
      "id": 165,
      "seek": 90732,
      "start": 2145.239997558594,
      "end": 2151.760017089844,
      "text": " erkaufen und auch ermöglichen, weil da ist da, wo das große Geld laufen wird und sonst irgendwas.",
      "tokens": [
        50926,
        31879,
        20748,
        674,
        2168,
        25253,
        16277,
        268,
        11,
        7689,
        1120,
        1418,
        1120,
        11,
        6020,
        1482,
        19691,
        16535,
        41647,
        4578,
        674,
        26309,
        47090,
        13,
        51252
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2713736891746521,
      "compression_ratio": 1.6587837934494019,
      "no_speech_prob": 0.5451110601425171
    },
    {
      "id": 166,
      "seek": 90732,
      "start": 2151.760017089844,
      "end": 2156.520026855469,
      "text": " Die, die in diesen Foulheitsmodus reingehen, die werden in der Mittelmäßigkeit ein bisschen",
      "tokens": [
        51252,
        3229,
        11,
        978,
        294,
        12862,
        479,
        3298,
        24260,
        8014,
        301,
        319,
        8735,
        2932,
        11,
        978,
        4604,
        294,
        1163,
        35079,
        32450,
        16626,
        1343,
        10763,
        51490
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2713736891746521,
      "compression_ratio": 1.6587837934494019,
      "no_speech_prob": 0.5451110601425171
    },
    {
      "id": 167,
      "seek": 90732,
      "start": 2156.520026855469,
      "end": 2161.239997558594,
      "text": " schwimmen, bis es halt dann irgendwann so weit ist, dass Mittelmäßigkeit Evo-Modellen komplett",
      "tokens": [
        51490,
        17932,
        32076,
        11,
        7393,
        785,
        12479,
        3594,
        34313,
        370,
        15306,
        1418,
        11,
        2658,
        35079,
        32450,
        16626,
        462,
        3080,
        12,
        44,
        378,
        8581,
        32261,
        51726
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2713736891746521,
      "compression_ratio": 1.6587837934494019,
      "no_speech_prob": 0.5451110601425171
    },
    {
      "id": 168,
      "seek": 93456,
      "start": 2161.239997558594,
      "end": 2165.760017089844,
      "text": " abgedeckt werden und dann sieht die Welt anders aus. Wir brauchen keine General Intelligence,",
      "tokens": [
        50364,
        410,
        3004,
        68,
        19951,
        4604,
        674,
        3594,
        14289,
        978,
        14761,
        17999,
        3437,
        13,
        4347,
        19543,
        9252,
        6996,
        27274,
        11,
        50590
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2292906790971756,
      "compression_ratio": 1.615646243095398,
      "no_speech_prob": 0.06266948580741882
    },
    {
      "id": 169,
      "seek": 93456,
      "start": 2165.760017089844,
      "end": 2169.719978027344,
      "text": " wenn alles auf Mittelmäßigkeit läuft. Und das ist das, was die Leute so ein bisschen",
      "tokens": [
        50590,
        4797,
        7874,
        2501,
        35079,
        32450,
        16626,
        31807,
        13,
        2719,
        1482,
        1418,
        1482,
        11,
        390,
        978,
        13495,
        370,
        1343,
        10763,
        50788
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2292906790971756,
      "compression_ratio": 1.615646243095398,
      "no_speech_prob": 0.06266948580741882
    },
    {
      "id": 170,
      "seek": 93456,
      "start": 2169.719978027344,
      "end": 2175.520026855469,
      "text": " unterschätzen und diese Aufwand in der Friktion und den Aufwand in solchen Systemen zu etablieren",
      "tokens": [
        50788,
        20983,
        339,
        45721,
        674,
        6705,
        9462,
        33114,
        294,
        1163,
        479,
        470,
        9780,
        674,
        1441,
        9462,
        33114,
        294,
        46281,
        8910,
        268,
        2164,
        1030,
        455,
        2753,
        268,
        51078
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2292906790971756,
      "compression_ratio": 1.615646243095398,
      "no_speech_prob": 0.06266948580741882
    },
    {
      "id": 171,
      "seek": 93456,
      "start": 2175.520026855469,
      "end": 2181.9600292968753,
      "text": " und umzusetzen. Okay, das heißt, wenn ich das jetzt mal, was wir bislang besprochen haben,",
      "tokens": [
        51078,
        674,
        1105,
        16236,
        24797,
        13,
        1033,
        11,
        1482,
        13139,
        11,
        4797,
        1893,
        1482,
        4354,
        2806,
        11,
        390,
        1987,
        272,
        5788,
        656,
        4097,
        23902,
        3084,
        11,
        51400
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2292906790971756,
      "compression_ratio": 1.615646243095398,
      "no_speech_prob": 0.06266948580741882
    },
    {
      "id": 172,
      "seek": 93456,
      "start": 2181.9600292968753,
      "end": 2188.719978027344,
      "text": " so ein bisschen zusammenfassen kann, ist, dass es so aussieht als, ja, wir haben jetzt große Modelle,",
      "tokens": [
        51400,
        370,
        1343,
        10763,
        14311,
        69,
        8356,
        4028,
        11,
        1418,
        11,
        2658,
        785,
        370,
        5730,
        39850,
        3907,
        11,
        2784,
        11,
        1987,
        3084,
        4354,
        19691,
        6583,
        4434,
        11,
        51738
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2292906790971756,
      "compression_ratio": 1.615646243095398,
      "no_speech_prob": 0.06266948580741882
    },
    {
      "id": 173,
      "seek": 96204,
      "start": 2188.719978027344,
      "end": 2193.799995117188,
      "text": " die leisten eigentlich das, was sie leisten sollen. Wir brauchen jetzt gar nicht irgendwie",
      "tokens": [
        50364,
        978,
        47013,
        10926,
        1482,
        11,
        390,
        2804,
        47013,
        24713,
        13,
        4347,
        19543,
        4354,
        3691,
        1979,
        20759,
        50618
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2839912176132202,
      "compression_ratio": 1.6859205961227417,
      "no_speech_prob": 0.011503227986395359
    },
    {
      "id": 174,
      "seek": 96204,
      "start": 2193.799995117188,
      "end": 2200.039985351563,
      "text": " auf GPT-6 irgendwie groß zu warten, sondern wir haben jetzt was, womit wir arbeiten können,",
      "tokens": [
        50618,
        2501,
        26039,
        51,
        12,
        21,
        20759,
        17253,
        2164,
        46907,
        11,
        11465,
        1987,
        3084,
        4354,
        390,
        11,
        1579,
        270,
        1987,
        23162,
        6310,
        11,
        50930
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2839912176132202,
      "compression_ratio": 1.6859205961227417,
      "no_speech_prob": 0.011503227986395359
    },
    {
      "id": 175,
      "seek": 96204,
      "start": 2200.039985351563,
      "end": 2207.8800122070315,
      "text": " aber wir müssen irgendwie gucken, dass wir ja den Context managen. Das Context Management,",
      "tokens": [
        50930,
        4340,
        1987,
        9013,
        20759,
        33135,
        11,
        2658,
        1987,
        2784,
        1441,
        4839,
        3828,
        587,
        4698,
        13,
        2846,
        4839,
        3828,
        14781,
        11,
        51322
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2839912176132202,
      "compression_ratio": 1.6859205961227417,
      "no_speech_prob": 0.011503227986395359
    },
    {
      "id": 176,
      "seek": 96204,
      "start": 2207.8800122070315,
      "end": 2212.760017089844,
      "text": " das wird jetzt wahrscheinlich groß werden, oder? Das ist jetzt schon groß. Also du siehst schon,",
      "tokens": [
        51322,
        1482,
        4578,
        4354,
        30957,
        17253,
        4604,
        11,
        4513,
        30,
        2846,
        1418,
        4354,
        4981,
        17253,
        13,
        2743,
        1581,
        2804,
        38857,
        4981,
        11,
        51566
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2839912176132202,
      "compression_ratio": 1.6859205961227417,
      "no_speech_prob": 0.011503227986395359
    },
    {
      "id": 177,
      "seek": 96204,
      "start": 2212.760017089844,
      "end": 2218.320014648438,
      "text": " die, die halt angefangen haben und vielleicht auch mehrfach gescheitert sind, so Stichworte",
      "tokens": [
        51566,
        978,
        11,
        978,
        12479,
        43907,
        10784,
        3084,
        674,
        12547,
        2168,
        5417,
        6749,
        5019,
        1876,
        270,
        911,
        3290,
        11,
        370,
        745,
        480,
        86,
        12752,
        51844
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2839912176132202,
      "compression_ratio": 1.6859205961227417,
      "no_speech_prob": 0.011503227986395359
    },
    {
      "id": 178,
      "seek": 99164,
      "start": 2218.4400097656253,
      "end": 2222.6400219726565,
      "text": " wie Rack und Knowledge Basis und wie wir sie alle nennen wollen, die da jetzt schon ein ganzes",
      "tokens": [
        50370,
        3355,
        497,
        501,
        674,
        32906,
        5859,
        271,
        674,
        3355,
        1987,
        2804,
        5430,
        297,
        16043,
        11253,
        11,
        978,
        1120,
        4354,
        4981,
        1343,
        6312,
        279,
        50580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2728838622570038,
      "compression_ratio": 1.5922330617904663,
      "no_speech_prob": 0.005286548752337694
    },
    {
      "id": 179,
      "seek": 99164,
      "start": 2222.6400219726565,
      "end": 2230.5999829101565,
      "text": " Stückchen weiter sind, haben einen Vorteil, ganz klar. Und das ist halt genau dieses Ding. Da wird",
      "tokens": [
        50580,
        31146,
        2470,
        8988,
        3290,
        11,
        3084,
        4891,
        46968,
        388,
        11,
        6312,
        14743,
        13,
        2719,
        1482,
        1418,
        12479,
        12535,
        12113,
        20558,
        13,
        3933,
        4578,
        50978
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2728838622570038,
      "compression_ratio": 1.5922330617904663,
      "no_speech_prob": 0.005286548752337694
    },
    {
      "id": 180,
      "seek": 99164,
      "start": 2230.5999829101565,
      "end": 2235.799995117188,
      "text": " sich nachher der Unterschied dann auch ergeben. Und gerade ist es ja immer noch, ich versuche den",
      "tokens": [
        50978,
        3041,
        5168,
        511,
        1163,
        41414,
        3594,
        2168,
        1189,
        16702,
        13,
        2719,
        12117,
        1418,
        785,
        2784,
        5578,
        3514,
        11,
        1893,
        1774,
        17545,
        1441,
        51238
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2728838622570038,
      "compression_ratio": 1.5922330617904663,
      "no_speech_prob": 0.005286548752337694
    },
    {
      "id": 181,
      "seek": 99164,
      "start": 2235.799995117188,
      "end": 2240.279975585938,
      "text": " Leuten mal zu sagen, ich kenne die Probleme, was man mit probabilistischen Modellen hat. Ihr wartet",
      "tokens": [
        51238,
        42301,
        2806,
        2164,
        8360,
        11,
        1893,
        350,
        13295,
        978,
        32891,
        11,
        390,
        587,
        2194,
        31959,
        468,
        6282,
        6583,
        8581,
        2385,
        13,
        14773,
        261,
        32347,
        51462
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2728838622570038,
      "compression_ratio": 1.5922330617904663,
      "no_speech_prob": 0.005286548752337694
    },
    {
      "id": 182,
      "seek": 99164,
      "start": 2240.279975585938,
      "end": 2248.279975585938,
      "text": " auf einen Mature Grad, wo das Ding deterministisch wird. Kinder, das kann gar nicht passieren. Also",
      "tokens": [
        51462,
        2501,
        4891,
        376,
        1503,
        16710,
        11,
        6020,
        1482,
        20558,
        15957,
        468,
        5494,
        4578,
        13,
        14193,
        11,
        1482,
        4028,
        3691,
        1979,
        46223,
        13,
        2743,
        51862
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2728838622570038,
      "compression_ratio": 1.5922330617904663,
      "no_speech_prob": 0.005286548752337694
    },
    {
      "id": 183,
      "seek": 102160,
      "start": 2248.760017089844,
      "end": 2255.6399609375003,
      "text": " dann wäre es kaputt, weil da kamen wir mal her und dann klang das, Large Language Modell,",
      "tokens": [
        50388,
        3594,
        14558,
        785,
        13816,
        13478,
        11,
        7689,
        1120,
        9727,
        268,
        1987,
        2806,
        720,
        674,
        3594,
        9671,
        656,
        1482,
        11,
        33092,
        24445,
        6583,
        898,
        11,
        50732
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29607436060905457,
      "compression_ratio": 1.399999976158142,
      "no_speech_prob": 0.0008423306862823665
    },
    {
      "id": 184,
      "seek": 102160,
      "start": 2255.6399609375003,
      "end": 2261.519965820313,
      "text": " nämlich genau so, wie wir das immer identifizieren können. Es klang wie eine Bedienungsanleitung vom",
      "tokens": [
        50732,
        21219,
        12535,
        370,
        11,
        3355,
        1987,
        1482,
        5578,
        2473,
        351,
        590,
        5695,
        6310,
        13,
        2313,
        9671,
        656,
        3355,
        3018,
        19893,
        1053,
        5846,
        282,
        306,
        35563,
        10135,
        51026
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29607436060905457,
      "compression_ratio": 1.399999976158142,
      "no_speech_prob": 0.0008423306862823665
    },
    {
      "id": 185,
      "seek": 102160,
      "start": 2261.519965820313,
      "end": 2272.600043945313,
      "text": " Ikea ins Chinesische übersetzt, durch den indischen Server gelaufen, auf Deutsch vorgelesen. Dafür",
      "tokens": [
        51026,
        286,
        39153,
        1028,
        761,
        1652,
        7864,
        45022,
        3524,
        11,
        7131,
        1441,
        1016,
        6282,
        25684,
        4087,
        20748,
        11,
        2501,
        12699,
        4245,
        70,
        6972,
        268,
        13,
        35865,
        51580
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29607436060905457,
      "compression_ratio": 1.399999976158142,
      "no_speech_prob": 0.0008423306862823665
    },
    {
      "id": 186,
      "seek": 104592,
      "start": 2272.6800000000003,
      "end": 2278.2000195312503,
      "text": " hatten wir nicht so ein Halluzinationsproblem. Also es ist ein Schwachsinn. Mature Enough",
      "tokens": [
        50368,
        20441,
        1987,
        1979,
        370,
        1343,
        5434,
        3334,
        10325,
        47419,
        13,
        2743,
        785,
        1418,
        1343,
        17576,
        608,
        82,
        7729,
        13,
        376,
        1503,
        19401,
        50644
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2554728090763092,
      "compression_ratio": 1.5945945978164673,
      "no_speech_prob": 0.06172173470258713
    },
    {
      "id": 187,
      "seek": 104592,
      "start": 2278.2000195312503,
      "end": 2282.560004882813,
      "text": " Technology heißt nicht, dass jedes Problem gelöst ist, sondern Mature Technology heißt,",
      "tokens": [
        50644,
        15037,
        13139,
        1979,
        11,
        2658,
        36119,
        11676,
        4087,
        36995,
        1418,
        11,
        11465,
        376,
        1503,
        15037,
        13139,
        11,
        50862
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2554728090763092,
      "compression_ratio": 1.5945945978164673,
      "no_speech_prob": 0.06172173470258713
    },
    {
      "id": 188,
      "seek": 104592,
      "start": 2282.560004882813,
      "end": 2287.560004882813,
      "text": " dass du die Limitierungen im Griff hast und Lösungen findest, die außerhalb der Technologie",
      "tokens": [
        50862,
        2658,
        1581,
        978,
        16406,
        270,
        811,
        5084,
        566,
        23765,
        6581,
        674,
        34642,
        5084,
        915,
        377,
        11,
        978,
        39428,
        13209,
        1163,
        8337,
        20121,
        51112
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2554728090763092,
      "compression_ratio": 1.5945945978164673,
      "no_speech_prob": 0.06172173470258713
    },
    {
      "id": 189,
      "seek": 104592,
      "start": 2287.560004882813,
      "end": 2294.1599804687503,
      "text": " lehrt. Und dann kannst du damit definitiv einen geilen Move machen. Wenn du es andersrum versuchst",
      "tokens": [
        51112,
        476,
        1703,
        83,
        13,
        2719,
        3594,
        20853,
        1581,
        9479,
        28781,
        592,
        4891,
        1519,
        17471,
        10475,
        7069,
        13,
        7899,
        1581,
        785,
        17999,
        6247,
        1774,
        625,
        372,
        51442
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2554728090763092,
      "compression_ratio": 1.5945945978164673,
      "no_speech_prob": 0.06172173470258713
    },
    {
      "id": 190,
      "seek": 104592,
      "start": 2294.1599804687503,
      "end": 2300.560004882813,
      "text": " und irgendwas wartest und insbesondere deine eigenen Fähigkeiten, dein eigenes Wissen anzündest,",
      "tokens": [
        51442,
        674,
        47090,
        45124,
        377,
        674,
        48694,
        28395,
        28702,
        479,
        6860,
        37545,
        11,
        25641,
        10446,
        279,
        343,
        10987,
        364,
        89,
        9541,
        377,
        11,
        51762
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2554728090763092,
      "compression_ratio": 1.5945945978164673,
      "no_speech_prob": 0.06172173470258713
    },
    {
      "id": 191,
      "seek": 107388,
      "start": 2300.560004882813,
      "end": 2309.840034179688,
      "text": " dann ist das egal. Du hast ja jetzt gesagt, Kontextmanagement ist schon ein großes Thema.",
      "tokens": [
        50364,
        3594,
        1418,
        1482,
        31528,
        13,
        5153,
        6581,
        2784,
        4354,
        12260,
        11,
        20629,
        3828,
        1601,
        11129,
        1418,
        4981,
        1343,
        48875,
        16306,
        13,
        50828
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37143683433532715,
      "compression_ratio": 1.5750000476837158,
      "no_speech_prob": 0.059104908257722855
    },
    {
      "id": 192,
      "seek": 107388,
      "start": 2309.840034179688,
      "end": 2318.080024414063,
      "text": " Ich sehe immer noch, dass man sagt, Rack dranhängen und dann läuft das. Das ist so die einfache",
      "tokens": [
        50828,
        3141,
        35995,
        5578,
        3514,
        11,
        2658,
        587,
        15764,
        11,
        497,
        501,
        32801,
        34591,
        268,
        674,
        3594,
        31807,
        1482,
        13,
        2846,
        1418,
        370,
        978,
        38627,
        6000,
        51240
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37143683433532715,
      "compression_ratio": 1.5750000476837158,
      "no_speech_prob": 0.059104908257722855
    },
    {
      "id": 193,
      "seek": 107388,
      "start": 2318.080024414063,
      "end": 2322.2400585937503,
      "text": " Version. Ja, ich nehme eine Vektordatenbank, aber wenn ich das jetzt so richtig sehe,",
      "tokens": [
        51240,
        35965,
        13,
        3530,
        11,
        1893,
        48276,
        3018,
        691,
        8192,
        765,
        7186,
        25423,
        11,
        4340,
        4797,
        1893,
        1482,
        4354,
        370,
        13129,
        35995,
        11,
        51448
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37143683433532715,
      "compression_ratio": 1.5750000476837158,
      "no_speech_prob": 0.059104908257722855
    },
    {
      "id": 194,
      "seek": 107388,
      "start": 2322.2400585937503,
      "end": 2327.560004882813,
      "text": " ist da viel mehr dahinter, auf was man achten muss. Ja und Rack ist keine einzelne Vektordatenbank. Ich",
      "tokens": [
        51448,
        1418,
        1120,
        5891,
        5417,
        16800,
        5106,
        11,
        2501,
        390,
        587,
        2800,
        1147,
        6425,
        13,
        3530,
        674,
        497,
        501,
        1418,
        9252,
        36731,
        716,
        691,
        8192,
        765,
        7186,
        25423,
        13,
        3141,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.37143683433532715,
      "compression_ratio": 1.5750000476837158,
      "no_speech_prob": 0.059104908257722855
    },
    {
      "id": 195,
      "seek": 110088,
      "start": 2327.560004882813,
      "end": 2330.759956054688,
      "text": " weiß gar nicht, wo dieser Mythos herkommt, dass ein Rack immer eine Vektordatenbank ist. Freunde,",
      "tokens": [
        50364,
        13385,
        3691,
        1979,
        11,
        6020,
        9053,
        26371,
        329,
        720,
        74,
        22230,
        11,
        2658,
        1343,
        497,
        501,
        5578,
        3018,
        691,
        8192,
        765,
        7186,
        25423,
        1418,
        13,
        40016,
        11,
        50524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3283846378326416,
      "compression_ratio": 1.618181824684143,
      "no_speech_prob": 0.1621880829334259
    },
    {
      "id": 196,
      "seek": 110088,
      "start": 2330.759956054688,
      "end": 2336.9199902343753,
      "text": " reichen Sie bitte diesen Zusammenhang. Aber der ist ja irgendwie vorhanden, oder? Ja,",
      "tokens": [
        50524,
        319,
        18613,
        3559,
        23231,
        12862,
        29442,
        23850,
        13,
        5992,
        1163,
        1418,
        2784,
        20759,
        4245,
        5543,
        268,
        11,
        4513,
        30,
        3530,
        11,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3283846378326416,
      "compression_ratio": 1.618181824684143,
      "no_speech_prob": 0.1621880829334259
    },
    {
      "id": 197,
      "seek": 110088,
      "start": 2336.9199902343753,
      "end": 2342.2400585937503,
      "text": " ich sage auch manchmal ja und dann nehmen wir halt einen Rack und definieren dann,",
      "tokens": [
        50832,
        1893,
        19721,
        2168,
        32092,
        2784,
        674,
        3594,
        19905,
        1987,
        12479,
        4891,
        497,
        501,
        674,
        1561,
        5695,
        3594,
        11,
        51098
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3283846378326416,
      "compression_ratio": 1.618181824684143,
      "no_speech_prob": 0.1621880829334259
    },
    {
      "id": 198,
      "seek": 110088,
      "start": 2342.2400585937503,
      "end": 2348.039985351563,
      "text": " was auch immer unser Rack ist. Kann ja auch mehr sein als eine Vektordatenbank. Okay,",
      "tokens": [
        51098,
        390,
        2168,
        5578,
        12977,
        497,
        501,
        1418,
        13,
        29074,
        2784,
        2168,
        5417,
        6195,
        3907,
        3018,
        691,
        8192,
        765,
        7186,
        25423,
        13,
        1033,
        11,
        51388
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3283846378326416,
      "compression_ratio": 1.618181824684143,
      "no_speech_prob": 0.1621880829334259
    },
    {
      "id": 199,
      "seek": 110088,
      "start": 2348.039985351563,
      "end": 2355.7200390625003,
      "text": " aber ich sehe schon, ich habe da den richtigen Punkt aus Versehen getriggert. Ja, also gut,",
      "tokens": [
        51388,
        4340,
        1893,
        35995,
        4981,
        11,
        1893,
        6015,
        1120,
        1441,
        22136,
        3213,
        25487,
        3437,
        4281,
        27750,
        483,
        81,
        6249,
        911,
        13,
        3530,
        11,
        611,
        5228,
        11,
        51772
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3283846378326416,
      "compression_ratio": 1.618181824684143,
      "no_speech_prob": 0.1621880829334259
    },
    {
      "id": 200,
      "seek": 112904,
      "start": 2355.7200390625003,
      "end": 2360.360053710938,
      "text": " dann nehmen wir noch eine Grafdatenbank. Ein Beispiel und wer weiß, was uns da sonst noch",
      "tokens": [
        50364,
        3594,
        19905,
        1987,
        3514,
        3018,
        8985,
        69,
        67,
        7186,
        25423,
        13,
        6391,
        13772,
        674,
        2612,
        13385,
        11,
        390,
        2693,
        1120,
        26309,
        3514,
        50596
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2817460298538208,
      "compression_ratio": 1.5816326141357422,
      "no_speech_prob": 0.005384238436818123
    },
    {
      "id": 201,
      "seek": 112904,
      "start": 2360.360053710938,
      "end": 2366.3999707031253,
      "text": " so alles einfällt. Genau, aber das ist dann ein Punkt, wo man jetzt echt kreativ werden kann,",
      "tokens": [
        50596,
        370,
        7874,
        38627,
        25333,
        13,
        22340,
        11,
        4340,
        1482,
        1418,
        3594,
        1343,
        25487,
        11,
        6020,
        587,
        4354,
        13972,
        350,
        620,
        592,
        4604,
        4028,
        11,
        50898
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2817460298538208,
      "compression_ratio": 1.5816326141357422,
      "no_speech_prob": 0.005384238436818123
    },
    {
      "id": 202,
      "seek": 112904,
      "start": 2366.3999707031253,
      "end": 2373.039985351563,
      "text": " oder? Dass man eben die richtigen Informationen zum richtigen Zeitpunkt hinzufügt, dass eben der",
      "tokens": [
        50898,
        4513,
        30,
        22306,
        587,
        11375,
        978,
        22136,
        3213,
        46753,
        5919,
        22136,
        3213,
        9394,
        31744,
        14102,
        39467,
        774,
        10463,
        11,
        2658,
        11375,
        1163,
        51230
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2817460298538208,
      "compression_ratio": 1.5816326141357422,
      "no_speech_prob": 0.005384238436818123
    },
    {
      "id": 203,
      "seek": 112904,
      "start": 2373.039985351563,
      "end": 2380.4800488281253,
      "text": " Kontext analysiert wird und eben das richtige Wissen augmentet. Genau, also im Prinzip, dass",
      "tokens": [
        51230,
        20629,
        3828,
        23014,
        4859,
        4578,
        674,
        11375,
        1482,
        41569,
        343,
        10987,
        1609,
        10433,
        302,
        13,
        22340,
        11,
        611,
        566,
        47572,
        11,
        2658,
        51602
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2817460298538208,
      "compression_ratio": 1.5816326141357422,
      "no_speech_prob": 0.005384238436818123
    },
    {
      "id": 204,
      "seek": 112904,
      "start": 2380.4800488281253,
      "end": 2385.279975585938,
      "text": " du halt weißt, die Limitierungen, die in der Technologie leben und es hat kein sauberes",
      "tokens": [
        51602,
        1581,
        12479,
        321,
        11539,
        11,
        978,
        16406,
        270,
        811,
        5084,
        11,
        978,
        294,
        1163,
        8337,
        20121,
        26392,
        674,
        785,
        2385,
        13424,
        601,
        10261,
        279,
        51842
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2817460298538208,
      "compression_ratio": 1.5816326141357422,
      "no_speech_prob": 0.005384238436818123
    },
    {
      "id": 205,
      "seek": 115860,
      "start": 2385.320014648438,
      "end": 2390.600043945313,
      "text": " Gedächtnis. Es hat ja nur ein Arbeitsgedächtnis maximal im Chat. Es hat kein mittel- und",
      "tokens": [
        50366,
        28166,
        737,
        4701,
        10661,
        13,
        2313,
        2385,
        2784,
        4343,
        1343,
        23262,
        3004,
        737,
        4701,
        10661,
        49336,
        566,
        27503,
        13,
        2313,
        2385,
        13424,
        19130,
        338,
        12,
        674,
        50630
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2671474814414978,
      "compression_ratio": 1.6572327613830566,
      "no_speech_prob": 0.0015480159781873226
    },
    {
      "id": 206,
      "seek": 115860,
      "start": 2390.600043945313,
      "end": 2395.039985351563,
      "text": " langfristiges Gedächtnis. Es kann kein Transfer-Learning, also von Transfer von der einen",
      "tokens": [
        50630,
        2265,
        48754,
        20609,
        28166,
        737,
        4701,
        10661,
        13,
        2313,
        4028,
        13424,
        35025,
        12,
        11020,
        2341,
        11,
        611,
        2957,
        35025,
        2957,
        1163,
        4891,
        50852
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2671474814414978,
      "compression_ratio": 1.6572327613830566,
      "no_speech_prob": 0.0015480159781873226
    },
    {
      "id": 207,
      "seek": 115860,
      "start": 2395.039985351563,
      "end": 2400.799995117188,
      "text": " auf die andere und es kann nicht überkontextualisieren. Und dann muss ich halt das",
      "tokens": [
        50852,
        2501,
        978,
        10490,
        674,
        785,
        4028,
        1979,
        4502,
        74,
        896,
        3828,
        901,
        271,
        5695,
        13,
        2719,
        3594,
        6425,
        1893,
        12479,
        1482,
        51140
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2671474814414978,
      "compression_ratio": 1.6572327613830566,
      "no_speech_prob": 0.0015480159781873226
    },
    {
      "id": 208,
      "seek": 115860,
      "start": 2400.799995117188,
      "end": 2405.080024414063,
      "text": " eventuell lösen. Und solange heißt das, das kann auch noch relativ viel Human-in-the-Loop",
      "tokens": [
        51140,
        2280,
        13789,
        25209,
        6748,
        13,
        2719,
        1404,
        933,
        13139,
        1482,
        11,
        1482,
        4028,
        2168,
        3514,
        21960,
        5891,
        10294,
        12,
        259,
        12,
        3322,
        12,
        43,
        15812,
        51354
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2671474814414978,
      "compression_ratio": 1.6572327613830566,
      "no_speech_prob": 0.0015480159781873226
    },
    {
      "id": 209,
      "seek": 115860,
      "start": 2405.080024414063,
      "end": 2408.4800488281253,
      "text": " bedeuten. Das kann ganz andere Techniken involvieren und das ist ja eigentlich der",
      "tokens": [
        51354,
        22466,
        7886,
        13,
        2846,
        4028,
        6312,
        10490,
        8337,
        19640,
        2499,
        85,
        5695,
        674,
        1482,
        1418,
        2784,
        10926,
        1163,
        51524
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2671474814414978,
      "compression_ratio": 1.6572327613830566,
      "no_speech_prob": 0.0015480159781873226
    },
    {
      "id": 210,
      "seek": 115860,
      "start": 2408.4800488281253,
      "end": 2412.560004882813,
      "text": " Spaß dran. Wir reden ja sonst auch in den klassischen anderen Varianten immer von dem",
      "tokens": [
        51524,
        27460,
        32801,
        13,
        4347,
        26447,
        2784,
        26309,
        2168,
        294,
        1441,
        42917,
        6282,
        11122,
        32511,
        29646,
        5578,
        2957,
        1371,
        51728
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2671474814414978,
      "compression_ratio": 1.6572327613830566,
      "no_speech_prob": 0.0015480159781873226
    },
    {
      "id": 211,
      "seek": 118588,
      "start": 2412.560004882813,
      "end": 2417.6399609375003,
      "text": " Model Ensemble und meinen damit auch nicht nur klassische Data Models, sondern auch sehr viel",
      "tokens": [
        50364,
        17105,
        2193,
        37227,
        674,
        22738,
        9479,
        2168,
        1979,
        4343,
        42917,
        7864,
        11888,
        6583,
        1625,
        11,
        11465,
        2168,
        5499,
        5891,
        50618
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968102693557739,
      "compression_ratio": 1.6776119470596313,
      "no_speech_prob": 0.02401604875922203
    },
    {
      "id": 212,
      "seek": 118588,
      "start": 2417.6399609375003,
      "end": 2422.6399609375003,
      "text": " mehr. Daran liegt der Gag und daran liegt auch die Zukunft. Und natürlich, dafür muss sie die",
      "tokens": [
        50618,
        5417,
        13,
        7803,
        282,
        22421,
        1163,
        460,
        559,
        674,
        24520,
        22421,
        2168,
        978,
        22782,
        13,
        2719,
        8762,
        11,
        13747,
        6425,
        2804,
        978,
        50868
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968102693557739,
      "compression_ratio": 1.6776119470596313,
      "no_speech_prob": 0.02401604875922203
    },
    {
      "id": 213,
      "seek": 118588,
      "start": 2422.6399609375003,
      "end": 2426.1199414062503,
      "text": " Daten haben. Und deswegen ist es so ein bisschen idiotisch, wenn die Firmen jetzt nicht ins Tun",
      "tokens": [
        50868,
        31126,
        3084,
        13,
        2719,
        26482,
        1418,
        785,
        370,
        1343,
        10763,
        14270,
        5494,
        11,
        4797,
        978,
        28164,
        2558,
        4354,
        1979,
        1028,
        21363,
        51042
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968102693557739,
      "compression_ratio": 1.6776119470596313,
      "no_speech_prob": 0.02401604875922203
    },
    {
      "id": 214,
      "seek": 118588,
      "start": 2426.1199414062503,
      "end": 2431.1199414062503,
      "text": " kommen, denn Daten verfallen wie Schimmel bis Toastbrot. Wenn die Daten halt irgendwann zu alt",
      "tokens": [
        51042,
        11729,
        11,
        10471,
        31126,
        1306,
        24425,
        3355,
        2065,
        6753,
        338,
        7393,
        1407,
        525,
        1443,
        310,
        13,
        7899,
        978,
        31126,
        12479,
        34313,
        2164,
        4955,
        51292
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968102693557739,
      "compression_ratio": 1.6776119470596313,
      "no_speech_prob": 0.02401604875922203
    },
    {
      "id": 215,
      "seek": 118588,
      "start": 2431.1199414062503,
      "end": 2436.279975585938,
      "text": " sind, haben sie eine Relevanz verloren, weil auch das, das Thema Zeitliche, also ich bin oft",
      "tokens": [
        51292,
        3290,
        11,
        3084,
        2804,
        3018,
        1300,
        28316,
        3910,
        44884,
        11,
        7689,
        2168,
        1482,
        11,
        1482,
        16306,
        9394,
        10185,
        11,
        611,
        1893,
        5171,
        11649,
        51550
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968102693557739,
      "compression_ratio": 1.6776119470596313,
      "no_speech_prob": 0.02401604875922203
    },
    {
      "id": 216,
      "seek": 118588,
      "start": 2436.279975585938,
      "end": 2439.9199902343753,
      "text": " genug gesehen, dass eine saisonale Zeitreihenanalyse gerechnet worden ist und keiner hat",
      "tokens": [
        51550,
        33194,
        21535,
        11,
        2658,
        3018,
        601,
        2770,
        1220,
        9394,
        10271,
        2932,
        282,
        5222,
        405,
        18635,
        46248,
        14054,
        1418,
        674,
        37767,
        2385,
        51732
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2968102693557739,
      "compression_ratio": 1.6776119470596313,
      "no_speech_prob": 0.02401604875922203
    },
    {
      "id": 217,
      "seek": 121324,
      "start": 2439.9199902343753,
      "end": 2443.4800488281253,
      "text": " dran gedacht, dass unsere Saison vielleicht außerhalb der zwölf Monate liegt und solche",
      "tokens": [
        50364,
        32801,
        33296,
        11,
        2658,
        14339,
        318,
        29527,
        12547,
        39428,
        13209,
        1163,
        11873,
        13072,
        69,
        44067,
        22421,
        674,
        29813,
        50542
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29205235838890076,
      "compression_ratio": 1.5800000429153442,
      "no_speech_prob": 0.042062871158123016
    },
    {
      "id": 218,
      "seek": 121324,
      "start": 2443.4800488281253,
      "end": 2449.799995117188,
      "text": " Späße. Deswegen, Daten können halt auch schimmeln. Und wenn ich die halt jetzt nicht nutze,",
      "tokens": [
        50542,
        1738,
        737,
        11451,
        13,
        24864,
        11,
        31126,
        6310,
        12479,
        2168,
        956,
        6753,
        9878,
        13,
        2719,
        4797,
        1893,
        978,
        12479,
        4354,
        1979,
        5393,
        1381,
        11,
        50858
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29205235838890076,
      "compression_ratio": 1.5800000429153442,
      "no_speech_prob": 0.042062871158123016
    },
    {
      "id": 219,
      "seek": 121324,
      "start": 2449.799995117188,
      "end": 2454.2000195312503,
      "text": " sodass die auch aktuell und relevant bleiben, dann habe ich ja genau die andere Seite. Aber das ist",
      "tokens": [
        50858,
        15047,
        640,
        978,
        2168,
        36267,
        674,
        7340,
        24912,
        11,
        3594,
        6015,
        1893,
        2784,
        12535,
        978,
        10490,
        19748,
        13,
        5992,
        1482,
        1418,
        51078
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29205235838890076,
      "compression_ratio": 1.5800000429153442,
      "no_speech_prob": 0.042062871158123016
    },
    {
      "id": 220,
      "seek": 121324,
      "start": 2454.2000195312503,
      "end": 2458.9600292968753,
      "text": " ein großes Thema. Aber ich meine, das haben die Hersteller ja schon im Griff. Sie nehmen den",
      "tokens": [
        51078,
        1343,
        48875,
        16306,
        13,
        5992,
        1893,
        10946,
        11,
        1482,
        3084,
        978,
        3204,
        372,
        14983,
        2784,
        4981,
        566,
        23765,
        13,
        3559,
        19905,
        1441,
        51316
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29205235838890076,
      "compression_ratio": 1.5800000429153442,
      "no_speech_prob": 0.042062871158123016
    },
    {
      "id": 221,
      "seek": 121324,
      "start": 2458.9600292968753,
      "end": 2464.3999707031253,
      "text": " System Prompt und schreiben rein, wer der aktuelle Präsident ist und wer dagegen verloren hat.",
      "tokens": [
        51316,
        8910,
        15833,
        662,
        674,
        48546,
        6561,
        11,
        2612,
        1163,
        13680,
        23635,
        27513,
        1418,
        674,
        2612,
        45387,
        44884,
        2385,
        13,
        51588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.29205235838890076,
      "compression_ratio": 1.5800000429153442,
      "no_speech_prob": 0.042062871158123016
    },
    {
      "id": 222,
      "seek": 123772,
      "start": 2464.999946289063,
      "end": 2472.6800000000003,
      "text": " Echt faszinierend. Aber in Sachen Zukunft. Wir haben ja jetzt, also bei der Softwareentwicklung",
      "tokens": [
        50394,
        462,
        4701,
        283,
        19601,
        259,
        811,
        521,
        13,
        5992,
        294,
        26074,
        22782,
        13,
        4347,
        3084,
        2784,
        4354,
        11,
        611,
        4643,
        1163,
        27428,
        317,
        16038,
        17850,
        50778
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2825939357280731,
      "compression_ratio": 1.5081300735473633,
      "no_speech_prob": 0.02476339414715767
    },
    {
      "id": 223,
      "seek": 123772,
      "start": 2472.6800000000003,
      "end": 2479.2000195312503,
      "text": " nehme ich es wahr, dass ja manche Entwickler haben Zugriff auf die großen Modelle, andere,",
      "tokens": [
        50778,
        48276,
        1893,
        785,
        21628,
        11,
        2658,
        2784,
        587,
        1876,
        29397,
        1918,
        3084,
        34722,
        81,
        3661,
        2501,
        978,
        23076,
        6583,
        4434,
        11,
        10490,
        11,
        51104
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2825939357280731,
      "compression_ratio": 1.5081300735473633,
      "no_speech_prob": 0.02476339414715767
    },
    {
      "id": 224,
      "seek": 123772,
      "start": 2479.2000195312503,
      "end": 2486.519965820313,
      "text": " da heißt es, wir müssen vorsichtig sein, wo die Daten hinfließen. Wir können maximal lokale",
      "tokens": [
        51104,
        1120,
        13139,
        785,
        11,
        1987,
        9013,
        48432,
        7334,
        6195,
        11,
        6020,
        978,
        31126,
        14102,
        3423,
        40687,
        13,
        4347,
        6310,
        49336,
        43578,
        1220,
        51470
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2825939357280731,
      "compression_ratio": 1.5081300735473633,
      "no_speech_prob": 0.02476339414715767
    },
    {
      "id": 225,
      "seek": 123772,
      "start": 2486.519965820313,
      "end": 2492.600043945313,
      "text": " Modelle verwenden. Lokale Modelle, habe ich immer so das Gefühl, das ist faszinierend.",
      "tokens": [
        51470,
        6583,
        4434,
        24615,
        8896,
        13,
        46278,
        1220,
        6583,
        4434,
        11,
        6015,
        1893,
        5578,
        370,
        1482,
        29715,
        11,
        1482,
        1418,
        283,
        19601,
        259,
        811,
        521,
        13,
        51774
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2825939357280731,
      "compression_ratio": 1.5081300735473633,
      "no_speech_prob": 0.02476339414715767
    },
    {
      "id": 226,
      "seek": 126592,
      "start": 2493.600043945313,
      "end": 2502.7200390625003,
      "text": " Aber es gibt ja ein paar Ansätze, Quantisierung, Model Destillation, die Modelle verkleinern",
      "tokens": [
        50414,
        5992,
        785,
        6089,
        2784,
        1343,
        16509,
        14590,
        30179,
        11,
        26968,
        32531,
        11,
        17105,
        16339,
        373,
        399,
        11,
        978,
        6583,
        4434,
        1306,
        14677,
        259,
        1248,
        50870
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2806193232536316,
      "compression_ratio": 1.50390625,
      "no_speech_prob": 0.001454895013011992
    },
    {
      "id": 227,
      "seek": 126592,
      "start": 2502.7200390625003,
      "end": 2507.6399609375003,
      "text": " können. Aber dann bin ich wieder an dem Punkt, wo ich sage, na ja, meine Zeit ist auch kostbar. Ich",
      "tokens": [
        50870,
        6310,
        13,
        5992,
        3594,
        5171,
        1893,
        6216,
        364,
        1371,
        25487,
        11,
        6020,
        1893,
        19721,
        11,
        1667,
        2784,
        11,
        10946,
        9394,
        1418,
        2168,
        27183,
        5356,
        13,
        3141,
        51116
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2806193232536316,
      "compression_ratio": 1.50390625,
      "no_speech_prob": 0.001454895013011992
    },
    {
      "id": 228,
      "seek": 126592,
      "start": 2507.6399609375003,
      "end": 2512.2400585937503,
      "text": " weiß nicht, wie gut die noch sind. Gibt es da, also ich meine, da gibt es die Benchmarks, aber.",
      "tokens": [
        51116,
        13385,
        1979,
        11,
        3355,
        5228,
        978,
        3514,
        3290,
        13,
        460,
        13651,
        785,
        1120,
        11,
        611,
        1893,
        10946,
        11,
        1120,
        6089,
        785,
        978,
        3964,
        339,
        37307,
        11,
        4340,
        13,
        51346
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2806193232536316,
      "compression_ratio": 1.50390625,
      "no_speech_prob": 0.001454895013011992
    },
    {
      "id": 229,
      "seek": 126592,
      "start": 2512.2400585937503,
      "end": 2519.039985351563,
      "text": " Also gut, ich meine grundsätzlich, wenn du nach dem Training quantifizierst, ist das Risiko,",
      "tokens": [
        51346,
        2743,
        5228,
        11,
        1893,
        10946,
        30886,
        82,
        33373,
        11,
        4797,
        1581,
        5168,
        1371,
        20620,
        4426,
        351,
        590,
        811,
        372,
        11,
        1418,
        1482,
        30897,
        10770,
        11,
        51686
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2806193232536316,
      "compression_ratio": 1.50390625,
      "no_speech_prob": 0.001454895013011992
    },
    {
      "id": 230,
      "seek": 129236,
      "start": 2519.039985351563,
      "end": 2525.360053710938,
      "text": " dass du wirklich Verluste hast, definitiv kleiner. Ich sage mal liebevoll, wir runden",
      "tokens": [
        50364,
        2658,
        1581,
        9696,
        4281,
        75,
        41389,
        6581,
        11,
        28781,
        592,
        39496,
        13,
        3141,
        19721,
        2806,
        31623,
        20654,
        11,
        1987,
        367,
        10028,
        50680
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35689038038253784,
      "compression_ratio": 1.6410256624221802,
      "no_speech_prob": 0.07154779881238937
    },
    {
      "id": 231,
      "seek": 129236,
      "start": 2525.360053710938,
      "end": 2531.4400097656253,
      "text": " ja auch sonst Beträge auf, weil am Ende des Tages ist ja quantisieren, ich sag mal quantifizieren,",
      "tokens": [
        50680,
        2784,
        2168,
        26309,
        6279,
        40917,
        2501,
        11,
        7689,
        669,
        15152,
        730,
        33601,
        1418,
        2784,
        4426,
        271,
        5695,
        11,
        1893,
        15274,
        2806,
        4426,
        351,
        590,
        5695,
        11,
        50984
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35689038038253784,
      "compression_ratio": 1.6410256624221802,
      "no_speech_prob": 0.07154779881238937
    },
    {
      "id": 232,
      "seek": 129236,
      "start": 2531.4400097656253,
      "end": 2535.1199414062503,
      "text": " quantisieren, da kommt das Fallens durch, quantisieren ja auch nichts anderes, als dass",
      "tokens": [
        50984,
        4426,
        271,
        5695,
        11,
        1120,
        10047,
        1482,
        7465,
        694,
        7131,
        11,
        4426,
        271,
        5695,
        2784,
        2168,
        13004,
        31426,
        11,
        3907,
        2658,
        51168
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35689038038253784,
      "compression_ratio": 1.6410256624221802,
      "no_speech_prob": 0.07154779881238937
    },
    {
      "id": 233,
      "seek": 129236,
      "start": 2535.1199414062503,
      "end": 2540.320014648438,
      "text": " wir die Nachkommastellen glatt ziehen. Das ist doch, wenn ich es richtig verstanden habe,",
      "tokens": [
        51168,
        1987,
        978,
        11815,
        74,
        1204,
        525,
        8581,
        1563,
        1591,
        40645,
        13,
        2846,
        1418,
        9243,
        11,
        4797,
        1893,
        785,
        13129,
        1306,
        33946,
        6015,
        11,
        51428
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35689038038253784,
      "compression_ratio": 1.6410256624221802,
      "no_speech_prob": 0.07154779881238937
    },
    {
      "id": 234,
      "seek": 129236,
      "start": 2540.320014648438,
      "end": 2545.759956054688,
      "text": " ich nehme zum Beispiel anstatt 16-Bit Floating Point, 8-Bit Floating Point, richtig?",
      "tokens": [
        51428,
        1893,
        48276,
        5919,
        13772,
        364,
        372,
        1591,
        3165,
        12,
        33,
        270,
        15153,
        990,
        12387,
        11,
        1649,
        12,
        33,
        270,
        15153,
        990,
        12387,
        11,
        13129,
        30,
        51700
      ],
      "temperature": 0.0,
      "avg_logprob": -0.35689038038253784,
      "compression_ratio": 1.6410256624221802,
      "no_speech_prob": 0.07154779881238937
    },
    {
      "id": 235,
      "seek": 131908,
      "start": 2545.759956054688,
      "end": 2552.6399609375003,
      "text": " Genau. Du kannst entweder in Floating bleiben, also in Gleitkommazahlen, aber du kannst auch",
      "tokens": [
        50364,
        22340,
        13,
        5153,
        20853,
        948,
        49070,
        294,
        15153,
        990,
        24912,
        11,
        611,
        294,
        460,
        306,
        270,
        74,
        1204,
        921,
        21128,
        11,
        4340,
        1581,
        20853,
        2168,
        50708
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25310859084129333,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.006094906944781542
    },
    {
      "id": 236,
      "seek": 131908,
      "start": 2552.6399609375003,
      "end": 2556.560004882813,
      "text": " in Ganzzahlen wandeln. Also es gibt ein paar Quantisierungsmethoden und so kommt auch immer",
      "tokens": [
        50708,
        294,
        32496,
        89,
        21128,
        14304,
        9878,
        13,
        2743,
        785,
        6089,
        1343,
        16509,
        26968,
        271,
        40908,
        5537,
        2926,
        268,
        674,
        370,
        10047,
        2168,
        5578,
        50904
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25310859084129333,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.006094906944781542
    },
    {
      "id": 237,
      "seek": 131908,
      "start": 2556.560004882813,
      "end": 2558.840034179688,
      "text": " mal ein schlechter Spruch zusammen, dann killen wir halt die Nachkommastellen. Ja,",
      "tokens": [
        50904,
        2806,
        1343,
        22664,
        26690,
        7702,
        625,
        14311,
        11,
        3594,
        1961,
        268,
        1987,
        12479,
        978,
        11815,
        74,
        1204,
        525,
        8581,
        13,
        3530,
        11,
        51018
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25310859084129333,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.006094906944781542
    },
    {
      "id": 238,
      "seek": 131908,
      "start": 2558.840034179688,
      "end": 2563.6399609375003,
      "text": " ich weiß, dass eine symmetrische Quantisierung und eine asymmetrische Quantisierung anders ist,",
      "tokens": [
        51018,
        1893,
        13385,
        11,
        2658,
        3018,
        14232,
        302,
        5714,
        1876,
        26968,
        32531,
        674,
        3018,
        37277,
        302,
        5714,
        1876,
        26968,
        32531,
        17999,
        1418,
        11,
        51258
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25310859084129333,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.006094906944781542
    },
    {
      "id": 239,
      "seek": 131908,
      "start": 2563.6399609375003,
      "end": 2571.1599804687503,
      "text": " aber so ist es einfacher erklärbar. Aber das ist im Prinzip, gehst du von 32 von mir aus auf 8 und",
      "tokens": [
        51258,
        4340,
        370,
        1418,
        785,
        38627,
        4062,
        27570,
        2713,
        5356,
        13,
        5992,
        1482,
        1418,
        566,
        47572,
        11,
        13218,
        372,
        1581,
        2957,
        8858,
        2957,
        3149,
        3437,
        2501,
        1649,
        674,
        51634
      ],
      "temperature": 0.0,
      "avg_logprob": -0.25310859084129333,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.006094906944781542
    },
    {
      "id": 240,
      "seek": 134448,
      "start": 2571.1599804687503,
      "end": 2576.759956054688,
      "text": " du entscheidest dich, wie viel niedrigere Präzision kann dein Use Case tragen und das ist",
      "tokens": [
        50364,
        1581,
        28398,
        327,
        377,
        10390,
        11,
        3355,
        5891,
        32488,
        7065,
        323,
        2114,
        737,
        89,
        1991,
        4028,
        25641,
        8278,
        17791,
        44737,
        674,
        1482,
        1418,
        50644
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23329326510429382,
      "compression_ratio": 1.6666666269302368,
      "no_speech_prob": 0.10643183439970016
    },
    {
      "id": 241,
      "seek": 134448,
      "start": 2576.759956054688,
      "end": 2581.6399609375003,
      "text": " im Prinzip oder wie sehr musst du niedrige Präzision tragen können, weil deine Architektur",
      "tokens": [
        50644,
        566,
        47572,
        4513,
        3355,
        5499,
        31716,
        1581,
        32488,
        81,
        3969,
        2114,
        737,
        89,
        1991,
        44737,
        6310,
        11,
        7689,
        28395,
        10984,
        642,
        2320,
        374,
        50888
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23329326510429382,
      "compression_ratio": 1.6666666269302368,
      "no_speech_prob": 0.10643183439970016
    },
    {
      "id": 242,
      "seek": 134448,
      "start": 2581.6399609375003,
      "end": 2587.759956054688,
      "text": " es nicht anders hergibt. Das ist im Prinzip das, was damit gemeint ist. Und für viele Use Cases",
      "tokens": [
        50888,
        785,
        1979,
        17999,
        720,
        70,
        13651,
        13,
        2846,
        1418,
        566,
        47572,
        1482,
        11,
        390,
        9479,
        18111,
        686,
        1418,
        13,
        2719,
        2959,
        9693,
        8278,
        383,
        1957,
        51194
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23329326510429382,
      "compression_ratio": 1.6666666269302368,
      "no_speech_prob": 0.10643183439970016
    },
    {
      "id": 243,
      "seek": 134448,
      "start": 2587.759956054688,
      "end": 2593.1599804687503,
      "text": " und zurück zu, es kommt halt auf den Case drauf an, kann es super stabil und super effizient sein",
      "tokens": [
        51194,
        674,
        15089,
        2164,
        11,
        785,
        10047,
        12479,
        2501,
        1441,
        17791,
        22763,
        364,
        11,
        4028,
        785,
        1687,
        11652,
        674,
        1687,
        1244,
        590,
        1196,
        6195,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23329326510429382,
      "compression_ratio": 1.6666666269302368,
      "no_speech_prob": 0.10643183439970016
    },
    {
      "id": 244,
      "seek": 134448,
      "start": 2593.1599804687503,
      "end": 2600.360053710938,
      "text": " und du brauchst gar nicht mehr diese Riesenmodelle. Du darfst ja nicht vergessen, dass wir aus einer",
      "tokens": [
        51464,
        674,
        1581,
        45522,
        372,
        3691,
        1979,
        5417,
        6705,
        497,
        30383,
        8014,
        4434,
        13,
        5153,
        19374,
        372,
        2784,
        1979,
        42418,
        11,
        2658,
        1987,
        3437,
        6850,
        51824
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23329326510429382,
      "compression_ratio": 1.6666666269302368,
      "no_speech_prob": 0.10643183439970016
    },
    {
      "id": 245,
      "seek": 137368,
      "start": 2600.360053710938,
      "end": 2606.320014648438,
      "text": " Welt kommen, wo wir von einem 110 Millionen oder 115 Millionen Parameter auf 1,5 Milliarden",
      "tokens": [
        50364,
        14761,
        11729,
        11,
        6020,
        1987,
        2957,
        6827,
        20154,
        26096,
        4513,
        39436,
        26096,
        34882,
        2398,
        2501,
        502,
        11,
        20,
        44784,
        50662
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27766793966293335,
      "compression_ratio": 1.6224489212036133,
      "no_speech_prob": 0.04267134889960289
    },
    {
      "id": 246,
      "seek": 137368,
      "start": 2606.320014648438,
      "end": 2610.320014648438,
      "text": " gesprungen sind und alle ziemlich dumm aus der Wäsche geguckt haben. Und jetzt sind wir ja",
      "tokens": [
        50662,
        5019,
        1424,
        5084,
        3290,
        674,
        5430,
        28901,
        16784,
        76,
        3437,
        1163,
        343,
        737,
        12287,
        23982,
        47800,
        3084,
        13,
        2719,
        4354,
        3290,
        1987,
        2784,
        50862
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27766793966293335,
      "compression_ratio": 1.6224489212036133,
      "no_speech_prob": 0.04267134889960289
    },
    {
      "id": 247,
      "seek": 137368,
      "start": 2610.320014648438,
      "end": 2614.759956054688,
      "text": " nochmal, ich weiß gar nicht, wie viele Milliarden und Trillionen, Milliarden Parameter wir",
      "tokens": [
        50862,
        26509,
        11,
        1893,
        13385,
        3691,
        1979,
        11,
        3355,
        9693,
        44784,
        674,
        1765,
        11836,
        268,
        11,
        44784,
        34882,
        2398,
        1987,
        51084
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27766793966293335,
      "compression_ratio": 1.6224489212036133,
      "no_speech_prob": 0.04267134889960289
    },
    {
      "id": 248,
      "seek": 137368,
      "start": 2614.759956054688,
      "end": 2620.1599804687503,
      "text": " zwischenzeitlich sind. Und da muss man halt sagen, dass das für manche Fachbereiche einfach overkill,",
      "tokens": [
        51084,
        19875,
        13712,
        1739,
        3290,
        13,
        2719,
        1120,
        6425,
        587,
        12479,
        8360,
        11,
        2658,
        1482,
        2959,
        587,
        1876,
        38213,
        18582,
        9304,
        7281,
        670,
        34213,
        11,
        51354
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27766793966293335,
      "compression_ratio": 1.6224489212036133,
      "no_speech_prob": 0.04267134889960289
    },
    {
      "id": 249,
      "seek": 137368,
      "start": 2620.1599804687503,
      "end": 2626.3999707031253,
      "text": " zurück zur Spezialisierung. Und das ist das, wo du in Probleme läufst, dass wenn du ein Frontier",
      "tokens": [
        51354,
        15089,
        7147,
        3550,
        17787,
        32531,
        13,
        2719,
        1482,
        1418,
        1482,
        11,
        6020,
        1581,
        294,
        32891,
        8235,
        2947,
        372,
        11,
        2658,
        4797,
        1581,
        1343,
        17348,
        811,
        51666
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27766793966293335,
      "compression_ratio": 1.6224489212036133,
      "no_speech_prob": 0.04267134889960289
    },
    {
      "id": 250,
      "seek": 139972,
      "start": 2626.519965820313,
      "end": 2632.1599804687503,
      "text": " Lab Model, was ja irgendwie alles kann, mit einem kleiner gemachten, quantisierten,",
      "tokens": [
        50370,
        10137,
        17105,
        11,
        390,
        2784,
        20759,
        7874,
        4028,
        11,
        2194,
        6827,
        39496,
        12293,
        268,
        11,
        4426,
        271,
        29632,
        11,
        50652
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26356127858161926,
      "compression_ratio": 1.5544217824935913,
      "no_speech_prob": 0.12036190181970596
    },
    {
      "id": 251,
      "seek": 139972,
      "start": 2632.1599804687503,
      "end": 2637.1599804687503,
      "text": " sonst irgendwie aufgesetzten On-Prem-Modell vergleichst, was auch alles kann, was aber",
      "tokens": [
        50652,
        26309,
        20759,
        2501,
        33506,
        1147,
        1282,
        12,
        47,
        2579,
        12,
        44,
        378,
        898,
        20209,
        8445,
        372,
        11,
        390,
        2168,
        7874,
        4028,
        11,
        390,
        4340,
        50902
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26356127858161926,
      "compression_ratio": 1.5544217824935913,
      "no_speech_prob": 0.12036190181970596
    },
    {
      "id": 252,
      "seek": 139972,
      "start": 2637.1599804687503,
      "end": 2641.6399609375003,
      "text": " meistens eine schlechte Idee ist, weil eigentlich sollte das für speziellere Fälle, also dass dieses",
      "tokens": [
        50902,
        36894,
        694,
        3018,
        22664,
        10553,
        32651,
        1418,
        11,
        7689,
        10926,
        18042,
        1482,
        2959,
        48682,
        285,
        323,
        479,
        31447,
        11,
        611,
        2658,
        12113,
        51126
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26356127858161926,
      "compression_ratio": 1.5544217824935913,
      "no_speech_prob": 0.12036190181970596
    },
    {
      "id": 253,
      "seek": 139972,
      "start": 2641.6399609375003,
      "end": 2647.560004882813,
      "text": " Modell dann halt ein oder drei Tätigkeiten abdeckt und nicht 35. Stell dir vor, ihr würdet",
      "tokens": [
        51126,
        6583,
        898,
        3594,
        12479,
        1343,
        4513,
        16809,
        314,
        3628,
        37545,
        410,
        1479,
        19951,
        674,
        1979,
        6976,
        13,
        37364,
        4746,
        4245,
        11,
        5553,
        9195,
        17863,
        51422
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26356127858161926,
      "compression_ratio": 1.5544217824935913,
      "no_speech_prob": 0.12036190181970596
    },
    {
      "id": 254,
      "seek": 139972,
      "start": 2647.560004882813,
      "end": 2651.4800488281253,
      "text": " In-House zum Beispiel sagen, okay, wir gehen das jetzt mal an. Dann würdest du natürlich",
      "tokens": [
        51422,
        682,
        12,
        39,
        1316,
        5919,
        13772,
        8360,
        11,
        1392,
        11,
        1987,
        13230,
        1482,
        4354,
        2806,
        364,
        13,
        7455,
        9195,
        23748,
        1581,
        8762,
        51618
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26356127858161926,
      "compression_ratio": 1.5544217824935913,
      "no_speech_prob": 0.12036190181970596
    },
    {
      "id": 255,
      "seek": 142480,
      "start": 2651.799995117188,
      "end": 2657.4800488281253,
      "text": " für das Dev eine eigene Struktur aufbauen, weil dann würdest du darauf alles optimieren. Und das",
      "tokens": [
        50380,
        2959,
        1482,
        9096,
        3018,
        38549,
        745,
        31543,
        2501,
        65,
        11715,
        11,
        7689,
        3594,
        9195,
        23748,
        1581,
        18654,
        7874,
        5028,
        5695,
        13,
        2719,
        1482,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2531193792819977,
      "compression_ratio": 1.6714285612106323,
      "no_speech_prob": 0.06179102510213852
    },
    {
      "id": 256,
      "seek": 142480,
      "start": 2657.4800488281253,
      "end": 2662.6399609375003,
      "text": " kann eine andere Variante sein, als wenn du sagst, okay, wir wollen aber auch Finance",
      "tokens": [
        50664,
        4028,
        3018,
        10490,
        32511,
        2879,
        6195,
        11,
        3907,
        4797,
        1581,
        15274,
        372,
        11,
        1392,
        11,
        1987,
        11253,
        4340,
        2168,
        25765,
        50922
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2531193792819977,
      "compression_ratio": 1.6714285612106323,
      "no_speech_prob": 0.06179102510213852
    },
    {
      "id": 257,
      "seek": 142480,
      "start": 2662.6399609375003,
      "end": 2667.4400097656253,
      "text": " enablen und dann würdest du wieder eine andere Variante nehmen. Und das ist das, wo wir heute",
      "tokens": [
        50922,
        9528,
        77,
        674,
        3594,
        9195,
        23748,
        1581,
        6216,
        3018,
        10490,
        32511,
        2879,
        19905,
        13,
        2719,
        1482,
        1418,
        1482,
        11,
        6020,
        1987,
        9801,
        51162
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2531193792819977,
      "compression_ratio": 1.6714285612106323,
      "no_speech_prob": 0.06179102510213852
    },
    {
      "id": 258,
      "seek": 142480,
      "start": 2667.4400097656253,
      "end": 2673.360053710938,
      "text": " noch nicht stark genug sind. Sicherlich aber die Firmen, die diesen Weg gehen, die Führenden in",
      "tokens": [
        51162,
        3514,
        1979,
        17417,
        33194,
        3290,
        13,
        25292,
        1739,
        4340,
        978,
        28164,
        2558,
        11,
        978,
        12862,
        18919,
        13230,
        11,
        978,
        479,
        7254,
        4542,
        268,
        294,
        51458
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2531193792819977,
      "compression_ratio": 1.6714285612106323,
      "no_speech_prob": 0.06179102510213852
    },
    {
      "id": 259,
      "seek": 142480,
      "start": 2673.360053710938,
      "end": 2678.4800488281253,
      "text": " der Zukunft sein werden. Habe ich das jetzt richtig verstanden? Also ich würde ein kleines",
      "tokens": [
        51458,
        1163,
        22782,
        6195,
        4604,
        13,
        389,
        4488,
        1893,
        1482,
        4354,
        13129,
        1306,
        33946,
        30,
        2743,
        1893,
        11942,
        1343,
        9318,
        1652,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2531193792819977,
      "compression_ratio": 1.6714285612106323,
      "no_speech_prob": 0.06179102510213852
    },
    {
      "id": 260,
      "seek": 145180,
      "start": 2678.4800488281253,
      "end": 2683.2400585937503,
      "text": " Modell spezialisieren. Softwareentwicklung hört sich jetzt erst mal so an, als wäre das ein",
      "tokens": [
        50364,
        6583,
        898,
        768,
        17787,
        271,
        5695,
        13,
        27428,
        317,
        16038,
        17850,
        42243,
        3041,
        4354,
        11301,
        2806,
        370,
        364,
        11,
        3907,
        14558,
        1482,
        1343,
        50602
      ],
      "temperature": 0.0,
      "avg_logprob": -0.371029794216156,
      "compression_ratio": 1.4322580099105835,
      "no_speech_prob": 0.532707691192627
    },
    {
      "id": 261,
      "seek": 145180,
      "start": 2683.2400585937503,
      "end": 2688.1199414062503,
      "text": " riesen Bereich. Da kann ich nichts spezialisieren. Aber eigentlich, wenn ich sage, ich mache nur",
      "tokens": [
        50602,
        23932,
        268,
        26489,
        13,
        3933,
        4028,
        1893,
        13004,
        768,
        17787,
        271,
        5695,
        13,
        5992,
        10926,
        11,
        4797,
        1893,
        19721,
        11,
        1893,
        28289,
        4343,
        50846
      ],
      "temperature": 0.0,
      "avg_logprob": -0.371029794216156,
      "compression_ratio": 1.4322580099105835,
      "no_speech_prob": 0.532707691192627
    },
    {
      "id": 262,
      "seek": 145180,
      "start": 2688.1199414062503,
      "end": 2689.2400585937503,
      "text": " Python und Softwareentwicklung.",
      "tokens": [
        50846,
        15329,
        674,
        27428,
        317,
        16038,
        17850,
        13,
        50902
      ],
      "temperature": 0.0,
      "avg_logprob": -0.371029794216156,
      "compression_ratio": 1.4322580099105835,
      "no_speech_prob": 0.532707691192627
    },
    {
      "id": 0,
      "seek": 0,
      "start": 2704.19,
      "end": 2714.19,
      "text": " verkleinert nicht das Richtige, dann ist dieses Model-Destillation, wo ich ein kleineres Modell über ein Trainer-Modell anlerne, eigentlich das Richtige, ne?",
      "tokens": [
        50364,
        1306,
        14677,
        259,
        911,
        1979,
        1482,
        22659,
        3969,
        11,
        3594,
        1418,
        12113,
        17105,
        12,
        35,
        377,
        373,
        399,
        11,
        6020,
        1893,
        1343,
        39496,
        279,
        6583,
        898,
        4502,
        1343,
        48494,
        12,
        44,
        378,
        898,
        364,
        1918,
        716,
        11,
        10926,
        1482,
        22659,
        3969,
        11,
        408,
        30,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3415273427963257,
      "compression_ratio": 1.6261261701583862,
      "no_speech_prob": 0.9064913988113403
    },
    {
      "id": 1,
      "seek": 0,
      "start": 2714.19,
      "end": 2724.19,
      "text": " Genau, das ist die Student-Teacher-Variante. Also das ist ja dann, die Quantisierung ist ein Modell, Ingestion ist ein anderes Modell und dann kommen ja noch so weitere Layers des Fine-Tunings mit dazu.",
      "tokens": [
        50864,
        22340,
        11,
        1482,
        1418,
        978,
        12464,
        12,
        14233,
        4062,
        12,
        53,
        3504,
        2879,
        13,
        2743,
        1482,
        1418,
        2784,
        3594,
        11,
        978,
        26968,
        32531,
        1418,
        1343,
        6583,
        898,
        11,
        682,
        2629,
        313,
        1418,
        1343,
        31426,
        6583,
        898,
        674,
        3594,
        11729,
        2784,
        3514,
        370,
        30020,
        20084,
        433,
        730,
        12024,
        12,
        51,
        409,
        1109,
        2194,
        13034,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.3415273427963257,
      "compression_ratio": 1.6261261701583862,
      "no_speech_prob": 0.9064913988113403
    },
    {
      "id": 2,
      "seek": 2000,
      "start": 2724.19,
      "end": 2734.19,
      "text": " Aber ja, das ist sicherlich etwas, wo einfach wie gesagt nochmal, ich mach doch keine Schraube mit dem Hammer rein, nur weil ich einen Hammer rumliegen habe.",
      "tokens": [
        50364,
        5992,
        2784,
        11,
        1482,
        1418,
        18623,
        1739,
        9569,
        11,
        6020,
        7281,
        3355,
        12260,
        26509,
        11,
        1893,
        2246,
        9243,
        9252,
        2065,
        424,
        1977,
        2194,
        1371,
        33722,
        6561,
        11,
        4343,
        7689,
        1893,
        4891,
        33722,
        8347,
        6302,
        1766,
        6015,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23996518552303314,
      "compression_ratio": 1.5335688591003418,
      "no_speech_prob": 0.5531706809997559
    },
    {
      "id": 3,
      "seek": 2000,
      "start": 2734.19,
      "end": 2741.19,
      "text": " Da hab ich nachher ein Loch in der Wand. Herzlichen Glückwunsch, kann man mal machen, wird ein Provisorien, hält bei Familien wie mir lange.",
      "tokens": [
        50864,
        3933,
        3025,
        1893,
        5168,
        511,
        1343,
        49912,
        294,
        1163,
        40772,
        13,
        24749,
        10193,
        33508,
        86,
        409,
        6145,
        11,
        4028,
        587,
        2806,
        7069,
        11,
        4578,
        1343,
        1705,
        16457,
        1053,
        11,
        40751,
        4643,
        36451,
        3355,
        3149,
        18131,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23996518552303314,
      "compression_ratio": 1.5335688591003418,
      "no_speech_prob": 0.5531706809997559
    },
    {
      "id": 4,
      "seek": 2000,
      "start": 2741.19,
      "end": 2748.19,
      "text": " Und danach denke ich mir, war wohl zu faul im Keller zu gehen, im wahrsten Sinne des Wortes, weil da hätte ich es ja machen müssen.",
      "tokens": [
        51214,
        2719,
        37784,
        27245,
        1893,
        3149,
        11,
        1516,
        24531,
        2164,
        2050,
        425,
        566,
        48352,
        2164,
        13230,
        11,
        566,
        21628,
        6266,
        47041,
        730,
        22748,
        279,
        11,
        7689,
        1120,
        20041,
        1893,
        785,
        2784,
        7069,
        9013,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23996518552303314,
      "compression_ratio": 1.5335688591003418,
      "no_speech_prob": 0.5531706809997559
    },
    {
      "id": 5,
      "seek": 4400,
      "start": 2749.19,
      "end": 2758.19,
      "text": " Aber das ist heute auch erst alles möglich und das ist so ein bisschen auch so ein schwieriges Umdenken, denn wir haben eine Entwicklung in den letzten drei Jahren gesehen,",
      "tokens": [
        50414,
        5992,
        1482,
        1418,
        9801,
        2168,
        11301,
        7874,
        16294,
        674,
        1482,
        1418,
        370,
        1343,
        10763,
        2168,
        370,
        1343,
        27546,
        20609,
        3301,
        1556,
        2653,
        11,
        10471,
        1987,
        3084,
        3018,
        39654,
        294,
        1441,
        18226,
        16809,
        13080,
        21535,
        11,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20536434650421143,
      "compression_ratio": 1.7056856155395508,
      "no_speech_prob": 0.5222401022911072
    },
    {
      "id": 6,
      "seek": 4400,
      "start": 2758.19,
      "end": 2766.19,
      "text": " die einfach so unglaublich war, dass ich, weiß ich nicht, jede, glaube ich, komische Aussage, die ich im Studium mal getroffen habe, alle kassieren musste.",
      "tokens": [
        50864,
        978,
        7281,
        370,
        49087,
        1739,
        1516,
        11,
        2658,
        1893,
        11,
        13385,
        1893,
        1979,
        11,
        34039,
        11,
        13756,
        1893,
        11,
        5207,
        7864,
        21286,
        609,
        11,
        978,
        1893,
        566,
        4541,
        2197,
        2806,
        483,
        30594,
        6015,
        11,
        5430,
        350,
        640,
        5695,
        34497,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20536434650421143,
      "compression_ratio": 1.7056856155395508,
      "no_speech_prob": 0.5222401022911072
    },
    {
      "id": 7,
      "seek": 4400,
      "start": 2766.19,
      "end": 2775.19,
      "text": " Ich hab mal gesagt, haltet euch fest, können jetzt alle lachen, dass die Maschine, ich weiß nicht, ob ich zu Lebzeiten hersehen würde, dass die Maschine die Maschine trainiert.",
      "tokens": [
        51264,
        3141,
        3025,
        2806,
        12260,
        11,
        12479,
        302,
        10403,
        6633,
        11,
        6310,
        4354,
        5430,
        287,
        11646,
        11,
        2658,
        978,
        5224,
        36675,
        11,
        1893,
        13385,
        1979,
        11,
        1111,
        1893,
        2164,
        19437,
        1381,
        6009,
        720,
        27750,
        11942,
        11,
        2658,
        978,
        5224,
        36675,
        978,
        5224,
        36675,
        3847,
        4859,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20536434650421143,
      "compression_ratio": 1.7056856155395508,
      "no_speech_prob": 0.5222401022911072
    },
    {
      "id": 8,
      "seek": 7100,
      "start": 2776.19,
      "end": 2782.19,
      "text": " Über was haben wir gerade gesprochen? Student-Teacher-Mode. Na, Lampe, herzlichen Glückwunsch.",
      "tokens": [
        50414,
        18086,
        390,
        3084,
        1987,
        12117,
        42714,
        30,
        12464,
        12,
        14233,
        4062,
        12,
        44,
        1429,
        13,
        6056,
        11,
        18825,
        494,
        11,
        720,
        89,
        10193,
        33508,
        86,
        409,
        6145,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26049456000328064,
      "compression_ratio": 1.7101911306381226,
      "no_speech_prob": 0.17285335063934326
    },
    {
      "id": 9,
      "seek": 7100,
      "start": 2784.19,
      "end": 2794.19,
      "text": " Und das ist halt das, was selbst für mich irgendwo echt krass ist und jetzt heißt es, von dem groß, groß, groß, allgemein, allgemein, allgemein musst du jetzt in klein, klein, klein denken.",
      "tokens": [
        50814,
        2719,
        1482,
        1418,
        12479,
        1482,
        11,
        390,
        13053,
        2959,
        6031,
        40865,
        13972,
        15913,
        640,
        1418,
        674,
        4354,
        13139,
        785,
        11,
        2957,
        1371,
        17253,
        11,
        17253,
        11,
        17253,
        11,
        439,
        31964,
        259,
        11,
        439,
        31964,
        259,
        11,
        439,
        31964,
        259,
        31716,
        1581,
        4354,
        294,
        29231,
        11,
        29231,
        11,
        29231,
        28780,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26049456000328064,
      "compression_ratio": 1.7101911306381226,
      "no_speech_prob": 0.17285335063934326
    },
    {
      "id": 10,
      "seek": 7100,
      "start": 2794.19,
      "end": 2798.19,
      "text": " Und diesen Switch, das musst du ja auch erst mal einem Kunden erklären.",
      "tokens": [
        51314,
        2719,
        12862,
        13893,
        11,
        1482,
        31716,
        1581,
        2784,
        2168,
        11301,
        2806,
        6827,
        38192,
        46528,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26049456000328064,
      "compression_ratio": 1.7101911306381226,
      "no_speech_prob": 0.17285335063934326
    },
    {
      "id": 11,
      "seek": 7100,
      "start": 2798.19,
      "end": 2804.19,
      "text": " Stell dir vor, du sitzt bei so einem lustigen Unternehmen, wo du jetzt vielleicht schon ein bisschen länger bist, wo du sagst, jetzt führen wir das mal alles in groß ein",
      "tokens": [
        51514,
        37364,
        4746,
        4245,
        11,
        1581,
        49734,
        4643,
        370,
        6827,
        24672,
        3213,
        27577,
        11,
        6020,
        1581,
        4354,
        12547,
        4981,
        1343,
        10763,
        40935,
        18209,
        11,
        6020,
        1581,
        15274,
        372,
        11,
        4354,
        35498,
        1987,
        1482,
        2806,
        7874,
        294,
        17253,
        1343,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26049456000328064,
      "compression_ratio": 1.7101911306381226,
      "no_speech_prob": 0.17285335063934326
    },
    {
      "id": 12,
      "seek": 10000,
      "start": 2804.19,
      "end": 2810.19,
      "text": " und dann kommst du übermorgen rein. Und jetzt spezialisieren wir. Läuft.",
      "tokens": [
        50364,
        674,
        3594,
        6669,
        372,
        1581,
        4502,
        40220,
        1766,
        6561,
        13,
        2719,
        4354,
        768,
        17787,
        271,
        5695,
        1987,
        13,
        441,
        737,
        25005,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26497915387153625,
      "compression_ratio": 1.4306219816207886,
      "no_speech_prob": 0.0028890427201986313
    },
    {
      "id": 13,
      "seek": 10000,
      "start": 2810.19,
      "end": 2815.19,
      "text": " Also ich meine, hey, in dem Fall hat Deutschland alles richtig gemacht. Die haben das mit dem Generalisieren nicht mitgemacht, können wir gleich spezialisieren.",
      "tokens": [
        50664,
        2743,
        1893,
        10946,
        11,
        4177,
        11,
        294,
        1371,
        7465,
        2385,
        14802,
        7874,
        13129,
        12293,
        13,
        3229,
        3084,
        1482,
        2194,
        1371,
        6996,
        271,
        5695,
        1979,
        2194,
        26322,
        3589,
        11,
        6310,
        1987,
        11699,
        768,
        17787,
        271,
        5695,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26497915387153625,
      "compression_ratio": 1.4306219816207886,
      "no_speech_prob": 0.0028890427201986313
    },
    {
      "id": 14,
      "seek": 10000,
      "start": 2817.19,
      "end": 2823.19,
      "text": " Okay. Ja, ja, spannend. Also das heißt, wir müssen umdenken.",
      "tokens": [
        51014,
        1033,
        13,
        3530,
        11,
        2784,
        11,
        49027,
        13,
        2743,
        1482,
        13139,
        11,
        1987,
        9013,
        1105,
        1556,
        2653,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26497915387153625,
      "compression_ratio": 1.4306219816207886,
      "no_speech_prob": 0.0028890427201986313
    },
    {
      "id": 15,
      "seek": 11900,
      "start": 2824.19,
      "end": 2833.19,
      "text": " Genau. Und du hast jetzt von vielen Möglichkeiten gesprochen. Und ich habe immer so wieder das Problem, ich habe es im Vorgespräch schon gesagt.",
      "tokens": [
        50414,
        22340,
        13,
        2719,
        1581,
        6581,
        4354,
        2957,
        19885,
        42627,
        42714,
        13,
        2719,
        1893,
        6015,
        5578,
        370,
        6216,
        1482,
        11676,
        11,
        1893,
        6015,
        785,
        566,
        12231,
        2880,
        1424,
        10168,
        4981,
        12260,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18025781214237213,
      "compression_ratio": 1.8774703741073608,
      "no_speech_prob": 0.0790197104215622
    },
    {
      "id": 16,
      "seek": 11900,
      "start": 2833.19,
      "end": 2841.19,
      "text": " Ich bin so einer, der sagt, wenn es wie eine Ente aussieht, wenn es läuft wie eine Ente, wenn es schwimmt wie eine Ente, wenn es quakt wie eine Ente, dann ist es eine Ente.",
      "tokens": [
        50864,
        3141,
        5171,
        370,
        6850,
        11,
        1163,
        15764,
        11,
        4797,
        785,
        3355,
        3018,
        3951,
        68,
        5730,
        39850,
        11,
        4797,
        785,
        31807,
        3355,
        3018,
        3951,
        68,
        11,
        4797,
        785,
        17932,
        15314,
        3355,
        3018,
        3951,
        68,
        11,
        4797,
        785,
        421,
        5886,
        3355,
        3018,
        3951,
        68,
        11,
        3594,
        1418,
        785,
        3018,
        3951,
        68,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18025781214237213,
      "compression_ratio": 1.8774703741073608,
      "no_speech_prob": 0.0790197104215622
    },
    {
      "id": 17,
      "seek": 11900,
      "start": 2841.19,
      "end": 2851.19,
      "text": " Wenn es sich intelligent verhält, wenn es mir Antworten gibt, wenn es mir Lösungen, wenn es mir Probleme löst, ja, dann scheint es intelligent zu sein.",
      "tokens": [
        51264,
        7899,
        785,
        3041,
        13232,
        1306,
        28068,
        11,
        4797,
        785,
        3149,
        34693,
        268,
        6089,
        11,
        4797,
        785,
        3149,
        34642,
        5084,
        11,
        4797,
        785,
        3149,
        32891,
        25209,
        372,
        11,
        2784,
        11,
        3594,
        47906,
        785,
        13232,
        2164,
        6195,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18025781214237213,
      "compression_ratio": 1.8774703741073608,
      "no_speech_prob": 0.0790197104215622
    },
    {
      "id": 18,
      "seek": 14700,
      "start": 2851.19,
      "end": 2862.19,
      "text": " Ja, jetzt Eberhard Wolf sagt immer, ja, Elisa wurde auch schon von vielen als echt angesehen und es geht schnell, dass man einen Menschen täuschen kann.",
      "tokens": [
        50364,
        3530,
        11,
        4354,
        462,
        607,
        21491,
        16634,
        15764,
        5578,
        11,
        2784,
        11,
        2699,
        3837,
        11191,
        2168,
        4981,
        2957,
        19885,
        3907,
        13972,
        2562,
        18380,
        674,
        785,
        7095,
        17589,
        11,
        2658,
        587,
        4891,
        8397,
        14619,
        301,
        2470,
        4028,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24269984662532806,
      "compression_ratio": 1.4801762104034424,
      "no_speech_prob": 0.04596661031246185
    },
    {
      "id": 19,
      "seek": 14700,
      "start": 2862.19,
      "end": 2869.19,
      "text": " Und da kommt vielleicht auch wieder deine Psychologie zur Hilfe. Wie siehst du das?",
      "tokens": [
        50914,
        2719,
        1120,
        10047,
        12547,
        2168,
        6216,
        28395,
        17303,
        20121,
        7147,
        37448,
        13,
        9233,
        2804,
        38857,
        1581,
        1482,
        30,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24269984662532806,
      "compression_ratio": 1.4801762104034424,
      "no_speech_prob": 0.04596661031246185
    },
    {
      "id": 20,
      "seek": 14700,
      "start": 2869.19,
      "end": 2878.19,
      "text": " Oder was ich eigentlich schon seit Jahren, zwei Jahren suche, sind die Grenzen dieser Technologie.",
      "tokens": [
        51264,
        20988,
        390,
        1893,
        10926,
        4981,
        16452,
        13080,
        11,
        12002,
        13080,
        1270,
        68,
        11,
        3290,
        978,
        24913,
        2904,
        9053,
        8337,
        20121,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24269984662532806,
      "compression_ratio": 1.4801762104034424,
      "no_speech_prob": 0.04596661031246185
    },
    {
      "id": 21,
      "seek": 17400,
      "start": 2879.19,
      "end": 2881.19,
      "text": " Gibt keine.",
      "tokens": [
        50414,
        460,
        13651,
        9252,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20494486391544342,
      "compression_ratio": 1.7582417726516724,
      "no_speech_prob": 0.096457339823246
    },
    {
      "id": 22,
      "seek": 17400,
      "start": 2881.19,
      "end": 2882.19,
      "text": " Gibt keine?",
      "tokens": [
        50514,
        460,
        13651,
        9252,
        30,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20494486391544342,
      "compression_ratio": 1.7582417726516724,
      "no_speech_prob": 0.096457339823246
    },
    {
      "id": 23,
      "seek": 17400,
      "start": 2882.19,
      "end": 2889.19,
      "text": " Gibt keine Grenzen der Technologie. Ich glaube, das Problem ist, ich bezeichne diese Maschinen auch als intelligent.",
      "tokens": [
        50564,
        460,
        13651,
        9252,
        24913,
        2904,
        1163,
        8337,
        20121,
        13,
        3141,
        13756,
        11,
        1482,
        11676,
        1418,
        11,
        1893,
        312,
        32338,
        716,
        6705,
        5224,
        339,
        5636,
        2168,
        3907,
        13232,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20494486391544342,
      "compression_ratio": 1.7582417726516724,
      "no_speech_prob": 0.096457339823246
    },
    {
      "id": 24,
      "seek": 17400,
      "start": 2889.19,
      "end": 2893.19,
      "text": " Aber ich fange schon an mit, ich bezeichne die Maschine als intelligent.",
      "tokens": [
        50914,
        5992,
        1893,
        283,
        933,
        4981,
        364,
        2194,
        11,
        1893,
        312,
        32338,
        716,
        978,
        5224,
        36675,
        3907,
        13232,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20494486391544342,
      "compression_ratio": 1.7582417726516724,
      "no_speech_prob": 0.096457339823246
    },
    {
      "id": 25,
      "seek": 17400,
      "start": 2893.19,
      "end": 2901.19,
      "text": " Ich bin kein Philosoph, wobei vielleicht, ich liebe Philosophie, aber ich habe nicht Philosophie studiert.",
      "tokens": [
        51114,
        3141,
        5171,
        13424,
        31182,
        5317,
        11,
        6020,
        21845,
        12547,
        11,
        1893,
        31623,
        31182,
        5317,
        414,
        11,
        4340,
        1893,
        6015,
        1979,
        31182,
        5317,
        414,
        972,
        4859,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20494486391544342,
      "compression_ratio": 1.7582417726516724,
      "no_speech_prob": 0.096457339823246
    },
    {
      "id": 26,
      "seek": 19700,
      "start": 2902.19,
      "end": 2913.19,
      "text": " Für mich ist diese Diskussion über Intelligenz, die menschliche Intelligenz versus die Maschinenintelligenz, eine unsinnige Diskussion, weil wir könnten es doch einfach nennen, menschliche Intelligenz und Maschinenintelligenz.",
      "tokens": [
        50414,
        14990,
        6031,
        1418,
        6705,
        45963,
        313,
        4502,
        18762,
        3213,
        89,
        11,
        978,
        10923,
        339,
        10185,
        18762,
        3213,
        89,
        5717,
        978,
        5224,
        339,
        5636,
        20761,
        3213,
        89,
        11,
        3018,
        2693,
        7729,
        3969,
        45963,
        313,
        11,
        7689,
        1987,
        37411,
        785,
        9243,
        7281,
        297,
        16043,
        11,
        10923,
        339,
        10185,
        18762,
        3213,
        89,
        674,
        5224,
        339,
        5636,
        20761,
        3213,
        89,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16831307113170624,
      "compression_ratio": 1.9055943489074707,
      "no_speech_prob": 0.2652801275253296
    },
    {
      "id": 27,
      "seek": 19700,
      "start": 2913.19,
      "end": 2920.19,
      "text": " Übrigens kommt der Begriff der Artificial Intelligence genau aus dieser Idee ja irgendwo auch her, die Intelligenz, die eine Maschine abbilden kann.",
      "tokens": [
        50964,
        10713,
        21674,
        694,
        10047,
        1163,
        879,
        32783,
        1163,
        5735,
        10371,
        27274,
        12535,
        3437,
        9053,
        32651,
        2784,
        40865,
        2168,
        720,
        11,
        978,
        18762,
        3213,
        89,
        11,
        978,
        3018,
        5224,
        36675,
        410,
        16248,
        268,
        4028,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16831307113170624,
      "compression_ratio": 1.9055943489074707,
      "no_speech_prob": 0.2652801275253296
    },
    {
      "id": 28,
      "seek": 19700,
      "start": 2920.19,
      "end": 2928.19,
      "text": " Weil dann gehen wir uns einig und sagen, okay, beides ist Intelligenz, beides hat seine Grenzen, beides hat seine Features, beides hat seine Stärken und Schwächen.",
      "tokens": [
        51314,
        18665,
        3594,
        13230,
        1987,
        2693,
        1343,
        328,
        674,
        8360,
        11,
        1392,
        11,
        312,
        1875,
        1418,
        18762,
        3213,
        89,
        11,
        312,
        1875,
        2385,
        15925,
        24913,
        2904,
        11,
        312,
        1875,
        2385,
        15925,
        3697,
        3377,
        11,
        312,
        1875,
        2385,
        15925,
        745,
        2713,
        2653,
        674,
        17576,
        45118,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16831307113170624,
      "compression_ratio": 1.9055943489074707,
      "no_speech_prob": 0.2652801275253296
    },
    {
      "id": 29,
      "seek": 22400,
      "start": 2929.19,
      "end": 2934.19,
      "text": " Aber wir versuchen das eine mit dem anderen. Also ihr könnt mir mal eine menschliche Intelligenz erklären und wie das Ding da oben passiert.",
      "tokens": [
        50414,
        5992,
        1987,
        34749,
        1482,
        3018,
        2194,
        1371,
        11122,
        13,
        2743,
        5553,
        22541,
        3149,
        2806,
        3018,
        10923,
        339,
        10185,
        18762,
        3213,
        89,
        46528,
        674,
        3355,
        1482,
        20558,
        1120,
        21279,
        21671,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2706991732120514,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.011491667479276657
    },
    {
      "id": 30,
      "seek": 22400,
      "start": 2934.19,
      "end": 2940.19,
      "text": " Fragt bitte die Neuroscience-Kollegen, gucken genauso dumm, wie ich auch einen Blackbox-Algorithmus von einem Large-Language-Modell gucke und mir denke,",
      "tokens": [
        50664,
        479,
        32243,
        23231,
        978,
        1734,
        8977,
        6699,
        12,
        42,
        1833,
        8113,
        11,
        33135,
        37694,
        16784,
        76,
        11,
        3355,
        1893,
        2168,
        4891,
        4076,
        4995,
        12,
        9171,
        70,
        6819,
        18761,
        2957,
        6827,
        33092,
        12,
        43,
        656,
        20473,
        12,
        44,
        378,
        898,
        695,
        18627,
        674,
        3149,
        27245,
        11,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2706991732120514,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.011491667479276657
    },
    {
      "id": 31,
      "seek": 22400,
      "start": 2940.19,
      "end": 2945.19,
      "text": " wir sind uns in vielen Dingen hier einig, aber irgendwie wissen wir auch trotzdem nicht, was wir hier tun, oder?",
      "tokens": [
        50964,
        1987,
        3290,
        2693,
        294,
        19885,
        49351,
        3296,
        1343,
        328,
        11,
        4340,
        20759,
        16331,
        1987,
        2168,
        28325,
        1979,
        11,
        390,
        1987,
        3296,
        4267,
        11,
        4513,
        30,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2706991732120514,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.011491667479276657
    },
    {
      "id": 32,
      "seek": 22400,
      "start": 2945.19,
      "end": 2956.19,
      "text": " Das ist für mich immer so eine, das können wir gerne bei einer 13 Flaschen Rotwein über Intelligenz diskutieren, bringt mich in meinem praktischen Leben nicht weiter.",
      "tokens": [
        51214,
        2846,
        1418,
        2959,
        6031,
        5578,
        370,
        3018,
        11,
        1482,
        6310,
        1987,
        15689,
        4643,
        6850,
        3705,
        3235,
        296,
        2470,
        17681,
        826,
        259,
        4502,
        18762,
        3213,
        89,
        36760,
        5695,
        11,
        36008,
        6031,
        294,
        24171,
        33721,
        6282,
        15399,
        1979,
        8988,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2706991732120514,
      "compression_ratio": 1.6011080741882324,
      "no_speech_prob": 0.011491667479276657
    },
    {
      "id": 33,
      "seek": 25200,
      "start": 2957.19,
      "end": 2965.19,
      "text": " Und das ist genau das, aber auf der anderen Seite ziehe ich einen harten Cut bei, es quakt, es wackelt, es sieht aus wie eine Ente, es verhält sich wie eine Ente,",
      "tokens": [
        50414,
        2719,
        1482,
        1418,
        12535,
        1482,
        11,
        4340,
        2501,
        1163,
        11122,
        19748,
        16503,
        675,
        1893,
        4891,
        276,
        11719,
        9431,
        4643,
        11,
        785,
        421,
        5886,
        11,
        785,
        261,
        326,
        25798,
        11,
        785,
        14289,
        3437,
        3355,
        3018,
        3951,
        68,
        11,
        785,
        1306,
        28068,
        3041,
        3355,
        3018,
        3951,
        68,
        11,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18841664493083954,
      "compression_ratio": 1.634920597076416,
      "no_speech_prob": 0.0439724363386631
    },
    {
      "id": 34,
      "seek": 25200,
      "start": 2965.19,
      "end": 2975.19,
      "text": " aber es ist halt keine echte Ente. Es ist eine Maschine, die so tut, als wäre sie eine Ente, die aber sehr intelligent ist in ihrem Entendasein.",
      "tokens": [
        50814,
        4340,
        785,
        1418,
        12479,
        9252,
        308,
        10553,
        3951,
        68,
        13,
        2313,
        1418,
        3018,
        5224,
        36675,
        11,
        978,
        370,
        3672,
        11,
        3907,
        14558,
        2804,
        3018,
        3951,
        68,
        11,
        978,
        4340,
        5499,
        13232,
        1418,
        294,
        30859,
        3951,
        521,
        651,
        259,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18841664493083954,
      "compression_ratio": 1.634920597076416,
      "no_speech_prob": 0.0439724363386631
    },
    {
      "id": 35,
      "seek": 27100,
      "start": 2976.19,
      "end": 2984.19,
      "text": " Das ist eine nette Neudefinition des Spruchs. Okay, es sieht aus wie eine Ente, quakt wie eine Ente, ist aber keine Ente.",
      "tokens": [
        50414,
        2846,
        1418,
        3018,
        2533,
        975,
        1734,
        2303,
        5194,
        849,
        730,
        7702,
        37503,
        13,
        1033,
        11,
        785,
        14289,
        3437,
        3355,
        3018,
        3951,
        68,
        11,
        421,
        5886,
        3355,
        3018,
        3951,
        68,
        11,
        1418,
        4340,
        9252,
        3951,
        68,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.181288480758667,
      "compression_ratio": 1.4556212425231934,
      "no_speech_prob": 0.5535988211631775
    },
    {
      "id": 36,
      "seek": 27100,
      "start": 2984.19,
      "end": 2996.19,
      "text": " Ich merke ja, manche Lösungen kriege ich nicht aus der Maschine, weil sie sich im Kreis dreht, weil sie etwas falsch macht,",
      "tokens": [
        50814,
        3141,
        3551,
        330,
        2784,
        11,
        587,
        1876,
        34642,
        5084,
        25766,
        432,
        1893,
        1979,
        3437,
        1163,
        5224,
        36675,
        11,
        7689,
        2804,
        3041,
        566,
        23625,
        271,
        22540,
        357,
        11,
        7689,
        2804,
        9569,
        43340,
        10857,
        11,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.181288480758667,
      "compression_ratio": 1.4556212425231934,
      "no_speech_prob": 0.5535988211631775
    },
    {
      "id": 37,
      "seek": 29200,
      "start": 2997.19,
      "end": 3009.19,
      "text": " weil ich nicht daran gedacht habe, dass sie nicht gut über Datenmengen iterieren kann, dass ich dann besser ein MCP dran schraube, dass die Maschine Code generiert und drüber iteriert und so.",
      "tokens": [
        50414,
        7689,
        1893,
        1979,
        24520,
        33296,
        6015,
        11,
        2658,
        2804,
        1979,
        5228,
        4502,
        31126,
        76,
        1501,
        268,
        17138,
        5695,
        4028,
        11,
        2658,
        1893,
        3594,
        18021,
        1343,
        8797,
        47,
        32801,
        956,
        424,
        1977,
        11,
        2658,
        978,
        5224,
        36675,
        15549,
        1337,
        4859,
        674,
        1224,
        12670,
        17138,
        4859,
        674,
        370,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21742287278175354,
      "compression_ratio": 1.5520833730697632,
      "no_speech_prob": 0.07365012913942337
    },
    {
      "id": 38,
      "seek": 29200,
      "start": 3009.19,
      "end": 3018.19,
      "text": " Aber deine Aussage, die kam jetzt recht schnell, dass es keine Limits gibt. Und das finde ich jetzt gut.",
      "tokens": [
        51014,
        5992,
        28395,
        21286,
        609,
        11,
        978,
        9727,
        4354,
        24261,
        17589,
        11,
        2658,
        785,
        9252,
        16406,
        1208,
        6089,
        13,
        2719,
        1482,
        17841,
        1893,
        4354,
        5228,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21742287278175354,
      "compression_ratio": 1.5520833730697632,
      "no_speech_prob": 0.07365012913942337
    },
    {
      "id": 39,
      "seek": 31400,
      "start": 3018.19,
      "end": 3026.19,
      "text": " Nein, es gibt keine Limits. Das ist das, was ich den Leuten immer versuche zu sagen. AI, müsst ihr Limitless denken. Es gibt halt nur sinnvolle und unsinnige Fälle.",
      "tokens": [
        50364,
        18878,
        11,
        785,
        6089,
        9252,
        16406,
        1208,
        13,
        2846,
        1418,
        1482,
        11,
        390,
        1893,
        1441,
        42301,
        5578,
        1774,
        17545,
        2164,
        8360,
        13,
        7318,
        11,
        49481,
        5553,
        16406,
        270,
        1832,
        28780,
        13,
        2313,
        6089,
        12479,
        4343,
        47066,
        20654,
        68,
        674,
        2693,
        7729,
        3969,
        479,
        31447,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26831936836242676,
      "compression_ratio": 1.6088560819625854,
      "no_speech_prob": 0.27103501558303833
    },
    {
      "id": 40,
      "seek": 31400,
      "start": 3026.19,
      "end": 3036.19,
      "text": " Aber das Ding kann alles. Es macht halt nur nicht alles Sinn. Und es kann auch nicht alles gut. So wie wir Menschen. Ich kann schwimmen.",
      "tokens": [
        50764,
        5992,
        1482,
        20558,
        4028,
        7874,
        13,
        2313,
        10857,
        12479,
        4343,
        1979,
        7874,
        37962,
        13,
        2719,
        785,
        4028,
        2168,
        1979,
        7874,
        5228,
        13,
        407,
        3355,
        1987,
        8397,
        13,
        3141,
        4028,
        17932,
        32076,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26831936836242676,
      "compression_ratio": 1.6088560819625854,
      "no_speech_prob": 0.27103501558303833
    },
    {
      "id": 41,
      "seek": 31400,
      "start": 3036.19,
      "end": 3043.19,
      "text": " Ich möchte aber jetzt nicht bei den 100 Meter bei den Olympischen Spielen antreten müssen. Da wäre ich richtig, richtig schlecht.",
      "tokens": [
        51264,
        3141,
        14570,
        4340,
        4354,
        1979,
        4643,
        1441,
        2319,
        38054,
        4643,
        1441,
        10395,
        6282,
        1738,
        12844,
        2511,
        35383,
        9013,
        13,
        3933,
        14558,
        1893,
        13129,
        11,
        13129,
        32427,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.26831936836242676,
      "compression_ratio": 1.6088560819625854,
      "no_speech_prob": 0.27103501558303833
    },
    {
      "id": 42,
      "seek": 33900,
      "start": 3044.19,
      "end": 3052.19,
      "text": " Und das ist halt genau das. Ich habe einen Führerschein. Ich habe schon mal in einem anderen Podcast gesagt, bekanntermaßen sind ja die Deutschen die besten Autofahrer und Autofahrerinnen der Welt.",
      "tokens": [
        50414,
        2719,
        1482,
        1418,
        12479,
        12535,
        1482,
        13,
        3141,
        6015,
        4891,
        479,
        11986,
        433,
        1876,
        259,
        13,
        3141,
        6015,
        4981,
        2806,
        294,
        6827,
        11122,
        29972,
        12260,
        11,
        9393,
        969,
        391,
        1696,
        8989,
        3290,
        2784,
        978,
        45070,
        978,
        30930,
        6049,
        2670,
        5398,
        260,
        674,
        6049,
        2670,
        5398,
        260,
        11399,
        1163,
        14761,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2565270960330963,
      "compression_ratio": 1.5968992710113525,
      "no_speech_prob": 0.15783388912677765
    },
    {
      "id": 43,
      "seek": 33900,
      "start": 3052.19,
      "end": 3059.19,
      "text": " Ich scheitere trotzdem im Formel-1-Wagen. Und das ist halt das, was wir uns irgendwo eingestehen müssen.",
      "tokens": [
        50814,
        3141,
        25690,
        270,
        323,
        28325,
        566,
        10126,
        338,
        12,
        16,
        12,
        54,
        4698,
        13,
        2719,
        1482,
        1418,
        12479,
        1482,
        11,
        390,
        1987,
        2693,
        40865,
        17002,
        8887,
        2932,
        9013,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2565270960330963,
      "compression_ratio": 1.5968992710113525,
      "no_speech_prob": 0.15783388912677765
    },
    {
      "id": 44,
      "seek": 33900,
      "start": 3059.19,
      "end": 3065.19,
      "text": " Dass diese Dinge halt wirklich skurrilerweise uns so viel ähnlicher sind, aber wir manchmal halt genauso.",
      "tokens": [
        51164,
        22306,
        6705,
        25102,
        12479,
        9696,
        1110,
        374,
        81,
        5441,
        13109,
        2693,
        370,
        5891,
        3078,
        12071,
        25215,
        3290,
        11,
        4340,
        1987,
        32092,
        12479,
        37694,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2565270960330963,
      "compression_ratio": 1.5968992710113525,
      "no_speech_prob": 0.15783388912677765
    },
    {
      "id": 45,
      "seek": 36100,
      "start": 3065.19,
      "end": 3074.19,
      "text": " Aber stell dir vor, und so erkläre ich es auch in unseren ganzen Enablement-Trainings, ich bin kein Freund, die Maschine zu vermenschlichen, aber ich finde, das Denkmodell hilft.",
      "tokens": [
        50364,
        5992,
        30787,
        4746,
        4245,
        11,
        674,
        370,
        27570,
        12277,
        1893,
        785,
        2168,
        294,
        25305,
        23966,
        2193,
        712,
        518,
        12,
        51,
        7146,
        1109,
        11,
        1893,
        5171,
        13424,
        29685,
        11,
        978,
        5224,
        36675,
        2164,
        26319,
        26590,
        10193,
        11,
        4340,
        1893,
        17841,
        11,
        1482,
        6458,
        74,
        8014,
        898,
        42493,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20859964191913605,
      "compression_ratio": 1.6200000047683716,
      "no_speech_prob": 0.49872201681137085
    },
    {
      "id": 46,
      "seek": 36100,
      "start": 3074.19,
      "end": 3082.19,
      "text": " Wenn bei dir morgen am Montag ein neuer Kollege eintritt ins Team, dann musst du den Kollegen kennenlernen.",
      "tokens": [
        50814,
        7899,
        4643,
        4746,
        36593,
        669,
        7947,
        559,
        1343,
        408,
        5486,
        28505,
        308,
        686,
        18579,
        1028,
        7606,
        11,
        3594,
        31716,
        1581,
        1441,
        23713,
        28445,
        75,
        25657,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20859964191913605,
      "compression_ratio": 1.6200000047683716,
      "no_speech_prob": 0.49872201681137085
    },
    {
      "id": 47,
      "seek": 36100,
      "start": 3082.19,
      "end": 3086.19,
      "text": " Dann musst du den Kollegen rausfinden, wo seine Stärken und seine Schwächen sind.",
      "tokens": [
        51214,
        7455,
        31716,
        1581,
        1441,
        23713,
        17202,
        43270,
        11,
        6020,
        15925,
        745,
        2713,
        2653,
        674,
        15925,
        17576,
        45118,
        3290,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20859964191913605,
      "compression_ratio": 1.6200000047683716,
      "no_speech_prob": 0.49872201681137085
    },
    {
      "id": 48,
      "seek": 36100,
      "start": 3086.19,
      "end": 3090.19,
      "text": " Und du kannst dich ja nicht ablenken lassen davon, wie dieser Mensch heißt, welche Jobbeschreibung und sonst was.",
      "tokens": [
        51414,
        2719,
        1581,
        20853,
        10390,
        2784,
        1979,
        410,
        6698,
        2653,
        16168,
        18574,
        11,
        3355,
        9053,
        27773,
        13139,
        11,
        24311,
        18602,
        6446,
        339,
        38606,
        1063,
        674,
        26309,
        390,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20859964191913605,
      "compression_ratio": 1.6200000047683716,
      "no_speech_prob": 0.49872201681137085
    },
    {
      "id": 49,
      "seek": 38600,
      "start": 3091.19,
      "end": 3097.19,
      "text": " Du wirst on the Job sehen, wo er gut reinpasst, wo seine Stärken liegen, wo er gut ins Team passt.",
      "tokens": [
        50414,
        5153,
        261,
        653,
        322,
        264,
        18602,
        11333,
        11,
        6020,
        1189,
        5228,
        6561,
        20990,
        372,
        11,
        6020,
        15925,
        745,
        2713,
        2653,
        35100,
        11,
        6020,
        1189,
        5228,
        1028,
        7606,
        37154,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21413198113441467,
      "compression_ratio": 1.617977499961853,
      "no_speech_prob": 0.15570983290672302
    },
    {
      "id": 50,
      "seek": 38600,
      "start": 3097.19,
      "end": 3101.19,
      "text": " Und AI verhält sich halt genauso. Das ist der Kollege, der zur Tür reingekommen ist.",
      "tokens": [
        50714,
        2719,
        7318,
        1306,
        28068,
        3041,
        12479,
        37694,
        13,
        2846,
        1418,
        1163,
        28505,
        11,
        1163,
        7147,
        16728,
        319,
        278,
        916,
        5132,
        1418,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21413198113441467,
      "compression_ratio": 1.617977499961853,
      "no_speech_prob": 0.15570983290672302
    },
    {
      "id": 51,
      "seek": 38600,
      "start": 3101.19,
      "end": 3108.19,
      "text": " Und bei mir kommt vielleicht ein anderer Kollege, weil ich andere Use Cases habe und denke mir, dafür ist es gar nicht zu gebrauchen, das muss ich jetzt woanders hinschicken.",
      "tokens": [
        50914,
        2719,
        4643,
        3149,
        10047,
        12547,
        1343,
        48108,
        28505,
        11,
        7689,
        1893,
        10490,
        8278,
        383,
        1957,
        6015,
        674,
        27245,
        3149,
        11,
        13747,
        1418,
        785,
        3691,
        1979,
        2164,
        1519,
        6198,
        11285,
        11,
        1482,
        6425,
        1893,
        4354,
        6020,
        41430,
        276,
        1292,
        339,
        3830,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21413198113441467,
      "compression_ratio": 1.617977499961853,
      "no_speech_prob": 0.15570983290672302
    },
    {
      "id": 52,
      "seek": 38600,
      "start": 3108.19,
      "end": 3114.19,
      "text": " Bei dir kommt er rein, sodass es halt an der Stelle funktionabel ist.",
      "tokens": [
        51264,
        16188,
        4746,
        10047,
        1189,
        6561,
        11,
        15047,
        640,
        785,
        12479,
        364,
        1163,
        26629,
        1019,
        2320,
        313,
        18657,
        1418,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21413198113441467,
      "compression_ratio": 1.617977499961853,
      "no_speech_prob": 0.15570983290672302
    },
    {
      "id": 53,
      "seek": 41000,
      "start": 3115.19,
      "end": 3119.19,
      "text": " Und wenn du das so rumdenkst, dann siehst du, dass es limitless ist, aber nicht alles macht Sinn.",
      "tokens": [
        50414,
        2719,
        4797,
        1581,
        1482,
        370,
        8347,
        1556,
        74,
        372,
        11,
        3594,
        2804,
        38857,
        1581,
        11,
        2658,
        785,
        4948,
        1832,
        1418,
        11,
        4340,
        1979,
        7874,
        10857,
        37962,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23085251450538635,
      "compression_ratio": 1.709433913230896,
      "no_speech_prob": 0.28054648637771606
    },
    {
      "id": 54,
      "seek": 41000,
      "start": 3119.19,
      "end": 3123.19,
      "text": " Nicht überall hat es seine Stärken. Und wir haben noch andere Kollegen, die wir auch einsetzen können.",
      "tokens": [
        50614,
        22629,
        38035,
        2385,
        785,
        15925,
        745,
        2713,
        2653,
        13,
        2719,
        1987,
        3084,
        3514,
        10490,
        23713,
        11,
        978,
        1987,
        2168,
        21889,
        24797,
        6310,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23085251450538635,
      "compression_ratio": 1.709433913230896,
      "no_speech_prob": 0.28054648637771606
    },
    {
      "id": 55,
      "seek": 41000,
      "start": 3123.19,
      "end": 3126.19,
      "text": " Und zusammen könnte es ein cooles Team geben.",
      "tokens": [
        50814,
        2719,
        14311,
        17646,
        785,
        1343,
        1627,
        279,
        7606,
        17191,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23085251450538635,
      "compression_ratio": 1.709433913230896,
      "no_speech_prob": 0.28054648637771606
    },
    {
      "id": 56,
      "seek": 41000,
      "start": 3126.19,
      "end": 3130.19,
      "text": " Und wenn wir das so rumdenken, dann verlieren wir auch unsere menschlichen Fähigkeiten nicht,",
      "tokens": [
        50964,
        2719,
        4797,
        1987,
        1482,
        370,
        8347,
        1556,
        2653,
        11,
        3594,
        49331,
        268,
        1987,
        2168,
        14339,
        10923,
        339,
        10193,
        479,
        6860,
        37545,
        1979,
        11,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23085251450538635,
      "compression_ratio": 1.709433913230896,
      "no_speech_prob": 0.28054648637771606
    },
    {
      "id": 57,
      "seek": 41000,
      "start": 3130.19,
      "end": 3137.19,
      "text": " weil ich der Maschine auch nicht Fähigkeiten zuweise, weil ich mich auch stark von ihr beeinflussen lasse,",
      "tokens": [
        51164,
        7689,
        1893,
        1163,
        5224,
        36675,
        2168,
        1979,
        479,
        6860,
        37545,
        2164,
        13109,
        11,
        7689,
        1893,
        6031,
        2168,
        17417,
        2957,
        5553,
        17479,
        27850,
        29202,
        2439,
        405,
        11,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23085251450538635,
      "compression_ratio": 1.709433913230896,
      "no_speech_prob": 0.28054648637771606
    },
    {
      "id": 58,
      "seek": 43300,
      "start": 3137.19,
      "end": 3142.19,
      "text": " weil ich es halt nicht so hypermenschlich personalisiere, sondern mehr so als Denkmodell,",
      "tokens": [
        50364,
        7689,
        1893,
        785,
        12479,
        1979,
        370,
        9848,
        76,
        26590,
        1739,
        2973,
        8021,
        323,
        11,
        11465,
        5417,
        370,
        3907,
        6458,
        74,
        8014,
        898,
        11,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24108070135116577,
      "compression_ratio": 1.5963636636734009,
      "no_speech_prob": 0.16654230654239655
    },
    {
      "id": 59,
      "seek": 43300,
      "start": 3142.19,
      "end": 3146.19,
      "text": " wie mit dem vielleicht auch manchmal sehr nervigen Kollegen Vorteil ist.",
      "tokens": [
        50614,
        3355,
        2194,
        1371,
        12547,
        2168,
        32092,
        5499,
        5724,
        3213,
        23713,
        46968,
        388,
        1418,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24108070135116577,
      "compression_ratio": 1.5963636636734009,
      "no_speech_prob": 0.16654230654239655
    },
    {
      "id": 60,
      "seek": 43300,
      "start": 3146.19,
      "end": 3149.19,
      "text": " Ich kann ihn einfach ausmachen.",
      "tokens": [
        50814,
        3141,
        4028,
        14534,
        7281,
        3437,
        43981,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24108070135116577,
      "compression_ratio": 1.5963636636734009,
      "no_speech_prob": 0.16654230654239655
    },
    {
      "id": 61,
      "seek": 43300,
      "start": 3149.19,
      "end": 3156.19,
      "text": " Okay, das heißt aber auch, ich muss weg davon, die Maschine als Maschine zu sehen, weil das machen ja viele, die sagen,",
      "tokens": [
        50964,
        1033,
        11,
        1482,
        13139,
        4340,
        2168,
        11,
        1893,
        6425,
        15565,
        18574,
        11,
        978,
        5224,
        36675,
        3907,
        5224,
        36675,
        2164,
        11333,
        11,
        7689,
        1482,
        7069,
        2784,
        9693,
        11,
        978,
        8360,
        11,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24108070135116577,
      "compression_ratio": 1.5963636636734009,
      "no_speech_prob": 0.16654230654239655
    },
    {
      "id": 62,
      "seek": 43300,
      "start": 3156.19,
      "end": 3160.19,
      "text": " hey, kann mir Fragen beantworten, aber guck mal, kann nicht rechnen.",
      "tokens": [
        51314,
        4177,
        11,
        4028,
        3149,
        25588,
        312,
        21655,
        268,
        11,
        4340,
        695,
        547,
        2806,
        11,
        4028,
        1979,
        319,
        1377,
        268,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24108070135116577,
      "compression_ratio": 1.5963636636734009,
      "no_speech_prob": 0.16654230654239655
    },
    {
      "id": 63,
      "seek": 43300,
      "start": 3160.19,
      "end": 3163.19,
      "text": " Ist ja ist ein Computer, müsste doch rechnen können.",
      "tokens": [
        51514,
        12810,
        2784,
        1418,
        1343,
        22289,
        11,
        42962,
        9243,
        319,
        1377,
        268,
        6310,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.24108070135116577,
      "compression_ratio": 1.5963636636734009,
      "no_speech_prob": 0.16654230654239655
    },
    {
      "id": 64,
      "seek": 45900,
      "start": 3163.19,
      "end": 3172.19,
      "text": " Kann ich nichts mit anfangen oder eben hey, das Teil muss doch in allen Sachen gut sein, weil es ist doch hier KI.",
      "tokens": [
        50364,
        29074,
        1893,
        13004,
        2194,
        33709,
        10784,
        4513,
        11375,
        4177,
        11,
        1482,
        16357,
        6425,
        9243,
        294,
        18440,
        26074,
        5228,
        6195,
        11,
        7689,
        785,
        1418,
        9243,
        3296,
        47261,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27544641494750977,
      "compression_ratio": 1.638157844543457,
      "no_speech_prob": 0.2683868110179901
    },
    {
      "id": 65,
      "seek": 45900,
      "start": 3172.19,
      "end": 3176.19,
      "text": " Von dieser Denkweise muss ich weg und dann passt es, oder?",
      "tokens": [
        50814,
        20700,
        9053,
        6458,
        74,
        13109,
        6425,
        1893,
        15565,
        674,
        3594,
        37154,
        785,
        11,
        4513,
        30,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27544641494750977,
      "compression_ratio": 1.638157844543457,
      "no_speech_prob": 0.2683868110179901
    },
    {
      "id": 66,
      "seek": 45900,
      "start": 3176.19,
      "end": 3179.19,
      "text": " Genau, also wenn ich halt so ein Mittelground ist, das ist kein Mensch.",
      "tokens": [
        51014,
        22340,
        11,
        611,
        4797,
        1893,
        12479,
        370,
        1343,
        35079,
        2921,
        1418,
        11,
        1482,
        1418,
        13424,
        27773,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27544641494750977,
      "compression_ratio": 1.638157844543457,
      "no_speech_prob": 0.2683868110179901
    },
    {
      "id": 67,
      "seek": 45900,
      "start": 3179.19,
      "end": 3184.19,
      "text": " Das ist auch keine deterministische Maschine wie ein Taschenrechner, wo 5 plus 3 immer 8 ist,",
      "tokens": [
        51164,
        2846,
        1418,
        2168,
        9252,
        15957,
        468,
        7864,
        5224,
        36675,
        3355,
        1343,
        27293,
        2470,
        265,
        339,
        1193,
        11,
        6020,
        1025,
        1804,
        805,
        5578,
        1649,
        1418,
        11,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27544641494750977,
      "compression_ratio": 1.638157844543457,
      "no_speech_prob": 0.2683868110179901
    },
    {
      "id": 68,
      "seek": 45900,
      "start": 3184.19,
      "end": 3187.19,
      "text": " egal welchen Taschenrechnerhersteller du benutzt.",
      "tokens": [
        51414,
        31528,
        2214,
        2470,
        27293,
        2470,
        265,
        339,
        1193,
        511,
        372,
        14983,
        1581,
        38424,
        2682,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27544641494750977,
      "compression_ratio": 1.638157844543457,
      "no_speech_prob": 0.2683868110179901
    },
    {
      "id": 69,
      "seek": 45900,
      "start": 3187.19,
      "end": 3192.19,
      "text": " Ist es eine menschliche Komponente, die natürlich auch, wir haben natürlich auch Quantifizierungsmethoden.",
      "tokens": [
        51564,
        12810,
        785,
        3018,
        10923,
        339,
        10185,
        591,
        8586,
        266,
        1576,
        11,
        978,
        8762,
        2168,
        11,
        1987,
        3084,
        8762,
        2168,
        26968,
        351,
        590,
        40908,
        5537,
        2926,
        268,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27544641494750977,
      "compression_ratio": 1.638157844543457,
      "no_speech_prob": 0.2683868110179901
    },
    {
      "id": 70,
      "seek": 48800,
      "start": 3192.19,
      "end": 3197.19,
      "text": " Wir haben mathematische und Code-Methoden, um auszutesten on scale, wo was Sinn macht.",
      "tokens": [
        50364,
        4347,
        3084,
        11619,
        7864,
        674,
        15549,
        12,
        44,
        302,
        2926,
        268,
        11,
        1105,
        3437,
        89,
        325,
        38743,
        322,
        4373,
        11,
        6020,
        390,
        37962,
        10857,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23756025731563568,
      "compression_ratio": 1.529411792755127,
      "no_speech_prob": 0.002590584335848689
    },
    {
      "id": 71,
      "seek": 48800,
      "start": 3197.19,
      "end": 3205.19,
      "text": " Das ist aber am Ende des Tages ist das unsere höchste Menschlichkeit, ist Sinnhaftigkeit und strategische Entscheidungskompetenz.",
      "tokens": [
        50614,
        2846,
        1418,
        4340,
        669,
        15152,
        730,
        33601,
        1418,
        1482,
        14339,
        13531,
        339,
        2941,
        27773,
        41096,
        11,
        1418,
        37962,
        25127,
        16626,
        674,
        5464,
        7864,
        44667,
        5161,
        298,
        7275,
        11368,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23756025731563568,
      "compression_ratio": 1.529411792755127,
      "no_speech_prob": 0.002590584335848689
    },
    {
      "id": 72,
      "seek": 48800,
      "start": 3205.19,
      "end": 3211.19,
      "text": " Und wenn ich die anfange zu delegieren, gut, dann, naja, gut, sollten wir nicht tun.",
      "tokens": [
        51014,
        2719,
        4797,
        1893,
        978,
        33709,
        933,
        2164,
        15824,
        5695,
        11,
        5228,
        11,
        3594,
        11,
        1667,
        2938,
        11,
        5228,
        11,
        29096,
        1987,
        1979,
        4267,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23756025731563568,
      "compression_ratio": 1.529411792755127,
      "no_speech_prob": 0.002590584335848689
    },
    {
      "id": 73,
      "seek": 48800,
      "start": 3211.19,
      "end": 3218.19,
      "text": " Das ist aber ein spannender Punkt, also sowas wie Motivation und Ethik in den Modellen,",
      "tokens": [
        51314,
        2846,
        1418,
        4340,
        1343,
        33360,
        3216,
        25487,
        11,
        611,
        19766,
        296,
        3355,
        8956,
        592,
        399,
        674,
        10540,
        1035,
        294,
        1441,
        6583,
        8581,
        11,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23756025731563568,
      "compression_ratio": 1.529411792755127,
      "no_speech_prob": 0.002590584335848689
    },
    {
      "id": 74,
      "seek": 51400,
      "start": 3219.19,
      "end": 3226.19,
      "text": " wo ich immer wieder gefragt werde und die Antworten sind, gerade die Motivation finde ich spannend.",
      "tokens": [
        50414,
        6020,
        1893,
        5578,
        6216,
        42638,
        24866,
        674,
        978,
        34693,
        268,
        3290,
        11,
        12117,
        978,
        8956,
        592,
        399,
        17841,
        1893,
        49027,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22764872014522552,
      "compression_ratio": 1.62109375,
      "no_speech_prob": 0.035108212381601334
    },
    {
      "id": 75,
      "seek": 51400,
      "start": 3226.19,
      "end": 3233.19,
      "text": " Die ist denen antrainiert. Die haben ja eigentlich keine eigene Motivation, dass sie Geld verdienen wollen oder sowas.",
      "tokens": [
        50764,
        3229,
        1418,
        19998,
        2511,
        7146,
        4859,
        13,
        3229,
        3084,
        2784,
        10926,
        9252,
        38549,
        8956,
        592,
        399,
        11,
        2658,
        2804,
        16535,
        6387,
        22461,
        11253,
        4513,
        19766,
        296,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22764872014522552,
      "compression_ratio": 1.62109375,
      "no_speech_prob": 0.035108212381601334
    },
    {
      "id": 76,
      "seek": 51400,
      "start": 3233.19,
      "end": 3238.19,
      "text": " Ja, naja, andersrum. Wechseln wir mal kurz die Mathematik.",
      "tokens": [
        51114,
        3530,
        11,
        1667,
        2938,
        11,
        17999,
        6247,
        13,
        492,
        21266,
        77,
        1987,
        2806,
        20465,
        978,
        15776,
        8615,
        1035,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22764872014522552,
      "compression_ratio": 1.62109375,
      "no_speech_prob": 0.035108212381601334
    },
    {
      "id": 77,
      "seek": 51400,
      "start": 3238.19,
      "end": 3241.19,
      "text": " Anstatt eines LLMs wechseln wir ins Reinforcement Learning.",
      "tokens": [
        51364,
        1107,
        372,
        1591,
        18599,
        441,
        43,
        26386,
        321,
        21266,
        77,
        1987,
        1028,
        42116,
        9382,
        15205,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22764872014522552,
      "compression_ratio": 1.62109375,
      "no_speech_prob": 0.035108212381601334
    },
    {
      "id": 78,
      "seek": 51400,
      "start": 3241.19,
      "end": 3246.19,
      "text": " Ein klassischer Fall eines Reinforcement Learning ist ein Staubsaugerroboter.",
      "tokens": [
        51514,
        6391,
        42917,
        19674,
        7465,
        18599,
        42116,
        9382,
        15205,
        1418,
        1343,
        16959,
        5432,
        20056,
        260,
        16614,
        21585,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22764872014522552,
      "compression_ratio": 1.62109375,
      "no_speech_prob": 0.035108212381601334
    },
    {
      "id": 79,
      "seek": 54200,
      "start": 3246.19,
      "end": 3251.19,
      "text": " Was ist die höchste Reward, also Reinforcement Learning?",
      "tokens": [
        50364,
        3027,
        1418,
        978,
        13531,
        339,
        2941,
        1300,
        1007,
        11,
        611,
        1300,
        19920,
        284,
        384,
        518,
        15205,
        30,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2577933073043823,
      "compression_ratio": 1.4502618312835693,
      "no_speech_prob": 0.04877282306551933
    },
    {
      "id": 80,
      "seek": 54200,
      "start": 3251.19,
      "end": 3260.19,
      "text": " Was hat den höchsten Reward, also was hat die höchste Relevanz und Reward-Funktion für deinen Staubsaugerroboter?",
      "tokens": [
        50614,
        3027,
        2385,
        1441,
        13531,
        339,
        6266,
        1300,
        1007,
        11,
        611,
        390,
        2385,
        978,
        13531,
        339,
        2941,
        1300,
        28316,
        3910,
        674,
        1300,
        1007,
        12,
        46947,
        9780,
        2959,
        49362,
        16959,
        5432,
        20056,
        260,
        16614,
        21585,
        30,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2577933073043823,
      "compression_ratio": 1.4502618312835693,
      "no_speech_prob": 0.04877282306551933
    },
    {
      "id": 81,
      "seek": 54200,
      "start": 3260.19,
      "end": 3265.19,
      "text": " Das komplette Zimmer gesäubert zu haben, also alles abgefahren, oder?",
      "tokens": [
        51064,
        2846,
        24526,
        3007,
        37243,
        5019,
        737,
        84,
        4290,
        2164,
        3084,
        11,
        611,
        7874,
        410,
        13529,
        7079,
        11,
        4513,
        30,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2577933073043823,
      "compression_ratio": 1.4502618312835693,
      "no_speech_prob": 0.04877282306551933
    },
    {
      "id": 82,
      "seek": 54200,
      "start": 3265.19,
      "end": 3268.19,
      "text": " Nein, das ist deine Definition.",
      "tokens": [
        51314,
        18878,
        11,
        1482,
        1418,
        28395,
        46245,
        849,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2577933073043823,
      "compression_ratio": 1.4502618312835693,
      "no_speech_prob": 0.04877282306551933
    },
    {
      "id": 83,
      "seek": 56400,
      "start": 3269.19,
      "end": 3277.19,
      "text": " Er muss erkennen, wann der Batteriestand im Größen bezogen auf den Weg zurück ist, so niedrig ist, dass er noch nach Hause findet, um sich wieder aufzuladen.",
      "tokens": [
        50414,
        3300,
        6425,
        45720,
        11,
        38064,
        1163,
        33066,
        6495,
        474,
        566,
        45778,
        8989,
        10782,
        8799,
        2501,
        1441,
        18919,
        15089,
        1418,
        11,
        370,
        32488,
        7065,
        1418,
        11,
        2658,
        1189,
        3514,
        5168,
        26217,
        27752,
        11,
        1105,
        3041,
        6216,
        2501,
        89,
        425,
        14771,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2343648076057434,
      "compression_ratio": 1.6313868761062622,
      "no_speech_prob": 0.5679659843444824
    },
    {
      "id": 84,
      "seek": 56400,
      "start": 3277.19,
      "end": 3280.19,
      "text": " Weil er überlebt ja nicht ohne Strom.",
      "tokens": [
        50814,
        18665,
        1189,
        4502,
        306,
        4517,
        2784,
        1979,
        15716,
        39126,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2343648076057434,
      "compression_ratio": 1.6313868761062622,
      "no_speech_prob": 0.5679659843444824
    },
    {
      "id": 85,
      "seek": 56400,
      "start": 3280.19,
      "end": 3288.19,
      "text": " Das heißt, die höchste Reward muss sein, der Punkt, wo er erkennt, meine Batterie ist so niedrig und so lange ist mein Weg noch zu meiner Ladestation,",
      "tokens": [
        50964,
        2846,
        13139,
        11,
        978,
        13531,
        339,
        2941,
        1300,
        1007,
        6425,
        6195,
        11,
        1163,
        25487,
        11,
        6020,
        1189,
        1189,
        41838,
        11,
        10946,
        33066,
        414,
        1418,
        370,
        32488,
        7065,
        674,
        370,
        18131,
        1418,
        10777,
        18919,
        3514,
        2164,
        20529,
        12106,
        377,
        399,
        11,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2343648076057434,
      "compression_ratio": 1.6313868761062622,
      "no_speech_prob": 0.5679659843444824
    },
    {
      "id": 86,
      "seek": 56400,
      "start": 3288.19,
      "end": 3295.19,
      "text": " dass er sich auf den Prozess abbricht, deinen Wunsch also überschreibt und nach Hause findet.",
      "tokens": [
        51364,
        2658,
        1189,
        3041,
        2501,
        1441,
        1705,
        37575,
        410,
        1443,
        1405,
        11,
        49362,
        343,
        409,
        6145,
        611,
        45022,
        339,
        31174,
        674,
        5168,
        26217,
        27752,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2343648076057434,
      "compression_ratio": 1.6313868761062622,
      "no_speech_prob": 0.5679659843444824
    },
    {
      "id": 87,
      "seek": 59100,
      "start": 3295.19,
      "end": 3299.19,
      "text": " Das muss die höchste Reward-Funktion sein, ansonsten bist du ein sehr unglücklicher Kunde.",
      "tokens": [
        50364,
        2846,
        6425,
        978,
        13531,
        339,
        2941,
        1300,
        1007,
        12,
        46947,
        9780,
        6195,
        11,
        1567,
        4068,
        268,
        18209,
        1581,
        1343,
        5499,
        517,
        7191,
        6536,
        25215,
        591,
        13271,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2638109028339386,
      "compression_ratio": 1.4954954385757446,
      "no_speech_prob": 0.014496279880404472
    },
    {
      "id": 88,
      "seek": 59100,
      "start": 3299.19,
      "end": 3304.19,
      "text": " Wenn der Akku niedrig wird, kriegt der Roboter dann Hunger?",
      "tokens": [
        50564,
        7899,
        1163,
        9629,
        5279,
        32488,
        7065,
        4578,
        11,
        25766,
        10463,
        1163,
        5424,
        21585,
        3594,
        46549,
        30,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2638109028339386,
      "compression_ratio": 1.4954954385757446,
      "no_speech_prob": 0.014496279880404472
    },
    {
      "id": 89,
      "seek": 59100,
      "start": 3304.19,
      "end": 3305.19,
      "text": " Genau.",
      "tokens": [
        50814,
        22340,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2638109028339386,
      "compression_ratio": 1.4954954385757446,
      "no_speech_prob": 0.014496279880404472
    },
    {
      "id": 90,
      "seek": 59100,
      "start": 3305.19,
      "end": 3307.19,
      "text": " Hat er dann das Hungergefühl?",
      "tokens": [
        50864,
        15867,
        1189,
        3594,
        1482,
        46549,
        13529,
        21692,
        30,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2638109028339386,
      "compression_ratio": 1.4954954385757446,
      "no_speech_prob": 0.014496279880404472
    },
    {
      "id": 91,
      "seek": 59100,
      "start": 3307.19,
      "end": 3313.19,
      "text": " Das ist eine Reward-Funktion, quasi triggert im Reinforcement Learning, ja.",
      "tokens": [
        50964,
        2846,
        1418,
        3018,
        1300,
        1007,
        12,
        46947,
        9780,
        11,
        20954,
        504,
        6249,
        911,
        566,
        42116,
        9382,
        15205,
        11,
        2784,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2638109028339386,
      "compression_ratio": 1.4954954385757446,
      "no_speech_prob": 0.014496279880404472
    },
    {
      "id": 92,
      "seek": 59100,
      "start": 3313.19,
      "end": 3319.19,
      "text": " Okay, jetzt haben wir nicht den Roboter, jetzt haben wir das LLM.",
      "tokens": [
        51264,
        1033,
        11,
        4354,
        3084,
        1987,
        1979,
        1441,
        5424,
        21585,
        11,
        4354,
        3084,
        1987,
        1482,
        441,
        43,
        44,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2638109028339386,
      "compression_ratio": 1.4954954385757446,
      "no_speech_prob": 0.014496279880404472
    },
    {
      "id": 93,
      "seek": 61500,
      "start": 3319.19,
      "end": 3332.19,
      "text": " Richtig. Und das LLM, zusammen mit dem Reinforcement Learning, deswegen sind die Reasoning-Modelle bei all ihrer Buggy-Haftigkeit und dass wir da noch ein bisschen mehr am Anfang weiter weg sind vom Ausgereift,",
      "tokens": [
        50364,
        497,
        7334,
        13,
        2719,
        1482,
        441,
        43,
        44,
        11,
        14311,
        2194,
        1371,
        42116,
        9382,
        15205,
        11,
        26482,
        3290,
        978,
        39693,
        278,
        12,
        44,
        378,
        4434,
        4643,
        439,
        23990,
        23821,
        1480,
        12,
        23745,
        844,
        16626,
        674,
        2658,
        1987,
        1120,
        3514,
        1343,
        10763,
        5417,
        669,
        25856,
        8988,
        15565,
        3290,
        10135,
        9039,
        34899,
        2008,
        11,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19965459406375885,
      "compression_ratio": 1.512635350227356,
      "no_speech_prob": 0.19145077466964722
    },
    {
      "id": 94,
      "seek": 61500,
      "start": 3332.19,
      "end": 3336.19,
      "text": " wie wir das bei einer klassischen LLM-Architektur sind, erzeugt jetzt genau das.",
      "tokens": [
        51014,
        3355,
        1987,
        1482,
        4643,
        6850,
        42917,
        6282,
        441,
        43,
        44,
        12,
        10683,
        339,
        642,
        2320,
        374,
        3290,
        11,
        1189,
        19303,
        83,
        4354,
        12535,
        1482,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19965459406375885,
      "compression_ratio": 1.512635350227356,
      "no_speech_prob": 0.19145077466964722
    },
    {
      "id": 95,
      "seek": 61500,
      "start": 3336.19,
      "end": 3344.19,
      "text": " Und auf einmal habe ich sowas wie Überlebensinstinkt, Achtung, bitte sehr hart in Anführungszeichen zu sehen, und Motivation.",
      "tokens": [
        51214,
        2719,
        2501,
        11078,
        6015,
        1893,
        19766,
        296,
        3355,
        18086,
        28512,
        694,
        13911,
        475,
        83,
        11,
        316,
        4701,
        1063,
        11,
        23231,
        5499,
        36644,
        294,
        1107,
        29189,
        5846,
        1381,
        18613,
        2164,
        11333,
        11,
        674,
        8956,
        592,
        399,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19965459406375885,
      "compression_ratio": 1.512635350227356,
      "no_speech_prob": 0.19145077466964722
    },
    {
      "id": 96,
      "seek": 64000,
      "start": 3344.19,
      "end": 3349.19,
      "text": " Weil jetzt ist es an der Stelle, dass es halt eine Eigenmotivation hat.",
      "tokens": [
        50364,
        18665,
        4354,
        1418,
        785,
        364,
        1163,
        26629,
        11,
        2658,
        785,
        12479,
        3018,
        30586,
        29778,
        592,
        399,
        2385,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982421875,
      "compression_ratio": 1.6793103218078613,
      "no_speech_prob": 0.12579430639743805
    },
    {
      "id": 97,
      "seek": 64000,
      "start": 3349.19,
      "end": 3352.19,
      "text": " Und das ist halt das, was immer so schwierig ist zu verstehen.",
      "tokens": [
        50614,
        2719,
        1482,
        1418,
        12479,
        1482,
        11,
        390,
        5578,
        370,
        37845,
        1418,
        2164,
        37352,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982421875,
      "compression_ratio": 1.6793103218078613,
      "no_speech_prob": 0.12579430639743805
    },
    {
      "id": 98,
      "seek": 64000,
      "start": 3352.19,
      "end": 3355.19,
      "text": " Du hast die Motivation, mein Zimmer soll sauber sein.",
      "tokens": [
        50764,
        5153,
        6581,
        978,
        8956,
        592,
        399,
        11,
        10777,
        37243,
        7114,
        601,
        10261,
        6195,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982421875,
      "compression_ratio": 1.6793103218078613,
      "no_speech_prob": 0.12579430639743805
    },
    {
      "id": 99,
      "seek": 64000,
      "start": 3355.19,
      "end": 3358.19,
      "text": " Ja, dafür ist ja, das ist die Grunddefinition des Dings.",
      "tokens": [
        50914,
        3530,
        11,
        13747,
        1418,
        2784,
        11,
        1482,
        1418,
        978,
        13941,
        1479,
        5194,
        849,
        730,
        413,
        1109,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982421875,
      "compression_ratio": 1.6793103218078613,
      "no_speech_prob": 0.12579430639743805
    },
    {
      "id": 100,
      "seek": 64000,
      "start": 3358.19,
      "end": 3369.19,
      "text": " Aber wenn es sauber putzt und ständig sauber putzt und dann im Raum rumliegt, dann hilft dir das genauso wenig was, wie wenn es quasi nicht putzt.",
      "tokens": [
        51064,
        5992,
        4797,
        785,
        601,
        10261,
        829,
        2682,
        674,
        342,
        38861,
        601,
        10261,
        829,
        2682,
        674,
        3594,
        566,
        31359,
        8347,
        6302,
        10463,
        11,
        3594,
        42493,
        4746,
        1482,
        37694,
        20911,
        390,
        11,
        3355,
        4797,
        785,
        20954,
        1979,
        829,
        2682,
        13,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982421875,
      "compression_ratio": 1.6793103218078613,
      "no_speech_prob": 0.12579430639743805
    },
    {
      "id": 101,
      "seek": 64000,
      "start": 3369.19,
      "end": 3371.19,
      "text": " Ich meine, dann würdest du ihn reklamieren.",
      "tokens": [
        51614,
        3141,
        10946,
        11,
        3594,
        9195,
        23748,
        1581,
        14534,
        33881,
        4326,
        5695,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982421875,
      "compression_ratio": 1.6793103218078613,
      "no_speech_prob": 0.12579430639743805
    },
    {
      "id": 102,
      "seek": 64000,
      "start": 3371.19,
      "end": 3373.19,
      "text": " Wenn es nicht putzt, dann ist das Geräteffekt.",
      "tokens": [
        51714,
        7899,
        785,
        1979,
        829,
        2682,
        11,
        3594,
        1418,
        1482,
        9409,
        737,
        975,
        602,
        8192,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982421875,
      "compression_ratio": 1.6793103218078613,
      "no_speech_prob": 0.12579430639743805
    },
    {
      "id": 103,
      "seek": 66900,
      "start": 3373.19,
      "end": 3378.19,
      "text": " Wenn es nicht nach Hause findet, ist die Reward-Function, das Reinforcement-Learning kaputt.",
      "tokens": [
        50364,
        7899,
        785,
        1979,
        5168,
        26217,
        27752,
        11,
        1418,
        978,
        1300,
        1007,
        12,
        37,
        32627,
        11,
        1482,
        42116,
        9382,
        12,
        11020,
        2341,
        13816,
        13478,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19105583429336548,
      "compression_ratio": 1.5858585834503174,
      "no_speech_prob": 0.003271517576649785
    },
    {
      "id": 104,
      "seek": 66900,
      "start": 3378.19,
      "end": 3379.19,
      "text": " Zwei unterschiedliche Dinge.",
      "tokens": [
        50614,
        1176,
        17849,
        30058,
        10185,
        25102,
        13,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19105583429336548,
      "compression_ratio": 1.5858585834503174,
      "no_speech_prob": 0.003271517576649785
    },
    {
      "id": 105,
      "seek": 66900,
      "start": 3379.19,
      "end": 3383.19,
      "text": " Und das ist für uns halt alles nicht ganz so trivial nachvollziehbar.",
      "tokens": [
        50664,
        2719,
        1482,
        1418,
        2959,
        2693,
        12479,
        7874,
        1979,
        6312,
        370,
        26703,
        5168,
        20654,
        28213,
        5356,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19105583429336548,
      "compression_ratio": 1.5858585834503174,
      "no_speech_prob": 0.003271517576649785
    },
    {
      "id": 106,
      "seek": 66900,
      "start": 3383.19,
      "end": 3388.19,
      "text": " Und wenn ich jetzt aber ein Reinforcement-Learning in ein Large-Language-Modell reinbringe,",
      "tokens": [
        50864,
        2719,
        4797,
        1893,
        4354,
        4340,
        1343,
        42116,
        9382,
        12,
        11020,
        2341,
        294,
        1343,
        33092,
        12,
        43,
        656,
        20473,
        12,
        44,
        378,
        898,
        6561,
        65,
        38895,
        11,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19105583429336548,
      "compression_ratio": 1.5858585834503174,
      "no_speech_prob": 0.003271517576649785
    },
    {
      "id": 107,
      "seek": 66900,
      "start": 3388.19,
      "end": 3394.19,
      "text": " dann habe ich auf einmal, was wir als Menschen als Motivation und Überlebensinstinkt identifizieren würden,",
      "tokens": [
        51114,
        3594,
        6015,
        1893,
        2501,
        11078,
        11,
        390,
        1987,
        3907,
        8397,
        3907,
        8956,
        592,
        399,
        674,
        18086,
        28512,
        694,
        13911,
        475,
        83,
        2473,
        351,
        590,
        5695,
        27621,
        11,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19105583429336548,
      "compression_ratio": 1.5858585834503174,
      "no_speech_prob": 0.003271517576649785
    },
    {
      "id": 108,
      "seek": 66900,
      "start": 3394.19,
      "end": 3400.19,
      "text": " weil das Modell nun eine Art eigene Instanz in Anführungszeichen sein kann.",
      "tokens": [
        51414,
        7689,
        1482,
        6583,
        898,
        8905,
        3018,
        5735,
        38549,
        2730,
        3910,
        294,
        1107,
        29189,
        5846,
        1381,
        18613,
        6195,
        4028,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19105583429336548,
      "compression_ratio": 1.5858585834503174,
      "no_speech_prob": 0.003271517576649785
    },
    {
      "id": 109,
      "seek": 69600,
      "start": 3400.19,
      "end": 3404.19,
      "text": " Damit muss es selber sich triggern, Dinge zu erledigen und zu tun.",
      "tokens": [
        50364,
        24495,
        6425,
        785,
        23888,
        3041,
        504,
        6249,
        1248,
        11,
        25102,
        2164,
        1189,
        1493,
        3213,
        674,
        2164,
        4267,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.173695906996727,
      "compression_ratio": 1.6487455368041992,
      "no_speech_prob": 0.09660610556602478
    },
    {
      "id": 110,
      "seek": 69600,
      "start": 3404.19,
      "end": 3408.19,
      "text": " Und ohne diese Trigger wird es gar nichts mit unseren lustigen Agenten,",
      "tokens": [
        50564,
        2719,
        15716,
        6705,
        1765,
        6812,
        4578,
        785,
        3691,
        13004,
        2194,
        25305,
        24672,
        3213,
        27174,
        268,
        11,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.173695906996727,
      "compression_ratio": 1.6487455368041992,
      "no_speech_prob": 0.09660610556602478
    },
    {
      "id": 111,
      "seek": 69600,
      "start": 3408.19,
      "end": 3412.19,
      "text": " weil sie nämlich sonst keine Motivationsfunktion haben, dass sie ihren Scheiß sauber erledigen",
      "tokens": [
        50764,
        7689,
        2804,
        21219,
        26309,
        9252,
        8956,
        592,
        763,
        15930,
        9780,
        3084,
        11,
        2658,
        2804,
        22347,
        25321,
        6230,
        601,
        10261,
        1189,
        1493,
        3213,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.173695906996727,
      "compression_ratio": 1.6487455368041992,
      "no_speech_prob": 0.09660610556602478
    },
    {
      "id": 112,
      "seek": 69600,
      "start": 3412.19,
      "end": 3416.19,
      "text": " und das vielleicht auch noch richtig machen und wissen, wann sie abbrechen müssen.",
      "tokens": [
        50964,
        674,
        1482,
        12547,
        2168,
        3514,
        13129,
        7069,
        674,
        16331,
        11,
        38064,
        2804,
        410,
        2672,
        2470,
        9013,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.173695906996727,
      "compression_ratio": 1.6487455368041992,
      "no_speech_prob": 0.09660610556602478
    },
    {
      "id": 113,
      "seek": 69600,
      "start": 3416.19,
      "end": 3419.19,
      "text": " Das ist diese Sachen, die da alle irgendwie so zusammenkommen.",
      "tokens": [
        51164,
        2846,
        1418,
        6705,
        26074,
        11,
        978,
        1120,
        5430,
        20759,
        370,
        14311,
        13675,
        13,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.173695906996727,
      "compression_ratio": 1.6487455368041992,
      "no_speech_prob": 0.09660610556602478
    },
    {
      "id": 114,
      "seek": 69600,
      "start": 3419.19,
      "end": 3426.19,
      "text": " Sehe ich das richtig, dass in diesen LLMs eigentlich so zwei Dinge drin sind?",
      "tokens": [
        51314,
        1100,
        675,
        1893,
        1482,
        13129,
        11,
        2658,
        294,
        12862,
        441,
        43,
        26386,
        10926,
        370,
        12002,
        25102,
        24534,
        3290,
        30,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.173695906996727,
      "compression_ratio": 1.6487455368041992,
      "no_speech_prob": 0.09660610556602478
    },
    {
      "id": 115,
      "seek": 72200,
      "start": 3426.19,
      "end": 3431.19,
      "text": " Das eine ist das Wissen und vielleicht auch Sprachverarbeitung.",
      "tokens": [
        50364,
        2846,
        3018,
        1418,
        1482,
        343,
        10987,
        674,
        12547,
        2168,
        7702,
        608,
        331,
        24024,
        1063,
        13,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2363981008529663,
      "compression_ratio": 1.4809523820877075,
      "no_speech_prob": 0.022969000041484833
    },
    {
      "id": 116,
      "seek": 72200,
      "start": 3431.19,
      "end": 3434.19,
      "text": " Und das andere ist das Verhalten.",
      "tokens": [
        50614,
        2719,
        1482,
        10490,
        1418,
        1482,
        4281,
        15022,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2363981008529663,
      "compression_ratio": 1.4809523820877075,
      "no_speech_prob": 0.022969000041484833
    },
    {
      "id": 117,
      "seek": 72200,
      "start": 3434.19,
      "end": 3440.19,
      "text": " Ich meine zum Beispiel, so ein Claude verhält sich agentischer als jetzt ein GPT-4.",
      "tokens": [
        50764,
        3141,
        10946,
        5919,
        13772,
        11,
        370,
        1343,
        12947,
        2303,
        1306,
        28068,
        3041,
        9461,
        19674,
        3907,
        4354,
        1343,
        26039,
        51,
        12,
        19,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2363981008529663,
      "compression_ratio": 1.4809523820877075,
      "no_speech_prob": 0.022969000041484833
    },
    {
      "id": 118,
      "seek": 72200,
      "start": 3440.19,
      "end": 3444.19,
      "text": " Und trotzdem ist das gleiche Wissen drin.",
      "tokens": [
        51064,
        2719,
        28325,
        1418,
        1482,
        11699,
        68,
        343,
        10987,
        24534,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2363981008529663,
      "compression_ratio": 1.4809523820877075,
      "no_speech_prob": 0.022969000041484833
    },
    {
      "id": 119,
      "seek": 72200,
      "start": 3444.19,
      "end": 3451.19,
      "text": " Und das Verhalten tatsächlich von dem menschlichen Training in großen Fällen kommt,",
      "tokens": [
        51264,
        2719,
        1482,
        4281,
        15022,
        20796,
        2957,
        1371,
        10923,
        339,
        10193,
        20620,
        294,
        23076,
        479,
        46181,
        10047,
        11,
        51614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2363981008529663,
      "compression_ratio": 1.4809523820877075,
      "no_speech_prob": 0.022969000041484833
    },
    {
      "id": 120,
      "seek": 74700,
      "start": 3451.19,
      "end": 3457.19,
      "text": " was man immer hört, dass da eben irgendwelche Klickformen sind, wo Leute entscheiden müssen,",
      "tokens": [
        50364,
        390,
        587,
        5578,
        42243,
        11,
        2658,
        1120,
        11375,
        26455,
        338,
        1876,
        16053,
        618,
        837,
        268,
        3290,
        11,
        6020,
        13495,
        44560,
        9013,
        11,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2244897186756134,
      "compression_ratio": 1.5136187076568604,
      "no_speech_prob": 0.037291038781404495
    },
    {
      "id": 121,
      "seek": 74700,
      "start": 3457.19,
      "end": 3460.19,
      "text": " war das jetzt eine gute Antwort, war das eine schlechte Antwort?",
      "tokens": [
        50664,
        1516,
        1482,
        4354,
        3018,
        21476,
        34693,
        11,
        1516,
        1482,
        3018,
        22664,
        10553,
        34693,
        30,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2244897186756134,
      "compression_ratio": 1.5136187076568604,
      "no_speech_prob": 0.037291038781404495
    },
    {
      "id": 122,
      "seek": 74700,
      "start": 3460.19,
      "end": 3465.19,
      "text": " Und das dann eben mit die Reward-Function ist, die er lernt.",
      "tokens": [
        50814,
        2719,
        1482,
        3594,
        11375,
        2194,
        978,
        1300,
        1007,
        12,
        37,
        32627,
        1418,
        11,
        978,
        1189,
        32068,
        580,
        13,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2244897186756134,
      "compression_ratio": 1.5136187076568604,
      "no_speech_prob": 0.037291038781404495
    },
    {
      "id": 123,
      "seek": 74700,
      "start": 3465.19,
      "end": 3473.19,
      "text": " Ja, eine Layer zusätzlich. Es ist nicht auf Sondaten trainiert, sondern auf Daten menschlichen Ursprungs.",
      "tokens": [
        51064,
        3530,
        11,
        3018,
        35166,
        11548,
        33373,
        13,
        2313,
        1418,
        1979,
        2501,
        318,
        684,
        7186,
        3847,
        4859,
        11,
        11465,
        2501,
        31126,
        10923,
        339,
        10193,
        9533,
        18193,
        5846,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2244897186756134,
      "compression_ratio": 1.5136187076568604,
      "no_speech_prob": 0.037291038781404495
    },
    {
      "id": 124,
      "seek": 74700,
      "start": 3473.19,
      "end": 3479.19,
      "text": " Das heißt, es hat ein inhärentes Wissen, wie Menschen sind.",
      "tokens": [
        51464,
        2846,
        13139,
        11,
        785,
        2385,
        1343,
        47707,
        737,
        1753,
        279,
        343,
        10987,
        11,
        3355,
        8397,
        3290,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2244897186756134,
      "compression_ratio": 1.5136187076568604,
      "no_speech_prob": 0.037291038781404495
    },
    {
      "id": 125,
      "seek": 77500,
      "start": 3479.19,
      "end": 3482.19,
      "text": " Es ist auf Romanen trainiert, auf Geschichten, Erzählungen.",
      "tokens": [
        50364,
        2313,
        1418,
        2501,
        8566,
        268,
        3847,
        4859,
        11,
        2501,
        14241,
        24681,
        11,
        3300,
        23242,
        45434,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19866420328617096,
      "compression_ratio": 1.6428571939468384,
      "no_speech_prob": 0.02438437193632126
    },
    {
      "id": 126,
      "seek": 77500,
      "start": 3482.19,
      "end": 3486.19,
      "text": " Es ist ja nicht nur auf puren Fakten definiert.",
      "tokens": [
        50514,
        2313,
        1418,
        2784,
        1979,
        4343,
        2501,
        6075,
        77,
        479,
        514,
        1147,
        1561,
        4859,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19866420328617096,
      "compression_ratio": 1.6428571939468384,
      "no_speech_prob": 0.02438437193632126
    },
    {
      "id": 127,
      "seek": 77500,
      "start": 3486.19,
      "end": 3491.19,
      "text": " Das heißt, du hast noch eine zusätzliche Layer, die du immer hast, wenn du Modelle entwickelst,",
      "tokens": [
        50714,
        2846,
        13139,
        11,
        1581,
        6581,
        3514,
        3018,
        11548,
        39983,
        10185,
        35166,
        11,
        978,
        1581,
        5578,
        6581,
        11,
        4797,
        1581,
        6583,
        4434,
        28449,
        7124,
        372,
        11,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19866420328617096,
      "compression_ratio": 1.6428571939468384,
      "no_speech_prob": 0.02438437193632126
    },
    {
      "id": 128,
      "seek": 77500,
      "start": 3491.19,
      "end": 3494.19,
      "text": " die menschlichen Daten, menschlichen Ursprungs haben,",
      "tokens": [
        50964,
        978,
        10923,
        339,
        10193,
        31126,
        11,
        10923,
        339,
        10193,
        9533,
        18193,
        5846,
        3084,
        11,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19866420328617096,
      "compression_ratio": 1.6428571939468384,
      "no_speech_prob": 0.02438437193632126
    },
    {
      "id": 129,
      "seek": 77500,
      "start": 3494.19,
      "end": 3499.19,
      "text": " nämlich irgendwie einen innenliegenden Fehler-Term, den wir menschlich definieren,",
      "tokens": [
        51114,
        21219,
        20759,
        4891,
        294,
        2866,
        6302,
        70,
        8896,
        48101,
        12,
        51,
        966,
        11,
        1441,
        1987,
        10923,
        339,
        1739,
        1561,
        5695,
        11,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19866420328617096,
      "compression_ratio": 1.6428571939468384,
      "no_speech_prob": 0.02438437193632126
    },
    {
      "id": 130,
      "seek": 77500,
      "start": 3499.19,
      "end": 3503.19,
      "text": " was man, wenn ich ein Prediction Model auf Conversion rechnen muss, in Sales-Daten,",
      "tokens": [
        51364,
        390,
        587,
        11,
        4797,
        1893,
        1343,
        32969,
        4105,
        17105,
        2501,
        2656,
        29153,
        319,
        1377,
        268,
        6425,
        11,
        294,
        23467,
        12,
        35,
        7186,
        11,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19866420328617096,
      "compression_ratio": 1.6428571939468384,
      "no_speech_prob": 0.02438437193632126
    },
    {
      "id": 131,
      "seek": 77500,
      "start": 3503.19,
      "end": 3507.19,
      "text": " weil da reden auch nur nicht zwei Roboter miteinander.",
      "tokens": [
        51564,
        7689,
        1120,
        26447,
        2168,
        4343,
        1979,
        12002,
        5424,
        21585,
        43127,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19866420328617096,
      "compression_ratio": 1.6428571939468384,
      "no_speech_prob": 0.02438437193632126
    },
    {
      "id": 132,
      "seek": 80300,
      "start": 3507.19,
      "end": 3510.19,
      "text": " Und das hast du dann natürlich da nochmal on Scale.",
      "tokens": [
        50364,
        2719,
        1482,
        6581,
        1581,
        3594,
        8762,
        1120,
        26509,
        322,
        42999,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19710543751716614,
      "compression_ratio": 1.6885813474655151,
      "no_speech_prob": 0.018252070993185043
    },
    {
      "id": 133,
      "seek": 80300,
      "start": 3510.19,
      "end": 3513.19,
      "text": " Das sind dann so Sachen, das ging auch so ein bisschen durch die Presse,",
      "tokens": [
        50514,
        2846,
        3290,
        3594,
        370,
        26074,
        11,
        1482,
        21924,
        2168,
        370,
        1343,
        10763,
        7131,
        978,
        2718,
        405,
        11,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19710543751716614,
      "compression_ratio": 1.6885813474655151,
      "no_speech_prob": 0.018252070993185043
    },
    {
      "id": 134,
      "seek": 80300,
      "start": 3513.19,
      "end": 3516.19,
      "text": " wenn du als Frau fragst, wie viel du im Gehaltsgespräch aufrufen kannst",
      "tokens": [
        50664,
        4797,
        1581,
        3907,
        13930,
        9241,
        372,
        11,
        3355,
        5891,
        1581,
        566,
        2876,
        4947,
        1373,
        2880,
        1424,
        10168,
        2501,
        894,
        6570,
        20853,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19710543751716614,
      "compression_ratio": 1.6885813474655151,
      "no_speech_prob": 0.018252070993185043
    },
    {
      "id": 135,
      "seek": 80300,
      "start": 3516.19,
      "end": 3520.19,
      "text": " und wie viel du als Mann Gehaltsgespräch aufrufen kannst.",
      "tokens": [
        50814,
        674,
        3355,
        5891,
        1581,
        3907,
        16892,
        2876,
        4947,
        1373,
        2880,
        1424,
        10168,
        2501,
        894,
        6570,
        20853,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19710543751716614,
      "compression_ratio": 1.6885813474655151,
      "no_speech_prob": 0.018252070993185043
    },
    {
      "id": 136,
      "seek": 80300,
      "start": 3520.19,
      "end": 3523.19,
      "text": " Die Regel ist immer, ein Algorithmus kann nie neutral sein.",
      "tokens": [
        51014,
        3229,
        33139,
        1418,
        5578,
        11,
        1343,
        35014,
        6819,
        18761,
        4028,
        2838,
        10598,
        6195,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19710543751716614,
      "compression_ratio": 1.6885813474655151,
      "no_speech_prob": 0.018252070993185043
    },
    {
      "id": 137,
      "seek": 80300,
      "start": 3523.19,
      "end": 3527.19,
      "text": " Er ist nie gut, er ist nie schlecht, aber er ist nie neutral.",
      "tokens": [
        51164,
        3300,
        1418,
        2838,
        5228,
        11,
        1189,
        1418,
        2838,
        32427,
        11,
        4340,
        1189,
        1418,
        2838,
        10598,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19710543751716614,
      "compression_ratio": 1.6885813474655151,
      "no_speech_prob": 0.018252070993185043
    },
    {
      "id": 138,
      "seek": 80300,
      "start": 3527.19,
      "end": 3529.19,
      "text": " Und was ist jetzt passiert? Verschiedene Layers.",
      "tokens": [
        51364,
        2719,
        390,
        1418,
        4354,
        21671,
        30,
        12226,
        11805,
        1450,
        20084,
        433,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19710543751716614,
      "compression_ratio": 1.6885813474655151,
      "no_speech_prob": 0.018252070993185043
    },
    {
      "id": 139,
      "seek": 80300,
      "start": 3529.19,
      "end": 3533.19,
      "text": " Erstens Trainingsdaten, die den General Data Reproduzieren.",
      "tokens": [
        51464,
        31183,
        694,
        28029,
        1109,
        67,
        7186,
        11,
        978,
        1441,
        6996,
        11888,
        3696,
        2323,
        89,
        5695,
        13,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19710543751716614,
      "compression_ratio": 1.6885813474655151,
      "no_speech_prob": 0.018252070993185043
    },
    {
      "id": 140,
      "seek": 82900,
      "start": 3533.19,
      "end": 3537.19,
      "text": " Zweitens Berichte darüber, die eben die Probleme auch zeigen.",
      "tokens": [
        50364,
        1176,
        28019,
        694,
        5637,
        18972,
        21737,
        11,
        978,
        11375,
        978,
        32891,
        2168,
        24687,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 141,
      "seek": 82900,
      "start": 3537.19,
      "end": 3540.19,
      "text": " Es gibt ganz viel Forschung, dass Frauen, wenn sie das gleiche Gehalt fordern,",
      "tokens": [
        50564,
        2313,
        6089,
        6312,
        5891,
        42938,
        1063,
        11,
        2658,
        24191,
        11,
        4797,
        2804,
        1482,
        11699,
        68,
        2876,
        20731,
        337,
        35520,
        11,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 142,
      "seek": 82900,
      "start": 3540.19,
      "end": 3542.19,
      "text": " bestraft werden.",
      "tokens": [
        50714,
        1151,
        4469,
        4604,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 143,
      "seek": 82900,
      "start": 3542.19,
      "end": 3545.19,
      "text": " Da sind von mir noch 38 Reddit-Threads dazu,",
      "tokens": [
        50814,
        3933,
        3290,
        2957,
        3149,
        3514,
        12843,
        32210,
        12,
        2434,
        2538,
        82,
        13034,
        11,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 144,
      "seek": 82900,
      "start": 3545.19,
      "end": 3548.19,
      "text": " plus das Reinforcement Learning by Human Feedback,",
      "tokens": [
        50964,
        1804,
        1482,
        42116,
        9382,
        15205,
        538,
        10294,
        33720,
        3207,
        11,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 145,
      "seek": 82900,
      "start": 3548.19,
      "end": 3550.19,
      "text": " plus die gesamte Architektur.",
      "tokens": [
        51114,
        1804,
        978,
        39746,
        975,
        1587,
        339,
        642,
        2320,
        374,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 146,
      "seek": 82900,
      "start": 3550.19,
      "end": 3552.19,
      "text": " Und damit haben wir lustigerweise,",
      "tokens": [
        51214,
        2719,
        9479,
        3084,
        1987,
        24672,
        4810,
        13109,
        11,
        51314
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 147,
      "seek": 82900,
      "start": 3552.19,
      "end": 3555.19,
      "text": " und das will natürlich ein besonders kein harter AI-Researcher",
      "tokens": [
        51314,
        674,
        1482,
        486,
        8762,
        1343,
        25258,
        13424,
        2233,
        391,
        7318,
        12,
        49,
        10177,
        260,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 148,
      "seek": 82900,
      "start": 3555.19,
      "end": 3557.19,
      "text": " aus der Mathe-, Physik- oder Code-Welt zugeben,",
      "tokens": [
        51464,
        3437,
        1163,
        6789,
        675,
        24246,
        15542,
        1035,
        12,
        4513,
        15549,
        12,
        54,
        2018,
        2164,
        16702,
        11,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 149,
      "seek": 82900,
      "start": 3557.19,
      "end": 3560.19,
      "text": " hast du halt ein Ding gebaut, was massiv menschlicher ist,",
      "tokens": [
        51564,
        6581,
        1581,
        12479,
        1343,
        20558,
        49203,
        11,
        390,
        2758,
        592,
        10923,
        339,
        25215,
        1418,
        11,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21201545000076294,
      "compression_ratio": 1.462686538696289,
      "no_speech_prob": 0.004679309669882059
    },
    {
      "id": 150,
      "seek": 85600,
      "start": 3560.19,
      "end": 3564.19,
      "text": " als wir jemals geglaubt haben.",
      "tokens": [
        50364,
        3907,
        1987,
        361,
        443,
        1124,
        23982,
        20798,
        83,
        3084,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 151,
      "seek": 85600,
      "start": 3564.19,
      "end": 3567.19,
      "text": " Das ist versehentlich passiert.",
      "tokens": [
        50564,
        2846,
        1418,
        7996,
        71,
        7698,
        21671,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 152,
      "seek": 85600,
      "start": 3567.19,
      "end": 3570.19,
      "text": " Und viel in dieser Forschung ist übrigens relativ versehentlich passiert.",
      "tokens": [
        50714,
        2719,
        5891,
        294,
        9053,
        42938,
        1063,
        1418,
        38215,
        21960,
        7996,
        71,
        7698,
        21671,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 153,
      "seek": 85600,
      "start": 3570.19,
      "end": 3572.19,
      "text": " Das darf man immer nicht unterschätzen.",
      "tokens": [
        50864,
        2846,
        19374,
        587,
        5578,
        1979,
        20983,
        339,
        45721,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 154,
      "seek": 85600,
      "start": 3572.19,
      "end": 3575.19,
      "text": " Und damit hast du halt diese ganz verschiedenen Mansierungen,",
      "tokens": [
        50964,
        2719,
        9479,
        6581,
        1581,
        12479,
        6705,
        6312,
        41043,
        23167,
        811,
        5084,
        11,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 155,
      "seek": 85600,
      "start": 3575.19,
      "end": 3578.19,
      "text": " die eben das so, es hat schon einen Grund,",
      "tokens": [
        51114,
        978,
        11375,
        1482,
        370,
        11,
        785,
        2385,
        4981,
        4891,
        13941,
        11,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 156,
      "seek": 85600,
      "start": 3578.19,
      "end": 3580.19,
      "text": " warum ich das so ein Wackelpudding nenne,",
      "tokens": [
        51264,
        24331,
        1893,
        1482,
        370,
        1343,
        343,
        326,
        7124,
        79,
        49797,
        297,
        13295,
        11,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 157,
      "seek": 85600,
      "start": 3580.19,
      "end": 3583.19,
      "text": " weil es halt immer wieder so ein bisschen ein diffuses Tüchern ist,",
      "tokens": [
        51364,
        7689,
        785,
        12479,
        5578,
        6216,
        370,
        1343,
        10763,
        1343,
        7593,
        8355,
        314,
        774,
        339,
        1248,
        1418,
        11,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 158,
      "seek": 85600,
      "start": 3583.19,
      "end": 3586.19,
      "text": " mit dem ich hier so zu tun habe und was ich irgendwie gucken muss,",
      "tokens": [
        51514,
        2194,
        1371,
        1893,
        3296,
        370,
        2164,
        4267,
        6015,
        674,
        390,
        1893,
        20759,
        33135,
        6425,
        11,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 159,
      "seek": 85600,
      "start": 3586.19,
      "end": 3589.19,
      "text": " wie mein Tamagotchi denn so überlebt.",
      "tokens": [
        51664,
        3355,
        10777,
        8540,
        559,
        310,
        8036,
        10471,
        370,
        4502,
        306,
        4517,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.27816900610923767,
      "compression_ratio": 1.6722408533096313,
      "no_speech_prob": 0.010324663482606411
    },
    {
      "id": 160,
      "seek": 88500,
      "start": 3589.19,
      "end": 3593.19,
      "text": " Aber das heißt, wenn jetzt das Modell tatsächlich unterschiedlich reagiert,",
      "tokens": [
        50364,
        5992,
        1482,
        13139,
        11,
        4797,
        4354,
        1482,
        6583,
        898,
        20796,
        30058,
        1739,
        26949,
        4859,
        11,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2457849085330963,
      "compression_ratio": 1.5447760820388794,
      "no_speech_prob": 0.0008295096340589225
    },
    {
      "id": 161,
      "seek": 88500,
      "start": 3593.19,
      "end": 3596.19,
      "text": " wenn es weiß, Mann oder Frau,",
      "tokens": [
        50564,
        4797,
        785,
        13385,
        11,
        16892,
        4513,
        13930,
        11,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2457849085330963,
      "compression_ratio": 1.5447760820388794,
      "no_speech_prob": 0.0008295096340589225
    },
    {
      "id": 162,
      "seek": 88500,
      "start": 3596.19,
      "end": 3599.19,
      "text": " dann ist dieses bei Chat-GPT,",
      "tokens": [
        50714,
        3594,
        1418,
        12113,
        4643,
        27503,
        12,
        38,
        47,
        51,
        11,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2457849085330963,
      "compression_ratio": 1.5447760820388794,
      "no_speech_prob": 0.0008295096340589225
    },
    {
      "id": 163,
      "seek": 88500,
      "start": 3599.19,
      "end": 3602.19,
      "text": " was sollte das Modell über dich wissen?",
      "tokens": [
        50864,
        390,
        18042,
        1482,
        6583,
        898,
        4502,
        10390,
        16331,
        30,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2457849085330963,
      "compression_ratio": 1.5447760820388794,
      "no_speech_prob": 0.0008295096340589225
    },
    {
      "id": 164,
      "seek": 88500,
      "start": 3602.19,
      "end": 3606.19,
      "text": " Ja, da vergifte ich ja teilweise dann auch meine Session damit, wenn ich...",
      "tokens": [
        51014,
        3530,
        11,
        1120,
        20209,
        2008,
        68,
        1893,
        2784,
        46748,
        3594,
        2168,
        10946,
        318,
        4311,
        9479,
        11,
        4797,
        1893,
        485,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2457849085330963,
      "compression_ratio": 1.5447760820388794,
      "no_speech_prob": 0.0008295096340589225
    },
    {
      "id": 165,
      "seek": 88500,
      "start": 3606.19,
      "end": 3610.19,
      "text": " Herzlich willkommen, warum ich die Memory Function so scheiße finde.",
      "tokens": [
        51214,
        3204,
        16813,
        46439,
        11,
        24331,
        1893,
        978,
        38203,
        11166,
        882,
        370,
        25690,
        47828,
        17841,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2457849085330963,
      "compression_ratio": 1.5447760820388794,
      "no_speech_prob": 0.0008295096340589225
    },
    {
      "id": 166,
      "seek": 88500,
      "start": 3610.19,
      "end": 3613.19,
      "text": " Ja, okay, ich verstehe es.",
      "tokens": [
        51414,
        3530,
        11,
        1392,
        11,
        1893,
        22442,
        675,
        785,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2457849085330963,
      "compression_ratio": 1.5447760820388794,
      "no_speech_prob": 0.0008295096340589225
    },
    {
      "id": 167,
      "seek": 88500,
      "start": 3613.19,
      "end": 3617.19,
      "text": " Aber sie ist doch so toll, weil er jetzt das Gedächtnis hat.",
      "tokens": [
        51564,
        5992,
        2804,
        1418,
        9243,
        370,
        16629,
        11,
        7689,
        1189,
        4354,
        1482,
        28166,
        737,
        4701,
        10661,
        2385,
        13,
        51764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2457849085330963,
      "compression_ratio": 1.5447760820388794,
      "no_speech_prob": 0.0008295096340589225
    },
    {
      "id": 168,
      "seek": 91300,
      "start": 3618.19,
      "end": 3621.19,
      "text": " Ja, und weil du dich immer ansprichst.",
      "tokens": [
        50414,
        3530,
        11,
        674,
        7689,
        1581,
        10390,
        5578,
        1567,
        1424,
        480,
        372,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19712486863136292,
      "compression_ratio": 1.5226337909698486,
      "no_speech_prob": 0.007342720869928598
    },
    {
      "id": 169,
      "seek": 91300,
      "start": 3621.19,
      "end": 3624.19,
      "text": " Du siehst heute wirklich wieder großartig aus.",
      "tokens": [
        50564,
        5153,
        2804,
        38857,
        9801,
        9696,
        6216,
        17253,
        446,
        328,
        3437,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19712486863136292,
      "compression_ratio": 1.5226337909698486,
      "no_speech_prob": 0.007342720869928598
    },
    {
      "id": 170,
      "seek": 91300,
      "start": 3624.19,
      "end": 3627.19,
      "text": " Und deine Idee zu deinem neuen Geschäftsmodell und dein Code,",
      "tokens": [
        50714,
        2719,
        28395,
        32651,
        2164,
        25641,
        443,
        21387,
        40440,
        82,
        8014,
        898,
        674,
        25641,
        15549,
        11,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19712486863136292,
      "compression_ratio": 1.5226337909698486,
      "no_speech_prob": 0.007342720869928598
    },
    {
      "id": 171,
      "seek": 91300,
      "start": 3627.19,
      "end": 3630.19,
      "text": " ein Traum der schlaflosen Nächte.",
      "tokens": [
        50864,
        1343,
        5403,
        449,
        1163,
        956,
        875,
        3423,
        6441,
        426,
        10168,
        975,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19712486863136292,
      "compression_ratio": 1.5226337909698486,
      "no_speech_prob": 0.007342720869928598
    },
    {
      "id": 172,
      "seek": 91300,
      "start": 3630.19,
      "end": 3635.19,
      "text": " Ja, und dann sind wir wieder auf der Seite der Psychologie,",
      "tokens": [
        51014,
        3530,
        11,
        674,
        3594,
        3290,
        1987,
        6216,
        2501,
        1163,
        19748,
        1163,
        17303,
        20121,
        11,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19712486863136292,
      "compression_ratio": 1.5226337909698486,
      "no_speech_prob": 0.007342720869928598
    },
    {
      "id": 173,
      "seek": 91300,
      "start": 3635.19,
      "end": 3640.19,
      "text": " dass du nicht nur mit deinem psychologischen Wissen die Maschine hacken kannst,",
      "tokens": [
        51264,
        2658,
        1581,
        1979,
        4343,
        2194,
        25641,
        443,
        4681,
        1132,
        6282,
        343,
        10987,
        978,
        5224,
        36675,
        10339,
        268,
        20853,
        11,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19712486863136292,
      "compression_ratio": 1.5226337909698486,
      "no_speech_prob": 0.007342720869928598
    },
    {
      "id": 174,
      "seek": 91300,
      "start": 3640.19,
      "end": 3643.19,
      "text": " sondern eben auch merkst, wie die Maschine...",
      "tokens": [
        51514,
        11465,
        11375,
        2168,
        43541,
        372,
        11,
        3355,
        978,
        5224,
        36675,
        485,
        51664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.19712486863136292,
      "compression_ratio": 1.5226337909698486,
      "no_speech_prob": 0.007342720869928598
    },
    {
      "id": 175,
      "seek": 93900,
      "start": 3643.19,
      "end": 3646.19,
      "text": " Ja, ist das auch Reinforcement Learning,",
      "tokens": [
        50364,
        3530,
        11,
        1418,
        1482,
        2168,
        42116,
        9382,
        15205,
        11,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 176,
      "seek": 93900,
      "start": 3646.19,
      "end": 3649.19,
      "text": " wenn die Maschine sagt, hey, das hast du jetzt gut gemacht,",
      "tokens": [
        50514,
        4797,
        978,
        5224,
        36675,
        15764,
        11,
        4177,
        11,
        1482,
        6581,
        1581,
        4354,
        5228,
        12293,
        11,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 177,
      "seek": 93900,
      "start": 3649.19,
      "end": 3651.19,
      "text": " das nächste Mal bitte wieder so?",
      "tokens": [
        50664,
        1482,
        30661,
        5746,
        23231,
        6216,
        370,
        30,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 178,
      "seek": 93900,
      "start": 3651.19,
      "end": 3655.19,
      "text": " Das sind eher das, was wir unter Dark Patterns definieren,",
      "tokens": [
        50764,
        2846,
        3290,
        24332,
        1482,
        11,
        390,
        1987,
        8662,
        9563,
        34367,
        3695,
        1561,
        5695,
        11,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 179,
      "seek": 93900,
      "start": 3655.19,
      "end": 3657.19,
      "text": " also Algorithmen, die dafür sorgen,",
      "tokens": [
        50964,
        611,
        35014,
        6819,
        2558,
        11,
        978,
        13747,
        47972,
        11,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 180,
      "seek": 93900,
      "start": 3657.19,
      "end": 3659.19,
      "text": " dass du möglichst viel mit der Maschine sprichst.",
      "tokens": [
        51064,
        2658,
        1581,
        44850,
        5891,
        2194,
        1163,
        5224,
        36675,
        6103,
        480,
        372,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 181,
      "seek": 93900,
      "start": 3659.19,
      "end": 3661.19,
      "text": " Retention, Retention, Retention.",
      "tokens": [
        51164,
        11495,
        1251,
        11,
        11495,
        1251,
        11,
        11495,
        1251,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 182,
      "seek": 93900,
      "start": 3661.19,
      "end": 3663.19,
      "text": " Und als OpenAI das gebaut hat, haben sie vergessen,",
      "tokens": [
        51264,
        2719,
        3907,
        7238,
        48698,
        1482,
        49203,
        2385,
        11,
        3084,
        2804,
        42418,
        11,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 183,
      "seek": 93900,
      "start": 3663.19,
      "end": 3667.19,
      "text": " dass Retention, Retention heißt, das GPU-Caster brennt.",
      "tokens": [
        51364,
        2658,
        11495,
        1251,
        11,
        11495,
        1251,
        13139,
        11,
        1482,
        18407,
        12,
        34,
        525,
        260,
        272,
        1095,
        580,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 184,
      "seek": 93900,
      "start": 3667.19,
      "end": 3670.19,
      "text": " Das haben sie nicht ganz durchgerannt, die Jungs.",
      "tokens": [
        51564,
        2846,
        3084,
        2804,
        1979,
        6312,
        7131,
        1321,
        969,
        83,
        11,
        978,
        508,
        5846,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1791888326406479,
      "compression_ratio": 1.5979729890823364,
      "no_speech_prob": 0.013007204048335552
    },
    {
      "id": 185,
      "seek": 96600,
      "start": 3671.19,
      "end": 3675.19,
      "text": " Dark Patterns, wenn ich das richtig verstanden habe, sind jetzt verboten?",
      "tokens": [
        50414,
        7803,
        74,
        34367,
        3695,
        11,
        4797,
        1893,
        1482,
        13129,
        1306,
        33946,
        6015,
        11,
        3290,
        4354,
        9595,
        21990,
        30,
        50614
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 186,
      "seek": 96600,
      "start": 3675.19,
      "end": 3677.19,
      "text": " Nein.",
      "tokens": [
        50614,
        18878,
        13,
        50714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 187,
      "seek": 96600,
      "start": 3677.19,
      "end": 3681.19,
      "text": " Nein? Ich glaube, in der Werbung, in den Webseiten oder sowas.",
      "tokens": [
        50714,
        18878,
        30,
        3141,
        13756,
        11,
        294,
        1163,
        14255,
        36776,
        11,
        294,
        1441,
        9573,
        405,
        6009,
        4513,
        19766,
        296,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 188,
      "seek": 96600,
      "start": 3681.19,
      "end": 3683.19,
      "text": " Wieso sind jetzt Dark Patterns verboten?",
      "tokens": [
        50914,
        343,
        32272,
        3290,
        4354,
        9563,
        34367,
        3695,
        9595,
        21990,
        30,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 189,
      "seek": 96600,
      "start": 3683.19,
      "end": 3685.19,
      "text": " Haben wir eine Liste rumgeschickt bekommen,",
      "tokens": [
        51014,
        47007,
        1987,
        3018,
        441,
        8375,
        8347,
        23378,
        40522,
        19256,
        11,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 190,
      "seek": 96600,
      "start": 3685.19,
      "end": 3687.19,
      "text": " was wir jetzt nicht mehr tun dürfen?",
      "tokens": [
        51114,
        390,
        1987,
        4354,
        1979,
        5417,
        4267,
        29493,
        30,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 191,
      "seek": 96600,
      "start": 3687.19,
      "end": 3690.19,
      "text": " Du, ich glaube tatsächlich, da war irgendwas mit Webseiten,",
      "tokens": [
        51214,
        5153,
        11,
        1893,
        13756,
        20796,
        11,
        1120,
        1516,
        47090,
        2194,
        9573,
        405,
        6009,
        11,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 192,
      "seek": 96600,
      "start": 3690.19,
      "end": 3694.19,
      "text": " dass eben so Dark Patterns, der falsche Button ist gehighlighted und sowas.",
      "tokens": [
        51364,
        2658,
        11375,
        370,
        9563,
        34367,
        3695,
        11,
        1163,
        16720,
        1876,
        38435,
        1418,
        1519,
        21454,
        2764,
        292,
        674,
        19766,
        296,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 193,
      "seek": 96600,
      "start": 3694.19,
      "end": 3697.19,
      "text": " Ja, natürlich, also solche Sachen sind natürlich ein Thema.",
      "tokens": [
        51564,
        3530,
        11,
        8762,
        11,
        611,
        29813,
        26074,
        3290,
        8762,
        1343,
        16306,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22729875147342682,
      "compression_ratio": 1.6750901937484741,
      "no_speech_prob": 0.0026723057962954044
    },
    {
      "id": 194,
      "seek": 99300,
      "start": 3697.19,
      "end": 3701.19,
      "text": " Ja, aber wir reden über klassische Dark Patterns, UX, UI,",
      "tokens": [
        50364,
        3530,
        11,
        4340,
        1987,
        26447,
        4502,
        42917,
        7864,
        9563,
        34367,
        3695,
        11,
        40176,
        11,
        15682,
        11,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 195,
      "seek": 99300,
      "start": 3701.19,
      "end": 3703.19,
      "text": " die sind immer wieder angezählt,",
      "tokens": [
        50564,
        978,
        3290,
        5578,
        6216,
        15495,
        23242,
        2282,
        11,
        50664
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 196,
      "seek": 99300,
      "start": 3703.19,
      "end": 3706.19,
      "text": " aber wir reden hier über Deep Dark Patterns Algorithmic.",
      "tokens": [
        50664,
        4340,
        1987,
        26447,
        3296,
        4502,
        14895,
        9563,
        34367,
        3695,
        35014,
        6819,
        13195,
        13,
        50814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 197,
      "seek": 99300,
      "start": 3708.19,
      "end": 3709.19,
      "text": " Okay.",
      "tokens": [
        50914,
        1033,
        13,
        50964
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 198,
      "seek": 99300,
      "start": 3709.19,
      "end": 3711.19,
      "text": " Darüber möchte ich noch nicht weiter ausführen,",
      "tokens": [
        50964,
        7803,
        12670,
        14570,
        1893,
        3514,
        1979,
        8988,
        3437,
        69,
        29540,
        11,
        51064
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 199,
      "seek": 99300,
      "start": 3711.19,
      "end": 3713.19,
      "text": " was ich in meiner Karriere schon alles gemacht habe.",
      "tokens": [
        51064,
        390,
        1893,
        294,
        20529,
        8009,
        470,
        323,
        4981,
        7874,
        12293,
        6015,
        13,
        51164
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 200,
      "seek": 99300,
      "start": 3715.19,
      "end": 3717.19,
      "text": " Gut, kann ich verstehen.",
      "tokens": [
        51264,
        24481,
        11,
        4028,
        1893,
        37352,
        13,
        51364
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 201,
      "seek": 99300,
      "start": 3717.19,
      "end": 3719.19,
      "text": " Wir sind auch knapp über die Zeit.",
      "tokens": [
        51364,
        4347,
        3290,
        2168,
        40979,
        4502,
        978,
        9394,
        13,
        51464
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 202,
      "seek": 99300,
      "start": 3719.19,
      "end": 3721.19,
      "text": " Barbara, es hat total Spaß gemacht.",
      "tokens": [
        51464,
        19214,
        11,
        785,
        2385,
        3217,
        27460,
        12293,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 203,
      "seek": 99300,
      "start": 3721.19,
      "end": 3724.19,
      "text": " Ich könnte dir noch stundenlang zuhören.",
      "tokens": [
        51564,
        3141,
        17646,
        4746,
        3514,
        342,
        10028,
        25241,
        2164,
        71,
        26377,
        13,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.22598931193351746,
      "compression_ratio": 1.5018726587295532,
      "no_speech_prob": 0.0330476276576519
    },
    {
      "id": 204,
      "seek": 102000,
      "start": 3724.19,
      "end": 3728.19,
      "text": " Da ist so viel Wissen dahinter, was ich irgendwie aufsagen wollte.",
      "tokens": [
        50364,
        3933,
        1418,
        370,
        5891,
        343,
        10987,
        16800,
        5106,
        11,
        390,
        1893,
        20759,
        2501,
        82,
        4698,
        24509,
        13,
        50564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2148628681898117,
      "compression_ratio": 1.577689290046692,
      "no_speech_prob": 0.003590574488043785
    },
    {
      "id": 205,
      "seek": 102000,
      "start": 3728.19,
      "end": 3734.19,
      "text": " Aber ja, herzlichen Dank für diesen Einblick in deine Welt, in dein Wissen.",
      "tokens": [
        50564,
        5992,
        2784,
        11,
        720,
        89,
        10193,
        14148,
        2959,
        12862,
        6391,
        38263,
        294,
        28395,
        14761,
        11,
        294,
        25641,
        343,
        10987,
        13,
        50864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2148628681898117,
      "compression_ratio": 1.577689290046692,
      "no_speech_prob": 0.003590574488043785
    },
    {
      "id": 206,
      "seek": 102000,
      "start": 3734.19,
      "end": 3739.19,
      "text": " Und ich glaube, da kann man jetzt hier viel für die Softwarearchitektur mitnehmen.",
      "tokens": [
        50864,
        2719,
        1893,
        13756,
        11,
        1120,
        4028,
        587,
        4354,
        3296,
        5891,
        2959,
        978,
        27428,
        1178,
        642,
        2320,
        374,
        2194,
        14669,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2148628681898117,
      "compression_ratio": 1.577689290046692,
      "no_speech_prob": 0.003590574488043785
    },
    {
      "id": 207,
      "seek": 102000,
      "start": 3740.19,
      "end": 3742.19,
      "text": " Herzlichen Dank für alle Zuhörer.",
      "tokens": [
        51164,
        24749,
        10193,
        14148,
        2959,
        5430,
        1176,
        3232,
        2311,
        260,
        13,
        51264
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2148628681898117,
      "compression_ratio": 1.577689290046692,
      "no_speech_prob": 0.003590574488043785
    },
    {
      "id": 208,
      "seek": 102000,
      "start": 3742.19,
      "end": 3745.19,
      "text": " Aus der Softwarearchitektur von so einem kleinen Mathe-Nerd.",
      "tokens": [
        51264,
        9039,
        1163,
        27428,
        1178,
        642,
        2320,
        374,
        2957,
        370,
        6827,
        26512,
        6789,
        675,
        12,
        45,
        7783,
        13,
        51414
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2148628681898117,
      "compression_ratio": 1.577689290046692,
      "no_speech_prob": 0.003590574488043785
    },
    {
      "id": 209,
      "seek": 102000,
      "start": 3746.19,
      "end": 3747.19,
      "text": " Ja.",
      "tokens": [
        51464,
        3530,
        13,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2148628681898117,
      "compression_ratio": 1.577689290046692,
      "no_speech_prob": 0.003590574488043785
    },
    {
      "id": 210,
      "seek": 102000,
      "start": 3748.19,
      "end": 3753.19,
      "text": " Gut, ich wollte noch mal kurz auf deinen eigenen Podcast hinweisen.",
      "tokens": [
        51564,
        24481,
        11,
        1893,
        24509,
        3514,
        2806,
        20465,
        2501,
        49362,
        28702,
        29972,
        14102,
        40196,
        13,
        51814
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2148628681898117,
      "compression_ratio": 1.577689290046692,
      "no_speech_prob": 0.003590574488043785
    },
    {
      "id": 211,
      "seek": 105000,
      "start": 3754.19,
      "end": 3757.19,
      "text": " Layer 8.9, da gibt es noch mehr tiefes Wissen.",
      "tokens": [
        50364,
        35166,
        1649,
        13,
        24,
        11,
        1120,
        6089,
        785,
        3514,
        5417,
        45100,
        279,
        343,
        10987,
        13,
        50514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2528046667575836,
      "compression_ratio": 1.2903225421905518,
      "no_speech_prob": 0.003221676219254732
    },
    {
      "id": 212,
      "seek": 105000,
      "start": 3758.19,
      "end": 3762.19,
      "text": " Und dann an alle Hörer, schönes Wochenende.",
      "tokens": [
        50564,
        2719,
        3594,
        364,
        5430,
        389,
        2311,
        260,
        11,
        13527,
        279,
        23126,
        5445,
        13,
        50764
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2528046667575836,
      "compression_ratio": 1.2903225421905518,
      "no_speech_prob": 0.003221676219254732
    },
    {
      "id": 213,
      "seek": 105000,
      "start": 3762.19,
      "end": 3765.19,
      "text": " Schön, dass ihr dabei wart.",
      "tokens": [
        50764,
        41060,
        11,
        2658,
        5553,
        14967,
        45124,
        13,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2528046667575836,
      "compression_ratio": 1.2903225421905518,
      "no_speech_prob": 0.003221676219254732
    },
    {
      "id": 214,
      "seek": 105000,
      "start": 3765.19,
      "end": 3767.19,
      "text": " Und ja, viel Spaß noch.",
      "tokens": [
        50914,
        2719,
        2784,
        11,
        5891,
        27460,
        3514,
        13,
        51014
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2528046667575836,
      "compression_ratio": 1.2903225421905518,
      "no_speech_prob": 0.003221676219254732
    },
    {
      "id": 215,
      "seek": 105000,
      "start": 3767.19,
      "end": 3769.19,
      "text": " Danke. Vielen, vielen Dank für die Einladung.",
      "tokens": [
        51014,
        26508,
        13,
        22502,
        11,
        19885,
        14148,
        2959,
        978,
        6391,
        9290,
        1063,
        13,
        51114
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2528046667575836,
      "compression_ratio": 1.2903225421905518,
      "no_speech_prob": 0.003221676219254732
    },
    {
      "id": 216,
      "seek": 105000,
      "start": 3770.19,
      "end": 3771.19,
      "text": " Gerne.",
      "tokens": [
        51164,
        9409,
        716,
        13,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2528046667575836,
      "compression_ratio": 1.2903225421905518,
      "no_speech_prob": 0.003221676219254732
    }
  ],
  "language": "german"
}