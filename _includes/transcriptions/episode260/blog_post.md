# KI und Bullshit - Eine kritische Betrachtung von Large Language Models

Large Language Models (LLMs) wie ChatGPT sind in aller Munde und werden oft als revolutionäre KI-Technologie gepriesen. Doch ein aktuelles Paper der University of Glasgow mit dem provokanten Titel "ChatGPT is Bullshit" wirft einen kritischen Blick auf diese Systeme und ihre Fähigkeiten. Zeit für eine differenzierte Analyse.

## Die Grundproblematik der LLMs

Der zentrale Punkt ist: LLMs sind im Kern Textgeneratoren, die darauf optimiert wurden, möglichst überzeugende und menschenähnlich wirkende Texte zu produzieren. Sie haben jedoch kein echtes Verständnis der Welt, keine Ziele und keine Intention - sie ahmen lediglich menschliche Sprache nach.

Dies führt zu einem fundamentalen Problem: Während Menschen normalerweise kommunizieren, um Wahrheit zu vermitteln oder bestimmte Ziele zu erreichen, geht es bei LLMs nur darum, plausibel klingende Texte zu generieren. Die Wahrheit spielt dabei eine untergeordnete Rolle.

## Das Konzept des "Bullshit"

Das Paper bezieht sich auf Harry Frankfurts philosophische Definition von "Bullshit": Äußerungen, bei denen es dem Sprecher nicht um Wahrheit geht, sondern nur darum, überzeugend zu wirken. Frankfurt unterscheidet dabei:

- Hard Bullshit: Bewusste Täuschung über die Absichten des Sprechers
- Soft Bullshit: Ohne bewusste Täuschungsabsicht, aber mit Gleichgültigkeit gegenüber der Wahrheit

LLMs produzieren nach dieser Definition mindestens "Soft Bullshit", da sie kein Konzept von Wahrheit haben und nur auf überzeugende Textgenerierung optimiert sind.

## Praktische Implikationen für die Softwareentwicklung

Diese theoretische Analyse hat wichtige praktische Konsequenzen für den Einsatz von LLMs in der Softwareentwicklung:

1. Generierter Code muss zwingend überprüft werden. Man kann sich nicht blind auf die Korrektheit verlassen.

2. Die Produktivitätsvorteile müssen gegen den zusätzlichen Kontrollaufwand abgewogen werden.

3. Bei komplexen Anforderungen wie Architekturentscheidungen können LLMs sogar kontraproduktiv sein, da sie echtes Domänenwissen nicht ersetzen können.

4. Die fehlende Nachvollziehbarkeit der Trainingsdaten ist problematisch, besonders bei sicherheitskritischen Anwendungen.

## Anthropomorphe Metaphern als Teil des Problems

Ein weiterer kritischer Punkt ist die Verwendung menschenähnlicher Metaphern wie "Halluzinationen" für Fehler der LLMs. Diese verschleiern die technische Realität und suggerieren fälschlicherweise menschenähnliche Fähigkeiten.

Stattdessen sollten wir diese Systeme nüchtern als das sehen, was sie sind: Textgeneratoren, die auf statistischer Basis arbeiten und deren Output grundsätzlich kritisch hinterfragt werden muss.

## Fazit und Ausblick

LLMs sind zweifellos mächtige Werkzeuge, die in bestimmten Kontexten sehr nützlich sein können. Wir müssen uns aber ihrer fundamentalen Limitierungen bewusst sein:

- Sie verstehen nicht wirklich, was sie ausgeben
- Sie sind nicht auf Wahrheit, sondern auf Plausibilität optimiert  
- Ihre Ausgaben müssen immer validiert werden
- Der Begriff "Künstliche Intelligenz" ist irreführend

Für die Praxis bedeutet das: LLMs können als Unterstützung dienen, aber nie kritisches Denken und echte Expertise ersetzen. Besonders in der Softwareentwicklung braucht es weiterhin Menschen, die Verantwortung übernehmen und Ergebnisse validieren.

Die Technologie wird sich weiterentwickeln, aber die grundsätzliche Problematik des "Bullshit" wird bleiben, solange diese Systeme nur auf überzeugende Textgenerierung und nicht auf echtes Verstehen optimiert sind.

Der bewusste und kritische Umgang mit diesen Limitierungen wird entscheidend sein für den sinnvollen Einsatz von LLMs in der Praxis.